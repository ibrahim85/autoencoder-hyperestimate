libdc1394 error: Failed to initialize libdc1394
I0623 12:17:20.615044  5064 caffe.cpp:99] Use GPU with device ID 0
I0623 12:17:20.733229  5064 caffe.cpp:107] Starting Optimization
I0623 12:17:20.733291  5064 solver.cpp:32] Initializing solver from parameters: 
test_iter: 5
test_interval: 1000
base_lr: 0.0001
display: 10
max_iter: 20000
lr_policy: "fixed"
momentum: 0.9
weight_decay: 0.004
snapshot: 1000
snapshot_prefix: "snapshots/16-06-21_16h05m31s_0_11_pretrainClassification"
solver_mode: GPU
net: "prototxt/16-06-21_16h05m31s_0_11_pretrainClassification_net.sh"
I0623 12:17:20.733309  5064 solver.cpp:70] Creating training net from net file: prototxt/16-06-21_16h05m31s_0_11_pretrainClassification_net.sh
I0623 12:17:20.733721  5064 net.cpp:277] The NetState phase (0) differed from the phase (1) specified by a rule in layer data
I0623 12:17:20.733739  5064 net.cpp:277] The NetState phase (0) differed from the phase (1) specified by a rule in layer accuracy_top_1
I0623 12:17:20.733743  5064 net.cpp:277] The NetState phase (0) differed from the phase (1) specified by a rule in layer accuracy_top_5
I0623 12:17:20.733834  5064 net.cpp:39] Initializing net from parameters: 
layers {
  top: "data"
  top: "label"
  name: "data"
  type: DATA
  data_param {
    source: "/dataset/cifar100_lmdb_lab/cifar100_train_lmdb"
    batch_size: 128
    backend: LMDB
  }
  include {
    phase: TRAIN
  }
  transform_param {
    mean_file: "/dataset/cifar100_lmdb_lab/mean.binaryproto"
  }
}
layers {
  bottom: "data"
  top: "0_0_conv"
  name: "0_0_conv"
  type: CONVOLUTION
  blobs_lr: 1
  blobs_lr: 2
  weight_decay: 0
  weight_decay: 0
  convolution_param {
    num_output: 32
    pad: 1
    kernel_size: 3
    stride: 1
    weight_filler {
      type: "xavier"
    }
    bias_filler {
      type: "constant"
      value: 0
    }
  }
}
layers {
  bottom: "0_0_conv"
  top: "0_0_conv"
  name: "0_0_conv_ReLU"
  type: RELU
}
layers {
  bottom: "0_0_conv"
  top: "0_1_conv"
  name: "0_1_conv"
  type: CONVOLUTION
  blobs_lr: 1
  blobs_lr: 2
  weight_decay: 0
  weight_decay: 0
  convolution_param {
    num_output: 32
    pad: 1
    kernel_size: 3
    stride: 1
    weight_filler {
      type: "xavier"
    }
    bias_filler {
      type: "constant"
      value: 0
    }
  }
}
layers {
  bottom: "0_1_conv"
  top: "0_1_conv"
  name: "0_1_conv_ReLU"
  type: RELU
}
layers {
  bottom: "0_1_conv"
  top: "0_pool"
  name: "0_pool"
  type: POOLING
  pooling_param {
    pool: MAX
    kernel_size: 2
    stride: 2
  }
}
layers {
  bottom: "0_pool"
  top: "1_0_conv"
  name: "1_0_conv"
  type: CONVOLUTION
  blobs_lr: 1
  blobs_lr: 2
  weight_decay: 0
  weight_decay: 0
  convolution_param {
    num_output: 32
    pad: 1
    kernel_size: 3
    stride: 1
    weight_filler {
      type: "xavier"
    }
    bias_filler {
      type: "constant"
      value: 0
    }
  }
}
layers {
  bottom: "1_0_conv"
  top: "1_0_conv"
  name: "1_0_conv_ReLU"
  type: RELU
}
layers {
  bottom: "1_0_conv"
  top: "1_1_conv"
  name: "1_1_conv"
  type: CONVOLUTION
  blobs_lr: 1
  blobs_lr: 2
  weight_decay: 0
  weight_decay: 0
  convolution_param {
    num_output: 32
    pad: 1
    kernel_size: 3
    stride: 1
    weight_filler {
      type: "xavier"
    }
    bias_filler {
      type: "constant"
      value: 0
    }
  }
}
layers {
  bottom: "1_1_conv"
  top: "1_1_conv"
  name: "1_1_conv_ReLU"
  type: RELU
}
layers {
  bottom: "1_1_conv"
  top: "1_pool"
  name: "1_pool"
  type: POOLING
  pooling_param {
    pool: MAX
    kernel_size: 2
    stride: 2
  }
}
layers {
  bottom: "1_pool"
  top: "2_0_conv"
  name: "2_0_conv"
  type: CONVOLUTION
  blobs_lr: 1
  blobs_lr: 2
  weight_decay: 0
  weight_decay: 0
  convolution_param {
    num_output: 32
    pad: 1
    kernel_size: 3
    stride: 1
    weight_filler {
      type: "xavier"
    }
    bias_filler {
      type: "constant"
      value: 0
    }
  }
}
layers {
  bottom: "2_0_conv"
  top: "2_0_conv"
  name: "2_0_conv_ReLU"
  type: RELU
}
layers {
  bottom: "2_0_conv"
  top: "2_1_conv"
  name: "2_1_conv"
  type: CONVOLUTION
  blobs_lr: 1
  blobs_lr: 2
  weight_decay: 0
  weight_decay: 0
  convolution_param {
    num_output: 32
    pad: 1
    kernel_size: 3
    stride: 1
    weight_filler {
      type: "xavier"
    }
    bias_filler {
      type: "constant"
      value: 0
    }
  }
}
layers {
  bottom: "2_1_conv"
  top: "2_1_conv"
  name: "2_1_conv_ReLU"
  type: RELU
}
layers {
  bottom: "2_1_conv"
  top: "2_pool"
  name: "2_pool"
  type: POOLING
  pooling_param {
    pool: MAX
    kernel_size: 2
    stride: 2
  }
}
layers {
  bottom: "2_pool"
  top: "middle_conv"
  name: "middle_conv"
  type: CONVOLUTION
  blobs_lr: 1
  blobs_lr: 2
  weight_decay: 0
  weight_decay: 0
  convolution_param {
    num_output: 50
    pad: 0
    kernel_size: 4
    stride: 1
    weight_filler {
      type: "xavier"
    }
    bias_filler {
      type: "constant"
      value: 0
    }
  }
}
layers {
  bottom: "middle_conv"
  top: "middle_conv"
  name: "middle_conv_ReLU"
  type: RELU
}
layers {
  bottom: "middle_conv"
  top: "fc1"
  name: "fc1"
  type: INNER_PRODUCT
  blobs_lr: 1
  blobs_lr: 2
  weight_decay: 0
  weight_decay: 0
  inner_product_param {
    num_output: 512
    weight_filler {
      type: "xavier"
    }
    bias_filler {
      type: "constant"
      value: 0
    }
  }
}
layers {
  bottom: "fc1"
  top: "fc1"
  name: "fc1_Dropout"
  type: DROPOUT
  dropout_param {
    dropout_ratio: 0.5
  }
  include {
    phase: TRAIN
  }
}
layers {
  bottom: "fc1"
  top: "fc1"
  name: "fc1_Dropout_ReLU"
  type: RELU
}
layers {
  bottom: "fc1"
  top: "fc2"
  name: "fc2"
  type: INNER_PRODUCT
  blobs_lr: 1
  blobs_lr: 2
  weight_decay: 0
  weight_decay: 0
  inner_product_param {
    num_output: 100
    weight_filler {
      type: "xavier"
    }
    bias_filler {
      type: "constant"
      value: 0
    }
  }
}
layers {
  bottom: "fc2"
  bottom: "label"
  top: "softmax"
  name: "softmax"
  type: SOFTMAX_LOSS
}
state {
  phase: TRAIN
}
I0623 12:17:20.733901  5064 layer_factory.hpp:78] Creating layer data
I0623 12:17:20.733913  5064 data_transformer.cpp:25] Loading mean file from/dataset/cifar100_lmdb_lab/mean.binaryproto
I0623 12:17:20.733959  5064 net.cpp:69] Creating Layer data
I0623 12:17:20.733964  5064 net.cpp:358] data -> data
I0623 12:17:20.733973  5064 net.cpp:358] data -> label
I0623 12:17:20.733978  5064 net.cpp:98] Setting up data
I0623 12:17:20.733981  5064 data_layer.cpp:32] Opening dataset /dataset/cifar100_lmdb_lab/cifar100_train_lmdb
I0623 12:17:20.734050  5064 data_layer.cpp:71] output data size: 128,3,32,32
I0623 12:17:20.734360  5064 net.cpp:105] Top shape: 128 3 32 32 (393216)
I0623 12:17:20.734366  5064 net.cpp:105] Top shape: 128 1 1 1 (128)
I0623 12:17:20.734370  5064 layer_factory.hpp:78] Creating layer 0_0_conv
I0623 12:17:20.734375  5064 net.cpp:69] Creating Layer 0_0_conv
I0623 12:17:20.734378  5064 net.cpp:396] 0_0_conv <- data
I0623 12:17:20.734385  5064 net.cpp:358] 0_0_conv -> 0_0_conv
I0623 12:17:20.734391  5064 net.cpp:98] Setting up 0_0_conv
I0623 12:17:20.734668  5064 net.cpp:105] Top shape: 128 32 32 32 (4194304)
I0623 12:17:20.734681  5064 layer_factory.hpp:78] Creating layer 0_0_conv_ReLU
I0623 12:17:20.734688  5064 net.cpp:69] Creating Layer 0_0_conv_ReLU
I0623 12:17:20.734690  5064 net.cpp:396] 0_0_conv_ReLU <- 0_0_conv
I0623 12:17:20.734694  5064 net.cpp:347] 0_0_conv_ReLU -> 0_0_conv (in-place)
I0623 12:17:20.734699  5064 net.cpp:98] Setting up 0_0_conv_ReLU
I0623 12:17:20.734701  5064 net.cpp:105] Top shape: 128 32 32 32 (4194304)
I0623 12:17:20.734704  5064 layer_factory.hpp:78] Creating layer 0_1_conv
I0623 12:17:20.734709  5064 net.cpp:69] Creating Layer 0_1_conv
I0623 12:17:20.734712  5064 net.cpp:396] 0_1_conv <- 0_0_conv
I0623 12:17:20.734716  5064 net.cpp:358] 0_1_conv -> 0_1_conv
I0623 12:17:20.734720  5064 net.cpp:98] Setting up 0_1_conv
I0623 12:17:20.734778  5064 net.cpp:105] Top shape: 128 32 32 32 (4194304)
I0623 12:17:20.734786  5064 layer_factory.hpp:78] Creating layer 0_1_conv_ReLU
I0623 12:17:20.734791  5064 net.cpp:69] Creating Layer 0_1_conv_ReLU
I0623 12:17:20.734793  5064 net.cpp:396] 0_1_conv_ReLU <- 0_1_conv
I0623 12:17:20.734797  5064 net.cpp:347] 0_1_conv_ReLU -> 0_1_conv (in-place)
I0623 12:17:20.734803  5064 net.cpp:98] Setting up 0_1_conv_ReLU
I0623 12:17:20.734807  5064 net.cpp:105] Top shape: 128 32 32 32 (4194304)
I0623 12:17:20.734815  5064 layer_factory.hpp:78] Creating layer 0_pool
I0623 12:17:20.734820  5064 net.cpp:69] Creating Layer 0_pool
I0623 12:17:20.734823  5064 net.cpp:396] 0_pool <- 0_1_conv
I0623 12:17:20.734827  5064 net.cpp:358] 0_pool -> 0_pool
I0623 12:17:20.734832  5064 net.cpp:98] Setting up 0_pool
I0623 12:17:20.734838  5064 net.cpp:105] Top shape: 128 32 16 16 (1048576)
I0623 12:17:20.734840  5064 layer_factory.hpp:78] Creating layer 1_0_conv
I0623 12:17:20.734844  5064 net.cpp:69] Creating Layer 1_0_conv
I0623 12:17:20.734846  5064 net.cpp:396] 1_0_conv <- 0_pool
I0623 12:17:20.734851  5064 net.cpp:358] 1_0_conv -> 1_0_conv
I0623 12:17:20.734855  5064 net.cpp:98] Setting up 1_0_conv
I0623 12:17:20.734918  5064 net.cpp:105] Top shape: 128 32 16 16 (1048576)
I0623 12:17:20.734925  5064 layer_factory.hpp:78] Creating layer 1_0_conv_ReLU
I0623 12:17:20.734930  5064 net.cpp:69] Creating Layer 1_0_conv_ReLU
I0623 12:17:20.734932  5064 net.cpp:396] 1_0_conv_ReLU <- 1_0_conv
I0623 12:17:20.734937  5064 net.cpp:347] 1_0_conv_ReLU -> 1_0_conv (in-place)
I0623 12:17:20.734941  5064 net.cpp:98] Setting up 1_0_conv_ReLU
I0623 12:17:20.734944  5064 net.cpp:105] Top shape: 128 32 16 16 (1048576)
I0623 12:17:20.734947  5064 layer_factory.hpp:78] Creating layer 1_1_conv
I0623 12:17:20.734951  5064 net.cpp:69] Creating Layer 1_1_conv
I0623 12:17:20.734954  5064 net.cpp:396] 1_1_conv <- 1_0_conv
I0623 12:17:20.734959  5064 net.cpp:358] 1_1_conv -> 1_1_conv
I0623 12:17:20.734963  5064 net.cpp:98] Setting up 1_1_conv
I0623 12:17:20.735021  5064 net.cpp:105] Top shape: 128 32 16 16 (1048576)
I0623 12:17:20.735028  5064 layer_factory.hpp:78] Creating layer 1_1_conv_ReLU
I0623 12:17:20.735033  5064 net.cpp:69] Creating Layer 1_1_conv_ReLU
I0623 12:17:20.735035  5064 net.cpp:396] 1_1_conv_ReLU <- 1_1_conv
I0623 12:17:20.735039  5064 net.cpp:347] 1_1_conv_ReLU -> 1_1_conv (in-place)
I0623 12:17:20.735044  5064 net.cpp:98] Setting up 1_1_conv_ReLU
I0623 12:17:20.735046  5064 net.cpp:105] Top shape: 128 32 16 16 (1048576)
I0623 12:17:20.735049  5064 layer_factory.hpp:78] Creating layer 1_pool
I0623 12:17:20.735052  5064 net.cpp:69] Creating Layer 1_pool
I0623 12:17:20.735055  5064 net.cpp:396] 1_pool <- 1_1_conv
I0623 12:17:20.735059  5064 net.cpp:358] 1_pool -> 1_pool
I0623 12:17:20.735064  5064 net.cpp:98] Setting up 1_pool
I0623 12:17:20.735066  5064 net.cpp:105] Top shape: 128 32 8 8 (262144)
I0623 12:17:20.735069  5064 layer_factory.hpp:78] Creating layer 2_0_conv
I0623 12:17:20.735072  5064 net.cpp:69] Creating Layer 2_0_conv
I0623 12:17:20.735075  5064 net.cpp:396] 2_0_conv <- 1_pool
I0623 12:17:20.735080  5064 net.cpp:358] 2_0_conv -> 2_0_conv
I0623 12:17:20.735085  5064 net.cpp:98] Setting up 2_0_conv
I0623 12:17:20.735137  5064 net.cpp:105] Top shape: 128 32 8 8 (262144)
I0623 12:17:20.735144  5064 layer_factory.hpp:78] Creating layer 2_0_conv_ReLU
I0623 12:17:20.735147  5064 net.cpp:69] Creating Layer 2_0_conv_ReLU
I0623 12:17:20.735151  5064 net.cpp:396] 2_0_conv_ReLU <- 2_0_conv
I0623 12:17:20.735154  5064 net.cpp:347] 2_0_conv_ReLU -> 2_0_conv (in-place)
I0623 12:17:20.735158  5064 net.cpp:98] Setting up 2_0_conv_ReLU
I0623 12:17:20.735162  5064 net.cpp:105] Top shape: 128 32 8 8 (262144)
I0623 12:17:20.735163  5064 layer_factory.hpp:78] Creating layer 2_1_conv
I0623 12:17:20.735167  5064 net.cpp:69] Creating Layer 2_1_conv
I0623 12:17:20.735170  5064 net.cpp:396] 2_1_conv <- 2_0_conv
I0623 12:17:20.735174  5064 net.cpp:358] 2_1_conv -> 2_1_conv
I0623 12:17:20.735178  5064 net.cpp:98] Setting up 2_1_conv
I0623 12:17:20.735229  5064 net.cpp:105] Top shape: 128 32 8 8 (262144)
I0623 12:17:20.735234  5064 layer_factory.hpp:78] Creating layer 2_1_conv_ReLU
I0623 12:17:20.735237  5064 net.cpp:69] Creating Layer 2_1_conv_ReLU
I0623 12:17:20.735240  5064 net.cpp:396] 2_1_conv_ReLU <- 2_1_conv
I0623 12:17:20.735244  5064 net.cpp:347] 2_1_conv_ReLU -> 2_1_conv (in-place)
I0623 12:17:20.735249  5064 net.cpp:98] Setting up 2_1_conv_ReLU
I0623 12:17:20.735252  5064 net.cpp:105] Top shape: 128 32 8 8 (262144)
I0623 12:17:20.735260  5064 layer_factory.hpp:78] Creating layer 2_pool
I0623 12:17:20.735263  5064 net.cpp:69] Creating Layer 2_pool
I0623 12:17:20.735265  5064 net.cpp:396] 2_pool <- 2_1_conv
I0623 12:17:20.735270  5064 net.cpp:358] 2_pool -> 2_pool
I0623 12:17:20.735275  5064 net.cpp:98] Setting up 2_pool
I0623 12:17:20.735277  5064 net.cpp:105] Top shape: 128 32 4 4 (65536)
I0623 12:17:20.735280  5064 layer_factory.hpp:78] Creating layer middle_conv
I0623 12:17:20.735285  5064 net.cpp:69] Creating Layer middle_conv
I0623 12:17:20.735287  5064 net.cpp:396] middle_conv <- 2_pool
I0623 12:17:20.735293  5064 net.cpp:358] middle_conv -> middle_conv
I0623 12:17:20.735297  5064 net.cpp:98] Setting up middle_conv
I0623 12:17:20.735445  5064 net.cpp:105] Top shape: 128 50 1 1 (6400)
I0623 12:17:20.735452  5064 layer_factory.hpp:78] Creating layer middle_conv_ReLU
I0623 12:17:20.735456  5064 net.cpp:69] Creating Layer middle_conv_ReLU
I0623 12:17:20.735460  5064 net.cpp:396] middle_conv_ReLU <- middle_conv
I0623 12:17:20.735463  5064 net.cpp:347] middle_conv_ReLU -> middle_conv (in-place)
I0623 12:17:20.735466  5064 net.cpp:98] Setting up middle_conv_ReLU
I0623 12:17:20.735469  5064 net.cpp:105] Top shape: 128 50 1 1 (6400)
I0623 12:17:20.735472  5064 layer_factory.hpp:78] Creating layer fc1
I0623 12:17:20.735476  5064 net.cpp:69] Creating Layer fc1
I0623 12:17:20.735478  5064 net.cpp:396] fc1 <- middle_conv
I0623 12:17:20.735483  5064 net.cpp:358] fc1 -> fc1
I0623 12:17:20.735487  5064 net.cpp:98] Setting up fc1
I0623 12:17:20.735625  5064 net.cpp:105] Top shape: 128 512 1 1 (65536)
I0623 12:17:20.735630  5064 layer_factory.hpp:78] Creating layer fc1_Dropout
I0623 12:17:20.735637  5064 net.cpp:69] Creating Layer fc1_Dropout
I0623 12:17:20.735641  5064 net.cpp:396] fc1_Dropout <- fc1
I0623 12:17:20.735646  5064 net.cpp:347] fc1_Dropout -> fc1 (in-place)
I0623 12:17:20.735649  5064 net.cpp:98] Setting up fc1_Dropout
I0623 12:17:20.735652  5064 net.cpp:105] Top shape: 128 512 1 1 (65536)
I0623 12:17:20.735656  5064 layer_factory.hpp:78] Creating layer fc1_Dropout_ReLU
I0623 12:17:20.735658  5064 net.cpp:69] Creating Layer fc1_Dropout_ReLU
I0623 12:17:20.735661  5064 net.cpp:396] fc1_Dropout_ReLU <- fc1
I0623 12:17:20.735666  5064 net.cpp:347] fc1_Dropout_ReLU -> fc1 (in-place)
I0623 12:17:20.735669  5064 net.cpp:98] Setting up fc1_Dropout_ReLU
I0623 12:17:20.735671  5064 net.cpp:105] Top shape: 128 512 1 1 (65536)
I0623 12:17:20.735674  5064 layer_factory.hpp:78] Creating layer fc2
I0623 12:17:20.735677  5064 net.cpp:69] Creating Layer fc2
I0623 12:17:20.735680  5064 net.cpp:396] fc2 <- fc1
I0623 12:17:20.735684  5064 net.cpp:358] fc2 -> fc2
I0623 12:17:20.735688  5064 net.cpp:98] Setting up fc2
I0623 12:17:20.735942  5064 net.cpp:105] Top shape: 128 100 1 1 (12800)
I0623 12:17:20.735950  5064 layer_factory.hpp:78] Creating layer softmax
I0623 12:17:20.735957  5064 net.cpp:69] Creating Layer softmax
I0623 12:17:20.735960  5064 net.cpp:396] softmax <- fc2
I0623 12:17:20.735963  5064 net.cpp:396] softmax <- label
I0623 12:17:20.735967  5064 net.cpp:358] softmax -> softmax
I0623 12:17:20.735972  5064 net.cpp:98] Setting up softmax
I0623 12:17:20.735980  5064 net.cpp:105] Top shape: 1 1 1 1 (1)
I0623 12:17:20.735983  5064 net.cpp:111]     with loss weight 1
I0623 12:17:20.735993  5064 net.cpp:172] softmax needs backward computation.
I0623 12:17:20.735996  5064 net.cpp:172] fc2 needs backward computation.
I0623 12:17:20.735999  5064 net.cpp:172] fc1_Dropout_ReLU needs backward computation.
I0623 12:17:20.736002  5064 net.cpp:172] fc1_Dropout needs backward computation.
I0623 12:17:20.736004  5064 net.cpp:172] fc1 needs backward computation.
I0623 12:17:20.736007  5064 net.cpp:172] middle_conv_ReLU needs backward computation.
I0623 12:17:20.736009  5064 net.cpp:172] middle_conv needs backward computation.
I0623 12:17:20.736012  5064 net.cpp:172] 2_pool needs backward computation.
I0623 12:17:20.736016  5064 net.cpp:172] 2_1_conv_ReLU needs backward computation.
I0623 12:17:20.736021  5064 net.cpp:172] 2_1_conv needs backward computation.
I0623 12:17:20.736027  5064 net.cpp:172] 2_0_conv_ReLU needs backward computation.
I0623 12:17:20.736029  5064 net.cpp:172] 2_0_conv needs backward computation.
I0623 12:17:20.736032  5064 net.cpp:172] 1_pool needs backward computation.
I0623 12:17:20.736035  5064 net.cpp:172] 1_1_conv_ReLU needs backward computation.
I0623 12:17:20.736037  5064 net.cpp:172] 1_1_conv needs backward computation.
I0623 12:17:20.736040  5064 net.cpp:172] 1_0_conv_ReLU needs backward computation.
I0623 12:17:20.736043  5064 net.cpp:172] 1_0_conv needs backward computation.
I0623 12:17:20.736047  5064 net.cpp:172] 0_pool needs backward computation.
I0623 12:17:20.736049  5064 net.cpp:172] 0_1_conv_ReLU needs backward computation.
I0623 12:17:20.736052  5064 net.cpp:172] 0_1_conv needs backward computation.
I0623 12:17:20.736054  5064 net.cpp:172] 0_0_conv_ReLU needs backward computation.
I0623 12:17:20.736057  5064 net.cpp:172] 0_0_conv needs backward computation.
I0623 12:17:20.736059  5064 net.cpp:174] data does not need backward computation.
I0623 12:17:20.736063  5064 net.cpp:210] This network produces output softmax
I0623 12:17:20.736073  5064 net.cpp:469] Collecting Learning Rate and Weight Decay.
I0623 12:17:20.736078  5064 net.cpp:221] Network initialization done.
I0623 12:17:20.736081  5064 net.cpp:222] Memory required for data: 96047620
I0623 12:17:20.736510  5064 solver.cpp:154] Creating test net (#0) specified by net file: prototxt/16-06-21_16h05m31s_0_11_pretrainClassification_net.sh
I0623 12:17:20.736536  5064 net.cpp:277] The NetState phase (1) differed from the phase (0) specified by a rule in layer data
I0623 12:17:20.736546  5064 net.cpp:277] The NetState phase (1) differed from the phase (0) specified by a rule in layer fc1_Dropout
I0623 12:17:20.736639  5064 net.cpp:39] Initializing net from parameters: 
layers {
  top: "data"
  top: "label"
  name: "data"
  type: DATA
  data_param {
    source: "/dataset/cifar100_lmdb_lab/cifar100_test_lmdb"
    batch_size: 128
    backend: LMDB
  }
  include {
    phase: TEST
  }
  transform_param {
    mean_file: "/dataset/cifar100_lmdb_lab/mean.binaryproto"
  }
}
layers {
  bottom: "data"
  top: "0_0_conv"
  name: "0_0_conv"
  type: CONVOLUTION
  blobs_lr: 1
  blobs_lr: 2
  weight_decay: 0
  weight_decay: 0
  convolution_param {
    num_output: 32
    pad: 1
    kernel_size: 3
    stride: 1
    weight_filler {
      type: "xavier"
    }
    bias_filler {
      type: "constant"
      value: 0
    }
  }
}
layers {
  bottom: "0_0_conv"
  top: "0_0_conv"
  name: "0_0_conv_ReLU"
  type: RELU
}
layers {
  bottom: "0_0_conv"
  top: "0_1_conv"
  name: "0_1_conv"
  type: CONVOLUTION
  blobs_lr: 1
  blobs_lr: 2
  weight_decay: 0
  weight_decay: 0
  convolution_param {
    num_output: 32
    pad: 1
    kernel_size: 3
    stride: 1
    weight_filler {
      type: "xavier"
    }
    bias_filler {
      type: "constant"
      value: 0
    }
  }
}
layers {
  bottom: "0_1_conv"
  top: "0_1_conv"
  name: "0_1_conv_ReLU"
  type: RELU
}
layers {
  bottom: "0_1_conv"
  top: "0_pool"
  name: "0_pool"
  type: POOLING
  pooling_param {
    pool: MAX
    kernel_size: 2
    stride: 2
  }
}
layers {
  bottom: "0_pool"
  top: "1_0_conv"
  name: "1_0_conv"
  type: CONVOLUTION
  blobs_lr: 1
  blobs_lr: 2
  weight_decay: 0
  weight_decay: 0
  convolution_param {
    num_output: 32
    pad: 1
    kernel_size: 3
    stride: 1
    weight_filler {
      type: "xavier"
    }
    bias_filler {
      type: "constant"
      value: 0
    }
  }
}
layers {
  bottom: "1_0_conv"
  top: "1_0_conv"
  name: "1_0_conv_ReLU"
  type: RELU
}
layers {
  bottom: "1_0_conv"
  top: "1_1_conv"
  name: "1_1_conv"
  type: CONVOLUTION
  blobs_lr: 1
  blobs_lr: 2
  weight_decay: 0
  weight_decay: 0
  convolution_param {
    num_output: 32
    pad: 1
    kernel_size: 3
    stride: 1
    weight_filler {
      type: "xavier"
    }
    bias_filler {
      type: "constant"
      value: 0
    }
  }
}
layers {
  bottom: "1_1_conv"
  top: "1_1_conv"
  name: "1_1_conv_ReLU"
  type: RELU
}
layers {
  bottom: "1_1_conv"
  top: "1_pool"
  name: "1_pool"
  type: POOLING
  pooling_param {
    pool: MAX
    kernel_size: 2
    stride: 2
  }
}
layers {
  bottom: "1_pool"
  top: "2_0_conv"
  name: "2_0_conv"
  type: CONVOLUTION
  blobs_lr: 1
  blobs_lr: 2
  weight_decay: 0
  weight_decay: 0
  convolution_param {
    num_output: 32
    pad: 1
    kernel_size: 3
    stride: 1
    weight_filler {
      type: "xavier"
    }
    bias_filler {
      type: "constant"
      value: 0
    }
  }
}
layers {
  bottom: "2_0_conv"
  top: "2_0_conv"
  name: "2_0_conv_ReLU"
  type: RELU
}
layers {
  bottom: "2_0_conv"
  top: "2_1_conv"
  name: "2_1_conv"
  type: CONVOLUTION
  blobs_lr: 1
  blobs_lr: 2
  weight_decay: 0
  weight_decay: 0
  convolution_param {
    num_output: 32
    pad: 1
    kernel_size: 3
    stride: 1
    weight_filler {
      type: "xavier"
    }
    bias_filler {
      type: "constant"
      value: 0
    }
  }
}
layers {
  bottom: "2_1_conv"
  top: "2_1_conv"
  name: "2_1_conv_ReLU"
  type: RELU
}
layers {
  bottom: "2_1_conv"
  top: "2_pool"
  name: "2_pool"
  type: POOLING
  pooling_param {
    pool: MAX
    kernel_size: 2
    stride: 2
  }
}
layers {
  bottom: "2_pool"
  top: "middle_conv"
  name: "middle_conv"
  type: CONVOLUTION
  blobs_lr: 1
  blobs_lr: 2
  weight_decay: 0
  weight_decay: 0
  convolution_param {
    num_output: 50
    pad: 0
    kernel_size: 4
    stride: 1
    weight_filler {
      type: "xavier"
    }
    bias_filler {
      type: "constant"
      value: 0
    }
  }
}
layers {
  bottom: "middle_conv"
  top: "middle_conv"
  name: "middle_conv_ReLU"
  type: RELU
}
layers {
  bottom: "middle_conv"
  top: "fc1"
  name: "fc1"
  type: INNER_PRODUCT
  blobs_lr: 1
  blobs_lr: 2
  weight_decay: 0
  weight_decay: 0
  inner_product_param {
    num_output: 512
    weight_filler {
      type: "xavier"
    }
    bias_filler {
      type: "constant"
      value: 0
    }
  }
}
layers {
  bottom: "fc1"
  top: "fc1"
  name: "fc1_Dropout_ReLU"
  type: RELU
}
layers {
  bottom: "fc1"
  top: "fc2"
  name: "fc2"
  type: INNER_PRODUCT
  blobs_lr: 1
  blobs_lr: 2
  weight_decay: 0
  weight_decay: 0
  inner_product_param {
    num_output: 100
    weight_filler {
      type: "xavier"
    }
    bias_filler {
      type: "constant"
      value: 0
    }
  }
}
layers {
  bottom: "fc2"
  bottom: "label"
  top: "softmax"
  name: "softmax"
  type: SOFTMAX_LOSS
}
layers {
  bottom: "fc2"
  bottom: "label"
  top: "accuracy_top_1"
  name: "accuracy_top_1"
  type: ACCURACY
  accuracy_param {
    top_k: 1
  }
  include {
    phase: TEST
  }
}
layers {
  bottom: "fc2"
  bottom: "label"
  top: "accuracy_top_5"
  name: "accuracy_top_5"
  type: ACCURACY
  accuracy_param {
    top_k: 5
  }
  include {
    phase: TEST
  }
}
state {
  phase: TEST
}
I0623 12:17:20.736721  5064 layer_factory.hpp:78] Creating layer data
I0623 12:17:20.736726  5064 data_transformer.cpp:25] Loading mean file from/dataset/cifar100_lmdb_lab/mean.binaryproto
I0623 12:17:20.736757  5064 net.cpp:69] Creating Layer data
I0623 12:17:20.736763  5064 net.cpp:358] data -> data
I0623 12:17:20.736768  5064 net.cpp:358] data -> label
I0623 12:17:20.736773  5064 net.cpp:98] Setting up data
I0623 12:17:20.736776  5064 data_layer.cpp:32] Opening dataset /dataset/cifar100_lmdb_lab/cifar100_test_lmdb
I0623 12:17:20.736812  5064 data_layer.cpp:71] output data size: 128,3,32,32
I0623 12:17:20.737149  5064 net.cpp:105] Top shape: 128 3 32 32 (393216)
I0623 12:17:20.737156  5064 net.cpp:105] Top shape: 128 1 1 1 (128)
I0623 12:17:20.737160  5064 layer_factory.hpp:78] Creating layer label_data_1_split
I0623 12:17:20.737165  5064 net.cpp:69] Creating Layer label_data_1_split
I0623 12:17:20.737169  5064 net.cpp:396] label_data_1_split <- label
I0623 12:17:20.737172  5064 net.cpp:358] label_data_1_split -> label_data_1_split_0
I0623 12:17:20.737179  5064 net.cpp:358] label_data_1_split -> label_data_1_split_1
I0623 12:17:20.737185  5064 net.cpp:358] label_data_1_split -> label_data_1_split_2
I0623 12:17:20.737200  5064 net.cpp:98] Setting up label_data_1_split
I0623 12:17:20.737205  5064 net.cpp:105] Top shape: 128 1 1 1 (128)
I0623 12:17:20.737211  5064 net.cpp:105] Top shape: 128 1 1 1 (128)
I0623 12:17:20.737215  5064 net.cpp:105] Top shape: 128 1 1 1 (128)
I0623 12:17:20.737217  5064 layer_factory.hpp:78] Creating layer 0_0_conv
I0623 12:17:20.737222  5064 net.cpp:69] Creating Layer 0_0_conv
I0623 12:17:20.737226  5064 net.cpp:396] 0_0_conv <- data
I0623 12:17:20.737231  5064 net.cpp:358] 0_0_conv -> 0_0_conv
I0623 12:17:20.737236  5064 net.cpp:98] Setting up 0_0_conv
I0623 12:17:20.737249  5064 net.cpp:105] Top shape: 128 32 32 32 (4194304)
I0623 12:17:20.737257  5064 layer_factory.hpp:78] Creating layer 0_0_conv_ReLU
I0623 12:17:20.737259  5064 net.cpp:69] Creating Layer 0_0_conv_ReLU
I0623 12:17:20.737262  5064 net.cpp:396] 0_0_conv_ReLU <- 0_0_conv
I0623 12:17:20.737267  5064 net.cpp:347] 0_0_conv_ReLU -> 0_0_conv (in-place)
I0623 12:17:20.737270  5064 net.cpp:98] Setting up 0_0_conv_ReLU
I0623 12:17:20.737273  5064 net.cpp:105] Top shape: 128 32 32 32 (4194304)
I0623 12:17:20.737277  5064 layer_factory.hpp:78] Creating layer 0_1_conv
I0623 12:17:20.737280  5064 net.cpp:69] Creating Layer 0_1_conv
I0623 12:17:20.737282  5064 net.cpp:396] 0_1_conv <- 0_0_conv
I0623 12:17:20.737287  5064 net.cpp:358] 0_1_conv -> 0_1_conv
I0623 12:17:20.737292  5064 net.cpp:98] Setting up 0_1_conv
I0623 12:17:20.737342  5064 net.cpp:105] Top shape: 128 32 32 32 (4194304)
I0623 12:17:20.737349  5064 layer_factory.hpp:78] Creating layer 0_1_conv_ReLU
I0623 12:17:20.737351  5064 net.cpp:69] Creating Layer 0_1_conv_ReLU
I0623 12:17:20.737354  5064 net.cpp:396] 0_1_conv_ReLU <- 0_1_conv
I0623 12:17:20.737360  5064 net.cpp:347] 0_1_conv_ReLU -> 0_1_conv (in-place)
I0623 12:17:20.737365  5064 net.cpp:98] Setting up 0_1_conv_ReLU
I0623 12:17:20.737367  5064 net.cpp:105] Top shape: 128 32 32 32 (4194304)
I0623 12:17:20.737370  5064 layer_factory.hpp:78] Creating layer 0_pool
I0623 12:17:20.737373  5064 net.cpp:69] Creating Layer 0_pool
I0623 12:17:20.737376  5064 net.cpp:396] 0_pool <- 0_1_conv
I0623 12:17:20.737380  5064 net.cpp:358] 0_pool -> 0_pool
I0623 12:17:20.737385  5064 net.cpp:98] Setting up 0_pool
I0623 12:17:20.737388  5064 net.cpp:105] Top shape: 128 32 16 16 (1048576)
I0623 12:17:20.737391  5064 layer_factory.hpp:78] Creating layer 1_0_conv
I0623 12:17:20.737395  5064 net.cpp:69] Creating Layer 1_0_conv
I0623 12:17:20.737397  5064 net.cpp:396] 1_0_conv <- 0_pool
I0623 12:17:20.737401  5064 net.cpp:358] 1_0_conv -> 1_0_conv
I0623 12:17:20.737406  5064 net.cpp:98] Setting up 1_0_conv
I0623 12:17:20.737458  5064 net.cpp:105] Top shape: 128 32 16 16 (1048576)
I0623 12:17:20.737464  5064 layer_factory.hpp:78] Creating layer 1_0_conv_ReLU
I0623 12:17:20.737468  5064 net.cpp:69] Creating Layer 1_0_conv_ReLU
I0623 12:17:20.737471  5064 net.cpp:396] 1_0_conv_ReLU <- 1_0_conv
I0623 12:17:20.737478  5064 net.cpp:347] 1_0_conv_ReLU -> 1_0_conv (in-place)
I0623 12:17:20.737483  5064 net.cpp:98] Setting up 1_0_conv_ReLU
I0623 12:17:20.737485  5064 net.cpp:105] Top shape: 128 32 16 16 (1048576)
I0623 12:17:20.737488  5064 layer_factory.hpp:78] Creating layer 1_1_conv
I0623 12:17:20.737491  5064 net.cpp:69] Creating Layer 1_1_conv
I0623 12:17:20.737494  5064 net.cpp:396] 1_1_conv <- 1_0_conv
I0623 12:17:20.737498  5064 net.cpp:358] 1_1_conv -> 1_1_conv
I0623 12:17:20.737503  5064 net.cpp:98] Setting up 1_1_conv
I0623 12:17:20.737553  5064 net.cpp:105] Top shape: 128 32 16 16 (1048576)
I0623 12:17:20.737558  5064 layer_factory.hpp:78] Creating layer 1_1_conv_ReLU
I0623 12:17:20.737562  5064 net.cpp:69] Creating Layer 1_1_conv_ReLU
I0623 12:17:20.737565  5064 net.cpp:396] 1_1_conv_ReLU <- 1_1_conv
I0623 12:17:20.737568  5064 net.cpp:347] 1_1_conv_ReLU -> 1_1_conv (in-place)
I0623 12:17:20.737571  5064 net.cpp:98] Setting up 1_1_conv_ReLU
I0623 12:17:20.737574  5064 net.cpp:105] Top shape: 128 32 16 16 (1048576)
I0623 12:17:20.737577  5064 layer_factory.hpp:78] Creating layer 1_pool
I0623 12:17:20.737581  5064 net.cpp:69] Creating Layer 1_pool
I0623 12:17:20.737586  5064 net.cpp:396] 1_pool <- 1_1_conv
I0623 12:17:20.737591  5064 net.cpp:358] 1_pool -> 1_pool
I0623 12:17:20.737598  5064 net.cpp:98] Setting up 1_pool
I0623 12:17:20.737602  5064 net.cpp:105] Top shape: 128 32 8 8 (262144)
I0623 12:17:20.737606  5064 layer_factory.hpp:78] Creating layer 2_0_conv
I0623 12:17:20.737608  5064 net.cpp:69] Creating Layer 2_0_conv
I0623 12:17:20.737612  5064 net.cpp:396] 2_0_conv <- 1_pool
I0623 12:17:20.737617  5064 net.cpp:358] 2_0_conv -> 2_0_conv
I0623 12:17:20.737620  5064 net.cpp:98] Setting up 2_0_conv
I0623 12:17:20.737670  5064 net.cpp:105] Top shape: 128 32 8 8 (262144)
I0623 12:17:20.737676  5064 layer_factory.hpp:78] Creating layer 2_0_conv_ReLU
I0623 12:17:20.737680  5064 net.cpp:69] Creating Layer 2_0_conv_ReLU
I0623 12:17:20.737682  5064 net.cpp:396] 2_0_conv_ReLU <- 2_0_conv
I0623 12:17:20.737686  5064 net.cpp:347] 2_0_conv_ReLU -> 2_0_conv (in-place)
I0623 12:17:20.737690  5064 net.cpp:98] Setting up 2_0_conv_ReLU
I0623 12:17:20.737692  5064 net.cpp:105] Top shape: 128 32 8 8 (262144)
I0623 12:17:20.737695  5064 layer_factory.hpp:78] Creating layer 2_1_conv
I0623 12:17:20.737699  5064 net.cpp:69] Creating Layer 2_1_conv
I0623 12:17:20.737702  5064 net.cpp:396] 2_1_conv <- 2_0_conv
I0623 12:17:20.737706  5064 net.cpp:358] 2_1_conv -> 2_1_conv
I0623 12:17:20.737710  5064 net.cpp:98] Setting up 2_1_conv
I0623 12:17:20.737778  5064 net.cpp:105] Top shape: 128 32 8 8 (262144)
I0623 12:17:20.737784  5064 layer_factory.hpp:78] Creating layer 2_1_conv_ReLU
I0623 12:17:20.737802  5064 net.cpp:69] Creating Layer 2_1_conv_ReLU
I0623 12:17:20.737805  5064 net.cpp:396] 2_1_conv_ReLU <- 2_1_conv
I0623 12:17:20.737809  5064 net.cpp:347] 2_1_conv_ReLU -> 2_1_conv (in-place)
I0623 12:17:20.737813  5064 net.cpp:98] Setting up 2_1_conv_ReLU
I0623 12:17:20.737815  5064 net.cpp:105] Top shape: 128 32 8 8 (262144)
I0623 12:17:20.737819  5064 layer_factory.hpp:78] Creating layer 2_pool
I0623 12:17:20.737824  5064 net.cpp:69] Creating Layer 2_pool
I0623 12:17:20.737830  5064 net.cpp:396] 2_pool <- 2_1_conv
I0623 12:17:20.737838  5064 net.cpp:358] 2_pool -> 2_pool
I0623 12:17:20.737844  5064 net.cpp:98] Setting up 2_pool
I0623 12:17:20.737848  5064 net.cpp:105] Top shape: 128 32 4 4 (65536)
I0623 12:17:20.737851  5064 layer_factory.hpp:78] Creating layer middle_conv
I0623 12:17:20.737856  5064 net.cpp:69] Creating Layer middle_conv
I0623 12:17:20.737859  5064 net.cpp:396] middle_conv <- 2_pool
I0623 12:17:20.737862  5064 net.cpp:358] middle_conv -> middle_conv
I0623 12:17:20.737866  5064 net.cpp:98] Setting up middle_conv
I0623 12:17:20.738000  5064 net.cpp:105] Top shape: 128 50 1 1 (6400)
I0623 12:17:20.738005  5064 layer_factory.hpp:78] Creating layer middle_conv_ReLU
I0623 12:17:20.738009  5064 net.cpp:69] Creating Layer middle_conv_ReLU
I0623 12:17:20.738013  5064 net.cpp:396] middle_conv_ReLU <- middle_conv
I0623 12:17:20.738016  5064 net.cpp:347] middle_conv_ReLU -> middle_conv (in-place)
I0623 12:17:20.738020  5064 net.cpp:98] Setting up middle_conv_ReLU
I0623 12:17:20.738023  5064 net.cpp:105] Top shape: 128 50 1 1 (6400)
I0623 12:17:20.738028  5064 layer_factory.hpp:78] Creating layer fc1
I0623 12:17:20.738031  5064 net.cpp:69] Creating Layer fc1
I0623 12:17:20.738034  5064 net.cpp:396] fc1 <- middle_conv
I0623 12:17:20.738039  5064 net.cpp:358] fc1 -> fc1
I0623 12:17:20.738044  5064 net.cpp:98] Setting up fc1
I0623 12:17:20.738178  5064 net.cpp:105] Top shape: 128 512 1 1 (65536)
I0623 12:17:20.738186  5064 layer_factory.hpp:78] Creating layer fc1_Dropout_ReLU
I0623 12:17:20.738190  5064 net.cpp:69] Creating Layer fc1_Dropout_ReLU
I0623 12:17:20.738193  5064 net.cpp:396] fc1_Dropout_ReLU <- fc1
I0623 12:17:20.738198  5064 net.cpp:347] fc1_Dropout_ReLU -> fc1 (in-place)
I0623 12:17:20.738201  5064 net.cpp:98] Setting up fc1_Dropout_ReLU
I0623 12:17:20.738204  5064 net.cpp:105] Top shape: 128 512 1 1 (65536)
I0623 12:17:20.738207  5064 layer_factory.hpp:78] Creating layer fc2
I0623 12:17:20.738212  5064 net.cpp:69] Creating Layer fc2
I0623 12:17:20.738215  5064 net.cpp:396] fc2 <- fc1
I0623 12:17:20.738224  5064 net.cpp:358] fc2 -> fc2
I0623 12:17:20.738229  5064 net.cpp:98] Setting up fc2
I0623 12:17:20.738497  5064 net.cpp:105] Top shape: 128 100 1 1 (12800)
I0623 12:17:20.738507  5064 layer_factory.hpp:78] Creating layer fc2_fc2_0_split
I0623 12:17:20.738512  5064 net.cpp:69] Creating Layer fc2_fc2_0_split
I0623 12:17:20.738514  5064 net.cpp:396] fc2_fc2_0_split <- fc2
I0623 12:17:20.738519  5064 net.cpp:358] fc2_fc2_0_split -> fc2_fc2_0_split_0
I0623 12:17:20.738525  5064 net.cpp:358] fc2_fc2_0_split -> fc2_fc2_0_split_1
I0623 12:17:20.738530  5064 net.cpp:358] fc2_fc2_0_split -> fc2_fc2_0_split_2
I0623 12:17:20.738534  5064 net.cpp:98] Setting up fc2_fc2_0_split
I0623 12:17:20.738538  5064 net.cpp:105] Top shape: 128 100 1 1 (12800)
I0623 12:17:20.738541  5064 net.cpp:105] Top shape: 128 100 1 1 (12800)
I0623 12:17:20.738544  5064 net.cpp:105] Top shape: 128 100 1 1 (12800)
I0623 12:17:20.738548  5064 layer_factory.hpp:78] Creating layer softmax
I0623 12:17:20.738554  5064 net.cpp:69] Creating Layer softmax
I0623 12:17:20.738557  5064 net.cpp:396] softmax <- fc2_fc2_0_split_0
I0623 12:17:20.738560  5064 net.cpp:396] softmax <- label_data_1_split_0
I0623 12:17:20.738566  5064 net.cpp:358] softmax -> softmax
I0623 12:17:20.738572  5064 net.cpp:98] Setting up softmax
I0623 12:17:20.738579  5064 net.cpp:105] Top shape: 1 1 1 1 (1)
I0623 12:17:20.738581  5064 net.cpp:111]     with loss weight 1
I0623 12:17:20.738586  5064 layer_factory.hpp:78] Creating layer accuracy_top_1
I0623 12:17:20.738591  5064 net.cpp:69] Creating Layer accuracy_top_1
I0623 12:17:20.738595  5064 net.cpp:396] accuracy_top_1 <- fc2_fc2_0_split_1
I0623 12:17:20.738597  5064 net.cpp:396] accuracy_top_1 <- label_data_1_split_1
I0623 12:17:20.738602  5064 net.cpp:358] accuracy_top_1 -> accuracy_top_1
I0623 12:17:20.738606  5064 net.cpp:98] Setting up accuracy_top_1
I0623 12:17:20.738610  5064 net.cpp:105] Top shape: 1 1 1 1 (1)
I0623 12:17:20.738613  5064 layer_factory.hpp:78] Creating layer accuracy_top_5
I0623 12:17:20.738617  5064 net.cpp:69] Creating Layer accuracy_top_5
I0623 12:17:20.738621  5064 net.cpp:396] accuracy_top_5 <- fc2_fc2_0_split_2
I0623 12:17:20.738626  5064 net.cpp:396] accuracy_top_5 <- label_data_1_split_2
I0623 12:17:20.738628  5064 net.cpp:358] accuracy_top_5 -> accuracy_top_5
I0623 12:17:20.738633  5064 net.cpp:98] Setting up accuracy_top_5
I0623 12:17:20.738638  5064 net.cpp:105] Top shape: 1 1 1 1 (1)
I0623 12:17:20.738646  5064 net.cpp:174] accuracy_top_5 does not need backward computation.
I0623 12:17:20.738651  5064 net.cpp:174] accuracy_top_1 does not need backward computation.
I0623 12:17:20.738652  5064 net.cpp:172] softmax needs backward computation.
I0623 12:17:20.738656  5064 net.cpp:172] fc2_fc2_0_split needs backward computation.
I0623 12:17:20.738658  5064 net.cpp:172] fc2 needs backward computation.
I0623 12:17:20.738662  5064 net.cpp:172] fc1_Dropout_ReLU needs backward computation.
I0623 12:17:20.738664  5064 net.cpp:172] fc1 needs backward computation.
I0623 12:17:20.738667  5064 net.cpp:172] middle_conv_ReLU needs backward computation.
I0623 12:17:20.738670  5064 net.cpp:172] middle_conv needs backward computation.
I0623 12:17:20.738673  5064 net.cpp:172] 2_pool needs backward computation.
I0623 12:17:20.738675  5064 net.cpp:172] 2_1_conv_ReLU needs backward computation.
I0623 12:17:20.738678  5064 net.cpp:172] 2_1_conv needs backward computation.
I0623 12:17:20.738682  5064 net.cpp:172] 2_0_conv_ReLU needs backward computation.
I0623 12:17:20.738684  5064 net.cpp:172] 2_0_conv needs backward computation.
I0623 12:17:20.738687  5064 net.cpp:172] 1_pool needs backward computation.
I0623 12:17:20.738690  5064 net.cpp:172] 1_1_conv_ReLU needs backward computation.
I0623 12:17:20.738693  5064 net.cpp:172] 1_1_conv needs backward computation.
I0623 12:17:20.738697  5064 net.cpp:172] 1_0_conv_ReLU needs backward computation.
I0623 12:17:20.738699  5064 net.cpp:172] 1_0_conv needs backward computation.
I0623 12:17:20.738703  5064 net.cpp:172] 0_pool needs backward computation.
I0623 12:17:20.738708  5064 net.cpp:172] 0_1_conv_ReLU needs backward computation.
I0623 12:17:20.738711  5064 net.cpp:172] 0_1_conv needs backward computation.
I0623 12:17:20.738718  5064 net.cpp:172] 0_0_conv_ReLU needs backward computation.
I0623 12:17:20.738723  5064 net.cpp:172] 0_0_conv needs backward computation.
I0623 12:17:20.738725  5064 net.cpp:174] label_data_1_split does not need backward computation.
I0623 12:17:20.738730  5064 net.cpp:174] data does not need backward computation.
I0623 12:17:20.738734  5064 net.cpp:210] This network produces output accuracy_top_1
I0623 12:17:20.738736  5064 net.cpp:210] This network produces output accuracy_top_5
I0623 12:17:20.738739  5064 net.cpp:210] This network produces output softmax
I0623 12:17:20.738756  5064 net.cpp:469] Collecting Learning Rate and Weight Decay.
I0623 12:17:20.738762  5064 net.cpp:221] Network initialization done.
I0623 12:17:20.738765  5064 net.cpp:222] Memory required for data: 95940620
I0623 12:17:20.738814  5064 solver.cpp:42] Solver scaffolding done.
I0623 12:17:20.738837  5064 caffe.cpp:115] Finetuning from snapshots/16-06-21_16h05m31s_0_10_pretrainClassificationFrozen_iter_3000.caffemodel
I0623 12:17:20.739622  5064 solver.cpp:247] Solving 
I0623 12:17:20.739629  5064 solver.cpp:248] Learning Rate Policy: fixed
I0623 12:17:20.739979  5064 solver.cpp:291] Iteration 0, Testing net (#0)
I0623 12:17:20.896805  5064 solver.cpp:342]     Test net output #0: accuracy_top_1 = 0.053125
I0623 12:17:20.896822  5064 solver.cpp:342]     Test net output #1: accuracy_top_5 = 0.20625
I0623 12:17:20.896829  5064 solver.cpp:342]     Test net output #2: softmax = 4.24478 (* 1 = 4.24478 loss)
I0623 12:17:20.972102  5064 solver.cpp:213] Iteration 0, loss = 4.27554
I0623 12:17:20.972120  5064 solver.cpp:228]     Train net output #0: softmax = 4.27554 (* 1 = 4.27554 loss)
I0623 12:17:20.972124  5064 solver.cpp:473] Iteration 0, lr = 0.0001
I0623 12:17:21.991653  5064 solver.cpp:213] Iteration 10, loss = 4.15485
I0623 12:17:21.991677  5064 solver.cpp:228]     Train net output #0: softmax = 4.15485 (* 1 = 4.15485 loss)
I0623 12:17:21.991683  5064 solver.cpp:473] Iteration 10, lr = 0.0001
I0623 12:17:23.011798  5064 solver.cpp:213] Iteration 20, loss = 4.17532
I0623 12:17:23.011816  5064 solver.cpp:228]     Train net output #0: softmax = 4.17532 (* 1 = 4.17532 loss)
I0623 12:17:23.011821  5064 solver.cpp:473] Iteration 20, lr = 0.0001
I0623 12:17:24.031447  5064 solver.cpp:213] Iteration 30, loss = 4.21129
I0623 12:17:24.031471  5064 solver.cpp:228]     Train net output #0: softmax = 4.21129 (* 1 = 4.21129 loss)
I0623 12:17:24.031476  5064 solver.cpp:473] Iteration 30, lr = 0.0001
I0623 12:17:25.051383  5064 solver.cpp:213] Iteration 40, loss = 4.16153
I0623 12:17:25.051416  5064 solver.cpp:228]     Train net output #0: softmax = 4.16153 (* 1 = 4.16153 loss)
I0623 12:17:25.051425  5064 solver.cpp:473] Iteration 40, lr = 0.0001
I0623 12:17:26.071867  5064 solver.cpp:213] Iteration 50, loss = 4.28156
I0623 12:17:26.071892  5064 solver.cpp:228]     Train net output #0: softmax = 4.28156 (* 1 = 4.28156 loss)
I0623 12:17:26.071897  5064 solver.cpp:473] Iteration 50, lr = 0.0001
I0623 12:17:27.091809  5064 solver.cpp:213] Iteration 60, loss = 4.37099
I0623 12:17:27.091828  5064 solver.cpp:228]     Train net output #0: softmax = 4.37099 (* 1 = 4.37099 loss)
I0623 12:17:27.091832  5064 solver.cpp:473] Iteration 60, lr = 0.0001
I0623 12:17:28.110918  5064 solver.cpp:213] Iteration 70, loss = 4.19359
I0623 12:17:28.110939  5064 solver.cpp:228]     Train net output #0: softmax = 4.19359 (* 1 = 4.19359 loss)
I0623 12:17:28.110944  5064 solver.cpp:473] Iteration 70, lr = 0.0001
I0623 12:17:29.130875  5064 solver.cpp:213] Iteration 80, loss = 4.13726
I0623 12:17:29.130894  5064 solver.cpp:228]     Train net output #0: softmax = 4.13726 (* 1 = 4.13726 loss)
I0623 12:17:29.130899  5064 solver.cpp:473] Iteration 80, lr = 0.0001
I0623 12:17:30.149541  5064 solver.cpp:213] Iteration 90, loss = 4.05923
I0623 12:17:30.149575  5064 solver.cpp:228]     Train net output #0: softmax = 4.05923 (* 1 = 4.05923 loss)
I0623 12:17:30.149590  5064 solver.cpp:473] Iteration 90, lr = 0.0001
I0623 12:17:31.169984  5064 solver.cpp:213] Iteration 100, loss = 4.18021
I0623 12:17:31.170024  5064 solver.cpp:228]     Train net output #0: softmax = 4.18021 (* 1 = 4.18021 loss)
I0623 12:17:31.170030  5064 solver.cpp:473] Iteration 100, lr = 0.0001
I0623 12:17:32.188611  5064 solver.cpp:213] Iteration 110, loss = 4.10574
I0623 12:17:32.188630  5064 solver.cpp:228]     Train net output #0: softmax = 4.10574 (* 1 = 4.10574 loss)
I0623 12:17:32.188635  5064 solver.cpp:473] Iteration 110, lr = 0.0001
I0623 12:17:33.208662  5064 solver.cpp:213] Iteration 120, loss = 4.11386
I0623 12:17:33.208680  5064 solver.cpp:228]     Train net output #0: softmax = 4.11386 (* 1 = 4.11386 loss)
I0623 12:17:33.208685  5064 solver.cpp:473] Iteration 120, lr = 0.0001
I0623 12:17:34.227403  5064 solver.cpp:213] Iteration 130, loss = 4.1035
I0623 12:17:34.227421  5064 solver.cpp:228]     Train net output #0: softmax = 4.1035 (* 1 = 4.1035 loss)
I0623 12:17:34.227426  5064 solver.cpp:473] Iteration 130, lr = 0.0001
I0623 12:17:35.247371  5064 solver.cpp:213] Iteration 140, loss = 4.28196
I0623 12:17:35.247400  5064 solver.cpp:228]     Train net output #0: softmax = 4.28196 (* 1 = 4.28196 loss)
I0623 12:17:35.247407  5064 solver.cpp:473] Iteration 140, lr = 0.0001
I0623 12:17:36.266762  5064 solver.cpp:213] Iteration 150, loss = 4.04819
I0623 12:17:36.266782  5064 solver.cpp:228]     Train net output #0: softmax = 4.04819 (* 1 = 4.04819 loss)
I0623 12:17:36.266788  5064 solver.cpp:473] Iteration 150, lr = 0.0001
I0623 12:17:37.286473  5064 solver.cpp:213] Iteration 160, loss = 4.10653
I0623 12:17:37.286489  5064 solver.cpp:228]     Train net output #0: softmax = 4.10653 (* 1 = 4.10653 loss)
I0623 12:17:37.286494  5064 solver.cpp:473] Iteration 160, lr = 0.0001
I0623 12:17:38.304359  5064 solver.cpp:213] Iteration 170, loss = 4.17949
I0623 12:17:38.304376  5064 solver.cpp:228]     Train net output #0: softmax = 4.17949 (* 1 = 4.17949 loss)
I0623 12:17:38.304381  5064 solver.cpp:473] Iteration 170, lr = 0.0001
I0623 12:17:39.324549  5064 solver.cpp:213] Iteration 180, loss = 4.17016
I0623 12:17:39.324566  5064 solver.cpp:228]     Train net output #0: softmax = 4.17016 (* 1 = 4.17016 loss)
I0623 12:17:39.324571  5064 solver.cpp:473] Iteration 180, lr = 0.0001
I0623 12:17:40.344884  5064 solver.cpp:213] Iteration 190, loss = 4.19004
I0623 12:17:40.344903  5064 solver.cpp:228]     Train net output #0: softmax = 4.19004 (* 1 = 4.19004 loss)
I0623 12:17:40.344908  5064 solver.cpp:473] Iteration 190, lr = 0.0001
I0623 12:17:41.364717  5064 solver.cpp:213] Iteration 200, loss = 4.04288
I0623 12:17:41.364737  5064 solver.cpp:228]     Train net output #0: softmax = 4.04288 (* 1 = 4.04288 loss)
I0623 12:17:41.364742  5064 solver.cpp:473] Iteration 200, lr = 0.0001
I0623 12:17:42.384482  5064 solver.cpp:213] Iteration 210, loss = 3.89061
I0623 12:17:42.384500  5064 solver.cpp:228]     Train net output #0: softmax = 3.89061 (* 1 = 3.89061 loss)
I0623 12:17:42.384505  5064 solver.cpp:473] Iteration 210, lr = 0.0001
I0623 12:17:43.404846  5064 solver.cpp:213] Iteration 220, loss = 3.9253
I0623 12:17:43.404862  5064 solver.cpp:228]     Train net output #0: softmax = 3.9253 (* 1 = 3.9253 loss)
I0623 12:17:43.404867  5064 solver.cpp:473] Iteration 220, lr = 0.0001
I0623 12:17:44.425220  5064 solver.cpp:213] Iteration 230, loss = 4.04133
I0623 12:17:44.425236  5064 solver.cpp:228]     Train net output #0: softmax = 4.04133 (* 1 = 4.04133 loss)
I0623 12:17:44.425241  5064 solver.cpp:473] Iteration 230, lr = 0.0001
I0623 12:17:45.444968  5064 solver.cpp:213] Iteration 240, loss = 4.05865
I0623 12:17:45.444988  5064 solver.cpp:228]     Train net output #0: softmax = 4.05865 (* 1 = 4.05865 loss)
I0623 12:17:45.444991  5064 solver.cpp:473] Iteration 240, lr = 0.0001
I0623 12:17:46.463327  5064 solver.cpp:213] Iteration 250, loss = 4.0529
I0623 12:17:46.463346  5064 solver.cpp:228]     Train net output #0: softmax = 4.0529 (* 1 = 4.0529 loss)
I0623 12:17:46.463351  5064 solver.cpp:473] Iteration 250, lr = 0.0001
I0623 12:17:47.483554  5064 solver.cpp:213] Iteration 260, loss = 4.05435
I0623 12:17:47.483587  5064 solver.cpp:228]     Train net output #0: softmax = 4.05435 (* 1 = 4.05435 loss)
I0623 12:17:47.483592  5064 solver.cpp:473] Iteration 260, lr = 0.0001
I0623 12:17:48.502858  5064 solver.cpp:213] Iteration 270, loss = 4.06319
I0623 12:17:48.502877  5064 solver.cpp:228]     Train net output #0: softmax = 4.06319 (* 1 = 4.06319 loss)
I0623 12:17:48.502882  5064 solver.cpp:473] Iteration 270, lr = 0.0001
I0623 12:17:49.523331  5064 solver.cpp:213] Iteration 280, loss = 4.10787
I0623 12:17:49.523347  5064 solver.cpp:228]     Train net output #0: softmax = 4.10787 (* 1 = 4.10787 loss)
I0623 12:17:49.523353  5064 solver.cpp:473] Iteration 280, lr = 0.0001
I0623 12:17:50.541774  5064 solver.cpp:213] Iteration 290, loss = 4.08684
I0623 12:17:50.541801  5064 solver.cpp:228]     Train net output #0: softmax = 4.08684 (* 1 = 4.08684 loss)
I0623 12:17:50.541815  5064 solver.cpp:473] Iteration 290, lr = 0.0001
I0623 12:17:51.562271  5064 solver.cpp:213] Iteration 300, loss = 4.10316
I0623 12:17:51.562330  5064 solver.cpp:228]     Train net output #0: softmax = 4.10316 (* 1 = 4.10316 loss)
I0623 12:17:51.562335  5064 solver.cpp:473] Iteration 300, lr = 0.0001
I0623 12:17:52.581156  5064 solver.cpp:213] Iteration 310, loss = 4.04305
I0623 12:17:52.581172  5064 solver.cpp:228]     Train net output #0: softmax = 4.04305 (* 1 = 4.04305 loss)
I0623 12:17:52.581177  5064 solver.cpp:473] Iteration 310, lr = 0.0001
I0623 12:17:53.600944  5064 solver.cpp:213] Iteration 320, loss = 3.93858
I0623 12:17:53.600966  5064 solver.cpp:228]     Train net output #0: softmax = 3.93858 (* 1 = 3.93858 loss)
I0623 12:17:53.600972  5064 solver.cpp:473] Iteration 320, lr = 0.0001
I0623 12:17:54.619099  5064 solver.cpp:213] Iteration 330, loss = 4.01214
I0623 12:17:54.619117  5064 solver.cpp:228]     Train net output #0: softmax = 4.01214 (* 1 = 4.01214 loss)
I0623 12:17:54.619122  5064 solver.cpp:473] Iteration 330, lr = 0.0001
I0623 12:17:55.639747  5064 solver.cpp:213] Iteration 340, loss = 4.27504
I0623 12:17:55.639766  5064 solver.cpp:228]     Train net output #0: softmax = 4.27504 (* 1 = 4.27504 loss)
I0623 12:17:55.639775  5064 solver.cpp:473] Iteration 340, lr = 0.0001
I0623 12:17:56.658984  5064 solver.cpp:213] Iteration 350, loss = 3.88351
I0623 12:17:56.659004  5064 solver.cpp:228]     Train net output #0: softmax = 3.88351 (* 1 = 3.88351 loss)
I0623 12:17:56.659009  5064 solver.cpp:473] Iteration 350, lr = 0.0001
I0623 12:17:57.678330  5064 solver.cpp:213] Iteration 360, loss = 4.03917
I0623 12:17:57.678347  5064 solver.cpp:228]     Train net output #0: softmax = 4.03917 (* 1 = 4.03917 loss)
I0623 12:17:57.678352  5064 solver.cpp:473] Iteration 360, lr = 0.0001
I0623 12:17:58.697659  5064 solver.cpp:213] Iteration 370, loss = 4.06837
I0623 12:17:58.697675  5064 solver.cpp:228]     Train net output #0: softmax = 4.06837 (* 1 = 4.06837 loss)
I0623 12:17:58.697680  5064 solver.cpp:473] Iteration 370, lr = 0.0001
I0623 12:17:59.718019  5064 solver.cpp:213] Iteration 380, loss = 4.07835
I0623 12:17:59.718036  5064 solver.cpp:228]     Train net output #0: softmax = 4.07835 (* 1 = 4.07835 loss)
I0623 12:17:59.718041  5064 solver.cpp:473] Iteration 380, lr = 0.0001
I0623 12:18:00.737619  5064 solver.cpp:213] Iteration 390, loss = 4.01342
I0623 12:18:00.737648  5064 solver.cpp:228]     Train net output #0: softmax = 4.01342 (* 1 = 4.01342 loss)
I0623 12:18:00.737656  5064 solver.cpp:473] Iteration 390, lr = 0.0001
I0623 12:18:01.758183  5064 solver.cpp:213] Iteration 400, loss = 4.03031
I0623 12:18:01.758201  5064 solver.cpp:228]     Train net output #0: softmax = 4.03031 (* 1 = 4.03031 loss)
I0623 12:18:01.758206  5064 solver.cpp:473] Iteration 400, lr = 0.0001
I0623 12:18:02.777426  5064 solver.cpp:213] Iteration 410, loss = 4.00542
I0623 12:18:02.777442  5064 solver.cpp:228]     Train net output #0: softmax = 4.00542 (* 1 = 4.00542 loss)
I0623 12:18:02.777447  5064 solver.cpp:473] Iteration 410, lr = 0.0001
I0623 12:18:03.798442  5064 solver.cpp:213] Iteration 420, loss = 3.94259
I0623 12:18:03.798461  5064 solver.cpp:228]     Train net output #0: softmax = 3.94259 (* 1 = 3.94259 loss)
I0623 12:18:03.798467  5064 solver.cpp:473] Iteration 420, lr = 0.0001
I0623 12:18:04.818002  5064 solver.cpp:213] Iteration 430, loss = 4.09904
I0623 12:18:04.818018  5064 solver.cpp:228]     Train net output #0: softmax = 4.09904 (* 1 = 4.09904 loss)
I0623 12:18:04.818023  5064 solver.cpp:473] Iteration 430, lr = 0.0001
I0623 12:18:05.836815  5064 solver.cpp:213] Iteration 440, loss = 4.25289
I0623 12:18:05.836836  5064 solver.cpp:228]     Train net output #0: softmax = 4.25289 (* 1 = 4.25289 loss)
I0623 12:18:05.836843  5064 solver.cpp:473] Iteration 440, lr = 0.0001
I0623 12:18:06.857301  5064 solver.cpp:213] Iteration 450, loss = 4.14086
I0623 12:18:06.857319  5064 solver.cpp:228]     Train net output #0: softmax = 4.14086 (* 1 = 4.14086 loss)
I0623 12:18:06.857323  5064 solver.cpp:473] Iteration 450, lr = 0.0001
I0623 12:18:07.876494  5064 solver.cpp:213] Iteration 460, loss = 4.03782
I0623 12:18:07.876510  5064 solver.cpp:228]     Train net output #0: softmax = 4.03782 (* 1 = 4.03782 loss)
I0623 12:18:07.876531  5064 solver.cpp:473] Iteration 460, lr = 0.0001
I0623 12:18:08.895946  5064 solver.cpp:213] Iteration 470, loss = 3.98933
I0623 12:18:08.895962  5064 solver.cpp:228]     Train net output #0: softmax = 3.98933 (* 1 = 3.98933 loss)
I0623 12:18:08.895967  5064 solver.cpp:473] Iteration 470, lr = 0.0001
I0623 12:18:09.915598  5064 solver.cpp:213] Iteration 480, loss = 3.8969
I0623 12:18:09.915614  5064 solver.cpp:228]     Train net output #0: softmax = 3.8969 (* 1 = 3.8969 loss)
I0623 12:18:09.915619  5064 solver.cpp:473] Iteration 480, lr = 0.0001
I0623 12:18:10.935500  5064 solver.cpp:213] Iteration 490, loss = 3.82025
I0623 12:18:10.935528  5064 solver.cpp:228]     Train net output #0: softmax = 3.82025 (* 1 = 3.82025 loss)
I0623 12:18:10.935535  5064 solver.cpp:473] Iteration 490, lr = 0.0001
I0623 12:18:11.955204  5064 solver.cpp:213] Iteration 500, loss = 3.95839
I0623 12:18:11.955220  5064 solver.cpp:228]     Train net output #0: softmax = 3.95839 (* 1 = 3.95839 loss)
I0623 12:18:11.955225  5064 solver.cpp:473] Iteration 500, lr = 0.0001
I0623 12:18:12.974119  5064 solver.cpp:213] Iteration 510, loss = 4.08695
I0623 12:18:12.974135  5064 solver.cpp:228]     Train net output #0: softmax = 4.08695 (* 1 = 4.08695 loss)
I0623 12:18:12.974140  5064 solver.cpp:473] Iteration 510, lr = 0.0001
I0623 12:18:13.993273  5064 solver.cpp:213] Iteration 520, loss = 4.12338
I0623 12:18:13.993289  5064 solver.cpp:228]     Train net output #0: softmax = 4.12338 (* 1 = 4.12338 loss)
I0623 12:18:13.993294  5064 solver.cpp:473] Iteration 520, lr = 0.0001
I0623 12:18:15.013422  5064 solver.cpp:213] Iteration 530, loss = 4.05581
I0623 12:18:15.013438  5064 solver.cpp:228]     Train net output #0: softmax = 4.05581 (* 1 = 4.05581 loss)
I0623 12:18:15.013442  5064 solver.cpp:473] Iteration 530, lr = 0.0001
I0623 12:18:16.032908  5064 solver.cpp:213] Iteration 540, loss = 3.89898
I0623 12:18:16.032935  5064 solver.cpp:228]     Train net output #0: softmax = 3.89898 (* 1 = 3.89898 loss)
I0623 12:18:16.032943  5064 solver.cpp:473] Iteration 540, lr = 0.0001
I0623 12:18:17.052830  5064 solver.cpp:213] Iteration 550, loss = 4.06198
I0623 12:18:17.052850  5064 solver.cpp:228]     Train net output #0: softmax = 4.06198 (* 1 = 4.06198 loss)
I0623 12:18:17.052855  5064 solver.cpp:473] Iteration 550, lr = 0.0001
I0623 12:18:18.071106  5064 solver.cpp:213] Iteration 560, loss = 4.02441
I0623 12:18:18.071125  5064 solver.cpp:228]     Train net output #0: softmax = 4.02441 (* 1 = 4.02441 loss)
I0623 12:18:18.071130  5064 solver.cpp:473] Iteration 560, lr = 0.0001
I0623 12:18:19.090998  5064 solver.cpp:213] Iteration 570, loss = 3.96682
I0623 12:18:19.091015  5064 solver.cpp:228]     Train net output #0: softmax = 3.96682 (* 1 = 3.96682 loss)
I0623 12:18:19.091019  5064 solver.cpp:473] Iteration 570, lr = 0.0001
I0623 12:18:20.109793  5064 solver.cpp:213] Iteration 580, loss = 4.14544
I0623 12:18:20.109815  5064 solver.cpp:228]     Train net output #0: softmax = 4.14544 (* 1 = 4.14544 loss)
I0623 12:18:20.109822  5064 solver.cpp:473] Iteration 580, lr = 0.0001
I0623 12:18:21.130534  5064 solver.cpp:213] Iteration 590, loss = 3.93777
I0623 12:18:21.130563  5064 solver.cpp:228]     Train net output #0: softmax = 3.93777 (* 1 = 3.93777 loss)
I0623 12:18:21.130571  5064 solver.cpp:473] Iteration 590, lr = 0.0001
I0623 12:18:22.150187  5064 solver.cpp:213] Iteration 600, loss = 3.87493
I0623 12:18:22.150230  5064 solver.cpp:228]     Train net output #0: softmax = 3.87493 (* 1 = 3.87493 loss)
I0623 12:18:22.150236  5064 solver.cpp:473] Iteration 600, lr = 0.0001
I0623 12:18:23.170554  5064 solver.cpp:213] Iteration 610, loss = 3.89965
I0623 12:18:23.170574  5064 solver.cpp:228]     Train net output #0: softmax = 3.89965 (* 1 = 3.89965 loss)
I0623 12:18:23.170579  5064 solver.cpp:473] Iteration 610, lr = 0.0001
I0623 12:18:24.190579  5064 solver.cpp:213] Iteration 620, loss = 4.05714
I0623 12:18:24.190596  5064 solver.cpp:228]     Train net output #0: softmax = 4.05714 (* 1 = 4.05714 loss)
I0623 12:18:24.190601  5064 solver.cpp:473] Iteration 620, lr = 0.0001
I0623 12:18:25.209604  5064 solver.cpp:213] Iteration 630, loss = 4.0491
I0623 12:18:25.209620  5064 solver.cpp:228]     Train net output #0: softmax = 4.0491 (* 1 = 4.0491 loss)
I0623 12:18:25.209625  5064 solver.cpp:473] Iteration 630, lr = 0.0001
I0623 12:18:26.228581  5064 solver.cpp:213] Iteration 640, loss = 4.01119
I0623 12:18:26.228598  5064 solver.cpp:228]     Train net output #0: softmax = 4.01119 (* 1 = 4.01119 loss)
I0623 12:18:26.228603  5064 solver.cpp:473] Iteration 640, lr = 0.0001
I0623 12:18:27.248427  5064 solver.cpp:213] Iteration 650, loss = 3.85693
I0623 12:18:27.248446  5064 solver.cpp:228]     Train net output #0: softmax = 3.85693 (* 1 = 3.85693 loss)
I0623 12:18:27.248451  5064 solver.cpp:473] Iteration 650, lr = 0.0001
I0623 12:18:28.268689  5064 solver.cpp:213] Iteration 660, loss = 3.82348
I0623 12:18:28.268707  5064 solver.cpp:228]     Train net output #0: softmax = 3.82348 (* 1 = 3.82348 loss)
I0623 12:18:28.268712  5064 solver.cpp:473] Iteration 660, lr = 0.0001
I0623 12:18:29.288308  5064 solver.cpp:213] Iteration 670, loss = 3.96959
I0623 12:18:29.288324  5064 solver.cpp:228]     Train net output #0: softmax = 3.96959 (* 1 = 3.96959 loss)
I0623 12:18:29.288329  5064 solver.cpp:473] Iteration 670, lr = 0.0001
I0623 12:18:30.306743  5064 solver.cpp:213] Iteration 680, loss = 3.96936
I0623 12:18:30.306759  5064 solver.cpp:228]     Train net output #0: softmax = 3.96936 (* 1 = 3.96936 loss)
I0623 12:18:30.306766  5064 solver.cpp:473] Iteration 680, lr = 0.0001
I0623 12:18:31.326820  5064 solver.cpp:213] Iteration 690, loss = 4.10736
I0623 12:18:31.326836  5064 solver.cpp:228]     Train net output #0: softmax = 4.10736 (* 1 = 4.10736 loss)
I0623 12:18:31.326841  5064 solver.cpp:473] Iteration 690, lr = 0.0001
I0623 12:18:32.346570  5064 solver.cpp:213] Iteration 700, loss = 4.08479
I0623 12:18:32.346598  5064 solver.cpp:228]     Train net output #0: softmax = 4.08479 (* 1 = 4.08479 loss)
I0623 12:18:32.346606  5064 solver.cpp:473] Iteration 700, lr = 0.0001
I0623 12:18:33.365988  5064 solver.cpp:213] Iteration 710, loss = 4.00261
I0623 12:18:33.366004  5064 solver.cpp:228]     Train net output #0: softmax = 4.00261 (* 1 = 4.00261 loss)
I0623 12:18:33.366009  5064 solver.cpp:473] Iteration 710, lr = 0.0001
I0623 12:18:34.384414  5064 solver.cpp:213] Iteration 720, loss = 3.79014
I0623 12:18:34.384430  5064 solver.cpp:228]     Train net output #0: softmax = 3.79014 (* 1 = 3.79014 loss)
I0623 12:18:34.384435  5064 solver.cpp:473] Iteration 720, lr = 0.0001
I0623 12:18:35.404603  5064 solver.cpp:213] Iteration 730, loss = 4.06589
I0623 12:18:35.404619  5064 solver.cpp:228]     Train net output #0: softmax = 4.06589 (* 1 = 4.06589 loss)
I0623 12:18:35.404624  5064 solver.cpp:473] Iteration 730, lr = 0.0001
I0623 12:18:36.424201  5064 solver.cpp:213] Iteration 740, loss = 3.81603
I0623 12:18:36.424218  5064 solver.cpp:228]     Train net output #0: softmax = 3.81603 (* 1 = 3.81603 loss)
I0623 12:18:36.424231  5064 solver.cpp:473] Iteration 740, lr = 0.0001
I0623 12:18:37.444324  5064 solver.cpp:213] Iteration 750, loss = 4.02693
I0623 12:18:37.444340  5064 solver.cpp:228]     Train net output #0: softmax = 4.02693 (* 1 = 4.02693 loss)
I0623 12:18:37.444345  5064 solver.cpp:473] Iteration 750, lr = 0.0001
I0623 12:18:38.463217  5064 solver.cpp:213] Iteration 760, loss = 4.00456
I0623 12:18:38.463232  5064 solver.cpp:228]     Train net output #0: softmax = 4.00456 (* 1 = 4.00456 loss)
I0623 12:18:38.463250  5064 solver.cpp:473] Iteration 760, lr = 0.0001
I0623 12:18:39.483834  5064 solver.cpp:213] Iteration 770, loss = 3.95795
I0623 12:18:39.483850  5064 solver.cpp:228]     Train net output #0: softmax = 3.95795 (* 1 = 3.95795 loss)
I0623 12:18:39.483855  5064 solver.cpp:473] Iteration 770, lr = 0.0001
I0623 12:18:40.504305  5064 solver.cpp:213] Iteration 780, loss = 3.9754
I0623 12:18:40.504322  5064 solver.cpp:228]     Train net output #0: softmax = 3.9754 (* 1 = 3.9754 loss)
I0623 12:18:40.504326  5064 solver.cpp:473] Iteration 780, lr = 0.0001
I0623 12:18:41.524495  5064 solver.cpp:213] Iteration 790, loss = 3.79381
I0623 12:18:41.524513  5064 solver.cpp:228]     Train net output #0: softmax = 3.79381 (* 1 = 3.79381 loss)
I0623 12:18:41.524518  5064 solver.cpp:473] Iteration 790, lr = 0.0001
I0623 12:18:42.544538  5064 solver.cpp:213] Iteration 800, loss = 3.9458
I0623 12:18:42.544556  5064 solver.cpp:228]     Train net output #0: softmax = 3.9458 (* 1 = 3.9458 loss)
I0623 12:18:42.544718  5064 solver.cpp:473] Iteration 800, lr = 0.0001
I0623 12:18:43.565171  5064 solver.cpp:213] Iteration 810, loss = 3.90937
I0623 12:18:43.565191  5064 solver.cpp:228]     Train net output #0: softmax = 3.90937 (* 1 = 3.90937 loss)
I0623 12:18:43.565196  5064 solver.cpp:473] Iteration 810, lr = 0.0001
I0623 12:18:44.585335  5064 solver.cpp:213] Iteration 820, loss = 3.82774
I0623 12:18:44.585355  5064 solver.cpp:228]     Train net output #0: softmax = 3.82774 (* 1 = 3.82774 loss)
I0623 12:18:44.585360  5064 solver.cpp:473] Iteration 820, lr = 0.0001
I0623 12:18:45.604838  5064 solver.cpp:213] Iteration 830, loss = 4.0152
I0623 12:18:45.604856  5064 solver.cpp:228]     Train net output #0: softmax = 4.0152 (* 1 = 4.0152 loss)
I0623 12:18:45.604861  5064 solver.cpp:473] Iteration 830, lr = 0.0001
I0623 12:18:46.625430  5064 solver.cpp:213] Iteration 840, loss = 3.90719
I0623 12:18:46.625447  5064 solver.cpp:228]     Train net output #0: softmax = 3.90719 (* 1 = 3.90719 loss)
I0623 12:18:46.625452  5064 solver.cpp:473] Iteration 840, lr = 0.0001
I0623 12:18:47.645359  5064 solver.cpp:213] Iteration 850, loss = 4.07147
I0623 12:18:47.645380  5064 solver.cpp:228]     Train net output #0: softmax = 4.07147 (* 1 = 4.07147 loss)
I0623 12:18:47.645508  5064 solver.cpp:473] Iteration 850, lr = 0.0001
I0623 12:18:48.665102  5064 solver.cpp:213] Iteration 860, loss = 3.86497
I0623 12:18:48.665118  5064 solver.cpp:228]     Train net output #0: softmax = 3.86497 (* 1 = 3.86497 loss)
I0623 12:18:48.665123  5064 solver.cpp:473] Iteration 860, lr = 0.0001
I0623 12:18:49.685519  5064 solver.cpp:213] Iteration 870, loss = 3.88143
I0623 12:18:49.685535  5064 solver.cpp:228]     Train net output #0: softmax = 3.88143 (* 1 = 3.88143 loss)
I0623 12:18:49.685540  5064 solver.cpp:473] Iteration 870, lr = 0.0001
I0623 12:18:50.705904  5064 solver.cpp:213] Iteration 880, loss = 3.87081
I0623 12:18:50.705920  5064 solver.cpp:228]     Train net output #0: softmax = 3.87081 (* 1 = 3.87081 loss)
I0623 12:18:50.705925  5064 solver.cpp:473] Iteration 880, lr = 0.0001
I0623 12:18:51.726205  5064 solver.cpp:213] Iteration 890, loss = 3.93642
I0623 12:18:51.726222  5064 solver.cpp:228]     Train net output #0: softmax = 3.93642 (* 1 = 3.93642 loss)
I0623 12:18:51.726227  5064 solver.cpp:473] Iteration 890, lr = 0.0001
I0623 12:18:52.746673  5064 solver.cpp:213] Iteration 900, loss = 4.05984
I0623 12:18:52.746840  5064 solver.cpp:228]     Train net output #0: softmax = 4.05984 (* 1 = 4.05984 loss)
I0623 12:18:52.746850  5064 solver.cpp:473] Iteration 900, lr = 0.0001
I0623 12:18:53.766885  5064 solver.cpp:213] Iteration 910, loss = 3.99886
I0623 12:18:53.766899  5064 solver.cpp:228]     Train net output #0: softmax = 3.99886 (* 1 = 3.99886 loss)
I0623 12:18:53.766904  5064 solver.cpp:473] Iteration 910, lr = 0.0001
I0623 12:18:54.786669  5064 solver.cpp:213] Iteration 920, loss = 3.8092
I0623 12:18:54.786684  5064 solver.cpp:228]     Train net output #0: softmax = 3.8092 (* 1 = 3.8092 loss)
I0623 12:18:54.786689  5064 solver.cpp:473] Iteration 920, lr = 0.0001
I0623 12:18:55.806954  5064 solver.cpp:213] Iteration 930, loss = 3.96779
I0623 12:18:55.806970  5064 solver.cpp:228]     Train net output #0: softmax = 3.96779 (* 1 = 3.96779 loss)
I0623 12:18:55.806975  5064 solver.cpp:473] Iteration 930, lr = 0.0001
I0623 12:18:56.827266  5064 solver.cpp:213] Iteration 940, loss = 3.99309
I0623 12:18:56.827281  5064 solver.cpp:228]     Train net output #0: softmax = 3.99309 (* 1 = 3.99309 loss)
I0623 12:18:56.827286  5064 solver.cpp:473] Iteration 940, lr = 0.0001
I0623 12:18:57.847702  5064 solver.cpp:213] Iteration 950, loss = 3.8583
I0623 12:18:57.847721  5064 solver.cpp:228]     Train net output #0: softmax = 3.8583 (* 1 = 3.8583 loss)
I0623 12:18:57.847849  5064 solver.cpp:473] Iteration 950, lr = 0.0001
I0623 12:18:58.868139  5064 solver.cpp:213] Iteration 960, loss = 3.80534
I0623 12:18:58.868165  5064 solver.cpp:228]     Train net output #0: softmax = 3.80534 (* 1 = 3.80534 loss)
I0623 12:18:58.868180  5064 solver.cpp:473] Iteration 960, lr = 0.0001
I0623 12:18:59.888185  5064 solver.cpp:213] Iteration 970, loss = 4.01558
I0623 12:18:59.888200  5064 solver.cpp:228]     Train net output #0: softmax = 4.01558 (* 1 = 4.01558 loss)
I0623 12:18:59.888206  5064 solver.cpp:473] Iteration 970, lr = 0.0001
I0623 12:19:00.907675  5064 solver.cpp:213] Iteration 980, loss = 3.85594
I0623 12:19:00.907691  5064 solver.cpp:228]     Train net output #0: softmax = 3.85594 (* 1 = 3.85594 loss)
I0623 12:19:00.907696  5064 solver.cpp:473] Iteration 980, lr = 0.0001
I0623 12:19:01.928123  5064 solver.cpp:213] Iteration 990, loss = 4.06669
I0623 12:19:01.928148  5064 solver.cpp:228]     Train net output #0: softmax = 4.06669 (* 1 = 4.06669 loss)
I0623 12:19:01.928153  5064 solver.cpp:473] Iteration 990, lr = 0.0001
I0623 12:19:02.878353  5064 solver.cpp:362] Snapshotting to snapshots/16-06-21_16h05m31s_0_11_pretrainClassification_iter_1000.caffemodel
I0623 12:19:02.879631  5064 solver.cpp:370] Snapshotting solver state to snapshots/16-06-21_16h05m31s_0_11_pretrainClassification_iter_1000.solverstate
I0623 12:19:02.880241  5064 solver.cpp:291] Iteration 1000, Testing net (#0)
I0623 12:19:03.041244  5064 solver.cpp:342]     Test net output #0: accuracy_top_1 = 0.104687
I0623 12:19:03.041260  5064 solver.cpp:342]     Test net output #1: accuracy_top_5 = 0.321875
I0623 12:19:03.041267  5064 solver.cpp:342]     Test net output #2: softmax = 3.8797 (* 1 = 3.8797 loss)
I0623 12:19:03.111834  5064 solver.cpp:213] Iteration 1000, loss = 3.93251
I0623 12:19:03.111848  5064 solver.cpp:228]     Train net output #0: softmax = 3.93251 (* 1 = 3.93251 loss)
I0623 12:19:03.111855  5064 solver.cpp:473] Iteration 1000, lr = 0.0001
I0623 12:19:04.132015  5064 solver.cpp:213] Iteration 1010, loss = 3.92907
I0623 12:19:04.132033  5064 solver.cpp:228]     Train net output #0: softmax = 3.92907 (* 1 = 3.92907 loss)
I0623 12:19:04.132038  5064 solver.cpp:473] Iteration 1010, lr = 0.0001
I0623 12:19:05.152200  5064 solver.cpp:213] Iteration 1020, loss = 3.99128
I0623 12:19:05.152217  5064 solver.cpp:228]     Train net output #0: softmax = 3.99128 (* 1 = 3.99128 loss)
I0623 12:19:05.152222  5064 solver.cpp:473] Iteration 1020, lr = 0.0001
I0623 12:19:06.172397  5064 solver.cpp:213] Iteration 1030, loss = 3.74969
I0623 12:19:06.172415  5064 solver.cpp:228]     Train net output #0: softmax = 3.74969 (* 1 = 3.74969 loss)
I0623 12:19:06.172420  5064 solver.cpp:473] Iteration 1030, lr = 0.0001
I0623 12:19:07.192982  5064 solver.cpp:213] Iteration 1040, loss = 3.73002
I0623 12:19:07.193004  5064 solver.cpp:228]     Train net output #0: softmax = 3.73002 (* 1 = 3.73002 loss)
I0623 12:19:07.193011  5064 solver.cpp:473] Iteration 1040, lr = 0.0001
I0623 12:19:08.212990  5064 solver.cpp:213] Iteration 1050, loss = 3.73416
I0623 12:19:08.213012  5064 solver.cpp:228]     Train net output #0: softmax = 3.73416 (* 1 = 3.73416 loss)
I0623 12:19:08.213214  5064 solver.cpp:473] Iteration 1050, lr = 0.0001
I0623 12:19:09.232967  5064 solver.cpp:213] Iteration 1060, loss = 3.81505
I0623 12:19:09.232985  5064 solver.cpp:228]     Train net output #0: softmax = 3.81505 (* 1 = 3.81505 loss)
I0623 12:19:09.232990  5064 solver.cpp:473] Iteration 1060, lr = 0.0001
I0623 12:19:10.252986  5064 solver.cpp:213] Iteration 1070, loss = 3.84584
I0623 12:19:10.253003  5064 solver.cpp:228]     Train net output #0: softmax = 3.84584 (* 1 = 3.84584 loss)
I0623 12:19:10.253007  5064 solver.cpp:473] Iteration 1070, lr = 0.0001
I0623 12:19:11.273344  5064 solver.cpp:213] Iteration 1080, loss = 3.91621
I0623 12:19:11.273360  5064 solver.cpp:228]     Train net output #0: softmax = 3.91621 (* 1 = 3.91621 loss)
I0623 12:19:11.273365  5064 solver.cpp:473] Iteration 1080, lr = 0.0001
I0623 12:19:12.293493  5064 solver.cpp:213] Iteration 1090, loss = 4.00846
I0623 12:19:12.293509  5064 solver.cpp:228]     Train net output #0: softmax = 4.00846 (* 1 = 4.00846 loss)
I0623 12:19:12.293514  5064 solver.cpp:473] Iteration 1090, lr = 0.0001
I0623 12:19:13.313954  5064 solver.cpp:213] Iteration 1100, loss = 3.87373
I0623 12:19:13.313974  5064 solver.cpp:228]     Train net output #0: softmax = 3.87373 (* 1 = 3.87373 loss)
I0623 12:19:13.314116  5064 solver.cpp:473] Iteration 1100, lr = 0.0001
I0623 12:19:14.334919  5064 solver.cpp:213] Iteration 1110, loss = 3.62645
I0623 12:19:14.334936  5064 solver.cpp:228]     Train net output #0: softmax = 3.62645 (* 1 = 3.62645 loss)
I0623 12:19:14.334941  5064 solver.cpp:473] Iteration 1110, lr = 0.0001
I0623 12:19:15.354869  5064 solver.cpp:213] Iteration 1120, loss = 4.06206
I0623 12:19:15.354887  5064 solver.cpp:228]     Train net output #0: softmax = 4.06206 (* 1 = 4.06206 loss)
I0623 12:19:15.354892  5064 solver.cpp:473] Iteration 1120, lr = 0.0001
I0623 12:19:16.375066  5064 solver.cpp:213] Iteration 1130, loss = 3.96461
I0623 12:19:16.375082  5064 solver.cpp:228]     Train net output #0: softmax = 3.96461 (* 1 = 3.96461 loss)
I0623 12:19:16.375087  5064 solver.cpp:473] Iteration 1130, lr = 0.0001
I0623 12:19:17.395853  5064 solver.cpp:213] Iteration 1140, loss = 3.85965
I0623 12:19:17.395894  5064 solver.cpp:228]     Train net output #0: softmax = 3.85965 (* 1 = 3.85965 loss)
I0623 12:19:17.395900  5064 solver.cpp:473] Iteration 1140, lr = 0.0001
I0623 12:19:18.417054  5064 solver.cpp:213] Iteration 1150, loss = 3.96204
I0623 12:19:18.417075  5064 solver.cpp:228]     Train net output #0: softmax = 3.96204 (* 1 = 3.96204 loss)
I0623 12:19:18.417081  5064 solver.cpp:473] Iteration 1150, lr = 0.0001
I0623 12:19:19.436897  5064 solver.cpp:213] Iteration 1160, loss = 3.87931
I0623 12:19:19.436916  5064 solver.cpp:228]     Train net output #0: softmax = 3.87931 (* 1 = 3.87931 loss)
I0623 12:19:19.436921  5064 solver.cpp:473] Iteration 1160, lr = 0.0001
I0623 12:19:20.457883  5064 solver.cpp:213] Iteration 1170, loss = 4.08099
I0623 12:19:20.457901  5064 solver.cpp:228]     Train net output #0: softmax = 4.08099 (* 1 = 4.08099 loss)
I0623 12:19:20.457906  5064 solver.cpp:473] Iteration 1170, lr = 0.0001
I0623 12:19:21.478960  5064 solver.cpp:213] Iteration 1180, loss = 3.58388
I0623 12:19:21.478976  5064 solver.cpp:228]     Train net output #0: softmax = 3.58388 (* 1 = 3.58388 loss)
I0623 12:19:21.478981  5064 solver.cpp:473] Iteration 1180, lr = 0.0001
I0623 12:19:22.499886  5064 solver.cpp:213] Iteration 1190, loss = 3.9451
I0623 12:19:22.499902  5064 solver.cpp:228]     Train net output #0: softmax = 3.9451 (* 1 = 3.9451 loss)
I0623 12:19:22.499908  5064 solver.cpp:473] Iteration 1190, lr = 0.0001
I0623 12:19:23.520803  5064 solver.cpp:213] Iteration 1200, loss = 3.93668
I0623 12:19:23.520967  5064 solver.cpp:228]     Train net output #0: softmax = 3.93668 (* 1 = 3.93668 loss)
I0623 12:19:23.520974  5064 solver.cpp:473] Iteration 1200, lr = 0.0001
I0623 12:19:24.541553  5064 solver.cpp:213] Iteration 1210, loss = 3.8575
I0623 12:19:24.541570  5064 solver.cpp:228]     Train net output #0: softmax = 3.8575 (* 1 = 3.8575 loss)
I0623 12:19:24.541575  5064 solver.cpp:473] Iteration 1210, lr = 0.0001
I0623 12:19:25.562263  5064 solver.cpp:213] Iteration 1220, loss = 3.87107
I0623 12:19:25.562279  5064 solver.cpp:228]     Train net output #0: softmax = 3.87107 (* 1 = 3.87107 loss)
I0623 12:19:25.562284  5064 solver.cpp:473] Iteration 1220, lr = 0.0001
I0623 12:19:26.583001  5064 solver.cpp:213] Iteration 1230, loss = 3.90529
I0623 12:19:26.583017  5064 solver.cpp:228]     Train net output #0: softmax = 3.90529 (* 1 = 3.90529 loss)
I0623 12:19:26.583022  5064 solver.cpp:473] Iteration 1230, lr = 0.0001
I0623 12:19:27.603443  5064 solver.cpp:213] Iteration 1240, loss = 3.88504
I0623 12:19:27.603459  5064 solver.cpp:228]     Train net output #0: softmax = 3.88504 (* 1 = 3.88504 loss)
I0623 12:19:27.603464  5064 solver.cpp:473] Iteration 1240, lr = 0.0001
I0623 12:19:28.623762  5064 solver.cpp:213] Iteration 1250, loss = 3.88265
I0623 12:19:28.623785  5064 solver.cpp:228]     Train net output #0: softmax = 3.88265 (* 1 = 3.88265 loss)
I0623 12:19:28.623904  5064 solver.cpp:473] Iteration 1250, lr = 0.0001
I0623 12:19:29.643762  5064 solver.cpp:213] Iteration 1260, loss = 3.75903
I0623 12:19:29.643779  5064 solver.cpp:228]     Train net output #0: softmax = 3.75903 (* 1 = 3.75903 loss)
I0623 12:19:29.643784  5064 solver.cpp:473] Iteration 1260, lr = 0.0001
I0623 12:19:30.663933  5064 solver.cpp:213] Iteration 1270, loss = 3.80235
I0623 12:19:30.663949  5064 solver.cpp:228]     Train net output #0: softmax = 3.80235 (* 1 = 3.80235 loss)
I0623 12:19:30.663954  5064 solver.cpp:473] Iteration 1270, lr = 0.0001
I0623 12:19:31.684010  5064 solver.cpp:213] Iteration 1280, loss = 3.78755
I0623 12:19:31.684026  5064 solver.cpp:228]     Train net output #0: softmax = 3.78755 (* 1 = 3.78755 loss)
I0623 12:19:31.684031  5064 solver.cpp:473] Iteration 1280, lr = 0.0001
I0623 12:19:32.705268  5064 solver.cpp:213] Iteration 1290, loss = 3.89315
I0623 12:19:32.705286  5064 solver.cpp:228]     Train net output #0: softmax = 3.89315 (* 1 = 3.89315 loss)
I0623 12:19:32.705289  5064 solver.cpp:473] Iteration 1290, lr = 0.0001
I0623 12:19:33.725555  5064 solver.cpp:213] Iteration 1300, loss = 3.98944
I0623 12:19:33.725579  5064 solver.cpp:228]     Train net output #0: softmax = 3.98944 (* 1 = 3.98944 loss)
I0623 12:19:33.725742  5064 solver.cpp:473] Iteration 1300, lr = 0.0001
I0623 12:19:34.746451  5064 solver.cpp:213] Iteration 1310, loss = 3.75187
I0623 12:19:34.746469  5064 solver.cpp:228]     Train net output #0: softmax = 3.75187 (* 1 = 3.75187 loss)
I0623 12:19:34.746474  5064 solver.cpp:473] Iteration 1310, lr = 0.0001
I0623 12:19:35.766571  5064 solver.cpp:213] Iteration 1320, loss = 3.88686
I0623 12:19:35.766587  5064 solver.cpp:228]     Train net output #0: softmax = 3.88686 (* 1 = 3.88686 loss)
I0623 12:19:35.766592  5064 solver.cpp:473] Iteration 1320, lr = 0.0001
I0623 12:19:36.787060  5064 solver.cpp:213] Iteration 1330, loss = 3.89919
I0623 12:19:36.787077  5064 solver.cpp:228]     Train net output #0: softmax = 3.89919 (* 1 = 3.89919 loss)
I0623 12:19:36.787082  5064 solver.cpp:473] Iteration 1330, lr = 0.0001
I0623 12:19:37.808027  5064 solver.cpp:213] Iteration 1340, loss = 3.92978
I0623 12:19:37.808043  5064 solver.cpp:228]     Train net output #0: softmax = 3.92978 (* 1 = 3.92978 loss)
I0623 12:19:37.808048  5064 solver.cpp:473] Iteration 1340, lr = 0.0001
I0623 12:19:38.828608  5064 solver.cpp:213] Iteration 1350, loss = 3.69678
I0623 12:19:38.828629  5064 solver.cpp:228]     Train net output #0: softmax = 3.69678 (* 1 = 3.69678 loss)
I0623 12:19:38.828760  5064 solver.cpp:473] Iteration 1350, lr = 0.0001
I0623 12:19:39.847404  5064 solver.cpp:213] Iteration 1360, loss = 3.78986
I0623 12:19:39.847424  5064 solver.cpp:228]     Train net output #0: softmax = 3.78986 (* 1 = 3.78986 loss)
I0623 12:19:39.847447  5064 solver.cpp:473] Iteration 1360, lr = 0.0001
I0623 12:19:40.868260  5064 solver.cpp:213] Iteration 1370, loss = 3.71779
I0623 12:19:40.868276  5064 solver.cpp:228]     Train net output #0: softmax = 3.71779 (* 1 = 3.71779 loss)
I0623 12:19:40.868281  5064 solver.cpp:473] Iteration 1370, lr = 0.0001
I0623 12:19:41.888943  5064 solver.cpp:213] Iteration 1380, loss = 3.79826
I0623 12:19:41.888970  5064 solver.cpp:228]     Train net output #0: softmax = 3.79826 (* 1 = 3.79826 loss)
I0623 12:19:41.888975  5064 solver.cpp:473] Iteration 1380, lr = 0.0001
I0623 12:19:42.909806  5064 solver.cpp:213] Iteration 1390, loss = 3.94504
I0623 12:19:42.909821  5064 solver.cpp:228]     Train net output #0: softmax = 3.94504 (* 1 = 3.94504 loss)
I0623 12:19:42.909826  5064 solver.cpp:473] Iteration 1390, lr = 0.0001
I0623 12:19:43.929505  5064 solver.cpp:213] Iteration 1400, loss = 3.68112
I0623 12:19:43.929524  5064 solver.cpp:228]     Train net output #0: softmax = 3.68112 (* 1 = 3.68112 loss)
I0623 12:19:43.929661  5064 solver.cpp:473] Iteration 1400, lr = 0.0001
I0623 12:19:44.948856  5064 solver.cpp:213] Iteration 1410, loss = 3.93635
I0623 12:19:44.948875  5064 solver.cpp:228]     Train net output #0: softmax = 3.93635 (* 1 = 3.93635 loss)
I0623 12:19:44.948880  5064 solver.cpp:473] Iteration 1410, lr = 0.0001
I0623 12:19:45.969599  5064 solver.cpp:213] Iteration 1420, loss = 3.68742
I0623 12:19:45.969615  5064 solver.cpp:228]     Train net output #0: softmax = 3.68742 (* 1 = 3.68742 loss)
I0623 12:19:45.969621  5064 solver.cpp:473] Iteration 1420, lr = 0.0001
I0623 12:19:46.990926  5064 solver.cpp:213] Iteration 1430, loss = 3.79538
I0623 12:19:46.990942  5064 solver.cpp:228]     Train net output #0: softmax = 3.79538 (* 1 = 3.79538 loss)
I0623 12:19:46.990947  5064 solver.cpp:473] Iteration 1430, lr = 0.0001
I0623 12:19:48.010881  5064 solver.cpp:213] Iteration 1440, loss = 3.97321
I0623 12:19:48.010897  5064 solver.cpp:228]     Train net output #0: softmax = 3.97321 (* 1 = 3.97321 loss)
I0623 12:19:48.010902  5064 solver.cpp:473] Iteration 1440, lr = 0.0001
I0623 12:19:49.031801  5064 solver.cpp:213] Iteration 1450, loss = 3.83214
I0623 12:19:49.031821  5064 solver.cpp:228]     Train net output #0: softmax = 3.83214 (* 1 = 3.83214 loss)
I0623 12:19:49.031944  5064 solver.cpp:473] Iteration 1450, lr = 0.0001
I0623 12:19:50.053011  5064 solver.cpp:213] Iteration 1460, loss = 3.80304
I0623 12:19:50.053030  5064 solver.cpp:228]     Train net output #0: softmax = 3.80304 (* 1 = 3.80304 loss)
I0623 12:19:50.053035  5064 solver.cpp:473] Iteration 1460, lr = 0.0001
I0623 12:19:51.073951  5064 solver.cpp:213] Iteration 1470, loss = 3.74448
I0623 12:19:51.073966  5064 solver.cpp:228]     Train net output #0: softmax = 3.74448 (* 1 = 3.74448 loss)
I0623 12:19:51.073971  5064 solver.cpp:473] Iteration 1470, lr = 0.0001
I0623 12:19:52.093886  5064 solver.cpp:213] Iteration 1480, loss = 4.0866
I0623 12:19:52.093902  5064 solver.cpp:228]     Train net output #0: softmax = 4.0866 (* 1 = 4.0866 loss)
I0623 12:19:52.093907  5064 solver.cpp:473] Iteration 1480, lr = 0.0001
I0623 12:19:53.114315  5064 solver.cpp:213] Iteration 1490, loss = 3.88274
I0623 12:19:53.114331  5064 solver.cpp:228]     Train net output #0: softmax = 3.88274 (* 1 = 3.88274 loss)
I0623 12:19:53.114336  5064 solver.cpp:473] Iteration 1490, lr = 0.0001
I0623 12:19:54.135294  5064 solver.cpp:213] Iteration 1500, loss = 3.70483
I0623 12:19:54.135462  5064 solver.cpp:228]     Train net output #0: softmax = 3.70483 (* 1 = 3.70483 loss)
I0623 12:19:54.135470  5064 solver.cpp:473] Iteration 1500, lr = 0.0001
I0623 12:19:55.155814  5064 solver.cpp:213] Iteration 1510, loss = 3.72835
I0623 12:19:55.155839  5064 solver.cpp:228]     Train net output #0: softmax = 3.72835 (* 1 = 3.72835 loss)
I0623 12:19:55.155846  5064 solver.cpp:473] Iteration 1510, lr = 0.0001
I0623 12:19:56.176676  5064 solver.cpp:213] Iteration 1520, loss = 3.77452
I0623 12:19:56.176693  5064 solver.cpp:228]     Train net output #0: softmax = 3.77452 (* 1 = 3.77452 loss)
I0623 12:19:56.176704  5064 solver.cpp:473] Iteration 1520, lr = 0.0001
I0623 12:19:57.197448  5064 solver.cpp:213] Iteration 1530, loss = 3.82023
I0623 12:19:57.197464  5064 solver.cpp:228]     Train net output #0: softmax = 3.82023 (* 1 = 3.82023 loss)
I0623 12:19:57.197469  5064 solver.cpp:473] Iteration 1530, lr = 0.0001
I0623 12:19:58.218435  5064 solver.cpp:213] Iteration 1540, loss = 4.01976
I0623 12:19:58.218452  5064 solver.cpp:228]     Train net output #0: softmax = 4.01976 (* 1 = 4.01976 loss)
I0623 12:19:58.218457  5064 solver.cpp:473] Iteration 1540, lr = 0.0001
I0623 12:19:59.238605  5064 solver.cpp:213] Iteration 1550, loss = 3.95432
I0623 12:19:59.238625  5064 solver.cpp:228]     Train net output #0: softmax = 3.95432 (* 1 = 3.95432 loss)
I0623 12:19:59.238754  5064 solver.cpp:473] Iteration 1550, lr = 0.0001
I0623 12:20:00.259474  5064 solver.cpp:213] Iteration 1560, loss = 3.80563
I0623 12:20:00.259502  5064 solver.cpp:228]     Train net output #0: softmax = 3.80563 (* 1 = 3.80563 loss)
I0623 12:20:00.259507  5064 solver.cpp:473] Iteration 1560, lr = 0.0001
I0623 12:20:01.280218  5064 solver.cpp:213] Iteration 1570, loss = 3.78534
I0623 12:20:01.280236  5064 solver.cpp:228]     Train net output #0: softmax = 3.78534 (* 1 = 3.78534 loss)
I0623 12:20:01.280241  5064 solver.cpp:473] Iteration 1570, lr = 0.0001
I0623 12:20:02.301167  5064 solver.cpp:213] Iteration 1580, loss = 3.75328
I0623 12:20:02.301183  5064 solver.cpp:228]     Train net output #0: softmax = 3.75328 (* 1 = 3.75328 loss)
I0623 12:20:02.301188  5064 solver.cpp:473] Iteration 1580, lr = 0.0001
I0623 12:20:03.320889  5064 solver.cpp:213] Iteration 1590, loss = 3.99657
I0623 12:20:03.320905  5064 solver.cpp:228]     Train net output #0: softmax = 3.99657 (* 1 = 3.99657 loss)
I0623 12:20:03.320910  5064 solver.cpp:473] Iteration 1590, lr = 0.0001
I0623 12:20:04.341923  5064 solver.cpp:213] Iteration 1600, loss = 3.85004
I0623 12:20:04.341943  5064 solver.cpp:228]     Train net output #0: softmax = 3.85004 (* 1 = 3.85004 loss)
I0623 12:20:04.341948  5064 solver.cpp:473] Iteration 1600, lr = 0.0001
I0623 12:20:05.363181  5064 solver.cpp:213] Iteration 1610, loss = 3.67549
I0623 12:20:05.363198  5064 solver.cpp:228]     Train net output #0: softmax = 3.67549 (* 1 = 3.67549 loss)
I0623 12:20:05.363204  5064 solver.cpp:473] Iteration 1610, lr = 0.0001
I0623 12:20:06.383988  5064 solver.cpp:213] Iteration 1620, loss = 3.87695
I0623 12:20:06.384008  5064 solver.cpp:228]     Train net output #0: softmax = 3.87695 (* 1 = 3.87695 loss)
I0623 12:20:06.384013  5064 solver.cpp:473] Iteration 1620, lr = 0.0001
I0623 12:20:07.404580  5064 solver.cpp:213] Iteration 1630, loss = 3.75465
I0623 12:20:07.404597  5064 solver.cpp:228]     Train net output #0: softmax = 3.75465 (* 1 = 3.75465 loss)
I0623 12:20:07.404602  5064 solver.cpp:473] Iteration 1630, lr = 0.0001
I0623 12:20:08.425705  5064 solver.cpp:213] Iteration 1640, loss = 3.87747
I0623 12:20:08.425722  5064 solver.cpp:228]     Train net output #0: softmax = 3.87747 (* 1 = 3.87747 loss)
I0623 12:20:08.425726  5064 solver.cpp:473] Iteration 1640, lr = 0.0001
I0623 12:20:09.445688  5064 solver.cpp:213] Iteration 1650, loss = 3.71092
I0623 12:20:09.445709  5064 solver.cpp:228]     Train net output #0: softmax = 3.71092 (* 1 = 3.71092 loss)
I0623 12:20:09.445829  5064 solver.cpp:473] Iteration 1650, lr = 0.0001
I0623 12:20:10.465834  5064 solver.cpp:213] Iteration 1660, loss = 3.85039
I0623 12:20:10.465850  5064 solver.cpp:228]     Train net output #0: softmax = 3.85039 (* 1 = 3.85039 loss)
I0623 12:20:10.465872  5064 solver.cpp:473] Iteration 1660, lr = 0.0001
I0623 12:20:11.486270  5064 solver.cpp:213] Iteration 1670, loss = 3.65041
I0623 12:20:11.486286  5064 solver.cpp:228]     Train net output #0: softmax = 3.65041 (* 1 = 3.65041 loss)
I0623 12:20:11.486291  5064 solver.cpp:473] Iteration 1670, lr = 0.0001
I0623 12:20:12.507253  5064 solver.cpp:213] Iteration 1680, loss = 3.94173
I0623 12:20:12.507272  5064 solver.cpp:228]     Train net output #0: softmax = 3.94173 (* 1 = 3.94173 loss)
I0623 12:20:12.507282  5064 solver.cpp:473] Iteration 1680, lr = 0.0001
I0623 12:20:13.528122  5064 solver.cpp:213] Iteration 1690, loss = 3.81008
I0623 12:20:13.528141  5064 solver.cpp:228]     Train net output #0: softmax = 3.81008 (* 1 = 3.81008 loss)
I0623 12:20:13.528146  5064 solver.cpp:473] Iteration 1690, lr = 0.0001
I0623 12:20:14.548971  5064 solver.cpp:213] Iteration 1700, loss = 3.7465
I0623 12:20:14.548993  5064 solver.cpp:228]     Train net output #0: softmax = 3.7465 (* 1 = 3.7465 loss)
I0623 12:20:14.549139  5064 solver.cpp:473] Iteration 1700, lr = 0.0001
I0623 12:20:15.567757  5064 solver.cpp:213] Iteration 1710, loss = 3.76863
I0623 12:20:15.567773  5064 solver.cpp:228]     Train net output #0: softmax = 3.76863 (* 1 = 3.76863 loss)
I0623 12:20:15.567778  5064 solver.cpp:473] Iteration 1710, lr = 0.0001
I0623 12:20:16.588435  5064 solver.cpp:213] Iteration 1720, loss = 3.6322
I0623 12:20:16.588452  5064 solver.cpp:228]     Train net output #0: softmax = 3.6322 (* 1 = 3.6322 loss)
I0623 12:20:16.588457  5064 solver.cpp:473] Iteration 1720, lr = 0.0001
I0623 12:20:17.608527  5064 solver.cpp:213] Iteration 1730, loss = 3.80122
I0623 12:20:17.608542  5064 solver.cpp:228]     Train net output #0: softmax = 3.80122 (* 1 = 3.80122 loss)
I0623 12:20:17.608547  5064 solver.cpp:473] Iteration 1730, lr = 0.0001
I0623 12:20:18.629400  5064 solver.cpp:213] Iteration 1740, loss = 3.7528
I0623 12:20:18.629418  5064 solver.cpp:228]     Train net output #0: softmax = 3.7528 (* 1 = 3.7528 loss)
I0623 12:20:18.629425  5064 solver.cpp:473] Iteration 1740, lr = 0.0001
I0623 12:20:19.650303  5064 solver.cpp:213] Iteration 1750, loss = 3.85216
I0623 12:20:19.650322  5064 solver.cpp:228]     Train net output #0: softmax = 3.85216 (* 1 = 3.85216 loss)
I0623 12:20:19.650435  5064 solver.cpp:473] Iteration 1750, lr = 0.0001
I0623 12:20:20.671015  5064 solver.cpp:213] Iteration 1760, loss = 3.69111
I0623 12:20:20.671032  5064 solver.cpp:228]     Train net output #0: softmax = 3.69111 (* 1 = 3.69111 loss)
I0623 12:20:20.671036  5064 solver.cpp:473] Iteration 1760, lr = 0.0001
I0623 12:20:21.692118  5064 solver.cpp:213] Iteration 1770, loss = 3.67727
I0623 12:20:21.692140  5064 solver.cpp:228]     Train net output #0: softmax = 3.67727 (* 1 = 3.67727 loss)
I0623 12:20:21.692145  5064 solver.cpp:473] Iteration 1770, lr = 0.0001
I0623 12:20:22.713533  5064 solver.cpp:213] Iteration 1780, loss = 3.69038
I0623 12:20:22.713551  5064 solver.cpp:228]     Train net output #0: softmax = 3.69038 (* 1 = 3.69038 loss)
I0623 12:20:22.713557  5064 solver.cpp:473] Iteration 1780, lr = 0.0001
I0623 12:20:23.734015  5064 solver.cpp:213] Iteration 1790, loss = 3.74132
I0623 12:20:23.734033  5064 solver.cpp:228]     Train net output #0: softmax = 3.74132 (* 1 = 3.74132 loss)
I0623 12:20:23.734040  5064 solver.cpp:473] Iteration 1790, lr = 0.0001
I0623 12:20:24.754222  5064 solver.cpp:213] Iteration 1800, loss = 3.72676
I0623 12:20:24.754387  5064 solver.cpp:228]     Train net output #0: softmax = 3.72676 (* 1 = 3.72676 loss)
I0623 12:20:24.754395  5064 solver.cpp:473] Iteration 1800, lr = 0.0001
I0623 12:20:25.775231  5064 solver.cpp:213] Iteration 1810, loss = 3.55144
I0623 12:20:25.775251  5064 solver.cpp:228]     Train net output #0: softmax = 3.55144 (* 1 = 3.55144 loss)
I0623 12:20:25.775257  5064 solver.cpp:473] Iteration 1810, lr = 0.0001
I0623 12:20:26.796103  5064 solver.cpp:213] Iteration 1820, loss = 3.87292
I0623 12:20:26.796120  5064 solver.cpp:228]     Train net output #0: softmax = 3.87292 (* 1 = 3.87292 loss)
I0623 12:20:26.796125  5064 solver.cpp:473] Iteration 1820, lr = 0.0001
I0623 12:20:27.817071  5064 solver.cpp:213] Iteration 1830, loss = 3.83315
I0623 12:20:27.817088  5064 solver.cpp:228]     Train net output #0: softmax = 3.83315 (* 1 = 3.83315 loss)
I0623 12:20:27.817093  5064 solver.cpp:473] Iteration 1830, lr = 0.0001
I0623 12:20:28.837410  5064 solver.cpp:213] Iteration 1840, loss = 3.89411
I0623 12:20:28.837427  5064 solver.cpp:228]     Train net output #0: softmax = 3.89411 (* 1 = 3.89411 loss)
I0623 12:20:28.837438  5064 solver.cpp:473] Iteration 1840, lr = 0.0001
I0623 12:20:29.858666  5064 solver.cpp:213] Iteration 1850, loss = 3.65972
I0623 12:20:29.858686  5064 solver.cpp:228]     Train net output #0: softmax = 3.65972 (* 1 = 3.65972 loss)
I0623 12:20:29.858830  5064 solver.cpp:473] Iteration 1850, lr = 0.0001
I0623 12:20:30.880100  5064 solver.cpp:213] Iteration 1860, loss = 3.71344
I0623 12:20:30.880117  5064 solver.cpp:228]     Train net output #0: softmax = 3.71344 (* 1 = 3.71344 loss)
I0623 12:20:30.880122  5064 solver.cpp:473] Iteration 1860, lr = 0.0001
I0623 12:20:31.901211  5064 solver.cpp:213] Iteration 1870, loss = 3.95857
I0623 12:20:31.901228  5064 solver.cpp:228]     Train net output #0: softmax = 3.95857 (* 1 = 3.95857 loss)
I0623 12:20:31.901233  5064 solver.cpp:473] Iteration 1870, lr = 0.0001
I0623 12:20:32.922112  5064 solver.cpp:213] Iteration 1880, loss = 3.72092
I0623 12:20:32.922130  5064 solver.cpp:228]     Train net output #0: softmax = 3.72092 (* 1 = 3.72092 loss)
I0623 12:20:32.922135  5064 solver.cpp:473] Iteration 1880, lr = 0.0001
I0623 12:20:33.942608  5064 solver.cpp:213] Iteration 1890, loss = 3.76384
I0623 12:20:33.942625  5064 solver.cpp:228]     Train net output #0: softmax = 3.76384 (* 1 = 3.76384 loss)
I0623 12:20:33.942631  5064 solver.cpp:473] Iteration 1890, lr = 0.0001
I0623 12:20:34.963383  5064 solver.cpp:213] Iteration 1900, loss = 3.73678
I0623 12:20:34.963403  5064 solver.cpp:228]     Train net output #0: softmax = 3.73678 (* 1 = 3.73678 loss)
I0623 12:20:34.963520  5064 solver.cpp:473] Iteration 1900, lr = 0.0001
I0623 12:20:35.983796  5064 solver.cpp:213] Iteration 1910, loss = 3.72823
I0623 12:20:35.983814  5064 solver.cpp:228]     Train net output #0: softmax = 3.72823 (* 1 = 3.72823 loss)
I0623 12:20:35.983819  5064 solver.cpp:473] Iteration 1910, lr = 0.0001
I0623 12:20:37.004668  5064 solver.cpp:213] Iteration 1920, loss = 3.85966
I0623 12:20:37.004688  5064 solver.cpp:228]     Train net output #0: softmax = 3.85966 (* 1 = 3.85966 loss)
I0623 12:20:37.004693  5064 solver.cpp:473] Iteration 1920, lr = 0.0001
I0623 12:20:38.025369  5064 solver.cpp:213] Iteration 1930, loss = 3.757
I0623 12:20:38.025385  5064 solver.cpp:228]     Train net output #0: softmax = 3.757 (* 1 = 3.757 loss)
I0623 12:20:38.025391  5064 solver.cpp:473] Iteration 1930, lr = 0.0001
I0623 12:20:39.045610  5064 solver.cpp:213] Iteration 1940, loss = 3.96261
I0623 12:20:39.045629  5064 solver.cpp:228]     Train net output #0: softmax = 3.96261 (* 1 = 3.96261 loss)
I0623 12:20:39.045634  5064 solver.cpp:473] Iteration 1940, lr = 0.0001
I0623 12:20:40.065968  5064 solver.cpp:213] Iteration 1950, loss = 3.7856
I0623 12:20:40.065987  5064 solver.cpp:228]     Train net output #0: softmax = 3.7856 (* 1 = 3.7856 loss)
I0623 12:20:40.066105  5064 solver.cpp:473] Iteration 1950, lr = 0.0001
I0623 12:20:41.085827  5064 solver.cpp:213] Iteration 1960, loss = 3.72237
I0623 12:20:41.085844  5064 solver.cpp:228]     Train net output #0: softmax = 3.72237 (* 1 = 3.72237 loss)
I0623 12:20:41.085866  5064 solver.cpp:473] Iteration 1960, lr = 0.0001
I0623 12:20:42.107019  5064 solver.cpp:213] Iteration 1970, loss = 3.56654
I0623 12:20:42.107048  5064 solver.cpp:228]     Train net output #0: softmax = 3.56654 (* 1 = 3.56654 loss)
I0623 12:20:42.107053  5064 solver.cpp:473] Iteration 1970, lr = 0.0001
I0623 12:20:43.127646  5064 solver.cpp:213] Iteration 1980, loss = 3.88316
I0623 12:20:43.127662  5064 solver.cpp:228]     Train net output #0: softmax = 3.88316 (* 1 = 3.88316 loss)
I0623 12:20:43.127667  5064 solver.cpp:473] Iteration 1980, lr = 0.0001
I0623 12:20:44.148041  5064 solver.cpp:213] Iteration 1990, loss = 3.76998
I0623 12:20:44.148057  5064 solver.cpp:228]     Train net output #0: softmax = 3.76998 (* 1 = 3.76998 loss)
I0623 12:20:44.148062  5064 solver.cpp:473] Iteration 1990, lr = 0.0001
I0623 12:20:45.098413  5064 solver.cpp:362] Snapshotting to snapshots/16-06-21_16h05m31s_0_11_pretrainClassification_iter_2000.caffemodel
I0623 12:20:45.099495  5064 solver.cpp:370] Snapshotting solver state to snapshots/16-06-21_16h05m31s_0_11_pretrainClassification_iter_2000.solverstate
I0623 12:20:45.100105  5064 solver.cpp:291] Iteration 2000, Testing net (#0)
I0623 12:20:45.261227  5064 solver.cpp:342]     Test net output #0: accuracy_top_1 = 0.132812
I0623 12:20:45.261242  5064 solver.cpp:342]     Test net output #1: accuracy_top_5 = 0.345313
I0623 12:20:45.261248  5064 solver.cpp:342]     Test net output #2: softmax = 3.7731 (* 1 = 3.7731 loss)
I0623 12:20:45.331514  5064 solver.cpp:213] Iteration 2000, loss = 3.66691
I0623 12:20:45.331528  5064 solver.cpp:228]     Train net output #0: softmax = 3.66691 (* 1 = 3.66691 loss)
I0623 12:20:45.331533  5064 solver.cpp:473] Iteration 2000, lr = 0.0001
I0623 12:20:46.352375  5064 solver.cpp:213] Iteration 2010, loss = 3.94428
I0623 12:20:46.352393  5064 solver.cpp:228]     Train net output #0: softmax = 3.94428 (* 1 = 3.94428 loss)
I0623 12:20:46.352398  5064 solver.cpp:473] Iteration 2010, lr = 0.0001
I0623 12:20:47.372439  5064 solver.cpp:213] Iteration 2020, loss = 3.82929
I0623 12:20:47.372455  5064 solver.cpp:228]     Train net output #0: softmax = 3.82929 (* 1 = 3.82929 loss)
I0623 12:20:47.372460  5064 solver.cpp:473] Iteration 2020, lr = 0.0001
I0623 12:20:48.393282  5064 solver.cpp:213] Iteration 2030, loss = 3.64691
I0623 12:20:48.393299  5064 solver.cpp:228]     Train net output #0: softmax = 3.64691 (* 1 = 3.64691 loss)
I0623 12:20:48.393304  5064 solver.cpp:473] Iteration 2030, lr = 0.0001
I0623 12:20:49.414232  5064 solver.cpp:213] Iteration 2040, loss = 3.75259
I0623 12:20:49.414247  5064 solver.cpp:228]     Train net output #0: softmax = 3.75259 (* 1 = 3.75259 loss)
I0623 12:20:49.414252  5064 solver.cpp:473] Iteration 2040, lr = 0.0001
I0623 12:20:50.435446  5064 solver.cpp:213] Iteration 2050, loss = 3.87916
I0623 12:20:50.435463  5064 solver.cpp:228]     Train net output #0: softmax = 3.87916 (* 1 = 3.87916 loss)
I0623 12:20:50.435469  5064 solver.cpp:473] Iteration 2050, lr = 0.0001
I0623 12:20:51.456090  5064 solver.cpp:213] Iteration 2060, loss = 3.79611
I0623 12:20:51.456109  5064 solver.cpp:228]     Train net output #0: softmax = 3.79611 (* 1 = 3.79611 loss)
I0623 12:20:51.456115  5064 solver.cpp:473] Iteration 2060, lr = 0.0001
I0623 12:20:52.476328  5064 solver.cpp:213] Iteration 2070, loss = 3.90733
I0623 12:20:52.476346  5064 solver.cpp:228]     Train net output #0: softmax = 3.90733 (* 1 = 3.90733 loss)
I0623 12:20:52.476351  5064 solver.cpp:473] Iteration 2070, lr = 0.0001
I0623 12:20:53.496732  5064 solver.cpp:213] Iteration 2080, loss = 3.86345
I0623 12:20:53.496749  5064 solver.cpp:228]     Train net output #0: softmax = 3.86345 (* 1 = 3.86345 loss)
I0623 12:20:53.496754  5064 solver.cpp:473] Iteration 2080, lr = 0.0001
I0623 12:20:54.518160  5064 solver.cpp:213] Iteration 2090, loss = 3.53013
I0623 12:20:54.518177  5064 solver.cpp:228]     Train net output #0: softmax = 3.53013 (* 1 = 3.53013 loss)
I0623 12:20:54.518182  5064 solver.cpp:473] Iteration 2090, lr = 0.0001
I0623 12:20:55.538935  5064 solver.cpp:213] Iteration 2100, loss = 3.74615
I0623 12:20:55.539093  5064 solver.cpp:228]     Train net output #0: softmax = 3.74615 (* 1 = 3.74615 loss)
I0623 12:20:55.539101  5064 solver.cpp:473] Iteration 2100, lr = 0.0001
I0623 12:20:56.559590  5064 solver.cpp:213] Iteration 2110, loss = 3.60163
I0623 12:20:56.559607  5064 solver.cpp:228]     Train net output #0: softmax = 3.60163 (* 1 = 3.60163 loss)
I0623 12:20:56.559612  5064 solver.cpp:473] Iteration 2110, lr = 0.0001
I0623 12:20:57.580902  5064 solver.cpp:213] Iteration 2120, loss = 3.71182
I0623 12:20:57.580920  5064 solver.cpp:228]     Train net output #0: softmax = 3.71182 (* 1 = 3.71182 loss)
I0623 12:20:57.580925  5064 solver.cpp:473] Iteration 2120, lr = 0.0001
I0623 12:20:58.601974  5064 solver.cpp:213] Iteration 2130, loss = 3.86009
I0623 12:20:58.601992  5064 solver.cpp:228]     Train net output #0: softmax = 3.86009 (* 1 = 3.86009 loss)
I0623 12:20:58.601997  5064 solver.cpp:473] Iteration 2130, lr = 0.0001
I0623 12:20:59.622367  5064 solver.cpp:213] Iteration 2140, loss = 3.71869
I0623 12:20:59.622391  5064 solver.cpp:228]     Train net output #0: softmax = 3.71869 (* 1 = 3.71869 loss)
I0623 12:20:59.622396  5064 solver.cpp:473] Iteration 2140, lr = 0.0001
I0623 12:21:00.643025  5064 solver.cpp:213] Iteration 2150, loss = 3.50576
I0623 12:21:00.643045  5064 solver.cpp:228]     Train net output #0: softmax = 3.50576 (* 1 = 3.50576 loss)
I0623 12:21:00.643163  5064 solver.cpp:473] Iteration 2150, lr = 0.0001
I0623 12:21:01.664023  5064 solver.cpp:213] Iteration 2160, loss = 3.43412
I0623 12:21:01.664041  5064 solver.cpp:228]     Train net output #0: softmax = 3.43412 (* 1 = 3.43412 loss)
I0623 12:21:01.664047  5064 solver.cpp:473] Iteration 2160, lr = 0.0001
I0623 12:21:02.684898  5064 solver.cpp:213] Iteration 2170, loss = 3.71798
I0623 12:21:02.684916  5064 solver.cpp:228]     Train net output #0: softmax = 3.71798 (* 1 = 3.71798 loss)
I0623 12:21:02.684922  5064 solver.cpp:473] Iteration 2170, lr = 0.0001
I0623 12:21:03.705690  5064 solver.cpp:213] Iteration 2180, loss = 3.90318
I0623 12:21:03.705708  5064 solver.cpp:228]     Train net output #0: softmax = 3.90318 (* 1 = 3.90318 loss)
I0623 12:21:03.705713  5064 solver.cpp:473] Iteration 2180, lr = 0.0001
I0623 12:21:04.726806  5064 solver.cpp:213] Iteration 2190, loss = 3.65108
I0623 12:21:04.726822  5064 solver.cpp:228]     Train net output #0: softmax = 3.65108 (* 1 = 3.65108 loss)
I0623 12:21:04.726827  5064 solver.cpp:473] Iteration 2190, lr = 0.0001
I0623 12:21:05.747395  5064 solver.cpp:213] Iteration 2200, loss = 3.61361
I0623 12:21:05.747413  5064 solver.cpp:228]     Train net output #0: softmax = 3.61361 (* 1 = 3.61361 loss)
I0623 12:21:05.747526  5064 solver.cpp:473] Iteration 2200, lr = 0.0001
I0623 12:21:06.768712  5064 solver.cpp:213] Iteration 2210, loss = 3.76432
I0623 12:21:06.768731  5064 solver.cpp:228]     Train net output #0: softmax = 3.76432 (* 1 = 3.76432 loss)
I0623 12:21:06.768736  5064 solver.cpp:473] Iteration 2210, lr = 0.0001
I0623 12:21:07.789484  5064 solver.cpp:213] Iteration 2220, loss = 3.64151
I0623 12:21:07.789502  5064 solver.cpp:228]     Train net output #0: softmax = 3.64151 (* 1 = 3.64151 loss)
I0623 12:21:07.789507  5064 solver.cpp:473] Iteration 2220, lr = 0.0001
I0623 12:21:08.810806  5064 solver.cpp:213] Iteration 2230, loss = 3.54511
I0623 12:21:08.810823  5064 solver.cpp:228]     Train net output #0: softmax = 3.54511 (* 1 = 3.54511 loss)
I0623 12:21:08.810828  5064 solver.cpp:473] Iteration 2230, lr = 0.0001
I0623 12:21:09.831619  5064 solver.cpp:213] Iteration 2240, loss = 3.55568
I0623 12:21:09.831636  5064 solver.cpp:228]     Train net output #0: softmax = 3.55568 (* 1 = 3.55568 loss)
I0623 12:21:09.831641  5064 solver.cpp:473] Iteration 2240, lr = 0.0001
I0623 12:21:10.853171  5064 solver.cpp:213] Iteration 2250, loss = 3.69891
I0623 12:21:10.853191  5064 solver.cpp:228]     Train net output #0: softmax = 3.69891 (* 1 = 3.69891 loss)
I0623 12:21:10.853315  5064 solver.cpp:473] Iteration 2250, lr = 0.0001
I0623 12:21:11.873431  5064 solver.cpp:213] Iteration 2260, loss = 4.14484
I0623 12:21:11.873448  5064 solver.cpp:228]     Train net output #0: softmax = 4.14484 (* 1 = 4.14484 loss)
I0623 12:21:11.873469  5064 solver.cpp:473] Iteration 2260, lr = 0.0001
I0623 12:21:12.894431  5064 solver.cpp:213] Iteration 2270, loss = 3.90813
I0623 12:21:12.894448  5064 solver.cpp:228]     Train net output #0: softmax = 3.90813 (* 1 = 3.90813 loss)
I0623 12:21:12.894454  5064 solver.cpp:473] Iteration 2270, lr = 0.0001
I0623 12:21:13.915230  5064 solver.cpp:213] Iteration 2280, loss = 3.71259
I0623 12:21:13.915246  5064 solver.cpp:228]     Train net output #0: softmax = 3.71259 (* 1 = 3.71259 loss)
I0623 12:21:13.915251  5064 solver.cpp:473] Iteration 2280, lr = 0.0001
I0623 12:21:14.936010  5064 solver.cpp:213] Iteration 2290, loss = 3.66564
I0623 12:21:14.936027  5064 solver.cpp:228]     Train net output #0: softmax = 3.66564 (* 1 = 3.66564 loss)
I0623 12:21:14.936031  5064 solver.cpp:473] Iteration 2290, lr = 0.0001
I0623 12:21:15.956645  5064 solver.cpp:213] Iteration 2300, loss = 3.76193
I0623 12:21:15.956676  5064 solver.cpp:228]     Train net output #0: softmax = 3.76193 (* 1 = 3.76193 loss)
I0623 12:21:15.956684  5064 solver.cpp:473] Iteration 2300, lr = 0.0001
I0623 12:21:16.977474  5064 solver.cpp:213] Iteration 2310, loss = 3.79734
I0623 12:21:16.977489  5064 solver.cpp:228]     Train net output #0: softmax = 3.79734 (* 1 = 3.79734 loss)
I0623 12:21:16.977494  5064 solver.cpp:473] Iteration 2310, lr = 0.0001
I0623 12:21:17.998320  5064 solver.cpp:213] Iteration 2320, loss = 3.65672
I0623 12:21:17.998338  5064 solver.cpp:228]     Train net output #0: softmax = 3.65672 (* 1 = 3.65672 loss)
I0623 12:21:17.998342  5064 solver.cpp:473] Iteration 2320, lr = 0.0001
I0623 12:21:19.019460  5064 solver.cpp:213] Iteration 2330, loss = 3.80078
I0623 12:21:19.019490  5064 solver.cpp:228]     Train net output #0: softmax = 3.80078 (* 1 = 3.80078 loss)
I0623 12:21:19.019495  5064 solver.cpp:473] Iteration 2330, lr = 0.0001
I0623 12:21:20.039433  5064 solver.cpp:213] Iteration 2340, loss = 3.75492
I0623 12:21:20.039450  5064 solver.cpp:228]     Train net output #0: softmax = 3.75492 (* 1 = 3.75492 loss)
I0623 12:21:20.039455  5064 solver.cpp:473] Iteration 2340, lr = 0.0001
I0623 12:21:21.060086  5064 solver.cpp:213] Iteration 2350, loss = 3.61505
I0623 12:21:21.060106  5064 solver.cpp:228]     Train net output #0: softmax = 3.61505 (* 1 = 3.61505 loss)
I0623 12:21:21.060247  5064 solver.cpp:473] Iteration 2350, lr = 0.0001
I0623 12:21:22.080817  5064 solver.cpp:213] Iteration 2360, loss = 3.63245
I0623 12:21:22.080834  5064 solver.cpp:228]     Train net output #0: softmax = 3.63245 (* 1 = 3.63245 loss)
I0623 12:21:22.080839  5064 solver.cpp:473] Iteration 2360, lr = 0.0001
I0623 12:21:23.101508  5064 solver.cpp:213] Iteration 2370, loss = 3.71485
I0623 12:21:23.101524  5064 solver.cpp:228]     Train net output #0: softmax = 3.71485 (* 1 = 3.71485 loss)
I0623 12:21:23.101529  5064 solver.cpp:473] Iteration 2370, lr = 0.0001
I0623 12:21:24.121991  5064 solver.cpp:213] Iteration 2380, loss = 3.76812
I0623 12:21:24.122011  5064 solver.cpp:228]     Train net output #0: softmax = 3.76812 (* 1 = 3.76812 loss)
I0623 12:21:24.122016  5064 solver.cpp:473] Iteration 2380, lr = 0.0001
I0623 12:21:25.142812  5064 solver.cpp:213] Iteration 2390, loss = 3.61222
I0623 12:21:25.142830  5064 solver.cpp:228]     Train net output #0: softmax = 3.61222 (* 1 = 3.61222 loss)
I0623 12:21:25.142835  5064 solver.cpp:473] Iteration 2390, lr = 0.0001
I0623 12:21:26.163226  5064 solver.cpp:213] Iteration 2400, loss = 3.69173
I0623 12:21:26.163401  5064 solver.cpp:228]     Train net output #0: softmax = 3.69173 (* 1 = 3.69173 loss)
I0623 12:21:26.163408  5064 solver.cpp:473] Iteration 2400, lr = 0.0001
I0623 12:21:27.183483  5064 solver.cpp:213] Iteration 2410, loss = 3.6526
I0623 12:21:27.183500  5064 solver.cpp:228]     Train net output #0: softmax = 3.6526 (* 1 = 3.6526 loss)
I0623 12:21:27.183504  5064 solver.cpp:473] Iteration 2410, lr = 0.0001
I0623 12:21:28.204504  5064 solver.cpp:213] Iteration 2420, loss = 3.69901
I0623 12:21:28.204520  5064 solver.cpp:228]     Train net output #0: softmax = 3.69901 (* 1 = 3.69901 loss)
I0623 12:21:28.204525  5064 solver.cpp:473] Iteration 2420, lr = 0.0001
I0623 12:21:29.224980  5064 solver.cpp:213] Iteration 2430, loss = 3.60653
I0623 12:21:29.224999  5064 solver.cpp:228]     Train net output #0: softmax = 3.60653 (* 1 = 3.60653 loss)
I0623 12:21:29.225004  5064 solver.cpp:473] Iteration 2430, lr = 0.0001
I0623 12:21:30.245584  5064 solver.cpp:213] Iteration 2440, loss = 3.83841
I0623 12:21:30.245600  5064 solver.cpp:228]     Train net output #0: softmax = 3.83841 (* 1 = 3.83841 loss)
I0623 12:21:30.245605  5064 solver.cpp:473] Iteration 2440, lr = 0.0001
I0623 12:21:31.266362  5064 solver.cpp:213] Iteration 2450, loss = 3.66901
I0623 12:21:31.266382  5064 solver.cpp:228]     Train net output #0: softmax = 3.66901 (* 1 = 3.66901 loss)
I0623 12:21:31.266510  5064 solver.cpp:473] Iteration 2450, lr = 0.0001
I0623 12:21:32.286638  5064 solver.cpp:213] Iteration 2460, loss = 3.66487
I0623 12:21:32.286661  5064 solver.cpp:228]     Train net output #0: softmax = 3.66487 (* 1 = 3.66487 loss)
I0623 12:21:32.286667  5064 solver.cpp:473] Iteration 2460, lr = 0.0001
I0623 12:21:33.307335  5064 solver.cpp:213] Iteration 2470, loss = 3.81357
I0623 12:21:33.307350  5064 solver.cpp:228]     Train net output #0: softmax = 3.81357 (* 1 = 3.81357 loss)
I0623 12:21:33.307355  5064 solver.cpp:473] Iteration 2470, lr = 0.0001
I0623 12:21:34.328125  5064 solver.cpp:213] Iteration 2480, loss = 3.52946
I0623 12:21:34.328145  5064 solver.cpp:228]     Train net output #0: softmax = 3.52946 (* 1 = 3.52946 loss)
I0623 12:21:34.328150  5064 solver.cpp:473] Iteration 2480, lr = 0.0001
I0623 12:21:35.348825  5064 solver.cpp:213] Iteration 2490, loss = 3.73162
I0623 12:21:35.348841  5064 solver.cpp:228]     Train net output #0: softmax = 3.73162 (* 1 = 3.73162 loss)
I0623 12:21:35.348846  5064 solver.cpp:473] Iteration 2490, lr = 0.0001
I0623 12:21:36.369094  5064 solver.cpp:213] Iteration 2500, loss = 3.60362
I0623 12:21:36.369112  5064 solver.cpp:228]     Train net output #0: softmax = 3.60362 (* 1 = 3.60362 loss)
I0623 12:21:36.369117  5064 solver.cpp:473] Iteration 2500, lr = 0.0001
I0623 12:21:37.390210  5064 solver.cpp:213] Iteration 2510, loss = 3.6833
I0623 12:21:37.390226  5064 solver.cpp:228]     Train net output #0: softmax = 3.6833 (* 1 = 3.6833 loss)
I0623 12:21:37.390230  5064 solver.cpp:473] Iteration 2510, lr = 0.0001
I0623 12:21:38.411308  5064 solver.cpp:213] Iteration 2520, loss = 3.72619
I0623 12:21:38.411324  5064 solver.cpp:228]     Train net output #0: softmax = 3.72619 (* 1 = 3.72619 loss)
I0623 12:21:38.411330  5064 solver.cpp:473] Iteration 2520, lr = 0.0001
I0623 12:21:39.431907  5064 solver.cpp:213] Iteration 2530, loss = 3.59885
I0623 12:21:39.431926  5064 solver.cpp:228]     Train net output #0: softmax = 3.59885 (* 1 = 3.59885 loss)
I0623 12:21:39.431931  5064 solver.cpp:473] Iteration 2530, lr = 0.0001
I0623 12:21:40.452360  5064 solver.cpp:213] Iteration 2540, loss = 3.55877
I0623 12:21:40.452378  5064 solver.cpp:228]     Train net output #0: softmax = 3.55877 (* 1 = 3.55877 loss)
I0623 12:21:40.452383  5064 solver.cpp:473] Iteration 2540, lr = 0.0001
I0623 12:21:41.472898  5064 solver.cpp:213] Iteration 2550, loss = 3.64862
I0623 12:21:41.472918  5064 solver.cpp:228]     Train net output #0: softmax = 3.64862 (* 1 = 3.64862 loss)
I0623 12:21:41.473037  5064 solver.cpp:473] Iteration 2550, lr = 0.0001
I0623 12:21:42.493958  5064 solver.cpp:213] Iteration 2560, loss = 3.51299
I0623 12:21:42.493974  5064 solver.cpp:228]     Train net output #0: softmax = 3.51299 (* 1 = 3.51299 loss)
I0623 12:21:42.493996  5064 solver.cpp:473] Iteration 2560, lr = 0.0001
I0623 12:21:43.515194  5064 solver.cpp:213] Iteration 2570, loss = 3.85226
I0623 12:21:43.515210  5064 solver.cpp:228]     Train net output #0: softmax = 3.85226 (* 1 = 3.85226 loss)
I0623 12:21:43.515215  5064 solver.cpp:473] Iteration 2570, lr = 0.0001
I0623 12:21:44.535279  5064 solver.cpp:213] Iteration 2580, loss = 3.66113
I0623 12:21:44.535296  5064 solver.cpp:228]     Train net output #0: softmax = 3.66113 (* 1 = 3.66113 loss)
I0623 12:21:44.535301  5064 solver.cpp:473] Iteration 2580, lr = 0.0001
I0623 12:21:45.556429  5064 solver.cpp:213] Iteration 2590, loss = 3.57762
I0623 12:21:45.556447  5064 solver.cpp:228]     Train net output #0: softmax = 3.57762 (* 1 = 3.57762 loss)
I0623 12:21:45.556452  5064 solver.cpp:473] Iteration 2590, lr = 0.0001
I0623 12:21:46.576381  5064 solver.cpp:213] Iteration 2600, loss = 3.66654
I0623 12:21:46.576401  5064 solver.cpp:228]     Train net output #0: softmax = 3.66654 (* 1 = 3.66654 loss)
I0623 12:21:46.576525  5064 solver.cpp:473] Iteration 2600, lr = 0.0001
I0623 12:21:47.596212  5064 solver.cpp:213] Iteration 2610, loss = 3.69853
I0623 12:21:47.596228  5064 solver.cpp:228]     Train net output #0: softmax = 3.69853 (* 1 = 3.69853 loss)
I0623 12:21:47.596233  5064 solver.cpp:473] Iteration 2610, lr = 0.0001
I0623 12:21:48.617265  5064 solver.cpp:213] Iteration 2620, loss = 3.49894
I0623 12:21:48.617282  5064 solver.cpp:228]     Train net output #0: softmax = 3.49894 (* 1 = 3.49894 loss)
I0623 12:21:48.617293  5064 solver.cpp:473] Iteration 2620, lr = 0.0001
I0623 12:21:49.637673  5064 solver.cpp:213] Iteration 2630, loss = 3.51638
I0623 12:21:49.637689  5064 solver.cpp:228]     Train net output #0: softmax = 3.51638 (* 1 = 3.51638 loss)
I0623 12:21:49.637694  5064 solver.cpp:473] Iteration 2630, lr = 0.0001
I0623 12:21:50.658483  5064 solver.cpp:213] Iteration 2640, loss = 3.8045
I0623 12:21:50.658498  5064 solver.cpp:228]     Train net output #0: softmax = 3.8045 (* 1 = 3.8045 loss)
I0623 12:21:50.658502  5064 solver.cpp:473] Iteration 2640, lr = 0.0001
I0623 12:21:51.679172  5064 solver.cpp:213] Iteration 2650, loss = 3.87989
I0623 12:21:51.679194  5064 solver.cpp:228]     Train net output #0: softmax = 3.87989 (* 1 = 3.87989 loss)
I0623 12:21:51.679309  5064 solver.cpp:473] Iteration 2650, lr = 0.0001
I0623 12:21:52.699795  5064 solver.cpp:213] Iteration 2660, loss = 3.97238
I0623 12:21:52.699810  5064 solver.cpp:228]     Train net output #0: softmax = 3.97238 (* 1 = 3.97238 loss)
I0623 12:21:52.699815  5064 solver.cpp:473] Iteration 2660, lr = 0.0001
I0623 12:21:53.720952  5064 solver.cpp:213] Iteration 2670, loss = 3.72056
I0623 12:21:53.720966  5064 solver.cpp:228]     Train net output #0: softmax = 3.72056 (* 1 = 3.72056 loss)
I0623 12:21:53.720971  5064 solver.cpp:473] Iteration 2670, lr = 0.0001
I0623 12:21:54.741575  5064 solver.cpp:213] Iteration 2680, loss = 3.65651
I0623 12:21:54.741590  5064 solver.cpp:228]     Train net output #0: softmax = 3.65651 (* 1 = 3.65651 loss)
I0623 12:21:54.741595  5064 solver.cpp:473] Iteration 2680, lr = 0.0001
I0623 12:21:55.761674  5064 solver.cpp:213] Iteration 2690, loss = 3.68422
I0623 12:21:55.761692  5064 solver.cpp:228]     Train net output #0: softmax = 3.68422 (* 1 = 3.68422 loss)
I0623 12:21:55.761696  5064 solver.cpp:473] Iteration 2690, lr = 0.0001
I0623 12:21:56.782435  5064 solver.cpp:213] Iteration 2700, loss = 3.77273
I0623 12:21:56.782600  5064 solver.cpp:228]     Train net output #0: softmax = 3.77273 (* 1 = 3.77273 loss)
I0623 12:21:56.782608  5064 solver.cpp:473] Iteration 2700, lr = 0.0001
I0623 12:21:57.802644  5064 solver.cpp:213] Iteration 2710, loss = 3.7248
I0623 12:21:57.802660  5064 solver.cpp:228]     Train net output #0: softmax = 3.7248 (* 1 = 3.7248 loss)
I0623 12:21:57.802665  5064 solver.cpp:473] Iteration 2710, lr = 0.0001
I0623 12:21:58.823572  5064 solver.cpp:213] Iteration 2720, loss = 3.7779
I0623 12:21:58.823592  5064 solver.cpp:228]     Train net output #0: softmax = 3.7779 (* 1 = 3.7779 loss)
I0623 12:21:58.823597  5064 solver.cpp:473] Iteration 2720, lr = 0.0001
I0623 12:21:59.844233  5064 solver.cpp:213] Iteration 2730, loss = 3.68969
I0623 12:21:59.844249  5064 solver.cpp:228]     Train net output #0: softmax = 3.68969 (* 1 = 3.68969 loss)
I0623 12:21:59.844255  5064 solver.cpp:473] Iteration 2730, lr = 0.0001
I0623 12:22:00.865067  5064 solver.cpp:213] Iteration 2740, loss = 3.54359
I0623 12:22:00.865083  5064 solver.cpp:228]     Train net output #0: softmax = 3.54359 (* 1 = 3.54359 loss)
I0623 12:22:00.865088  5064 solver.cpp:473] Iteration 2740, lr = 0.0001
I0623 12:22:01.885550  5064 solver.cpp:213] Iteration 2750, loss = 3.5493
I0623 12:22:01.885570  5064 solver.cpp:228]     Train net output #0: softmax = 3.5493 (* 1 = 3.5493 loss)
I0623 12:22:01.885711  5064 solver.cpp:473] Iteration 2750, lr = 0.0001
I0623 12:22:02.906045  5064 solver.cpp:213] Iteration 2760, loss = 3.59737
I0623 12:22:02.906061  5064 solver.cpp:228]     Train net output #0: softmax = 3.59737 (* 1 = 3.59737 loss)
I0623 12:22:02.906066  5064 solver.cpp:473] Iteration 2760, lr = 0.0001
I0623 12:22:03.926476  5064 solver.cpp:213] Iteration 2770, loss = 3.70409
I0623 12:22:03.926496  5064 solver.cpp:228]     Train net output #0: softmax = 3.70409 (* 1 = 3.70409 loss)
I0623 12:22:03.926501  5064 solver.cpp:473] Iteration 2770, lr = 0.0001
I0623 12:22:04.947229  5064 solver.cpp:213] Iteration 2780, loss = 3.51528
I0623 12:22:04.947247  5064 solver.cpp:228]     Train net output #0: softmax = 3.51528 (* 1 = 3.51528 loss)
I0623 12:22:04.947258  5064 solver.cpp:473] Iteration 2780, lr = 0.0001
I0623 12:22:05.967612  5064 solver.cpp:213] Iteration 2790, loss = 3.56094
I0623 12:22:05.967629  5064 solver.cpp:228]     Train net output #0: softmax = 3.56094 (* 1 = 3.56094 loss)
I0623 12:22:05.967634  5064 solver.cpp:473] Iteration 2790, lr = 0.0001
I0623 12:22:06.988450  5064 solver.cpp:213] Iteration 2800, loss = 3.60683
I0623 12:22:06.988468  5064 solver.cpp:228]     Train net output #0: softmax = 3.60683 (* 1 = 3.60683 loss)
I0623 12:22:06.988589  5064 solver.cpp:473] Iteration 2800, lr = 0.0001
I0623 12:22:08.008816  5064 solver.cpp:213] Iteration 2810, loss = 3.60519
I0623 12:22:08.008832  5064 solver.cpp:228]     Train net output #0: softmax = 3.60519 (* 1 = 3.60519 loss)
I0623 12:22:08.008837  5064 solver.cpp:473] Iteration 2810, lr = 0.0001
I0623 12:22:09.028586  5064 solver.cpp:213] Iteration 2820, loss = 3.71081
I0623 12:22:09.028604  5064 solver.cpp:228]     Train net output #0: softmax = 3.71081 (* 1 = 3.71081 loss)
I0623 12:22:09.028610  5064 solver.cpp:473] Iteration 2820, lr = 0.0001
I0623 12:22:10.048660  5064 solver.cpp:213] Iteration 2830, loss = 3.78046
I0623 12:22:10.048678  5064 solver.cpp:228]     Train net output #0: softmax = 3.78046 (* 1 = 3.78046 loss)
I0623 12:22:10.048683  5064 solver.cpp:473] Iteration 2830, lr = 0.0001
I0623 12:22:11.068920  5064 solver.cpp:213] Iteration 2840, loss = 3.41757
I0623 12:22:11.068936  5064 solver.cpp:228]     Train net output #0: softmax = 3.41757 (* 1 = 3.41757 loss)
I0623 12:22:11.068941  5064 solver.cpp:473] Iteration 2840, lr = 0.0001
I0623 12:22:12.088794  5064 solver.cpp:213] Iteration 2850, loss = 3.51107
I0623 12:22:12.088814  5064 solver.cpp:228]     Train net output #0: softmax = 3.51107 (* 1 = 3.51107 loss)
I0623 12:22:12.088930  5064 solver.cpp:473] Iteration 2850, lr = 0.0001
I0623 12:22:13.108842  5064 solver.cpp:213] Iteration 2860, loss = 3.74135
I0623 12:22:13.108860  5064 solver.cpp:228]     Train net output #0: softmax = 3.74135 (* 1 = 3.74135 loss)
I0623 12:22:13.108881  5064 solver.cpp:473] Iteration 2860, lr = 0.0001
I0623 12:22:14.129258  5064 solver.cpp:213] Iteration 2870, loss = 3.75096
I0623 12:22:14.129274  5064 solver.cpp:228]     Train net output #0: softmax = 3.75096 (* 1 = 3.75096 loss)
I0623 12:22:14.129279  5064 solver.cpp:473] Iteration 2870, lr = 0.0001
I0623 12:22:15.149750  5064 solver.cpp:213] Iteration 2880, loss = 3.61192
I0623 12:22:15.149766  5064 solver.cpp:228]     Train net output #0: softmax = 3.61192 (* 1 = 3.61192 loss)
I0623 12:22:15.149771  5064 solver.cpp:473] Iteration 2880, lr = 0.0001
I0623 12:22:16.170660  5064 solver.cpp:213] Iteration 2890, loss = 3.66764
I0623 12:22:16.170680  5064 solver.cpp:228]     Train net output #0: softmax = 3.66764 (* 1 = 3.66764 loss)
I0623 12:22:16.170684  5064 solver.cpp:473] Iteration 2890, lr = 0.0001
I0623 12:22:17.190646  5064 solver.cpp:213] Iteration 2900, loss = 3.78872
I0623 12:22:17.190665  5064 solver.cpp:228]     Train net output #0: softmax = 3.78872 (* 1 = 3.78872 loss)
I0623 12:22:17.190794  5064 solver.cpp:473] Iteration 2900, lr = 0.0001
I0623 12:22:18.211719  5064 solver.cpp:213] Iteration 2910, loss = 3.66998
I0623 12:22:18.211737  5064 solver.cpp:228]     Train net output #0: softmax = 3.66998 (* 1 = 3.66998 loss)
I0623 12:22:18.211742  5064 solver.cpp:473] Iteration 2910, lr = 0.0001
I0623 12:22:19.232199  5064 solver.cpp:213] Iteration 2920, loss = 3.61824
I0623 12:22:19.232218  5064 solver.cpp:228]     Train net output #0: softmax = 3.61824 (* 1 = 3.61824 loss)
I0623 12:22:19.232223  5064 solver.cpp:473] Iteration 2920, lr = 0.0001
I0623 12:22:20.252728  5064 solver.cpp:213] Iteration 2930, loss = 3.62601
I0623 12:22:20.252753  5064 solver.cpp:228]     Train net output #0: softmax = 3.62601 (* 1 = 3.62601 loss)
I0623 12:22:20.252758  5064 solver.cpp:473] Iteration 2930, lr = 0.0001
I0623 12:22:21.273597  5064 solver.cpp:213] Iteration 2940, loss = 3.68974
I0623 12:22:21.273613  5064 solver.cpp:228]     Train net output #0: softmax = 3.68974 (* 1 = 3.68974 loss)
I0623 12:22:21.273624  5064 solver.cpp:473] Iteration 2940, lr = 0.0001
I0623 12:22:22.294100  5064 solver.cpp:213] Iteration 2950, loss = 3.40167
I0623 12:22:22.294121  5064 solver.cpp:228]     Train net output #0: softmax = 3.40167 (* 1 = 3.40167 loss)
I0623 12:22:22.294241  5064 solver.cpp:473] Iteration 2950, lr = 0.0001
I0623 12:22:23.314751  5064 solver.cpp:213] Iteration 2960, loss = 3.72328
I0623 12:22:23.314771  5064 solver.cpp:228]     Train net output #0: softmax = 3.72328 (* 1 = 3.72328 loss)
I0623 12:22:23.314776  5064 solver.cpp:473] Iteration 2960, lr = 0.0001
I0623 12:22:24.335780  5064 solver.cpp:213] Iteration 2970, loss = 3.60175
I0623 12:22:24.335798  5064 solver.cpp:228]     Train net output #0: softmax = 3.60175 (* 1 = 3.60175 loss)
I0623 12:22:24.335803  5064 solver.cpp:473] Iteration 2970, lr = 0.0001
I0623 12:22:25.356181  5064 solver.cpp:213] Iteration 2980, loss = 3.45696
I0623 12:22:25.356197  5064 solver.cpp:228]     Train net output #0: softmax = 3.45696 (* 1 = 3.45696 loss)
I0623 12:22:25.356202  5064 solver.cpp:473] Iteration 2980, lr = 0.0001
I0623 12:22:26.377809  5064 solver.cpp:213] Iteration 2990, loss = 3.51281
I0623 12:22:26.377825  5064 solver.cpp:228]     Train net output #0: softmax = 3.51281 (* 1 = 3.51281 loss)
I0623 12:22:26.377830  5064 solver.cpp:473] Iteration 2990, lr = 0.0001
I0623 12:22:27.327953  5064 solver.cpp:362] Snapshotting to snapshots/16-06-21_16h05m31s_0_11_pretrainClassification_iter_3000.caffemodel
I0623 12:22:27.329124  5064 solver.cpp:370] Snapshotting solver state to snapshots/16-06-21_16h05m31s_0_11_pretrainClassification_iter_3000.solverstate
I0623 12:22:27.329730  5064 solver.cpp:291] Iteration 3000, Testing net (#0)
I0623 12:22:27.490638  5064 solver.cpp:342]     Test net output #0: accuracy_top_1 = 0.176563
I0623 12:22:27.490651  5064 solver.cpp:342]     Test net output #1: accuracy_top_5 = 0.435937
I0623 12:22:27.490658  5064 solver.cpp:342]     Test net output #2: softmax = 3.49149 (* 1 = 3.49149 loss)
I0623 12:22:27.561588  5064 solver.cpp:213] Iteration 3000, loss = 3.653
I0623 12:22:27.561602  5064 solver.cpp:228]     Train net output #0: softmax = 3.653 (* 1 = 3.653 loss)
I0623 12:22:27.561607  5064 solver.cpp:473] Iteration 3000, lr = 0.0001
I0623 12:22:28.582878  5064 solver.cpp:213] Iteration 3010, loss = 3.42062
I0623 12:22:28.582906  5064 solver.cpp:228]     Train net output #0: softmax = 3.42062 (* 1 = 3.42062 loss)
I0623 12:22:28.582911  5064 solver.cpp:473] Iteration 3010, lr = 0.0001
I0623 12:22:29.603531  5064 solver.cpp:213] Iteration 3020, loss = 3.58282
I0623 12:22:29.603551  5064 solver.cpp:228]     Train net output #0: softmax = 3.58282 (* 1 = 3.58282 loss)
I0623 12:22:29.603556  5064 solver.cpp:473] Iteration 3020, lr = 0.0001
I0623 12:22:30.623597  5064 solver.cpp:213] Iteration 3030, loss = 3.75081
I0623 12:22:30.623615  5064 solver.cpp:228]     Train net output #0: softmax = 3.75081 (* 1 = 3.75081 loss)
I0623 12:22:30.623620  5064 solver.cpp:473] Iteration 3030, lr = 0.0001
I0623 12:22:31.644047  5064 solver.cpp:213] Iteration 3040, loss = 3.51711
I0623 12:22:31.644063  5064 solver.cpp:228]     Train net output #0: softmax = 3.51711 (* 1 = 3.51711 loss)
I0623 12:22:31.644068  5064 solver.cpp:473] Iteration 3040, lr = 0.0001
I0623 12:22:32.664513  5064 solver.cpp:213] Iteration 3050, loss = 3.68354
I0623 12:22:32.664531  5064 solver.cpp:228]     Train net output #0: softmax = 3.68354 (* 1 = 3.68354 loss)
I0623 12:22:32.664536  5064 solver.cpp:473] Iteration 3050, lr = 0.0001
I0623 12:22:33.685195  5064 solver.cpp:213] Iteration 3060, loss = 3.6371
I0623 12:22:33.685212  5064 solver.cpp:228]     Train net output #0: softmax = 3.6371 (* 1 = 3.6371 loss)
I0623 12:22:33.685219  5064 solver.cpp:473] Iteration 3060, lr = 0.0001
I0623 12:22:34.705090  5064 solver.cpp:213] Iteration 3070, loss = 3.4238
I0623 12:22:34.705109  5064 solver.cpp:228]     Train net output #0: softmax = 3.4238 (* 1 = 3.4238 loss)
I0623 12:22:34.705116  5064 solver.cpp:473] Iteration 3070, lr = 0.0001
I0623 12:22:35.725595  5064 solver.cpp:213] Iteration 3080, loss = 3.5028
I0623 12:22:35.725620  5064 solver.cpp:228]     Train net output #0: softmax = 3.5028 (* 1 = 3.5028 loss)
I0623 12:22:35.725625  5064 solver.cpp:473] Iteration 3080, lr = 0.0001
I0623 12:22:36.746171  5064 solver.cpp:213] Iteration 3090, loss = 3.61952
I0623 12:22:36.746188  5064 solver.cpp:228]     Train net output #0: softmax = 3.61952 (* 1 = 3.61952 loss)
I0623 12:22:36.746193  5064 solver.cpp:473] Iteration 3090, lr = 0.0001
I0623 12:22:37.766374  5064 solver.cpp:213] Iteration 3100, loss = 3.80341
I0623 12:22:37.766394  5064 solver.cpp:228]     Train net output #0: softmax = 3.80341 (* 1 = 3.80341 loss)
I0623 12:22:37.766512  5064 solver.cpp:473] Iteration 3100, lr = 0.0001
I0623 12:22:38.787005  5064 solver.cpp:213] Iteration 3110, loss = 3.61624
I0623 12:22:38.787021  5064 solver.cpp:228]     Train net output #0: softmax = 3.61624 (* 1 = 3.61624 loss)
I0623 12:22:38.787026  5064 solver.cpp:473] Iteration 3110, lr = 0.0001
I0623 12:22:39.807953  5064 solver.cpp:213] Iteration 3120, loss = 3.70612
I0623 12:22:39.807971  5064 solver.cpp:228]     Train net output #0: softmax = 3.70612 (* 1 = 3.70612 loss)
I0623 12:22:39.807976  5064 solver.cpp:473] Iteration 3120, lr = 0.0001
I0623 12:22:40.828374  5064 solver.cpp:213] Iteration 3130, loss = 3.76871
I0623 12:22:40.828392  5064 solver.cpp:228]     Train net output #0: softmax = 3.76871 (* 1 = 3.76871 loss)
I0623 12:22:40.828397  5064 solver.cpp:473] Iteration 3130, lr = 0.0001
I0623 12:22:41.849068  5064 solver.cpp:213] Iteration 3140, loss = 3.7477
I0623 12:22:41.849102  5064 solver.cpp:228]     Train net output #0: softmax = 3.7477 (* 1 = 3.7477 loss)
I0623 12:22:41.849107  5064 solver.cpp:473] Iteration 3140, lr = 0.0001
I0623 12:22:42.869774  5064 solver.cpp:213] Iteration 3150, loss = 3.46439
I0623 12:22:42.869793  5064 solver.cpp:228]     Train net output #0: softmax = 3.46439 (* 1 = 3.46439 loss)
I0623 12:22:42.869925  5064 solver.cpp:473] Iteration 3150, lr = 0.0001
I0623 12:22:43.890743  5064 solver.cpp:213] Iteration 3160, loss = 3.70374
I0623 12:22:43.890758  5064 solver.cpp:228]     Train net output #0: softmax = 3.70374 (* 1 = 3.70374 loss)
I0623 12:22:43.890763  5064 solver.cpp:473] Iteration 3160, lr = 0.0001
I0623 12:22:44.911592  5064 solver.cpp:213] Iteration 3170, loss = 3.66963
I0623 12:22:44.911609  5064 solver.cpp:228]     Train net output #0: softmax = 3.66963 (* 1 = 3.66963 loss)
I0623 12:22:44.911615  5064 solver.cpp:473] Iteration 3170, lr = 0.0001
I0623 12:22:45.932030  5064 solver.cpp:213] Iteration 3180, loss = 3.49959
I0623 12:22:45.932047  5064 solver.cpp:228]     Train net output #0: softmax = 3.49959 (* 1 = 3.49959 loss)
I0623 12:22:45.932052  5064 solver.cpp:473] Iteration 3180, lr = 0.0001
I0623 12:22:46.952828  5064 solver.cpp:213] Iteration 3190, loss = 3.54033
I0623 12:22:46.952843  5064 solver.cpp:228]     Train net output #0: softmax = 3.54033 (* 1 = 3.54033 loss)
I0623 12:22:46.952848  5064 solver.cpp:473] Iteration 3190, lr = 0.0001
I0623 12:22:47.973520  5064 solver.cpp:213] Iteration 3200, loss = 3.45578
I0623 12:22:47.973539  5064 solver.cpp:228]     Train net output #0: softmax = 3.45578 (* 1 = 3.45578 loss)
I0623 12:22:47.973686  5064 solver.cpp:473] Iteration 3200, lr = 0.0001
I0623 12:22:48.994237  5064 solver.cpp:213] Iteration 3210, loss = 3.54014
I0623 12:22:48.994254  5064 solver.cpp:228]     Train net output #0: softmax = 3.54014 (* 1 = 3.54014 loss)
I0623 12:22:48.994259  5064 solver.cpp:473] Iteration 3210, lr = 0.0001
I0623 12:22:50.014987  5064 solver.cpp:213] Iteration 3220, loss = 3.76493
I0623 12:22:50.015005  5064 solver.cpp:228]     Train net output #0: softmax = 3.76493 (* 1 = 3.76493 loss)
I0623 12:22:50.015010  5064 solver.cpp:473] Iteration 3220, lr = 0.0001
I0623 12:22:51.035854  5064 solver.cpp:213] Iteration 3230, loss = 3.49088
I0623 12:22:51.035871  5064 solver.cpp:228]     Train net output #0: softmax = 3.49088 (* 1 = 3.49088 loss)
I0623 12:22:51.035876  5064 solver.cpp:473] Iteration 3230, lr = 0.0001
I0623 12:22:52.056210  5064 solver.cpp:213] Iteration 3240, loss = 3.46254
I0623 12:22:52.056232  5064 solver.cpp:228]     Train net output #0: softmax = 3.46254 (* 1 = 3.46254 loss)
I0623 12:22:52.056237  5064 solver.cpp:473] Iteration 3240, lr = 0.0001
I0623 12:22:53.076514  5064 solver.cpp:213] Iteration 3250, loss = 3.74279
I0623 12:22:53.076534  5064 solver.cpp:228]     Train net output #0: softmax = 3.74279 (* 1 = 3.74279 loss)
I0623 12:22:53.076656  5064 solver.cpp:473] Iteration 3250, lr = 0.0001
I0623 12:22:54.097273  5064 solver.cpp:213] Iteration 3260, loss = 3.69524
I0623 12:22:54.097290  5064 solver.cpp:228]     Train net output #0: softmax = 3.69524 (* 1 = 3.69524 loss)
I0623 12:22:54.097295  5064 solver.cpp:473] Iteration 3260, lr = 0.0001
I0623 12:22:55.117525  5064 solver.cpp:213] Iteration 3270, loss = 3.53551
I0623 12:22:55.117540  5064 solver.cpp:228]     Train net output #0: softmax = 3.53551 (* 1 = 3.53551 loss)
I0623 12:22:55.117545  5064 solver.cpp:473] Iteration 3270, lr = 0.0001
I0623 12:22:56.138077  5064 solver.cpp:213] Iteration 3280, loss = 3.50977
I0623 12:22:56.138095  5064 solver.cpp:228]     Train net output #0: softmax = 3.50977 (* 1 = 3.50977 loss)
I0623 12:22:56.138100  5064 solver.cpp:473] Iteration 3280, lr = 0.0001
I0623 12:22:57.158737  5064 solver.cpp:213] Iteration 3290, loss = 3.62037
I0623 12:22:57.158754  5064 solver.cpp:228]     Train net output #0: softmax = 3.62037 (* 1 = 3.62037 loss)
I0623 12:22:57.158759  5064 solver.cpp:473] Iteration 3290, lr = 0.0001
I0623 12:22:58.179420  5064 solver.cpp:213] Iteration 3300, loss = 3.72043
I0623 12:22:58.179584  5064 solver.cpp:228]     Train net output #0: softmax = 3.72043 (* 1 = 3.72043 loss)
I0623 12:22:58.179592  5064 solver.cpp:473] Iteration 3300, lr = 0.0001
I0623 12:22:59.199849  5064 solver.cpp:213] Iteration 3310, loss = 3.6622
I0623 12:22:59.199867  5064 solver.cpp:228]     Train net output #0: softmax = 3.6622 (* 1 = 3.6622 loss)
I0623 12:22:59.199872  5064 solver.cpp:473] Iteration 3310, lr = 0.0001
I0623 12:23:00.220868  5064 solver.cpp:213] Iteration 3320, loss = 3.54875
I0623 12:23:00.220885  5064 solver.cpp:228]     Train net output #0: softmax = 3.54875 (* 1 = 3.54875 loss)
I0623 12:23:00.220891  5064 solver.cpp:473] Iteration 3320, lr = 0.0001
I0623 12:23:01.241649  5064 solver.cpp:213] Iteration 3330, loss = 3.503
I0623 12:23:01.241665  5064 solver.cpp:228]     Train net output #0: softmax = 3.503 (* 1 = 3.503 loss)
I0623 12:23:01.241670  5064 solver.cpp:473] Iteration 3330, lr = 0.0001
I0623 12:23:02.262374  5064 solver.cpp:213] Iteration 3340, loss = 3.57038
I0623 12:23:02.262392  5064 solver.cpp:228]     Train net output #0: softmax = 3.57038 (* 1 = 3.57038 loss)
I0623 12:23:02.262397  5064 solver.cpp:473] Iteration 3340, lr = 0.0001
I0623 12:23:03.283083  5064 solver.cpp:213] Iteration 3350, loss = 3.36177
I0623 12:23:03.283105  5064 solver.cpp:228]     Train net output #0: softmax = 3.36177 (* 1 = 3.36177 loss)
I0623 12:23:03.283259  5064 solver.cpp:473] Iteration 3350, lr = 0.0001
I0623 12:23:04.303477  5064 solver.cpp:213] Iteration 3360, loss = 3.41362
I0623 12:23:04.303493  5064 solver.cpp:228]     Train net output #0: softmax = 3.41362 (* 1 = 3.41362 loss)
I0623 12:23:04.303498  5064 solver.cpp:473] Iteration 3360, lr = 0.0001
I0623 12:23:05.324009  5064 solver.cpp:213] Iteration 3370, loss = 3.50245
I0623 12:23:05.324025  5064 solver.cpp:228]     Train net output #0: softmax = 3.50245 (* 1 = 3.50245 loss)
I0623 12:23:05.324031  5064 solver.cpp:473] Iteration 3370, lr = 0.0001
I0623 12:23:06.344743  5064 solver.cpp:213] Iteration 3380, loss = 3.54132
I0623 12:23:06.344759  5064 solver.cpp:228]     Train net output #0: softmax = 3.54132 (* 1 = 3.54132 loss)
I0623 12:23:06.344764  5064 solver.cpp:473] Iteration 3380, lr = 0.0001
I0623 12:23:07.365607  5064 solver.cpp:213] Iteration 3390, loss = 3.56706
I0623 12:23:07.365623  5064 solver.cpp:228]     Train net output #0: softmax = 3.56706 (* 1 = 3.56706 loss)
I0623 12:23:07.365628  5064 solver.cpp:473] Iteration 3390, lr = 0.0001
I0623 12:23:08.386070  5064 solver.cpp:213] Iteration 3400, loss = 3.62426
I0623 12:23:08.386096  5064 solver.cpp:228]     Train net output #0: softmax = 3.62426 (* 1 = 3.62426 loss)
I0623 12:23:08.386101  5064 solver.cpp:473] Iteration 3400, lr = 0.0001
I0623 12:23:09.406167  5064 solver.cpp:213] Iteration 3410, loss = 3.45142
I0623 12:23:09.406184  5064 solver.cpp:228]     Train net output #0: softmax = 3.45142 (* 1 = 3.45142 loss)
I0623 12:23:09.406189  5064 solver.cpp:473] Iteration 3410, lr = 0.0001
I0623 12:23:10.426216  5064 solver.cpp:213] Iteration 3420, loss = 3.4567
I0623 12:23:10.426234  5064 solver.cpp:228]     Train net output #0: softmax = 3.4567 (* 1 = 3.4567 loss)
I0623 12:23:10.426239  5064 solver.cpp:473] Iteration 3420, lr = 0.0001
I0623 12:23:11.446847  5064 solver.cpp:213] Iteration 3430, loss = 3.51254
I0623 12:23:11.446864  5064 solver.cpp:228]     Train net output #0: softmax = 3.51254 (* 1 = 3.51254 loss)
I0623 12:23:11.446869  5064 solver.cpp:473] Iteration 3430, lr = 0.0001
I0623 12:23:12.466815  5064 solver.cpp:213] Iteration 3440, loss = 3.55746
I0623 12:23:12.466835  5064 solver.cpp:228]     Train net output #0: softmax = 3.55746 (* 1 = 3.55746 loss)
I0623 12:23:12.466840  5064 solver.cpp:473] Iteration 3440, lr = 0.0001
I0623 12:23:13.487889  5064 solver.cpp:213] Iteration 3450, loss = 3.62879
I0623 12:23:13.487910  5064 solver.cpp:228]     Train net output #0: softmax = 3.62879 (* 1 = 3.62879 loss)
I0623 12:23:13.488029  5064 solver.cpp:473] Iteration 3450, lr = 0.0001
I0623 12:23:14.508456  5064 solver.cpp:213] Iteration 3460, loss = 3.4788
I0623 12:23:14.508473  5064 solver.cpp:228]     Train net output #0: softmax = 3.4788 (* 1 = 3.4788 loss)
I0623 12:23:14.508493  5064 solver.cpp:473] Iteration 3460, lr = 0.0001
I0623 12:23:15.528779  5064 solver.cpp:213] Iteration 3470, loss = 3.4617
I0623 12:23:15.528795  5064 solver.cpp:228]     Train net output #0: softmax = 3.4617 (* 1 = 3.4617 loss)
I0623 12:23:15.528800  5064 solver.cpp:473] Iteration 3470, lr = 0.0001
I0623 12:23:16.548941  5064 solver.cpp:213] Iteration 3480, loss = 3.62005
I0623 12:23:16.548957  5064 solver.cpp:228]     Train net output #0: softmax = 3.62005 (* 1 = 3.62005 loss)
I0623 12:23:16.548962  5064 solver.cpp:473] Iteration 3480, lr = 0.0001
I0623 12:23:17.569603  5064 solver.cpp:213] Iteration 3490, loss = 3.5555
I0623 12:23:17.569620  5064 solver.cpp:228]     Train net output #0: softmax = 3.5555 (* 1 = 3.5555 loss)
I0623 12:23:17.569625  5064 solver.cpp:473] Iteration 3490, lr = 0.0001
I0623 12:23:18.590436  5064 solver.cpp:213] Iteration 3500, loss = 3.64042
I0623 12:23:18.590458  5064 solver.cpp:228]     Train net output #0: softmax = 3.64042 (* 1 = 3.64042 loss)
I0623 12:23:18.590586  5064 solver.cpp:473] Iteration 3500, lr = 0.0001
I0623 12:23:19.611318  5064 solver.cpp:213] Iteration 3510, loss = 3.55603
I0623 12:23:19.611348  5064 solver.cpp:228]     Train net output #0: softmax = 3.55603 (* 1 = 3.55603 loss)
I0623 12:23:19.611354  5064 solver.cpp:473] Iteration 3510, lr = 0.0001
I0623 12:23:20.631994  5064 solver.cpp:213] Iteration 3520, loss = 3.81295
I0623 12:23:20.632012  5064 solver.cpp:228]     Train net output #0: softmax = 3.81295 (* 1 = 3.81295 loss)
I0623 12:23:20.632017  5064 solver.cpp:473] Iteration 3520, lr = 0.0001
I0623 12:23:21.652818  5064 solver.cpp:213] Iteration 3530, loss = 3.7192
I0623 12:23:21.652835  5064 solver.cpp:228]     Train net output #0: softmax = 3.7192 (* 1 = 3.7192 loss)
I0623 12:23:21.652840  5064 solver.cpp:473] Iteration 3530, lr = 0.0001
I0623 12:23:22.673944  5064 solver.cpp:213] Iteration 3540, loss = 3.3773
I0623 12:23:22.673962  5064 solver.cpp:228]     Train net output #0: softmax = 3.3773 (* 1 = 3.3773 loss)
I0623 12:23:22.673967  5064 solver.cpp:473] Iteration 3540, lr = 0.0001
I0623 12:23:23.694274  5064 solver.cpp:213] Iteration 3550, loss = 3.52579
I0623 12:23:23.694298  5064 solver.cpp:228]     Train net output #0: softmax = 3.52579 (* 1 = 3.52579 loss)
I0623 12:23:23.694425  5064 solver.cpp:473] Iteration 3550, lr = 0.0001
I0623 12:23:24.714799  5064 solver.cpp:213] Iteration 3560, loss = 3.59783
I0623 12:23:24.714817  5064 solver.cpp:228]     Train net output #0: softmax = 3.59783 (* 1 = 3.59783 loss)
I0623 12:23:24.714828  5064 solver.cpp:473] Iteration 3560, lr = 0.0001
I0623 12:23:25.735633  5064 solver.cpp:213] Iteration 3570, loss = 3.30109
I0623 12:23:25.735649  5064 solver.cpp:228]     Train net output #0: softmax = 3.30109 (* 1 = 3.30109 loss)
I0623 12:23:25.735654  5064 solver.cpp:473] Iteration 3570, lr = 0.0001
I0623 12:23:26.756105  5064 solver.cpp:213] Iteration 3580, loss = 3.49239
I0623 12:23:26.756122  5064 solver.cpp:228]     Train net output #0: softmax = 3.49239 (* 1 = 3.49239 loss)
I0623 12:23:26.756127  5064 solver.cpp:473] Iteration 3580, lr = 0.0001
I0623 12:23:27.776790  5064 solver.cpp:213] Iteration 3590, loss = 3.3043
I0623 12:23:27.776808  5064 solver.cpp:228]     Train net output #0: softmax = 3.3043 (* 1 = 3.3043 loss)
I0623 12:23:27.776813  5064 solver.cpp:473] Iteration 3590, lr = 0.0001
I0623 12:23:28.797963  5064 solver.cpp:213] Iteration 3600, loss = 3.4636
I0623 12:23:28.798125  5064 solver.cpp:228]     Train net output #0: softmax = 3.4636 (* 1 = 3.4636 loss)
I0623 12:23:28.798133  5064 solver.cpp:473] Iteration 3600, lr = 0.0001
I0623 12:23:29.817756  5064 solver.cpp:213] Iteration 3610, loss = 3.76574
I0623 12:23:29.817772  5064 solver.cpp:228]     Train net output #0: softmax = 3.76574 (* 1 = 3.76574 loss)
I0623 12:23:29.817778  5064 solver.cpp:473] Iteration 3610, lr = 0.0001
I0623 12:23:30.838291  5064 solver.cpp:213] Iteration 3620, loss = 3.71614
I0623 12:23:30.838306  5064 solver.cpp:228]     Train net output #0: softmax = 3.71614 (* 1 = 3.71614 loss)
I0623 12:23:30.838311  5064 solver.cpp:473] Iteration 3620, lr = 0.0001
I0623 12:23:31.858090  5064 solver.cpp:213] Iteration 3630, loss = 3.69002
I0623 12:23:31.858108  5064 solver.cpp:228]     Train net output #0: softmax = 3.69002 (* 1 = 3.69002 loss)
I0623 12:23:31.858113  5064 solver.cpp:473] Iteration 3630, lr = 0.0001
I0623 12:23:32.879207  5064 solver.cpp:213] Iteration 3640, loss = 3.61432
I0623 12:23:32.879223  5064 solver.cpp:228]     Train net output #0: softmax = 3.61432 (* 1 = 3.61432 loss)
I0623 12:23:32.879228  5064 solver.cpp:473] Iteration 3640, lr = 0.0001
I0623 12:23:33.899824  5064 solver.cpp:213] Iteration 3650, loss = 3.57613
I0623 12:23:33.899842  5064 solver.cpp:228]     Train net output #0: softmax = 3.57613 (* 1 = 3.57613 loss)
I0623 12:23:33.899987  5064 solver.cpp:473] Iteration 3650, lr = 0.0001
I0623 12:23:34.920728  5064 solver.cpp:213] Iteration 3660, loss = 3.55451
I0623 12:23:34.920744  5064 solver.cpp:228]     Train net output #0: softmax = 3.55451 (* 1 = 3.55451 loss)
I0623 12:23:34.920748  5064 solver.cpp:473] Iteration 3660, lr = 0.0001
I0623 12:23:35.941284  5064 solver.cpp:213] Iteration 3670, loss = 3.49057
I0623 12:23:35.941303  5064 solver.cpp:228]     Train net output #0: softmax = 3.49057 (* 1 = 3.49057 loss)
I0623 12:23:35.941308  5064 solver.cpp:473] Iteration 3670, lr = 0.0001
I0623 12:23:36.962437  5064 solver.cpp:213] Iteration 3680, loss = 3.48615
I0623 12:23:36.962455  5064 solver.cpp:228]     Train net output #0: softmax = 3.48615 (* 1 = 3.48615 loss)
I0623 12:23:36.962458  5064 solver.cpp:473] Iteration 3680, lr = 0.0001
I0623 12:23:37.983499  5064 solver.cpp:213] Iteration 3690, loss = 3.60344
I0623 12:23:37.983515  5064 solver.cpp:228]     Train net output #0: softmax = 3.60344 (* 1 = 3.60344 loss)
I0623 12:23:37.983520  5064 solver.cpp:473] Iteration 3690, lr = 0.0001
I0623 12:23:39.004575  5064 solver.cpp:213] Iteration 3700, loss = 3.65924
I0623 12:23:39.004590  5064 solver.cpp:228]     Train net output #0: softmax = 3.65924 (* 1 = 3.65924 loss)
I0623 12:23:39.004595  5064 solver.cpp:473] Iteration 3700, lr = 0.0001
I0623 12:23:40.025235  5064 solver.cpp:213] Iteration 3710, loss = 3.54863
I0623 12:23:40.025251  5064 solver.cpp:228]     Train net output #0: softmax = 3.54863 (* 1 = 3.54863 loss)
I0623 12:23:40.025256  5064 solver.cpp:473] Iteration 3710, lr = 0.0001
I0623 12:23:41.046124  5064 solver.cpp:213] Iteration 3720, loss = 3.50569
I0623 12:23:41.046140  5064 solver.cpp:228]     Train net output #0: softmax = 3.50569 (* 1 = 3.50569 loss)
I0623 12:23:41.046150  5064 solver.cpp:473] Iteration 3720, lr = 0.0001
I0623 12:23:42.066639  5064 solver.cpp:213] Iteration 3730, loss = 3.38085
I0623 12:23:42.066654  5064 solver.cpp:228]     Train net output #0: softmax = 3.38085 (* 1 = 3.38085 loss)
I0623 12:23:42.066659  5064 solver.cpp:473] Iteration 3730, lr = 0.0001
I0623 12:23:43.087558  5064 solver.cpp:213] Iteration 3740, loss = 3.64363
I0623 12:23:43.087574  5064 solver.cpp:228]     Train net output #0: softmax = 3.64363 (* 1 = 3.64363 loss)
I0623 12:23:43.087579  5064 solver.cpp:473] Iteration 3740, lr = 0.0001
I0623 12:23:44.108661  5064 solver.cpp:213] Iteration 3750, loss = 3.52776
I0623 12:23:44.108680  5064 solver.cpp:228]     Train net output #0: softmax = 3.52776 (* 1 = 3.52776 loss)
I0623 12:23:44.108839  5064 solver.cpp:473] Iteration 3750, lr = 0.0001
I0623 12:23:45.130031  5064 solver.cpp:213] Iteration 3760, loss = 3.42488
I0623 12:23:45.130046  5064 solver.cpp:228]     Train net output #0: softmax = 3.42488 (* 1 = 3.42488 loss)
I0623 12:23:45.130069  5064 solver.cpp:473] Iteration 3760, lr = 0.0001
I0623 12:23:46.149960  5064 solver.cpp:213] Iteration 3770, loss = 3.40811
I0623 12:23:46.149976  5064 solver.cpp:228]     Train net output #0: softmax = 3.40811 (* 1 = 3.40811 loss)
I0623 12:23:46.149981  5064 solver.cpp:473] Iteration 3770, lr = 0.0001
I0623 12:23:47.170409  5064 solver.cpp:213] Iteration 3780, loss = 3.58316
I0623 12:23:47.170425  5064 solver.cpp:228]     Train net output #0: softmax = 3.58316 (* 1 = 3.58316 loss)
I0623 12:23:47.170430  5064 solver.cpp:473] Iteration 3780, lr = 0.0001
I0623 12:23:48.190263  5064 solver.cpp:213] Iteration 3790, loss = 3.66734
I0623 12:23:48.190284  5064 solver.cpp:228]     Train net output #0: softmax = 3.66734 (* 1 = 3.66734 loss)
I0623 12:23:48.190289  5064 solver.cpp:473] Iteration 3790, lr = 0.0001
I0623 12:23:49.211112  5064 solver.cpp:213] Iteration 3800, loss = 3.58166
I0623 12:23:49.211132  5064 solver.cpp:228]     Train net output #0: softmax = 3.58166 (* 1 = 3.58166 loss)
I0623 12:23:49.211253  5064 solver.cpp:473] Iteration 3800, lr = 0.0001
I0623 12:23:50.231644  5064 solver.cpp:213] Iteration 3810, loss = 3.54349
I0623 12:23:50.231660  5064 solver.cpp:228]     Train net output #0: softmax = 3.54349 (* 1 = 3.54349 loss)
I0623 12:23:50.231665  5064 solver.cpp:473] Iteration 3810, lr = 0.0001
I0623 12:23:51.252579  5064 solver.cpp:213] Iteration 3820, loss = 3.48272
I0623 12:23:51.252595  5064 solver.cpp:228]     Train net output #0: softmax = 3.48272 (* 1 = 3.48272 loss)
I0623 12:23:51.252600  5064 solver.cpp:473] Iteration 3820, lr = 0.0001
I0623 12:23:52.273587  5064 solver.cpp:213] Iteration 3830, loss = 3.69331
I0623 12:23:52.273603  5064 solver.cpp:228]     Train net output #0: softmax = 3.69331 (* 1 = 3.69331 loss)
I0623 12:23:52.273608  5064 solver.cpp:473] Iteration 3830, lr = 0.0001
I0623 12:23:53.294085  5064 solver.cpp:213] Iteration 3840, loss = 3.60383
I0623 12:23:53.294100  5064 solver.cpp:228]     Train net output #0: softmax = 3.60383 (* 1 = 3.60383 loss)
I0623 12:23:53.294106  5064 solver.cpp:473] Iteration 3840, lr = 0.0001
I0623 12:23:54.315150  5064 solver.cpp:213] Iteration 3850, loss = 3.65297
I0623 12:23:54.315168  5064 solver.cpp:228]     Train net output #0: softmax = 3.65297 (* 1 = 3.65297 loss)
I0623 12:23:54.315284  5064 solver.cpp:473] Iteration 3850, lr = 0.0001
I0623 12:23:55.336199  5064 solver.cpp:213] Iteration 3860, loss = 3.47392
I0623 12:23:55.336215  5064 solver.cpp:228]     Train net output #0: softmax = 3.47392 (* 1 = 3.47392 loss)
I0623 12:23:55.336220  5064 solver.cpp:473] Iteration 3860, lr = 0.0001
I0623 12:23:56.356294  5064 solver.cpp:213] Iteration 3870, loss = 3.30852
I0623 12:23:56.356310  5064 solver.cpp:228]     Train net output #0: softmax = 3.30852 (* 1 = 3.30852 loss)
I0623 12:23:56.356315  5064 solver.cpp:473] Iteration 3870, lr = 0.0001
I0623 12:23:57.376665  5064 solver.cpp:213] Iteration 3880, loss = 3.4583
I0623 12:23:57.376682  5064 solver.cpp:228]     Train net output #0: softmax = 3.4583 (* 1 = 3.4583 loss)
I0623 12:23:57.376693  5064 solver.cpp:473] Iteration 3880, lr = 0.0001
I0623 12:23:58.397270  5064 solver.cpp:213] Iteration 3890, loss = 3.55376
I0623 12:23:58.397286  5064 solver.cpp:228]     Train net output #0: softmax = 3.55376 (* 1 = 3.55376 loss)
I0623 12:23:58.397291  5064 solver.cpp:473] Iteration 3890, lr = 0.0001
I0623 12:23:59.418047  5064 solver.cpp:213] Iteration 3900, loss = 3.5158
I0623 12:23:59.418087  5064 solver.cpp:228]     Train net output #0: softmax = 3.5158 (* 1 = 3.5158 loss)
I0623 12:23:59.418092  5064 solver.cpp:473] Iteration 3900, lr = 0.0001
I0623 12:24:00.438259  5064 solver.cpp:213] Iteration 3910, loss = 3.7423
I0623 12:24:00.438276  5064 solver.cpp:228]     Train net output #0: softmax = 3.7423 (* 1 = 3.7423 loss)
I0623 12:24:00.438279  5064 solver.cpp:473] Iteration 3910, lr = 0.0001
I0623 12:24:01.458554  5064 solver.cpp:213] Iteration 3920, loss = 3.55077
I0623 12:24:01.458570  5064 solver.cpp:228]     Train net output #0: softmax = 3.55077 (* 1 = 3.55077 loss)
I0623 12:24:01.458575  5064 solver.cpp:473] Iteration 3920, lr = 0.0001
I0623 12:24:02.478916  5064 solver.cpp:213] Iteration 3930, loss = 3.77518
I0623 12:24:02.478934  5064 solver.cpp:228]     Train net output #0: softmax = 3.77518 (* 1 = 3.77518 loss)
I0623 12:24:02.478938  5064 solver.cpp:473] Iteration 3930, lr = 0.0001
I0623 12:24:03.500058  5064 solver.cpp:213] Iteration 3940, loss = 3.66877
I0623 12:24:03.500075  5064 solver.cpp:228]     Train net output #0: softmax = 3.66877 (* 1 = 3.66877 loss)
I0623 12:24:03.500080  5064 solver.cpp:473] Iteration 3940, lr = 0.0001
I0623 12:24:04.519778  5064 solver.cpp:213] Iteration 3950, loss = 3.72253
I0623 12:24:04.519798  5064 solver.cpp:228]     Train net output #0: softmax = 3.72253 (* 1 = 3.72253 loss)
I0623 12:24:04.519922  5064 solver.cpp:473] Iteration 3950, lr = 0.0001
I0623 12:24:05.540040  5064 solver.cpp:213] Iteration 3960, loss = 3.57718
I0623 12:24:05.540055  5064 solver.cpp:228]     Train net output #0: softmax = 3.57718 (* 1 = 3.57718 loss)
I0623 12:24:05.540060  5064 solver.cpp:473] Iteration 3960, lr = 0.0001
I0623 12:24:06.561100  5064 solver.cpp:213] Iteration 3970, loss = 3.49742
I0623 12:24:06.561115  5064 solver.cpp:228]     Train net output #0: softmax = 3.49742 (* 1 = 3.49742 loss)
I0623 12:24:06.561120  5064 solver.cpp:473] Iteration 3970, lr = 0.0001
I0623 12:24:07.581657  5064 solver.cpp:213] Iteration 3980, loss = 3.76192
I0623 12:24:07.581673  5064 solver.cpp:228]     Train net output #0: softmax = 3.76192 (* 1 = 3.76192 loss)
I0623 12:24:07.581678  5064 solver.cpp:473] Iteration 3980, lr = 0.0001
I0623 12:24:08.602562  5064 solver.cpp:213] Iteration 3990, loss = 3.62555
I0623 12:24:08.602579  5064 solver.cpp:228]     Train net output #0: softmax = 3.62555 (* 1 = 3.62555 loss)
I0623 12:24:08.602583  5064 solver.cpp:473] Iteration 3990, lr = 0.0001
I0623 12:24:09.552817  5064 solver.cpp:362] Snapshotting to snapshots/16-06-21_16h05m31s_0_11_pretrainClassification_iter_4000.caffemodel
I0623 12:24:09.553860  5064 solver.cpp:370] Snapshotting solver state to snapshots/16-06-21_16h05m31s_0_11_pretrainClassification_iter_4000.solverstate
I0623 12:24:09.554462  5064 solver.cpp:291] Iteration 4000, Testing net (#0)
I0623 12:24:09.715461  5064 solver.cpp:342]     Test net output #0: accuracy_top_1 = 0.178125
I0623 12:24:09.715476  5064 solver.cpp:342]     Test net output #1: accuracy_top_5 = 0.417188
I0623 12:24:09.715483  5064 solver.cpp:342]     Test net output #2: softmax = 3.49092 (* 1 = 3.49092 loss)
I0623 12:24:09.786236  5064 solver.cpp:213] Iteration 4000, loss = 3.4817
I0623 12:24:09.786249  5064 solver.cpp:228]     Train net output #0: softmax = 3.4817 (* 1 = 3.4817 loss)
I0623 12:24:09.786254  5064 solver.cpp:473] Iteration 4000, lr = 0.0001
I0623 12:24:10.806197  5064 solver.cpp:213] Iteration 4010, loss = 3.54425
I0623 12:24:10.806213  5064 solver.cpp:228]     Train net output #0: softmax = 3.54425 (* 1 = 3.54425 loss)
I0623 12:24:10.806218  5064 solver.cpp:473] Iteration 4010, lr = 0.0001
I0623 12:24:11.826727  5064 solver.cpp:213] Iteration 4020, loss = 3.80192
I0623 12:24:11.826750  5064 solver.cpp:228]     Train net output #0: softmax = 3.80192 (* 1 = 3.80192 loss)
I0623 12:24:11.826756  5064 solver.cpp:473] Iteration 4020, lr = 0.0001
I0623 12:24:12.847131  5064 solver.cpp:213] Iteration 4030, loss = 3.51946
I0623 12:24:12.847146  5064 solver.cpp:228]     Train net output #0: softmax = 3.51946 (* 1 = 3.51946 loss)
I0623 12:24:12.847151  5064 solver.cpp:473] Iteration 4030, lr = 0.0001
I0623 12:24:13.867854  5064 solver.cpp:213] Iteration 4040, loss = 3.56792
I0623 12:24:13.867871  5064 solver.cpp:228]     Train net output #0: softmax = 3.56792 (* 1 = 3.56792 loss)
I0623 12:24:13.867875  5064 solver.cpp:473] Iteration 4040, lr = 0.0001
I0623 12:24:14.889065  5064 solver.cpp:213] Iteration 4050, loss = 3.53501
I0623 12:24:14.889083  5064 solver.cpp:228]     Train net output #0: softmax = 3.53501 (* 1 = 3.53501 loss)
I0623 12:24:14.889093  5064 solver.cpp:473] Iteration 4050, lr = 0.0001
I0623 12:24:15.909250  5064 solver.cpp:213] Iteration 4060, loss = 3.63091
I0623 12:24:15.909265  5064 solver.cpp:228]     Train net output #0: softmax = 3.63091 (* 1 = 3.63091 loss)
I0623 12:24:15.909271  5064 solver.cpp:473] Iteration 4060, lr = 0.0001
I0623 12:24:16.929505  5064 solver.cpp:213] Iteration 4070, loss = 3.28909
I0623 12:24:16.929522  5064 solver.cpp:228]     Train net output #0: softmax = 3.28909 (* 1 = 3.28909 loss)
I0623 12:24:16.929527  5064 solver.cpp:473] Iteration 4070, lr = 0.0001
I0623 12:24:17.949447  5064 solver.cpp:213] Iteration 4080, loss = 3.43664
I0623 12:24:17.949463  5064 solver.cpp:228]     Train net output #0: softmax = 3.43664 (* 1 = 3.43664 loss)
I0623 12:24:17.949468  5064 solver.cpp:473] Iteration 4080, lr = 0.0001
I0623 12:24:18.970856  5064 solver.cpp:213] Iteration 4090, loss = 3.69167
I0623 12:24:18.970873  5064 solver.cpp:228]     Train net output #0: softmax = 3.69167 (* 1 = 3.69167 loss)
I0623 12:24:18.970878  5064 solver.cpp:473] Iteration 4090, lr = 0.0001
I0623 12:24:19.991286  5064 solver.cpp:213] Iteration 4100, loss = 3.58385
I0623 12:24:19.991305  5064 solver.cpp:228]     Train net output #0: softmax = 3.58385 (* 1 = 3.58385 loss)
I0623 12:24:19.999934  5064 solver.cpp:473] Iteration 4100, lr = 0.0001
I0623 12:24:21.011240  5064 solver.cpp:213] Iteration 4110, loss = 3.4794
I0623 12:24:21.011255  5064 solver.cpp:228]     Train net output #0: softmax = 3.4794 (* 1 = 3.4794 loss)
I0623 12:24:21.011260  5064 solver.cpp:473] Iteration 4110, lr = 0.0001
I0623 12:24:22.032258  5064 solver.cpp:213] Iteration 4120, loss = 3.45441
I0623 12:24:22.032274  5064 solver.cpp:228]     Train net output #0: softmax = 3.45441 (* 1 = 3.45441 loss)
I0623 12:24:22.032279  5064 solver.cpp:473] Iteration 4120, lr = 0.0001
I0623 12:24:23.052644  5064 solver.cpp:213] Iteration 4130, loss = 3.68586
I0623 12:24:23.052660  5064 solver.cpp:228]     Train net output #0: softmax = 3.68586 (* 1 = 3.68586 loss)
I0623 12:24:23.052665  5064 solver.cpp:473] Iteration 4130, lr = 0.0001
I0623 12:24:24.072242  5064 solver.cpp:213] Iteration 4140, loss = 3.721
I0623 12:24:24.072257  5064 solver.cpp:228]     Train net output #0: softmax = 3.721 (* 1 = 3.721 loss)
I0623 12:24:24.072263  5064 solver.cpp:473] Iteration 4140, lr = 0.0001
I0623 12:24:25.092685  5064 solver.cpp:213] Iteration 4150, loss = 3.45739
I0623 12:24:25.092705  5064 solver.cpp:228]     Train net output #0: softmax = 3.45739 (* 1 = 3.45739 loss)
I0623 12:24:25.092823  5064 solver.cpp:473] Iteration 4150, lr = 0.0001
I0623 12:24:26.113199  5064 solver.cpp:213] Iteration 4160, loss = 3.57131
I0623 12:24:26.113214  5064 solver.cpp:228]     Train net output #0: softmax = 3.57131 (* 1 = 3.57131 loss)
I0623 12:24:26.113219  5064 solver.cpp:473] Iteration 4160, lr = 0.0001
I0623 12:24:27.133744  5064 solver.cpp:213] Iteration 4170, loss = 3.73502
I0623 12:24:27.133761  5064 solver.cpp:228]     Train net output #0: softmax = 3.73502 (* 1 = 3.73502 loss)
I0623 12:24:27.133766  5064 solver.cpp:473] Iteration 4170, lr = 0.0001
I0623 12:24:28.154369  5064 solver.cpp:213] Iteration 4180, loss = 3.6419
I0623 12:24:28.154392  5064 solver.cpp:228]     Train net output #0: softmax = 3.6419 (* 1 = 3.6419 loss)
I0623 12:24:28.154397  5064 solver.cpp:473] Iteration 4180, lr = 0.0001
I0623 12:24:29.175292  5064 solver.cpp:213] Iteration 4190, loss = 3.60155
I0623 12:24:29.175308  5064 solver.cpp:228]     Train net output #0: softmax = 3.60155 (* 1 = 3.60155 loss)
I0623 12:24:29.175313  5064 solver.cpp:473] Iteration 4190, lr = 0.0001
I0623 12:24:30.195564  5064 solver.cpp:213] Iteration 4200, loss = 3.55455
I0623 12:24:30.195719  5064 solver.cpp:228]     Train net output #0: softmax = 3.55455 (* 1 = 3.55455 loss)
I0623 12:24:30.195725  5064 solver.cpp:473] Iteration 4200, lr = 0.0001
I0623 12:24:31.216533  5064 solver.cpp:213] Iteration 4210, loss = 3.364
I0623 12:24:31.216549  5064 solver.cpp:228]     Train net output #0: softmax = 3.364 (* 1 = 3.364 loss)
I0623 12:24:31.216555  5064 solver.cpp:473] Iteration 4210, lr = 0.0001
I0623 12:24:32.237181  5064 solver.cpp:213] Iteration 4220, loss = 3.50071
I0623 12:24:32.237197  5064 solver.cpp:228]     Train net output #0: softmax = 3.50071 (* 1 = 3.50071 loss)
I0623 12:24:32.237202  5064 solver.cpp:473] Iteration 4220, lr = 0.0001
I0623 12:24:33.257802  5064 solver.cpp:213] Iteration 4230, loss = 3.41641
I0623 12:24:33.257817  5064 solver.cpp:228]     Train net output #0: softmax = 3.41641 (* 1 = 3.41641 loss)
I0623 12:24:33.257822  5064 solver.cpp:473] Iteration 4230, lr = 0.0001
I0623 12:24:34.278090  5064 solver.cpp:213] Iteration 4240, loss = 3.60247
I0623 12:24:34.278106  5064 solver.cpp:228]     Train net output #0: softmax = 3.60247 (* 1 = 3.60247 loss)
I0623 12:24:34.278111  5064 solver.cpp:473] Iteration 4240, lr = 0.0001
I0623 12:24:35.299149  5064 solver.cpp:213] Iteration 4250, loss = 3.5028
I0623 12:24:35.299168  5064 solver.cpp:228]     Train net output #0: softmax = 3.5028 (* 1 = 3.5028 loss)
I0623 12:24:35.299283  5064 solver.cpp:473] Iteration 4250, lr = 0.0001
I0623 12:24:36.320029  5064 solver.cpp:213] Iteration 4260, loss = 3.38989
I0623 12:24:36.320045  5064 solver.cpp:228]     Train net output #0: softmax = 3.38989 (* 1 = 3.38989 loss)
I0623 12:24:36.320050  5064 solver.cpp:473] Iteration 4260, lr = 0.0001
I0623 12:24:37.341172  5064 solver.cpp:213] Iteration 4270, loss = 3.44956
I0623 12:24:37.341188  5064 solver.cpp:228]     Train net output #0: softmax = 3.44956 (* 1 = 3.44956 loss)
I0623 12:24:37.341193  5064 solver.cpp:473] Iteration 4270, lr = 0.0001
I0623 12:24:38.361774  5064 solver.cpp:213] Iteration 4280, loss = 3.60348
I0623 12:24:38.361790  5064 solver.cpp:228]     Train net output #0: softmax = 3.60348 (* 1 = 3.60348 loss)
I0623 12:24:38.361795  5064 solver.cpp:473] Iteration 4280, lr = 0.0001
I0623 12:24:39.381960  5064 solver.cpp:213] Iteration 4290, loss = 3.5291
I0623 12:24:39.381976  5064 solver.cpp:228]     Train net output #0: softmax = 3.5291 (* 1 = 3.5291 loss)
I0623 12:24:39.381981  5064 solver.cpp:473] Iteration 4290, lr = 0.0001
I0623 12:24:40.402801  5064 solver.cpp:213] Iteration 4300, loss = 3.46796
I0623 12:24:40.402817  5064 solver.cpp:228]     Train net output #0: softmax = 3.46796 (* 1 = 3.46796 loss)
I0623 12:24:40.402822  5064 solver.cpp:473] Iteration 4300, lr = 0.0001
I0623 12:24:41.423900  5064 solver.cpp:213] Iteration 4310, loss = 3.37697
I0623 12:24:41.423916  5064 solver.cpp:228]     Train net output #0: softmax = 3.37697 (* 1 = 3.37697 loss)
I0623 12:24:41.423921  5064 solver.cpp:473] Iteration 4310, lr = 0.0001
I0623 12:24:42.445220  5064 solver.cpp:213] Iteration 4320, loss = 3.78977
I0623 12:24:42.445237  5064 solver.cpp:228]     Train net output #0: softmax = 3.78977 (* 1 = 3.78977 loss)
I0623 12:24:42.445241  5064 solver.cpp:473] Iteration 4320, lr = 0.0001
I0623 12:24:43.465759  5064 solver.cpp:213] Iteration 4330, loss = 3.49768
I0623 12:24:43.465773  5064 solver.cpp:228]     Train net output #0: softmax = 3.49768 (* 1 = 3.49768 loss)
I0623 12:24:43.465778  5064 solver.cpp:473] Iteration 4330, lr = 0.0001
I0623 12:24:44.485939  5064 solver.cpp:213] Iteration 4340, loss = 3.55706
I0623 12:24:44.485961  5064 solver.cpp:228]     Train net output #0: softmax = 3.55706 (* 1 = 3.55706 loss)
I0623 12:24:44.485966  5064 solver.cpp:473] Iteration 4340, lr = 0.0001
I0623 12:24:45.506806  5064 solver.cpp:213] Iteration 4350, loss = 3.51979
I0623 12:24:45.506825  5064 solver.cpp:228]     Train net output #0: softmax = 3.51979 (* 1 = 3.51979 loss)
I0623 12:24:45.506991  5064 solver.cpp:473] Iteration 4350, lr = 0.0001
I0623 12:24:46.527843  5064 solver.cpp:213] Iteration 4360, loss = 3.4411
I0623 12:24:46.527858  5064 solver.cpp:228]     Train net output #0: softmax = 3.4411 (* 1 = 3.4411 loss)
I0623 12:24:46.527878  5064 solver.cpp:473] Iteration 4360, lr = 0.0001
I0623 12:24:47.548043  5064 solver.cpp:213] Iteration 4370, loss = 3.37704
I0623 12:24:47.548058  5064 solver.cpp:228]     Train net output #0: softmax = 3.37704 (* 1 = 3.37704 loss)
I0623 12:24:47.548063  5064 solver.cpp:473] Iteration 4370, lr = 0.0001
I0623 12:24:48.568606  5064 solver.cpp:213] Iteration 4380, loss = 3.67619
I0623 12:24:48.568622  5064 solver.cpp:228]     Train net output #0: softmax = 3.67619 (* 1 = 3.67619 loss)
I0623 12:24:48.568627  5064 solver.cpp:473] Iteration 4380, lr = 0.0001
I0623 12:24:49.590091  5064 solver.cpp:213] Iteration 4390, loss = 3.52331
I0623 12:24:49.590108  5064 solver.cpp:228]     Train net output #0: softmax = 3.52331 (* 1 = 3.52331 loss)
I0623 12:24:49.590112  5064 solver.cpp:473] Iteration 4390, lr = 0.0001
I0623 12:24:50.610584  5064 solver.cpp:213] Iteration 4400, loss = 3.30633
I0623 12:24:50.610602  5064 solver.cpp:228]     Train net output #0: softmax = 3.30633 (* 1 = 3.30633 loss)
I0623 12:24:50.610728  5064 solver.cpp:473] Iteration 4400, lr = 0.0001
I0623 12:24:51.630465  5064 solver.cpp:213] Iteration 4410, loss = 3.58037
I0623 12:24:51.630481  5064 solver.cpp:228]     Train net output #0: softmax = 3.58037 (* 1 = 3.58037 loss)
I0623 12:24:51.630486  5064 solver.cpp:473] Iteration 4410, lr = 0.0001
I0623 12:24:52.651410  5064 solver.cpp:213] Iteration 4420, loss = 3.50072
I0623 12:24:52.651427  5064 solver.cpp:228]     Train net output #0: softmax = 3.50072 (* 1 = 3.50072 loss)
I0623 12:24:52.651432  5064 solver.cpp:473] Iteration 4420, lr = 0.0001
I0623 12:24:53.672451  5064 solver.cpp:213] Iteration 4430, loss = 3.45081
I0623 12:24:53.672467  5064 solver.cpp:228]     Train net output #0: softmax = 3.45081 (* 1 = 3.45081 loss)
I0623 12:24:53.672472  5064 solver.cpp:473] Iteration 4430, lr = 0.0001
I0623 12:24:54.693224  5064 solver.cpp:213] Iteration 4440, loss = 3.57888
I0623 12:24:54.693241  5064 solver.cpp:228]     Train net output #0: softmax = 3.57888 (* 1 = 3.57888 loss)
I0623 12:24:54.693246  5064 solver.cpp:473] Iteration 4440, lr = 0.0001
I0623 12:24:55.714076  5064 solver.cpp:213] Iteration 4450, loss = 3.61351
I0623 12:24:55.714097  5064 solver.cpp:228]     Train net output #0: softmax = 3.61351 (* 1 = 3.61351 loss)
I0623 12:24:55.714212  5064 solver.cpp:473] Iteration 4450, lr = 0.0001
I0623 12:24:56.734151  5064 solver.cpp:213] Iteration 4460, loss = 3.30293
I0623 12:24:56.734169  5064 solver.cpp:228]     Train net output #0: softmax = 3.30293 (* 1 = 3.30293 loss)
I0623 12:24:56.734174  5064 solver.cpp:473] Iteration 4460, lr = 0.0001
I0623 12:24:57.755120  5064 solver.cpp:213] Iteration 4470, loss = 3.59095
I0623 12:24:57.755137  5064 solver.cpp:228]     Train net output #0: softmax = 3.59095 (* 1 = 3.59095 loss)
I0623 12:24:57.755141  5064 solver.cpp:473] Iteration 4470, lr = 0.0001
I0623 12:24:58.776033  5064 solver.cpp:213] Iteration 4480, loss = 3.4188
I0623 12:24:58.776049  5064 solver.cpp:228]     Train net output #0: softmax = 3.4188 (* 1 = 3.4188 loss)
I0623 12:24:58.776054  5064 solver.cpp:473] Iteration 4480, lr = 0.0001
I0623 12:24:59.796952  5064 solver.cpp:213] Iteration 4490, loss = 3.48756
I0623 12:24:59.796967  5064 solver.cpp:228]     Train net output #0: softmax = 3.48756 (* 1 = 3.48756 loss)
I0623 12:24:59.796973  5064 solver.cpp:473] Iteration 4490, lr = 0.0001
I0623 12:25:00.817405  5064 solver.cpp:213] Iteration 4500, loss = 3.5075
I0623 12:25:00.817564  5064 solver.cpp:228]     Train net output #0: softmax = 3.5075 (* 1 = 3.5075 loss)
I0623 12:25:00.817574  5064 solver.cpp:473] Iteration 4500, lr = 0.0001
I0623 12:25:01.838634  5064 solver.cpp:213] Iteration 4510, loss = 3.43799
I0623 12:25:01.838651  5064 solver.cpp:228]     Train net output #0: softmax = 3.43799 (* 1 = 3.43799 loss)
I0623 12:25:01.838656  5064 solver.cpp:473] Iteration 4510, lr = 0.0001
I0623 12:25:02.859212  5064 solver.cpp:213] Iteration 4520, loss = 3.45808
I0623 12:25:02.859230  5064 solver.cpp:228]     Train net output #0: softmax = 3.45808 (* 1 = 3.45808 loss)
I0623 12:25:02.859233  5064 solver.cpp:473] Iteration 4520, lr = 0.0001
I0623 12:25:03.879843  5064 solver.cpp:213] Iteration 4530, loss = 3.50293
I0623 12:25:03.879859  5064 solver.cpp:228]     Train net output #0: softmax = 3.50293 (* 1 = 3.50293 loss)
I0623 12:25:03.879864  5064 solver.cpp:473] Iteration 4530, lr = 0.0001
I0623 12:25:04.900378  5064 solver.cpp:213] Iteration 4540, loss = 3.53576
I0623 12:25:04.900394  5064 solver.cpp:228]     Train net output #0: softmax = 3.53576 (* 1 = 3.53576 loss)
I0623 12:25:04.900399  5064 solver.cpp:473] Iteration 4540, lr = 0.0001
I0623 12:25:05.922034  5064 solver.cpp:213] Iteration 4550, loss = 3.47706
I0623 12:25:05.922052  5064 solver.cpp:228]     Train net output #0: softmax = 3.47706 (* 1 = 3.47706 loss)
I0623 12:25:05.922194  5064 solver.cpp:473] Iteration 4550, lr = 0.0001
I0623 12:25:06.943164  5064 solver.cpp:213] Iteration 4560, loss = 3.66778
I0623 12:25:06.943181  5064 solver.cpp:228]     Train net output #0: softmax = 3.66778 (* 1 = 3.66778 loss)
I0623 12:25:06.943186  5064 solver.cpp:473] Iteration 4560, lr = 0.0001
I0623 12:25:07.963500  5064 solver.cpp:213] Iteration 4570, loss = 3.77584
I0623 12:25:07.963516  5064 solver.cpp:228]     Train net output #0: softmax = 3.77584 (* 1 = 3.77584 loss)
I0623 12:25:07.963521  5064 solver.cpp:473] Iteration 4570, lr = 0.0001
I0623 12:25:08.984637  5064 solver.cpp:213] Iteration 4580, loss = 3.43167
I0623 12:25:08.984657  5064 solver.cpp:228]     Train net output #0: softmax = 3.43167 (* 1 = 3.43167 loss)
I0623 12:25:08.984661  5064 solver.cpp:473] Iteration 4580, lr = 0.0001
I0623 12:25:10.005285  5064 solver.cpp:213] Iteration 4590, loss = 3.53099
I0623 12:25:10.005301  5064 solver.cpp:228]     Train net output #0: softmax = 3.53099 (* 1 = 3.53099 loss)
I0623 12:25:10.005306  5064 solver.cpp:473] Iteration 4590, lr = 0.0001
I0623 12:25:11.026336  5064 solver.cpp:213] Iteration 4600, loss = 3.50778
I0623 12:25:11.026355  5064 solver.cpp:228]     Train net output #0: softmax = 3.50778 (* 1 = 3.50778 loss)
I0623 12:25:11.026474  5064 solver.cpp:473] Iteration 4600, lr = 0.0001
I0623 12:25:12.047341  5064 solver.cpp:213] Iteration 4610, loss = 3.23977
I0623 12:25:12.047356  5064 solver.cpp:228]     Train net output #0: softmax = 3.23977 (* 1 = 3.23977 loss)
I0623 12:25:12.047361  5064 solver.cpp:473] Iteration 4610, lr = 0.0001
I0623 12:25:13.068135  5064 solver.cpp:213] Iteration 4620, loss = 3.42132
I0623 12:25:13.068151  5064 solver.cpp:228]     Train net output #0: softmax = 3.42132 (* 1 = 3.42132 loss)
I0623 12:25:13.068156  5064 solver.cpp:473] Iteration 4620, lr = 0.0001
I0623 12:25:14.089179  5064 solver.cpp:213] Iteration 4630, loss = 3.50729
I0623 12:25:14.089195  5064 solver.cpp:228]     Train net output #0: softmax = 3.50729 (* 1 = 3.50729 loss)
I0623 12:25:14.089200  5064 solver.cpp:473] Iteration 4630, lr = 0.0001
I0623 12:25:15.109993  5064 solver.cpp:213] Iteration 4640, loss = 3.62699
I0623 12:25:15.110009  5064 solver.cpp:228]     Train net output #0: softmax = 3.62699 (* 1 = 3.62699 loss)
I0623 12:25:15.110014  5064 solver.cpp:473] Iteration 4640, lr = 0.0001
I0623 12:25:16.130213  5064 solver.cpp:213] Iteration 4650, loss = 3.56665
I0623 12:25:16.130233  5064 solver.cpp:228]     Train net output #0: softmax = 3.56665 (* 1 = 3.56665 loss)
I0623 12:25:16.130347  5064 solver.cpp:473] Iteration 4650, lr = 0.0001
I0623 12:25:17.149281  5064 solver.cpp:213] Iteration 4660, loss = 3.56735
I0623 12:25:17.149303  5064 solver.cpp:228]     Train net output #0: softmax = 3.56735 (* 1 = 3.56735 loss)
I0623 12:25:17.149327  5064 solver.cpp:473] Iteration 4660, lr = 0.0001
I0623 12:25:18.169736  5064 solver.cpp:213] Iteration 4670, loss = 3.28773
I0623 12:25:18.169754  5064 solver.cpp:228]     Train net output #0: softmax = 3.28773 (* 1 = 3.28773 loss)
I0623 12:25:18.169759  5064 solver.cpp:473] Iteration 4670, lr = 0.0001
I0623 12:25:19.190189  5064 solver.cpp:213] Iteration 4680, loss = 3.62737
I0623 12:25:19.190206  5064 solver.cpp:228]     Train net output #0: softmax = 3.62737 (* 1 = 3.62737 loss)
I0623 12:25:19.190210  5064 solver.cpp:473] Iteration 4680, lr = 0.0001
I0623 12:25:20.210566  5064 solver.cpp:213] Iteration 4690, loss = 3.36202
I0623 12:25:20.210584  5064 solver.cpp:228]     Train net output #0: softmax = 3.36202 (* 1 = 3.36202 loss)
I0623 12:25:20.210590  5064 solver.cpp:473] Iteration 4690, lr = 0.0001
I0623 12:25:21.231874  5064 solver.cpp:213] Iteration 4700, loss = 3.53912
I0623 12:25:21.231911  5064 solver.cpp:228]     Train net output #0: softmax = 3.53912 (* 1 = 3.53912 loss)
I0623 12:25:21.243674  5064 solver.cpp:473] Iteration 4700, lr = 0.0001
I0623 12:25:22.252962  5064 solver.cpp:213] Iteration 4710, loss = 3.58736
I0623 12:25:22.252979  5064 solver.cpp:228]     Train net output #0: softmax = 3.58736 (* 1 = 3.58736 loss)
I0623 12:25:22.252985  5064 solver.cpp:473] Iteration 4710, lr = 0.0001
I0623 12:25:23.273342  5064 solver.cpp:213] Iteration 4720, loss = 3.51497
I0623 12:25:23.273358  5064 solver.cpp:228]     Train net output #0: softmax = 3.51497 (* 1 = 3.51497 loss)
I0623 12:25:23.273363  5064 solver.cpp:473] Iteration 4720, lr = 0.0001
I0623 12:25:24.294283  5064 solver.cpp:213] Iteration 4730, loss = 3.54975
I0623 12:25:24.294298  5064 solver.cpp:228]     Train net output #0: softmax = 3.54975 (* 1 = 3.54975 loss)
I0623 12:25:24.294304  5064 solver.cpp:473] Iteration 4730, lr = 0.0001
I0623 12:25:25.314812  5064 solver.cpp:213] Iteration 4740, loss = 3.3545
I0623 12:25:25.314828  5064 solver.cpp:228]     Train net output #0: softmax = 3.3545 (* 1 = 3.3545 loss)
I0623 12:25:25.314833  5064 solver.cpp:473] Iteration 4740, lr = 0.0001
I0623 12:25:26.335785  5064 solver.cpp:213] Iteration 4750, loss = 3.36133
I0623 12:25:26.335808  5064 solver.cpp:228]     Train net output #0: softmax = 3.36133 (* 1 = 3.36133 loss)
I0623 12:25:26.335937  5064 solver.cpp:473] Iteration 4750, lr = 0.0001
I0623 12:25:27.357280  5064 solver.cpp:213] Iteration 4760, loss = 3.45208
I0623 12:25:27.357311  5064 solver.cpp:228]     Train net output #0: softmax = 3.45208 (* 1 = 3.45208 loss)
I0623 12:25:27.357316  5064 solver.cpp:473] Iteration 4760, lr = 0.0001
I0623 12:25:28.378423  5064 solver.cpp:213] Iteration 4770, loss = 3.49119
I0623 12:25:28.378440  5064 solver.cpp:228]     Train net output #0: softmax = 3.49119 (* 1 = 3.49119 loss)
I0623 12:25:28.378445  5064 solver.cpp:473] Iteration 4770, lr = 0.0001
I0623 12:25:29.399044  5064 solver.cpp:213] Iteration 4780, loss = 3.39015
I0623 12:25:29.399058  5064 solver.cpp:228]     Train net output #0: softmax = 3.39015 (* 1 = 3.39015 loss)
I0623 12:25:29.399063  5064 solver.cpp:473] Iteration 4780, lr = 0.0001
I0623 12:25:30.419335  5064 solver.cpp:213] Iteration 4790, loss = 3.2949
I0623 12:25:30.419350  5064 solver.cpp:228]     Train net output #0: softmax = 3.2949 (* 1 = 3.2949 loss)
I0623 12:25:30.419355  5064 solver.cpp:473] Iteration 4790, lr = 0.0001
I0623 12:25:31.439776  5064 solver.cpp:213] Iteration 4800, loss = 3.59796
I0623 12:25:31.439971  5064 solver.cpp:228]     Train net output #0: softmax = 3.59796 (* 1 = 3.59796 loss)
I0623 12:25:31.439980  5064 solver.cpp:473] Iteration 4800, lr = 0.0001
I0623 12:25:32.460587  5064 solver.cpp:213] Iteration 4810, loss = 3.61632
I0623 12:25:32.460602  5064 solver.cpp:228]     Train net output #0: softmax = 3.61632 (* 1 = 3.61632 loss)
I0623 12:25:32.460608  5064 solver.cpp:473] Iteration 4810, lr = 0.0001
I0623 12:25:33.481557  5064 solver.cpp:213] Iteration 4820, loss = 3.59762
I0623 12:25:33.481572  5064 solver.cpp:228]     Train net output #0: softmax = 3.59762 (* 1 = 3.59762 loss)
I0623 12:25:33.481585  5064 solver.cpp:473] Iteration 4820, lr = 0.0001
I0623 12:25:34.502069  5064 solver.cpp:213] Iteration 4830, loss = 3.58322
I0623 12:25:34.502084  5064 solver.cpp:228]     Train net output #0: softmax = 3.58322 (* 1 = 3.58322 loss)
I0623 12:25:34.502089  5064 solver.cpp:473] Iteration 4830, lr = 0.0001
I0623 12:25:35.522976  5064 solver.cpp:213] Iteration 4840, loss = 3.67675
I0623 12:25:35.522992  5064 solver.cpp:228]     Train net output #0: softmax = 3.67675 (* 1 = 3.67675 loss)
I0623 12:25:35.522997  5064 solver.cpp:473] Iteration 4840, lr = 0.0001
I0623 12:25:36.543936  5064 solver.cpp:213] Iteration 4850, loss = 3.17299
I0623 12:25:36.543953  5064 solver.cpp:228]     Train net output #0: softmax = 3.17299 (* 1 = 3.17299 loss)
I0623 12:25:36.544080  5064 solver.cpp:473] Iteration 4850, lr = 0.0001
I0623 12:25:37.564605  5064 solver.cpp:213] Iteration 4860, loss = 3.4635
I0623 12:25:37.564636  5064 solver.cpp:228]     Train net output #0: softmax = 3.4635 (* 1 = 3.4635 loss)
I0623 12:25:37.564642  5064 solver.cpp:473] Iteration 4860, lr = 0.0001
I0623 12:25:38.585573  5064 solver.cpp:213] Iteration 4870, loss = 3.31233
I0623 12:25:38.585590  5064 solver.cpp:228]     Train net output #0: softmax = 3.31233 (* 1 = 3.31233 loss)
I0623 12:25:38.585595  5064 solver.cpp:473] Iteration 4870, lr = 0.0001
I0623 12:25:39.606273  5064 solver.cpp:213] Iteration 4880, loss = 3.39082
I0623 12:25:39.606290  5064 solver.cpp:228]     Train net output #0: softmax = 3.39082 (* 1 = 3.39082 loss)
I0623 12:25:39.606295  5064 solver.cpp:473] Iteration 4880, lr = 0.0001
I0623 12:25:40.626664  5064 solver.cpp:213] Iteration 4890, loss = 3.47564
I0623 12:25:40.626682  5064 solver.cpp:228]     Train net output #0: softmax = 3.47564 (* 1 = 3.47564 loss)
I0623 12:25:40.626687  5064 solver.cpp:473] Iteration 4890, lr = 0.0001
I0623 12:25:41.647783  5064 solver.cpp:213] Iteration 4900, loss = 3.31652
I0623 12:25:41.647804  5064 solver.cpp:228]     Train net output #0: softmax = 3.31652 (* 1 = 3.31652 loss)
I0623 12:25:41.647814  5064 solver.cpp:473] Iteration 4900, lr = 0.0001
I0623 12:25:42.668558  5064 solver.cpp:213] Iteration 4910, loss = 3.57194
I0623 12:25:42.668577  5064 solver.cpp:228]     Train net output #0: softmax = 3.57194 (* 1 = 3.57194 loss)
I0623 12:25:42.668582  5064 solver.cpp:473] Iteration 4910, lr = 0.0001
I0623 12:25:43.689497  5064 solver.cpp:213] Iteration 4920, loss = 3.2655
I0623 12:25:43.689514  5064 solver.cpp:228]     Train net output #0: softmax = 3.2655 (* 1 = 3.2655 loss)
I0623 12:25:43.689518  5064 solver.cpp:473] Iteration 4920, lr = 0.0001
I0623 12:25:44.709851  5064 solver.cpp:213] Iteration 4930, loss = 3.5167
I0623 12:25:44.709867  5064 solver.cpp:228]     Train net output #0: softmax = 3.5167 (* 1 = 3.5167 loss)
I0623 12:25:44.709872  5064 solver.cpp:473] Iteration 4930, lr = 0.0001
I0623 12:25:45.730586  5064 solver.cpp:213] Iteration 4940, loss = 3.60076
I0623 12:25:45.730602  5064 solver.cpp:228]     Train net output #0: softmax = 3.60076 (* 1 = 3.60076 loss)
I0623 12:25:45.730607  5064 solver.cpp:473] Iteration 4940, lr = 0.0001
I0623 12:25:46.750710  5064 solver.cpp:213] Iteration 4950, loss = 3.63035
I0623 12:25:46.750728  5064 solver.cpp:228]     Train net output #0: softmax = 3.63035 (* 1 = 3.63035 loss)
I0623 12:25:46.750845  5064 solver.cpp:473] Iteration 4950, lr = 0.0001
I0623 12:25:47.771536  5064 solver.cpp:213] Iteration 4960, loss = 3.52938
I0623 12:25:47.771564  5064 solver.cpp:228]     Train net output #0: softmax = 3.52938 (* 1 = 3.52938 loss)
I0623 12:25:47.771587  5064 solver.cpp:473] Iteration 4960, lr = 0.0001
I0623 12:25:48.792526  5064 solver.cpp:213] Iteration 4970, loss = 3.46052
I0623 12:25:48.792544  5064 solver.cpp:228]     Train net output #0: softmax = 3.46052 (* 1 = 3.46052 loss)
I0623 12:25:48.792549  5064 solver.cpp:473] Iteration 4970, lr = 0.0001
I0623 12:25:49.813136  5064 solver.cpp:213] Iteration 4980, loss = 3.69177
I0623 12:25:49.813153  5064 solver.cpp:228]     Train net output #0: softmax = 3.69177 (* 1 = 3.69177 loss)
I0623 12:25:49.813163  5064 solver.cpp:473] Iteration 4980, lr = 0.0001
I0623 12:25:50.833786  5064 solver.cpp:213] Iteration 4990, loss = 3.44482
I0623 12:25:50.833802  5064 solver.cpp:228]     Train net output #0: softmax = 3.44482 (* 1 = 3.44482 loss)
I0623 12:25:50.833807  5064 solver.cpp:473] Iteration 4990, lr = 0.0001
I0623 12:25:51.784245  5064 solver.cpp:362] Snapshotting to snapshots/16-06-21_16h05m31s_0_11_pretrainClassification_iter_5000.caffemodel
I0623 12:25:51.785313  5064 solver.cpp:370] Snapshotting solver state to snapshots/16-06-21_16h05m31s_0_11_pretrainClassification_iter_5000.solverstate
I0623 12:25:51.785936  5064 solver.cpp:291] Iteration 5000, Testing net (#0)
I0623 12:25:51.946863  5064 solver.cpp:342]     Test net output #0: accuracy_top_1 = 0.196875
I0623 12:25:51.946877  5064 solver.cpp:342]     Test net output #1: accuracy_top_5 = 0.464063
I0623 12:25:51.946884  5064 solver.cpp:342]     Test net output #2: softmax = 3.41281 (* 1 = 3.41281 loss)
I0623 12:25:52.017508  5064 solver.cpp:213] Iteration 5000, loss = 3.50175
I0623 12:25:52.017523  5064 solver.cpp:228]     Train net output #0: softmax = 3.50175 (* 1 = 3.50175 loss)
I0623 12:25:52.017527  5064 solver.cpp:473] Iteration 5000, lr = 0.0001
I0623 12:25:53.038074  5064 solver.cpp:213] Iteration 5010, loss = 3.31427
I0623 12:25:53.038090  5064 solver.cpp:228]     Train net output #0: softmax = 3.31427 (* 1 = 3.31427 loss)
I0623 12:25:53.038095  5064 solver.cpp:473] Iteration 5010, lr = 0.0001
I0623 12:25:54.058755  5064 solver.cpp:213] Iteration 5020, loss = 3.57853
I0623 12:25:54.058771  5064 solver.cpp:228]     Train net output #0: softmax = 3.57853 (* 1 = 3.57853 loss)
I0623 12:25:54.058775  5064 solver.cpp:473] Iteration 5020, lr = 0.0001
I0623 12:25:55.079524  5064 solver.cpp:213] Iteration 5030, loss = 3.65791
I0623 12:25:55.079540  5064 solver.cpp:228]     Train net output #0: softmax = 3.65791 (* 1 = 3.65791 loss)
I0623 12:25:55.079545  5064 solver.cpp:473] Iteration 5030, lr = 0.0001
I0623 12:25:56.100111  5064 solver.cpp:213] Iteration 5040, loss = 3.57358
I0623 12:25:56.100126  5064 solver.cpp:228]     Train net output #0: softmax = 3.57358 (* 1 = 3.57358 loss)
I0623 12:25:56.100145  5064 solver.cpp:473] Iteration 5040, lr = 0.0001
I0623 12:25:57.120820  5064 solver.cpp:213] Iteration 5050, loss = 3.61503
I0623 12:25:57.120856  5064 solver.cpp:228]     Train net output #0: softmax = 3.61503 (* 1 = 3.61503 loss)
I0623 12:25:57.121096  5064 solver.cpp:473] Iteration 5050, lr = 0.0001
I0623 12:25:58.141615  5064 solver.cpp:213] Iteration 5060, loss = 3.39401
I0623 12:25:58.141633  5064 solver.cpp:228]     Train net output #0: softmax = 3.39401 (* 1 = 3.39401 loss)
I0623 12:25:58.141638  5064 solver.cpp:473] Iteration 5060, lr = 0.0001
I0623 12:25:59.162595  5064 solver.cpp:213] Iteration 5070, loss = 3.37672
I0623 12:25:59.162611  5064 solver.cpp:228]     Train net output #0: softmax = 3.37672 (* 1 = 3.37672 loss)
I0623 12:25:59.162617  5064 solver.cpp:473] Iteration 5070, lr = 0.0001
I0623 12:26:00.182859  5064 solver.cpp:213] Iteration 5080, loss = 3.39458
I0623 12:26:00.182876  5064 solver.cpp:228]     Train net output #0: softmax = 3.39458 (* 1 = 3.39458 loss)
I0623 12:26:00.182881  5064 solver.cpp:473] Iteration 5080, lr = 0.0001
I0623 12:26:01.203279  5064 solver.cpp:213] Iteration 5090, loss = 3.4695
I0623 12:26:01.203295  5064 solver.cpp:228]     Train net output #0: softmax = 3.4695 (* 1 = 3.4695 loss)
I0623 12:26:01.203300  5064 solver.cpp:473] Iteration 5090, lr = 0.0001
I0623 12:26:02.223883  5064 solver.cpp:213] Iteration 5100, loss = 3.28479
I0623 12:26:02.224042  5064 solver.cpp:228]     Train net output #0: softmax = 3.28479 (* 1 = 3.28479 loss)
I0623 12:26:02.224050  5064 solver.cpp:473] Iteration 5100, lr = 0.0001
I0623 12:26:03.243863  5064 solver.cpp:213] Iteration 5110, loss = 3.38834
I0623 12:26:03.243880  5064 solver.cpp:228]     Train net output #0: softmax = 3.38834 (* 1 = 3.38834 loss)
I0623 12:26:03.243885  5064 solver.cpp:473] Iteration 5110, lr = 0.0001
I0623 12:26:04.263959  5064 solver.cpp:213] Iteration 5120, loss = 3.59972
I0623 12:26:04.263981  5064 solver.cpp:228]     Train net output #0: softmax = 3.59972 (* 1 = 3.59972 loss)
I0623 12:26:04.263988  5064 solver.cpp:473] Iteration 5120, lr = 0.0001
I0623 12:26:05.284358  5064 solver.cpp:213] Iteration 5130, loss = 3.46096
I0623 12:26:05.284374  5064 solver.cpp:228]     Train net output #0: softmax = 3.46096 (* 1 = 3.46096 loss)
I0623 12:26:05.284379  5064 solver.cpp:473] Iteration 5130, lr = 0.0001
I0623 12:26:06.304050  5064 solver.cpp:213] Iteration 5140, loss = 3.28975
I0623 12:26:06.304067  5064 solver.cpp:228]     Train net output #0: softmax = 3.28975 (* 1 = 3.28975 loss)
I0623 12:26:06.304072  5064 solver.cpp:473] Iteration 5140, lr = 0.0001
I0623 12:26:07.324901  5064 solver.cpp:213] Iteration 5150, loss = 3.44099
I0623 12:26:07.324937  5064 solver.cpp:228]     Train net output #0: softmax = 3.44099 (* 1 = 3.44099 loss)
I0623 12:26:07.325078  5064 solver.cpp:473] Iteration 5150, lr = 0.0001
I0623 12:26:08.345098  5064 solver.cpp:213] Iteration 5160, loss = 3.25183
I0623 12:26:08.345118  5064 solver.cpp:228]     Train net output #0: softmax = 3.25183 (* 1 = 3.25183 loss)
I0623 12:26:08.345123  5064 solver.cpp:473] Iteration 5160, lr = 0.0001
I0623 12:26:09.365408  5064 solver.cpp:213] Iteration 5170, loss = 3.36837
I0623 12:26:09.365424  5064 solver.cpp:228]     Train net output #0: softmax = 3.36837 (* 1 = 3.36837 loss)
I0623 12:26:09.365429  5064 solver.cpp:473] Iteration 5170, lr = 0.0001
I0623 12:26:10.386091  5064 solver.cpp:213] Iteration 5180, loss = 3.22261
I0623 12:26:10.386107  5064 solver.cpp:228]     Train net output #0: softmax = 3.22261 (* 1 = 3.22261 loss)
I0623 12:26:10.386112  5064 solver.cpp:473] Iteration 5180, lr = 0.0001
I0623 12:26:11.406661  5064 solver.cpp:213] Iteration 5190, loss = 3.52997
I0623 12:26:11.406677  5064 solver.cpp:228]     Train net output #0: softmax = 3.52997 (* 1 = 3.52997 loss)
I0623 12:26:11.406682  5064 solver.cpp:473] Iteration 5190, lr = 0.0001
I0623 12:26:12.427148  5064 solver.cpp:213] Iteration 5200, loss = 3.5694
I0623 12:26:12.427168  5064 solver.cpp:228]     Train net output #0: softmax = 3.5694 (* 1 = 3.5694 loss)
I0623 12:26:12.427173  5064 solver.cpp:473] Iteration 5200, lr = 0.0001
I0623 12:26:13.448124  5064 solver.cpp:213] Iteration 5210, loss = 3.50359
I0623 12:26:13.448142  5064 solver.cpp:228]     Train net output #0: softmax = 3.50359 (* 1 = 3.50359 loss)
I0623 12:26:13.448148  5064 solver.cpp:473] Iteration 5210, lr = 0.0001
I0623 12:26:14.469420  5064 solver.cpp:213] Iteration 5220, loss = 3.34117
I0623 12:26:14.469437  5064 solver.cpp:228]     Train net output #0: softmax = 3.34117 (* 1 = 3.34117 loss)
I0623 12:26:14.469442  5064 solver.cpp:473] Iteration 5220, lr = 0.0001
I0623 12:26:15.490418  5064 solver.cpp:213] Iteration 5230, loss = 3.61901
I0623 12:26:15.490434  5064 solver.cpp:228]     Train net output #0: softmax = 3.61901 (* 1 = 3.61901 loss)
I0623 12:26:15.490439  5064 solver.cpp:473] Iteration 5230, lr = 0.0001
I0623 12:26:16.510764  5064 solver.cpp:213] Iteration 5240, loss = 3.32636
I0623 12:26:16.510783  5064 solver.cpp:228]     Train net output #0: softmax = 3.32636 (* 1 = 3.32636 loss)
I0623 12:26:16.510788  5064 solver.cpp:473] Iteration 5240, lr = 0.0001
I0623 12:26:17.530815  5064 solver.cpp:213] Iteration 5250, loss = 3.50972
I0623 12:26:17.530851  5064 solver.cpp:228]     Train net output #0: softmax = 3.50972 (* 1 = 3.50972 loss)
I0623 12:26:17.530989  5064 solver.cpp:473] Iteration 5250, lr = 0.0001
I0623 12:26:18.551271  5064 solver.cpp:213] Iteration 5260, loss = 3.42428
I0623 12:26:18.551287  5064 solver.cpp:228]     Train net output #0: softmax = 3.42428 (* 1 = 3.42428 loss)
I0623 12:26:18.551308  5064 solver.cpp:473] Iteration 5260, lr = 0.0001
I0623 12:26:19.571833  5064 solver.cpp:213] Iteration 5270, loss = 3.44149
I0623 12:26:19.571848  5064 solver.cpp:228]     Train net output #0: softmax = 3.44149 (* 1 = 3.44149 loss)
I0623 12:26:19.571853  5064 solver.cpp:473] Iteration 5270, lr = 0.0001
I0623 12:26:20.592067  5064 solver.cpp:213] Iteration 5280, loss = 3.34926
I0623 12:26:20.592092  5064 solver.cpp:228]     Train net output #0: softmax = 3.34926 (* 1 = 3.34926 loss)
I0623 12:26:20.592097  5064 solver.cpp:473] Iteration 5280, lr = 0.0001
I0623 12:26:21.612557  5064 solver.cpp:213] Iteration 5290, loss = 3.41367
I0623 12:26:21.612576  5064 solver.cpp:228]     Train net output #0: softmax = 3.41367 (* 1 = 3.41367 loss)
I0623 12:26:21.612581  5064 solver.cpp:473] Iteration 5290, lr = 0.0001
I0623 12:26:22.632628  5064 solver.cpp:213] Iteration 5300, loss = 3.49682
I0623 12:26:22.632652  5064 solver.cpp:228]     Train net output #0: softmax = 3.49682 (* 1 = 3.49682 loss)
I0623 12:26:22.632786  5064 solver.cpp:473] Iteration 5300, lr = 0.0001
I0623 12:26:23.653496  5064 solver.cpp:213] Iteration 5310, loss = 3.208
I0623 12:26:23.653515  5064 solver.cpp:228]     Train net output #0: softmax = 3.208 (* 1 = 3.208 loss)
I0623 12:26:23.653520  5064 solver.cpp:473] Iteration 5310, lr = 0.0001
I0623 12:26:24.673617  5064 solver.cpp:213] Iteration 5320, loss = 3.49553
I0623 12:26:24.673635  5064 solver.cpp:228]     Train net output #0: softmax = 3.49553 (* 1 = 3.49553 loss)
I0623 12:26:24.673640  5064 solver.cpp:473] Iteration 5320, lr = 0.0001
I0623 12:26:25.694506  5064 solver.cpp:213] Iteration 5330, loss = 3.5635
I0623 12:26:25.694521  5064 solver.cpp:228]     Train net output #0: softmax = 3.5635 (* 1 = 3.5635 loss)
I0623 12:26:25.694526  5064 solver.cpp:473] Iteration 5330, lr = 0.0001
I0623 12:26:26.715564  5064 solver.cpp:213] Iteration 5340, loss = 3.54991
I0623 12:26:26.715579  5064 solver.cpp:228]     Train net output #0: softmax = 3.54991 (* 1 = 3.54991 loss)
I0623 12:26:26.715584  5064 solver.cpp:473] Iteration 5340, lr = 0.0001
I0623 12:26:27.736500  5064 solver.cpp:213] Iteration 5350, loss = 3.56984
I0623 12:26:27.736528  5064 solver.cpp:228]     Train net output #0: softmax = 3.56984 (* 1 = 3.56984 loss)
I0623 12:26:27.736680  5064 solver.cpp:473] Iteration 5350, lr = 0.0001
I0623 12:26:28.757081  5064 solver.cpp:213] Iteration 5360, loss = 3.30035
I0623 12:26:28.757103  5064 solver.cpp:228]     Train net output #0: softmax = 3.30035 (* 1 = 3.30035 loss)
I0623 12:26:28.757108  5064 solver.cpp:473] Iteration 5360, lr = 0.0001
I0623 12:26:29.777451  5064 solver.cpp:213] Iteration 5370, loss = 3.17809
I0623 12:26:29.777472  5064 solver.cpp:228]     Train net output #0: softmax = 3.17809 (* 1 = 3.17809 loss)
I0623 12:26:29.777477  5064 solver.cpp:473] Iteration 5370, lr = 0.0001
I0623 12:26:30.797505  5064 solver.cpp:213] Iteration 5380, loss = 3.39316
I0623 12:26:30.797525  5064 solver.cpp:228]     Train net output #0: softmax = 3.39316 (* 1 = 3.39316 loss)
I0623 12:26:30.797530  5064 solver.cpp:473] Iteration 5380, lr = 0.0001
I0623 12:26:31.817409  5064 solver.cpp:213] Iteration 5390, loss = 3.46176
I0623 12:26:31.817425  5064 solver.cpp:228]     Train net output #0: softmax = 3.46176 (* 1 = 3.46176 loss)
I0623 12:26:31.817430  5064 solver.cpp:473] Iteration 5390, lr = 0.0001
I0623 12:26:32.837517  5064 solver.cpp:213] Iteration 5400, loss = 3.36544
I0623 12:26:32.837726  5064 solver.cpp:228]     Train net output #0: softmax = 3.36544 (* 1 = 3.36544 loss)
I0623 12:26:32.837733  5064 solver.cpp:473] Iteration 5400, lr = 0.0001
I0623 12:26:33.857887  5064 solver.cpp:213] Iteration 5410, loss = 3.38058
I0623 12:26:33.857905  5064 solver.cpp:228]     Train net output #0: softmax = 3.38058 (* 1 = 3.38058 loss)
I0623 12:26:33.857910  5064 solver.cpp:473] Iteration 5410, lr = 0.0001
I0623 12:26:34.879086  5064 solver.cpp:213] Iteration 5420, loss = 3.52883
I0623 12:26:34.879106  5064 solver.cpp:228]     Train net output #0: softmax = 3.52883 (* 1 = 3.52883 loss)
I0623 12:26:34.879112  5064 solver.cpp:473] Iteration 5420, lr = 0.0001
I0623 12:26:35.899230  5064 solver.cpp:213] Iteration 5430, loss = 3.54822
I0623 12:26:35.899250  5064 solver.cpp:228]     Train net output #0: softmax = 3.54822 (* 1 = 3.54822 loss)
I0623 12:26:35.899255  5064 solver.cpp:473] Iteration 5430, lr = 0.0001
I0623 12:26:36.920375  5064 solver.cpp:213] Iteration 5440, loss = 3.41263
I0623 12:26:36.920403  5064 solver.cpp:228]     Train net output #0: softmax = 3.41263 (* 1 = 3.41263 loss)
I0623 12:26:36.920408  5064 solver.cpp:473] Iteration 5440, lr = 0.0001
I0623 12:26:37.941751  5064 solver.cpp:213] Iteration 5450, loss = 3.46842
I0623 12:26:37.941771  5064 solver.cpp:228]     Train net output #0: softmax = 3.46842 (* 1 = 3.46842 loss)
I0623 12:26:37.941920  5064 solver.cpp:473] Iteration 5450, lr = 0.0001
I0623 12:26:38.963299  5064 solver.cpp:213] Iteration 5460, loss = 3.40566
I0623 12:26:38.963315  5064 solver.cpp:228]     Train net output #0: softmax = 3.40566 (* 1 = 3.40566 loss)
I0623 12:26:38.963320  5064 solver.cpp:473] Iteration 5460, lr = 0.0001
I0623 12:26:39.984616  5064 solver.cpp:213] Iteration 5470, loss = 3.54623
I0623 12:26:39.984632  5064 solver.cpp:228]     Train net output #0: softmax = 3.54623 (* 1 = 3.54623 loss)
I0623 12:26:39.984637  5064 solver.cpp:473] Iteration 5470, lr = 0.0001
I0623 12:26:41.005090  5064 solver.cpp:213] Iteration 5480, loss = 3.33117
I0623 12:26:41.005108  5064 solver.cpp:228]     Train net output #0: softmax = 3.33117 (* 1 = 3.33117 loss)
I0623 12:26:41.005113  5064 solver.cpp:473] Iteration 5480, lr = 0.0001
I0623 12:26:42.025753  5064 solver.cpp:213] Iteration 5490, loss = 3.17254
I0623 12:26:42.025774  5064 solver.cpp:228]     Train net output #0: softmax = 3.17254 (* 1 = 3.17254 loss)
I0623 12:26:42.025779  5064 solver.cpp:473] Iteration 5490, lr = 0.0001
I0623 12:26:43.046754  5064 solver.cpp:213] Iteration 5500, loss = 3.49239
I0623 12:26:43.046777  5064 solver.cpp:228]     Train net output #0: softmax = 3.49239 (* 1 = 3.49239 loss)
I0623 12:26:43.046901  5064 solver.cpp:473] Iteration 5500, lr = 0.0001
I0623 12:26:44.067566  5064 solver.cpp:213] Iteration 5510, loss = 3.37977
I0623 12:26:44.067584  5064 solver.cpp:228]     Train net output #0: softmax = 3.37977 (* 1 = 3.37977 loss)
I0623 12:26:44.067589  5064 solver.cpp:473] Iteration 5510, lr = 0.0001
I0623 12:26:45.088258  5064 solver.cpp:213] Iteration 5520, loss = 3.60113
I0623 12:26:45.088275  5064 solver.cpp:228]     Train net output #0: softmax = 3.60113 (* 1 = 3.60113 loss)
I0623 12:26:45.088280  5064 solver.cpp:473] Iteration 5520, lr = 0.0001
I0623 12:26:46.108681  5064 solver.cpp:213] Iteration 5530, loss = 3.36531
I0623 12:26:46.108700  5064 solver.cpp:228]     Train net output #0: softmax = 3.36531 (* 1 = 3.36531 loss)
I0623 12:26:46.108705  5064 solver.cpp:473] Iteration 5530, lr = 0.0001
I0623 12:26:47.128927  5064 solver.cpp:213] Iteration 5540, loss = 3.38097
I0623 12:26:47.128947  5064 solver.cpp:228]     Train net output #0: softmax = 3.38097 (* 1 = 3.38097 loss)
I0623 12:26:47.128952  5064 solver.cpp:473] Iteration 5540, lr = 0.0001
I0623 12:26:48.149695  5064 solver.cpp:213] Iteration 5550, loss = 3.15228
I0623 12:26:48.149718  5064 solver.cpp:228]     Train net output #0: softmax = 3.15228 (* 1 = 3.15228 loss)
I0623 12:26:48.149838  5064 solver.cpp:473] Iteration 5550, lr = 0.0001
I0623 12:26:49.170873  5064 solver.cpp:213] Iteration 5560, loss = 3.39862
I0623 12:26:49.170895  5064 solver.cpp:228]     Train net output #0: softmax = 3.39862 (* 1 = 3.39862 loss)
I0623 12:26:49.170918  5064 solver.cpp:473] Iteration 5560, lr = 0.0001
I0623 12:26:50.191179  5064 solver.cpp:213] Iteration 5570, loss = 3.38667
I0623 12:26:50.191197  5064 solver.cpp:228]     Train net output #0: softmax = 3.38667 (* 1 = 3.38667 loss)
I0623 12:26:50.191202  5064 solver.cpp:473] Iteration 5570, lr = 0.0001
I0623 12:26:51.211987  5064 solver.cpp:213] Iteration 5580, loss = 3.2469
I0623 12:26:51.212005  5064 solver.cpp:228]     Train net output #0: softmax = 3.2469 (* 1 = 3.2469 loss)
I0623 12:26:51.212010  5064 solver.cpp:473] Iteration 5580, lr = 0.0001
I0623 12:26:52.232517  5064 solver.cpp:213] Iteration 5590, loss = 3.39812
I0623 12:26:52.232533  5064 solver.cpp:228]     Train net output #0: softmax = 3.39812 (* 1 = 3.39812 loss)
I0623 12:26:52.232538  5064 solver.cpp:473] Iteration 5590, lr = 0.0001
I0623 12:26:53.253444  5064 solver.cpp:213] Iteration 5600, loss = 3.32693
I0623 12:26:53.253468  5064 solver.cpp:228]     Train net output #0: softmax = 3.32693 (* 1 = 3.32693 loss)
I0623 12:26:53.253605  5064 solver.cpp:473] Iteration 5600, lr = 0.0001
I0623 12:26:54.273957  5064 solver.cpp:213] Iteration 5610, loss = 3.55044
I0623 12:26:54.273977  5064 solver.cpp:228]     Train net output #0: softmax = 3.55044 (* 1 = 3.55044 loss)
I0623 12:26:54.273982  5064 solver.cpp:473] Iteration 5610, lr = 0.0001
I0623 12:26:55.294487  5064 solver.cpp:213] Iteration 5620, loss = 3.50647
I0623 12:26:55.294508  5064 solver.cpp:228]     Train net output #0: softmax = 3.50647 (* 1 = 3.50647 loss)
I0623 12:26:55.294513  5064 solver.cpp:473] Iteration 5620, lr = 0.0001
I0623 12:26:56.314903  5064 solver.cpp:213] Iteration 5630, loss = 2.9998
I0623 12:26:56.314920  5064 solver.cpp:228]     Train net output #0: softmax = 2.9998 (* 1 = 2.9998 loss)
I0623 12:26:56.314925  5064 solver.cpp:473] Iteration 5630, lr = 0.0001
I0623 12:26:57.335368  5064 solver.cpp:213] Iteration 5640, loss = 3.44195
I0623 12:26:57.335384  5064 solver.cpp:228]     Train net output #0: softmax = 3.44195 (* 1 = 3.44195 loss)
I0623 12:26:57.335389  5064 solver.cpp:473] Iteration 5640, lr = 0.0001
I0623 12:26:58.356695  5064 solver.cpp:213] Iteration 5650, loss = 3.38254
I0623 12:26:58.356714  5064 solver.cpp:228]     Train net output #0: softmax = 3.38254 (* 1 = 3.38254 loss)
I0623 12:26:58.356719  5064 solver.cpp:473] Iteration 5650, lr = 0.0001
I0623 12:26:59.377171  5064 solver.cpp:213] Iteration 5660, loss = 3.66248
I0623 12:26:59.377187  5064 solver.cpp:228]     Train net output #0: softmax = 3.66248 (* 1 = 3.66248 loss)
I0623 12:26:59.377192  5064 solver.cpp:473] Iteration 5660, lr = 0.0001
I0623 12:27:00.397547  5064 solver.cpp:213] Iteration 5670, loss = 3.52818
I0623 12:27:00.397563  5064 solver.cpp:228]     Train net output #0: softmax = 3.52818 (* 1 = 3.52818 loss)
I0623 12:27:00.397568  5064 solver.cpp:473] Iteration 5670, lr = 0.0001
I0623 12:27:01.418064  5064 solver.cpp:213] Iteration 5680, loss = 3.44184
I0623 12:27:01.418088  5064 solver.cpp:228]     Train net output #0: softmax = 3.44184 (* 1 = 3.44184 loss)
I0623 12:27:01.418093  5064 solver.cpp:473] Iteration 5680, lr = 0.0001
I0623 12:27:02.439283  5064 solver.cpp:213] Iteration 5690, loss = 3.4029
I0623 12:27:02.439303  5064 solver.cpp:228]     Train net output #0: softmax = 3.4029 (* 1 = 3.4029 loss)
I0623 12:27:02.439308  5064 solver.cpp:473] Iteration 5690, lr = 0.0001
I0623 12:27:03.460337  5064 solver.cpp:213] Iteration 5700, loss = 3.2698
I0623 12:27:03.460511  5064 solver.cpp:228]     Train net output #0: softmax = 3.2698 (* 1 = 3.2698 loss)
I0623 12:27:03.460518  5064 solver.cpp:473] Iteration 5700, lr = 0.0001
I0623 12:27:04.480846  5064 solver.cpp:213] Iteration 5710, loss = 3.44556
I0623 12:27:04.480865  5064 solver.cpp:228]     Train net output #0: softmax = 3.44556 (* 1 = 3.44556 loss)
I0623 12:27:04.480871  5064 solver.cpp:473] Iteration 5710, lr = 0.0001
I0623 12:27:05.501385  5064 solver.cpp:213] Iteration 5720, loss = 3.26395
I0623 12:27:05.501407  5064 solver.cpp:228]     Train net output #0: softmax = 3.26395 (* 1 = 3.26395 loss)
I0623 12:27:05.501413  5064 solver.cpp:473] Iteration 5720, lr = 0.0001
I0623 12:27:06.521746  5064 solver.cpp:213] Iteration 5730, loss = 3.46658
I0623 12:27:06.521765  5064 solver.cpp:228]     Train net output #0: softmax = 3.46658 (* 1 = 3.46658 loss)
I0623 12:27:06.521771  5064 solver.cpp:473] Iteration 5730, lr = 0.0001
I0623 12:27:07.541750  5064 solver.cpp:213] Iteration 5740, loss = 3.41063
I0623 12:27:07.541767  5064 solver.cpp:228]     Train net output #0: softmax = 3.41063 (* 1 = 3.41063 loss)
I0623 12:27:07.541774  5064 solver.cpp:473] Iteration 5740, lr = 0.0001
I0623 12:27:08.561759  5064 solver.cpp:213] Iteration 5750, loss = 3.15368
I0623 12:27:08.561785  5064 solver.cpp:228]     Train net output #0: softmax = 3.15368 (* 1 = 3.15368 loss)
I0623 12:27:08.561944  5064 solver.cpp:473] Iteration 5750, lr = 0.0001
I0623 12:27:09.582382  5064 solver.cpp:213] Iteration 5760, loss = 3.23195
I0623 12:27:09.582406  5064 solver.cpp:228]     Train net output #0: softmax = 3.23195 (* 1 = 3.23195 loss)
I0623 12:27:09.582417  5064 solver.cpp:473] Iteration 5760, lr = 0.0001
I0623 12:27:10.602856  5064 solver.cpp:213] Iteration 5770, loss = 3.58561
I0623 12:27:10.602877  5064 solver.cpp:228]     Train net output #0: softmax = 3.58561 (* 1 = 3.58561 loss)
I0623 12:27:10.602882  5064 solver.cpp:473] Iteration 5770, lr = 0.0001
I0623 12:27:11.622690  5064 solver.cpp:213] Iteration 5780, loss = 3.36575
I0623 12:27:11.622710  5064 solver.cpp:228]     Train net output #0: softmax = 3.36575 (* 1 = 3.36575 loss)
I0623 12:27:11.622715  5064 solver.cpp:473] Iteration 5780, lr = 0.0001
I0623 12:27:12.642946  5064 solver.cpp:213] Iteration 5790, loss = 3.3051
I0623 12:27:12.642963  5064 solver.cpp:228]     Train net output #0: softmax = 3.3051 (* 1 = 3.3051 loss)
I0623 12:27:12.642968  5064 solver.cpp:473] Iteration 5790, lr = 0.0001
I0623 12:27:13.663926  5064 solver.cpp:213] Iteration 5800, loss = 3.4568
I0623 12:27:13.663949  5064 solver.cpp:228]     Train net output #0: softmax = 3.4568 (* 1 = 3.4568 loss)
I0623 12:27:13.664072  5064 solver.cpp:473] Iteration 5800, lr = 0.0001
I0623 12:27:14.684072  5064 solver.cpp:213] Iteration 5810, loss = 3.43259
I0623 12:27:14.684092  5064 solver.cpp:228]     Train net output #0: softmax = 3.43259 (* 1 = 3.43259 loss)
I0623 12:27:14.684097  5064 solver.cpp:473] Iteration 5810, lr = 0.0001
I0623 12:27:15.705036  5064 solver.cpp:213] Iteration 5820, loss = 3.30829
I0623 12:27:15.705056  5064 solver.cpp:228]     Train net output #0: softmax = 3.30829 (* 1 = 3.30829 loss)
I0623 12:27:15.705061  5064 solver.cpp:473] Iteration 5820, lr = 0.0001
I0623 12:27:16.725297  5064 solver.cpp:213] Iteration 5830, loss = 3.24501
I0623 12:27:16.725317  5064 solver.cpp:228]     Train net output #0: softmax = 3.24501 (* 1 = 3.24501 loss)
I0623 12:27:16.725322  5064 solver.cpp:473] Iteration 5830, lr = 0.0001
I0623 12:27:17.745890  5064 solver.cpp:213] Iteration 5840, loss = 3.49238
I0623 12:27:17.745911  5064 solver.cpp:228]     Train net output #0: softmax = 3.49238 (* 1 = 3.49238 loss)
I0623 12:27:17.745916  5064 solver.cpp:473] Iteration 5840, lr = 0.0001
I0623 12:27:18.766126  5064 solver.cpp:213] Iteration 5850, loss = 3.47508
I0623 12:27:18.766147  5064 solver.cpp:228]     Train net output #0: softmax = 3.47508 (* 1 = 3.47508 loss)
I0623 12:27:18.766273  5064 solver.cpp:473] Iteration 5850, lr = 0.0001
I0623 12:27:19.786401  5064 solver.cpp:213] Iteration 5860, loss = 3.52133
I0623 12:27:19.786419  5064 solver.cpp:228]     Train net output #0: softmax = 3.52133 (* 1 = 3.52133 loss)
I0623 12:27:19.786438  5064 solver.cpp:473] Iteration 5860, lr = 0.0001
I0623 12:27:20.807221  5064 solver.cpp:213] Iteration 5870, loss = 3.2681
I0623 12:27:20.807240  5064 solver.cpp:228]     Train net output #0: softmax = 3.2681 (* 1 = 3.2681 loss)
I0623 12:27:20.807245  5064 solver.cpp:473] Iteration 5870, lr = 0.0001
I0623 12:27:21.827807  5064 solver.cpp:213] Iteration 5880, loss = 3.28167
I0623 12:27:21.827838  5064 solver.cpp:228]     Train net output #0: softmax = 3.28167 (* 1 = 3.28167 loss)
I0623 12:27:21.827844  5064 solver.cpp:473] Iteration 5880, lr = 0.0001
I0623 12:27:22.848521  5064 solver.cpp:213] Iteration 5890, loss = 3.68134
I0623 12:27:22.848543  5064 solver.cpp:228]     Train net output #0: softmax = 3.68134 (* 1 = 3.68134 loss)
I0623 12:27:22.848548  5064 solver.cpp:473] Iteration 5890, lr = 0.0001
I0623 12:27:23.868482  5064 solver.cpp:213] Iteration 5900, loss = 3.27255
I0623 12:27:23.868505  5064 solver.cpp:228]     Train net output #0: softmax = 3.27255 (* 1 = 3.27255 loss)
I0623 12:27:23.868664  5064 solver.cpp:473] Iteration 5900, lr = 0.0001
I0623 12:27:24.888797  5064 solver.cpp:213] Iteration 5910, loss = 3.51549
I0623 12:27:24.888816  5064 solver.cpp:228]     Train net output #0: softmax = 3.51549 (* 1 = 3.51549 loss)
I0623 12:27:24.888821  5064 solver.cpp:473] Iteration 5910, lr = 0.0001
I0623 12:27:25.908843  5064 solver.cpp:213] Iteration 5920, loss = 3.49078
I0623 12:27:25.908860  5064 solver.cpp:228]     Train net output #0: softmax = 3.49078 (* 1 = 3.49078 loss)
I0623 12:27:25.908872  5064 solver.cpp:473] Iteration 5920, lr = 0.0001
I0623 12:27:26.929461  5064 solver.cpp:213] Iteration 5930, loss = 3.45556
I0623 12:27:26.929482  5064 solver.cpp:228]     Train net output #0: softmax = 3.45556 (* 1 = 3.45556 loss)
I0623 12:27:26.929487  5064 solver.cpp:473] Iteration 5930, lr = 0.0001
I0623 12:27:27.950006  5064 solver.cpp:213] Iteration 5940, loss = 3.46039
I0623 12:27:27.950026  5064 solver.cpp:228]     Train net output #0: softmax = 3.46039 (* 1 = 3.46039 loss)
I0623 12:27:27.950031  5064 solver.cpp:473] Iteration 5940, lr = 0.0001
I0623 12:27:28.970815  5064 solver.cpp:213] Iteration 5950, loss = 3.44993
I0623 12:27:28.970842  5064 solver.cpp:228]     Train net output #0: softmax = 3.44993 (* 1 = 3.44993 loss)
I0623 12:27:28.970964  5064 solver.cpp:473] Iteration 5950, lr = 0.0001
I0623 12:27:29.990728  5064 solver.cpp:213] Iteration 5960, loss = 3.43962
I0623 12:27:29.990746  5064 solver.cpp:228]     Train net output #0: softmax = 3.43962 (* 1 = 3.43962 loss)
I0623 12:27:29.990751  5064 solver.cpp:473] Iteration 5960, lr = 0.0001
I0623 12:27:31.011528  5064 solver.cpp:213] Iteration 5970, loss = 3.26043
I0623 12:27:31.011546  5064 solver.cpp:228]     Train net output #0: softmax = 3.26043 (* 1 = 3.26043 loss)
I0623 12:27:31.011550  5064 solver.cpp:473] Iteration 5970, lr = 0.0001
I0623 12:27:32.031024  5064 solver.cpp:213] Iteration 5980, loss = 3.22104
I0623 12:27:32.031044  5064 solver.cpp:228]     Train net output #0: softmax = 3.22104 (* 1 = 3.22104 loss)
I0623 12:27:32.031049  5064 solver.cpp:473] Iteration 5980, lr = 0.0001
I0623 12:27:33.052207  5064 solver.cpp:213] Iteration 5990, loss = 3.04146
I0623 12:27:33.052228  5064 solver.cpp:228]     Train net output #0: softmax = 3.04146 (* 1 = 3.04146 loss)
I0623 12:27:33.052233  5064 solver.cpp:473] Iteration 5990, lr = 0.0001
I0623 12:27:34.002631  5064 solver.cpp:362] Snapshotting to snapshots/16-06-21_16h05m31s_0_11_pretrainClassification_iter_6000.caffemodel
I0623 12:27:34.003829  5064 solver.cpp:370] Snapshotting solver state to snapshots/16-06-21_16h05m31s_0_11_pretrainClassification_iter_6000.solverstate
I0623 12:27:34.004472  5064 solver.cpp:291] Iteration 6000, Testing net (#0)
I0623 12:27:34.165364  5064 solver.cpp:342]     Test net output #0: accuracy_top_1 = 0.229687
I0623 12:27:34.165380  5064 solver.cpp:342]     Test net output #1: accuracy_top_5 = 0.485938
I0623 12:27:34.165387  5064 solver.cpp:342]     Test net output #2: softmax = 3.32383 (* 1 = 3.32383 loss)
I0623 12:27:34.236101  5064 solver.cpp:213] Iteration 6000, loss = 3.45276
I0623 12:27:34.236116  5064 solver.cpp:228]     Train net output #0: softmax = 3.45276 (* 1 = 3.45276 loss)
I0623 12:27:34.236121  5064 solver.cpp:473] Iteration 6000, lr = 0.0001
I0623 12:27:35.254887  5064 solver.cpp:213] Iteration 6010, loss = 3.53931
I0623 12:27:35.254914  5064 solver.cpp:228]     Train net output #0: softmax = 3.53931 (* 1 = 3.53931 loss)
I0623 12:27:35.254920  5064 solver.cpp:473] Iteration 6010, lr = 0.0001
I0623 12:27:36.275807  5064 solver.cpp:213] Iteration 6020, loss = 3.29647
I0623 12:27:36.275828  5064 solver.cpp:228]     Train net output #0: softmax = 3.29647 (* 1 = 3.29647 loss)
I0623 12:27:36.275835  5064 solver.cpp:473] Iteration 6020, lr = 0.0001
I0623 12:27:37.296639  5064 solver.cpp:213] Iteration 6030, loss = 3.52539
I0623 12:27:37.296656  5064 solver.cpp:228]     Train net output #0: softmax = 3.52539 (* 1 = 3.52539 loss)
I0623 12:27:37.296661  5064 solver.cpp:473] Iteration 6030, lr = 0.0001
I0623 12:27:38.317221  5064 solver.cpp:213] Iteration 6040, loss = 3.48025
I0623 12:27:38.317237  5064 solver.cpp:228]     Train net output #0: softmax = 3.48025 (* 1 = 3.48025 loss)
I0623 12:27:38.317252  5064 solver.cpp:473] Iteration 6040, lr = 0.0001
I0623 12:27:39.337605  5064 solver.cpp:213] Iteration 6050, loss = 3.60649
I0623 12:27:39.337627  5064 solver.cpp:228]     Train net output #0: softmax = 3.60649 (* 1 = 3.60649 loss)
I0623 12:27:39.337638  5064 solver.cpp:473] Iteration 6050, lr = 0.0001
I0623 12:27:40.358665  5064 solver.cpp:213] Iteration 6060, loss = 3.39282
I0623 12:27:40.358685  5064 solver.cpp:228]     Train net output #0: softmax = 3.39282 (* 1 = 3.39282 loss)
I0623 12:27:40.358690  5064 solver.cpp:473] Iteration 6060, lr = 0.0001
I0623 12:27:41.379372  5064 solver.cpp:213] Iteration 6070, loss = 3.30246
I0623 12:27:41.379395  5064 solver.cpp:228]     Train net output #0: softmax = 3.30246 (* 1 = 3.30246 loss)
I0623 12:27:41.379400  5064 solver.cpp:473] Iteration 6070, lr = 0.0001
I0623 12:27:42.399852  5064 solver.cpp:213] Iteration 6080, loss = 3.24179
I0623 12:27:42.399871  5064 solver.cpp:228]     Train net output #0: softmax = 3.24179 (* 1 = 3.24179 loss)
I0623 12:27:42.399876  5064 solver.cpp:473] Iteration 6080, lr = 0.0001
I0623 12:27:43.419663  5064 solver.cpp:213] Iteration 6090, loss = 3.29887
I0623 12:27:43.419682  5064 solver.cpp:228]     Train net output #0: softmax = 3.29887 (* 1 = 3.29887 loss)
I0623 12:27:43.419687  5064 solver.cpp:473] Iteration 6090, lr = 0.0001
I0623 12:27:44.439936  5064 solver.cpp:213] Iteration 6100, loss = 3.45887
I0623 12:27:44.439960  5064 solver.cpp:228]     Train net output #0: softmax = 3.45887 (* 1 = 3.45887 loss)
I0623 12:27:44.440112  5064 solver.cpp:473] Iteration 6100, lr = 0.0001
I0623 12:27:45.460234  5064 solver.cpp:213] Iteration 6110, loss = 3.46916
I0623 12:27:45.460255  5064 solver.cpp:228]     Train net output #0: softmax = 3.46916 (* 1 = 3.46916 loss)
I0623 12:27:45.460260  5064 solver.cpp:473] Iteration 6110, lr = 0.0001
I0623 12:27:46.480958  5064 solver.cpp:213] Iteration 6120, loss = 3.39064
I0623 12:27:46.480978  5064 solver.cpp:228]     Train net output #0: softmax = 3.39064 (* 1 = 3.39064 loss)
I0623 12:27:46.480983  5064 solver.cpp:473] Iteration 6120, lr = 0.0001
I0623 12:27:47.500778  5064 solver.cpp:213] Iteration 6130, loss = 3.44339
I0623 12:27:47.500797  5064 solver.cpp:228]     Train net output #0: softmax = 3.44339 (* 1 = 3.44339 loss)
I0623 12:27:47.500802  5064 solver.cpp:473] Iteration 6130, lr = 0.0001
I0623 12:27:48.521221  5064 solver.cpp:213] Iteration 6140, loss = 3.21944
I0623 12:27:48.521257  5064 solver.cpp:228]     Train net output #0: softmax = 3.21944 (* 1 = 3.21944 loss)
I0623 12:27:48.521263  5064 solver.cpp:473] Iteration 6140, lr = 0.0001
I0623 12:27:49.541942  5064 solver.cpp:213] Iteration 6150, loss = 3.38593
I0623 12:27:49.541968  5064 solver.cpp:228]     Train net output #0: softmax = 3.38593 (* 1 = 3.38593 loss)
I0623 12:27:49.542107  5064 solver.cpp:473] Iteration 6150, lr = 0.0001
I0623 12:27:50.562139  5064 solver.cpp:213] Iteration 6160, loss = 3.51077
I0623 12:27:50.562156  5064 solver.cpp:228]     Train net output #0: softmax = 3.51077 (* 1 = 3.51077 loss)
I0623 12:27:50.562161  5064 solver.cpp:473] Iteration 6160, lr = 0.0001
I0623 12:27:51.583402  5064 solver.cpp:213] Iteration 6170, loss = 3.22215
I0623 12:27:51.583421  5064 solver.cpp:228]     Train net output #0: softmax = 3.22215 (* 1 = 3.22215 loss)
I0623 12:27:51.583426  5064 solver.cpp:473] Iteration 6170, lr = 0.0001
I0623 12:27:52.604120  5064 solver.cpp:213] Iteration 6180, loss = 3.15743
I0623 12:27:52.604145  5064 solver.cpp:228]     Train net output #0: softmax = 3.15743 (* 1 = 3.15743 loss)
I0623 12:27:52.604151  5064 solver.cpp:473] Iteration 6180, lr = 0.0001
I0623 12:27:53.625124  5064 solver.cpp:213] Iteration 6190, loss = 3.51378
I0623 12:27:53.625146  5064 solver.cpp:228]     Train net output #0: softmax = 3.51378 (* 1 = 3.51378 loss)
I0623 12:27:53.625151  5064 solver.cpp:473] Iteration 6190, lr = 0.0001
I0623 12:27:54.645457  5064 solver.cpp:213] Iteration 6200, loss = 3.43989
I0623 12:27:54.645478  5064 solver.cpp:228]     Train net output #0: softmax = 3.43989 (* 1 = 3.43989 loss)
I0623 12:27:54.645615  5064 solver.cpp:473] Iteration 6200, lr = 0.0001
I0623 12:27:55.665967  5064 solver.cpp:213] Iteration 6210, loss = 3.31694
I0623 12:27:55.665987  5064 solver.cpp:228]     Train net output #0: softmax = 3.31694 (* 1 = 3.31694 loss)
I0623 12:27:55.665993  5064 solver.cpp:473] Iteration 6210, lr = 0.0001
I0623 12:27:56.686214  5064 solver.cpp:213] Iteration 6220, loss = 3.24655
I0623 12:27:56.686241  5064 solver.cpp:228]     Train net output #0: softmax = 3.24655 (* 1 = 3.24655 loss)
I0623 12:27:56.686246  5064 solver.cpp:473] Iteration 6220, lr = 0.0001
I0623 12:27:57.707171  5064 solver.cpp:213] Iteration 6230, loss = 3.27932
I0623 12:27:57.707187  5064 solver.cpp:228]     Train net output #0: softmax = 3.27932 (* 1 = 3.27932 loss)
I0623 12:27:57.707192  5064 solver.cpp:473] Iteration 6230, lr = 0.0001
I0623 12:27:58.727816  5064 solver.cpp:213] Iteration 6240, loss = 3.39142
I0623 12:27:58.727833  5064 solver.cpp:228]     Train net output #0: softmax = 3.39142 (* 1 = 3.39142 loss)
I0623 12:27:58.727838  5064 solver.cpp:473] Iteration 6240, lr = 0.0001
I0623 12:27:59.748234  5064 solver.cpp:213] Iteration 6250, loss = 3.38569
I0623 12:27:59.748252  5064 solver.cpp:228]     Train net output #0: softmax = 3.38569 (* 1 = 3.38569 loss)
I0623 12:27:59.748374  5064 solver.cpp:473] Iteration 6250, lr = 0.0001
I0623 12:28:00.768556  5064 solver.cpp:213] Iteration 6260, loss = 3.25828
I0623 12:28:00.768573  5064 solver.cpp:228]     Train net output #0: softmax = 3.25828 (* 1 = 3.25828 loss)
I0623 12:28:00.768579  5064 solver.cpp:473] Iteration 6260, lr = 0.0001
I0623 12:28:01.789554  5064 solver.cpp:213] Iteration 6270, loss = 3.39321
I0623 12:28:01.789571  5064 solver.cpp:228]     Train net output #0: softmax = 3.39321 (* 1 = 3.39321 loss)
I0623 12:28:01.789575  5064 solver.cpp:473] Iteration 6270, lr = 0.0001
I0623 12:28:02.810972  5064 solver.cpp:213] Iteration 6280, loss = 3.43959
I0623 12:28:02.810988  5064 solver.cpp:228]     Train net output #0: softmax = 3.43959 (* 1 = 3.43959 loss)
I0623 12:28:02.810993  5064 solver.cpp:473] Iteration 6280, lr = 0.0001
I0623 12:28:03.831717  5064 solver.cpp:213] Iteration 6290, loss = 3.35416
I0623 12:28:03.831734  5064 solver.cpp:228]     Train net output #0: softmax = 3.35416 (* 1 = 3.35416 loss)
I0623 12:28:03.831739  5064 solver.cpp:473] Iteration 6290, lr = 0.0001
I0623 12:28:04.852159  5064 solver.cpp:213] Iteration 6300, loss = 3.55251
I0623 12:28:04.852344  5064 solver.cpp:228]     Train net output #0: softmax = 3.55251 (* 1 = 3.55251 loss)
I0623 12:28:04.852350  5064 solver.cpp:473] Iteration 6300, lr = 0.0001
I0623 12:28:05.872776  5064 solver.cpp:213] Iteration 6310, loss = 3.5479
I0623 12:28:05.872792  5064 solver.cpp:228]     Train net output #0: softmax = 3.5479 (* 1 = 3.5479 loss)
I0623 12:28:05.872797  5064 solver.cpp:473] Iteration 6310, lr = 0.0001
I0623 12:28:06.893591  5064 solver.cpp:213] Iteration 6320, loss = 3.48646
I0623 12:28:06.893610  5064 solver.cpp:228]     Train net output #0: softmax = 3.48646 (* 1 = 3.48646 loss)
I0623 12:28:06.893615  5064 solver.cpp:473] Iteration 6320, lr = 0.0001
I0623 12:28:07.913306  5064 solver.cpp:213] Iteration 6330, loss = 3.32148
I0623 12:28:07.913324  5064 solver.cpp:228]     Train net output #0: softmax = 3.32148 (* 1 = 3.32148 loss)
I0623 12:28:07.913329  5064 solver.cpp:473] Iteration 6330, lr = 0.0001
I0623 12:28:08.934352  5064 solver.cpp:213] Iteration 6340, loss = 3.34876
I0623 12:28:08.934373  5064 solver.cpp:228]     Train net output #0: softmax = 3.34876 (* 1 = 3.34876 loss)
I0623 12:28:08.934379  5064 solver.cpp:473] Iteration 6340, lr = 0.0001
I0623 12:28:09.955638  5064 solver.cpp:213] Iteration 6350, loss = 3.33547
I0623 12:28:09.955660  5064 solver.cpp:228]     Train net output #0: softmax = 3.33547 (* 1 = 3.33547 loss)
I0623 12:28:09.955796  5064 solver.cpp:473] Iteration 6350, lr = 0.0001
I0623 12:28:10.976076  5064 solver.cpp:213] Iteration 6360, loss = 3.25965
I0623 12:28:10.976095  5064 solver.cpp:228]     Train net output #0: softmax = 3.25965 (* 1 = 3.25965 loss)
I0623 12:28:10.976100  5064 solver.cpp:473] Iteration 6360, lr = 0.0001
I0623 12:28:11.996621  5064 solver.cpp:213] Iteration 6370, loss = 3.37762
I0623 12:28:11.996639  5064 solver.cpp:228]     Train net output #0: softmax = 3.37762 (* 1 = 3.37762 loss)
I0623 12:28:11.996644  5064 solver.cpp:473] Iteration 6370, lr = 0.0001
I0623 12:28:13.017477  5064 solver.cpp:213] Iteration 6380, loss = 3.31357
I0623 12:28:13.017505  5064 solver.cpp:228]     Train net output #0: softmax = 3.31357 (* 1 = 3.31357 loss)
I0623 12:28:13.017511  5064 solver.cpp:473] Iteration 6380, lr = 0.0001
I0623 12:28:14.038465  5064 solver.cpp:213] Iteration 6390, loss = 3.71413
I0623 12:28:14.038486  5064 solver.cpp:228]     Train net output #0: softmax = 3.71413 (* 1 = 3.71413 loss)
I0623 12:28:14.038491  5064 solver.cpp:473] Iteration 6390, lr = 0.0001
I0623 12:28:15.058913  5064 solver.cpp:213] Iteration 6400, loss = 3.49085
I0623 12:28:15.058938  5064 solver.cpp:228]     Train net output #0: softmax = 3.49085 (* 1 = 3.49085 loss)
I0623 12:28:15.059072  5064 solver.cpp:473] Iteration 6400, lr = 0.0001
I0623 12:28:16.079061  5064 solver.cpp:213] Iteration 6410, loss = 3.53132
I0623 12:28:16.079078  5064 solver.cpp:228]     Train net output #0: softmax = 3.53132 (* 1 = 3.53132 loss)
I0623 12:28:16.079083  5064 solver.cpp:473] Iteration 6410, lr = 0.0001
I0623 12:28:17.099725  5064 solver.cpp:213] Iteration 6420, loss = 3.56187
I0623 12:28:17.099745  5064 solver.cpp:228]     Train net output #0: softmax = 3.56187 (* 1 = 3.56187 loss)
I0623 12:28:17.099750  5064 solver.cpp:473] Iteration 6420, lr = 0.0001
I0623 12:28:18.119761  5064 solver.cpp:213] Iteration 6430, loss = 3.67601
I0623 12:28:18.119784  5064 solver.cpp:228]     Train net output #0: softmax = 3.67601 (* 1 = 3.67601 loss)
I0623 12:28:18.119789  5064 solver.cpp:473] Iteration 6430, lr = 0.0001
I0623 12:28:19.140437  5064 solver.cpp:213] Iteration 6440, loss = 3.61774
I0623 12:28:19.140460  5064 solver.cpp:228]     Train net output #0: softmax = 3.61774 (* 1 = 3.61774 loss)
I0623 12:28:19.140465  5064 solver.cpp:473] Iteration 6440, lr = 0.0001
I0623 12:28:20.160598  5064 solver.cpp:213] Iteration 6450, loss = 3.25403
I0623 12:28:20.160621  5064 solver.cpp:228]     Train net output #0: softmax = 3.25403 (* 1 = 3.25403 loss)
I0623 12:28:20.160739  5064 solver.cpp:473] Iteration 6450, lr = 0.0001
I0623 12:28:21.181263  5064 solver.cpp:213] Iteration 6460, loss = 2.95694
I0623 12:28:21.181282  5064 solver.cpp:228]     Train net output #0: softmax = 2.95694 (* 1 = 2.95694 loss)
I0623 12:28:21.181303  5064 solver.cpp:473] Iteration 6460, lr = 0.0001
I0623 12:28:22.201297  5064 solver.cpp:213] Iteration 6470, loss = 3.10783
I0623 12:28:22.201315  5064 solver.cpp:228]     Train net output #0: softmax = 3.10783 (* 1 = 3.10783 loss)
I0623 12:28:22.201320  5064 solver.cpp:473] Iteration 6470, lr = 0.0001
I0623 12:28:23.222046  5064 solver.cpp:213] Iteration 6480, loss = 3.28721
I0623 12:28:23.222069  5064 solver.cpp:228]     Train net output #0: softmax = 3.28721 (* 1 = 3.28721 loss)
I0623 12:28:23.222074  5064 solver.cpp:473] Iteration 6480, lr = 0.0001
I0623 12:28:24.242722  5064 solver.cpp:213] Iteration 6490, loss = 3.24686
I0623 12:28:24.242743  5064 solver.cpp:228]     Train net output #0: softmax = 3.24686 (* 1 = 3.24686 loss)
I0623 12:28:24.242748  5064 solver.cpp:473] Iteration 6490, lr = 0.0001
I0623 12:28:25.263988  5064 solver.cpp:213] Iteration 6500, loss = 3.31117
I0623 12:28:25.264010  5064 solver.cpp:228]     Train net output #0: softmax = 3.31117 (* 1 = 3.31117 loss)
I0623 12:28:25.264156  5064 solver.cpp:473] Iteration 6500, lr = 0.0001
I0623 12:28:26.285018  5064 solver.cpp:213] Iteration 6510, loss = 3.21231
I0623 12:28:26.285037  5064 solver.cpp:228]     Train net output #0: softmax = 3.21231 (* 1 = 3.21231 loss)
I0623 12:28:26.285043  5064 solver.cpp:473] Iteration 6510, lr = 0.0001
I0623 12:28:27.305105  5064 solver.cpp:213] Iteration 6520, loss = 3.13464
I0623 12:28:27.305125  5064 solver.cpp:228]     Train net output #0: softmax = 3.13464 (* 1 = 3.13464 loss)
I0623 12:28:27.305131  5064 solver.cpp:473] Iteration 6520, lr = 0.0001
I0623 12:28:28.325598  5064 solver.cpp:213] Iteration 6530, loss = 3.41598
I0623 12:28:28.325618  5064 solver.cpp:228]     Train net output #0: softmax = 3.41598 (* 1 = 3.41598 loss)
I0623 12:28:28.325623  5064 solver.cpp:473] Iteration 6530, lr = 0.0001
I0623 12:28:29.346362  5064 solver.cpp:213] Iteration 6540, loss = 3.47004
I0623 12:28:29.346391  5064 solver.cpp:228]     Train net output #0: softmax = 3.47004 (* 1 = 3.47004 loss)
I0623 12:28:29.346396  5064 solver.cpp:473] Iteration 6540, lr = 0.0001
I0623 12:28:30.367199  5064 solver.cpp:213] Iteration 6550, loss = 3.50096
I0623 12:28:30.367219  5064 solver.cpp:228]     Train net output #0: softmax = 3.50096 (* 1 = 3.50096 loss)
I0623 12:28:30.367224  5064 solver.cpp:473] Iteration 6550, lr = 0.0001
I0623 12:28:31.387545  5064 solver.cpp:213] Iteration 6560, loss = 3.25746
I0623 12:28:31.387564  5064 solver.cpp:228]     Train net output #0: softmax = 3.25746 (* 1 = 3.25746 loss)
I0623 12:28:31.387570  5064 solver.cpp:473] Iteration 6560, lr = 0.0001
I0623 12:28:32.407857  5064 solver.cpp:213] Iteration 6570, loss = 3.25971
I0623 12:28:32.407876  5064 solver.cpp:228]     Train net output #0: softmax = 3.25971 (* 1 = 3.25971 loss)
I0623 12:28:32.407881  5064 solver.cpp:473] Iteration 6570, lr = 0.0001
I0623 12:28:33.428030  5064 solver.cpp:213] Iteration 6580, loss = 3.34796
I0623 12:28:33.428047  5064 solver.cpp:228]     Train net output #0: softmax = 3.34796 (* 1 = 3.34796 loss)
I0623 12:28:33.428052  5064 solver.cpp:473] Iteration 6580, lr = 0.0001
I0623 12:28:34.448612  5064 solver.cpp:213] Iteration 6590, loss = 3.73934
I0623 12:28:34.448632  5064 solver.cpp:228]     Train net output #0: softmax = 3.73934 (* 1 = 3.73934 loss)
I0623 12:28:34.448637  5064 solver.cpp:473] Iteration 6590, lr = 0.0001
I0623 12:28:35.468652  5064 solver.cpp:213] Iteration 6600, loss = 3.16146
I0623 12:28:35.468823  5064 solver.cpp:228]     Train net output #0: softmax = 3.16146 (* 1 = 3.16146 loss)
I0623 12:28:35.468830  5064 solver.cpp:473] Iteration 6600, lr = 0.0001
I0623 12:28:36.489176  5064 solver.cpp:213] Iteration 6610, loss = 3.32112
I0623 12:28:36.489197  5064 solver.cpp:228]     Train net output #0: softmax = 3.32112 (* 1 = 3.32112 loss)
I0623 12:28:36.489202  5064 solver.cpp:473] Iteration 6610, lr = 0.0001
I0623 12:28:37.509305  5064 solver.cpp:213] Iteration 6620, loss = 3.49213
I0623 12:28:37.509328  5064 solver.cpp:228]     Train net output #0: softmax = 3.49213 (* 1 = 3.49213 loss)
I0623 12:28:37.509335  5064 solver.cpp:473] Iteration 6620, lr = 0.0001
I0623 12:28:38.530203  5064 solver.cpp:213] Iteration 6630, loss = 3.44908
I0623 12:28:38.530223  5064 solver.cpp:228]     Train net output #0: softmax = 3.44908 (* 1 = 3.44908 loss)
I0623 12:28:38.530228  5064 solver.cpp:473] Iteration 6630, lr = 0.0001
I0623 12:28:39.550503  5064 solver.cpp:213] Iteration 6640, loss = 3.3395
I0623 12:28:39.550523  5064 solver.cpp:228]     Train net output #0: softmax = 3.3395 (* 1 = 3.3395 loss)
I0623 12:28:39.550528  5064 solver.cpp:473] Iteration 6640, lr = 0.0001
I0623 12:28:40.571511  5064 solver.cpp:213] Iteration 6650, loss = 3.37277
I0623 12:28:40.571532  5064 solver.cpp:228]     Train net output #0: softmax = 3.37277 (* 1 = 3.37277 loss)
I0623 12:28:40.571676  5064 solver.cpp:473] Iteration 6650, lr = 0.0001
I0623 12:28:41.592751  5064 solver.cpp:213] Iteration 6660, loss = 3.4079
I0623 12:28:41.592770  5064 solver.cpp:228]     Train net output #0: softmax = 3.4079 (* 1 = 3.4079 loss)
I0623 12:28:41.592775  5064 solver.cpp:473] Iteration 6660, lr = 0.0001
I0623 12:28:42.613111  5064 solver.cpp:213] Iteration 6670, loss = 3.20642
I0623 12:28:42.613129  5064 solver.cpp:228]     Train net output #0: softmax = 3.20642 (* 1 = 3.20642 loss)
I0623 12:28:42.613134  5064 solver.cpp:473] Iteration 6670, lr = 0.0001
I0623 12:28:43.634071  5064 solver.cpp:213] Iteration 6680, loss = 3.40692
I0623 12:28:43.634091  5064 solver.cpp:228]     Train net output #0: softmax = 3.40692 (* 1 = 3.40692 loss)
I0623 12:28:43.634096  5064 solver.cpp:473] Iteration 6680, lr = 0.0001
I0623 12:28:44.654479  5064 solver.cpp:213] Iteration 6690, loss = 3.61033
I0623 12:28:44.654498  5064 solver.cpp:228]     Train net output #0: softmax = 3.61033 (* 1 = 3.61033 loss)
I0623 12:28:44.654503  5064 solver.cpp:473] Iteration 6690, lr = 0.0001
I0623 12:28:45.675554  5064 solver.cpp:213] Iteration 6700, loss = 3.59301
I0623 12:28:45.675585  5064 solver.cpp:228]     Train net output #0: softmax = 3.59301 (* 1 = 3.59301 loss)
I0623 12:28:45.675595  5064 solver.cpp:473] Iteration 6700, lr = 0.0001
I0623 12:28:46.696486  5064 solver.cpp:213] Iteration 6710, loss = 3.25638
I0623 12:28:46.696502  5064 solver.cpp:228]     Train net output #0: softmax = 3.25638 (* 1 = 3.25638 loss)
I0623 12:28:46.696507  5064 solver.cpp:473] Iteration 6710, lr = 0.0001
I0623 12:28:47.717710  5064 solver.cpp:213] Iteration 6720, loss = 3.27491
I0623 12:28:47.717725  5064 solver.cpp:228]     Train net output #0: softmax = 3.27491 (* 1 = 3.27491 loss)
I0623 12:28:47.717730  5064 solver.cpp:473] Iteration 6720, lr = 0.0001
I0623 12:28:48.738185  5064 solver.cpp:213] Iteration 6730, loss = 3.21933
I0623 12:28:48.738204  5064 solver.cpp:228]     Train net output #0: softmax = 3.21933 (* 1 = 3.21933 loss)
I0623 12:28:48.738209  5064 solver.cpp:473] Iteration 6730, lr = 0.0001
I0623 12:28:49.759094  5064 solver.cpp:213] Iteration 6740, loss = 3.14607
I0623 12:28:49.759112  5064 solver.cpp:228]     Train net output #0: softmax = 3.14607 (* 1 = 3.14607 loss)
I0623 12:28:49.759117  5064 solver.cpp:473] Iteration 6740, lr = 0.0001
I0623 12:28:50.779520  5064 solver.cpp:213] Iteration 6750, loss = 3.13174
I0623 12:28:50.779541  5064 solver.cpp:228]     Train net output #0: softmax = 3.13174 (* 1 = 3.13174 loss)
I0623 12:28:50.779682  5064 solver.cpp:473] Iteration 6750, lr = 0.0001
I0623 12:28:51.800038  5064 solver.cpp:213] Iteration 6760, loss = 3.46777
I0623 12:28:51.800055  5064 solver.cpp:228]     Train net output #0: softmax = 3.46777 (* 1 = 3.46777 loss)
I0623 12:28:51.800078  5064 solver.cpp:473] Iteration 6760, lr = 0.0001
I0623 12:28:52.820686  5064 solver.cpp:213] Iteration 6770, loss = 3.31129
I0623 12:28:52.820706  5064 solver.cpp:228]     Train net output #0: softmax = 3.31129 (* 1 = 3.31129 loss)
I0623 12:28:52.820713  5064 solver.cpp:473] Iteration 6770, lr = 0.0001
I0623 12:28:53.841125  5064 solver.cpp:213] Iteration 6780, loss = 3.4146
I0623 12:28:53.841145  5064 solver.cpp:228]     Train net output #0: softmax = 3.4146 (* 1 = 3.4146 loss)
I0623 12:28:53.841150  5064 solver.cpp:473] Iteration 6780, lr = 0.0001
I0623 12:28:54.861608  5064 solver.cpp:213] Iteration 6790, loss = 3.19303
I0623 12:28:54.861629  5064 solver.cpp:228]     Train net output #0: softmax = 3.19303 (* 1 = 3.19303 loss)
I0623 12:28:54.861634  5064 solver.cpp:473] Iteration 6790, lr = 0.0001
I0623 12:28:55.882050  5064 solver.cpp:213] Iteration 6800, loss = 3.41966
I0623 12:28:55.882071  5064 solver.cpp:228]     Train net output #0: softmax = 3.41966 (* 1 = 3.41966 loss)
I0623 12:28:55.882205  5064 solver.cpp:473] Iteration 6800, lr = 0.0001
I0623 12:28:56.901924  5064 solver.cpp:213] Iteration 6810, loss = 3.24722
I0623 12:28:56.901943  5064 solver.cpp:228]     Train net output #0: softmax = 3.24722 (* 1 = 3.24722 loss)
I0623 12:28:56.901948  5064 solver.cpp:473] Iteration 6810, lr = 0.0001
I0623 12:28:57.922111  5064 solver.cpp:213] Iteration 6820, loss = 3.24357
I0623 12:28:57.922133  5064 solver.cpp:228]     Train net output #0: softmax = 3.24357 (* 1 = 3.24357 loss)
I0623 12:28:57.922138  5064 solver.cpp:473] Iteration 6820, lr = 0.0001
I0623 12:28:58.942663  5064 solver.cpp:213] Iteration 6830, loss = 3.6557
I0623 12:28:58.942683  5064 solver.cpp:228]     Train net output #0: softmax = 3.6557 (* 1 = 3.6557 loss)
I0623 12:28:58.942688  5064 solver.cpp:473] Iteration 6830, lr = 0.0001
I0623 12:28:59.962792  5064 solver.cpp:213] Iteration 6840, loss = 3.41226
I0623 12:28:59.962811  5064 solver.cpp:228]     Train net output #0: softmax = 3.41226 (* 1 = 3.41226 loss)
I0623 12:28:59.962816  5064 solver.cpp:473] Iteration 6840, lr = 0.0001
I0623 12:29:00.983248  5064 solver.cpp:213] Iteration 6850, loss = 3.18316
I0623 12:29:00.983273  5064 solver.cpp:228]     Train net output #0: softmax = 3.18316 (* 1 = 3.18316 loss)
I0623 12:29:00.983392  5064 solver.cpp:473] Iteration 6850, lr = 0.0001
I0623 12:29:02.003880  5064 solver.cpp:213] Iteration 6860, loss = 3.16218
I0623 12:29:02.003898  5064 solver.cpp:228]     Train net output #0: softmax = 3.16218 (* 1 = 3.16218 loss)
I0623 12:29:02.003911  5064 solver.cpp:473] Iteration 6860, lr = 0.0001
I0623 12:29:03.024219  5064 solver.cpp:213] Iteration 6870, loss = 3.33843
I0623 12:29:03.024237  5064 solver.cpp:228]     Train net output #0: softmax = 3.33843 (* 1 = 3.33843 loss)
I0623 12:29:03.024242  5064 solver.cpp:473] Iteration 6870, lr = 0.0001
I0623 12:29:04.044556  5064 solver.cpp:213] Iteration 6880, loss = 3.57664
I0623 12:29:04.044574  5064 solver.cpp:228]     Train net output #0: softmax = 3.57664 (* 1 = 3.57664 loss)
I0623 12:29:04.044580  5064 solver.cpp:473] Iteration 6880, lr = 0.0001
I0623 12:29:05.065470  5064 solver.cpp:213] Iteration 6890, loss = 3.38081
I0623 12:29:05.065492  5064 solver.cpp:228]     Train net output #0: softmax = 3.38081 (* 1 = 3.38081 loss)
I0623 12:29:05.065497  5064 solver.cpp:473] Iteration 6890, lr = 0.0001
I0623 12:29:06.085808  5064 solver.cpp:213] Iteration 6900, loss = 3.04024
I0623 12:29:06.085981  5064 solver.cpp:228]     Train net output #0: softmax = 3.04024 (* 1 = 3.04024 loss)
I0623 12:29:06.085989  5064 solver.cpp:473] Iteration 6900, lr = 0.0001
I0623 12:29:07.106319  5064 solver.cpp:213] Iteration 6910, loss = 3.1431
I0623 12:29:07.106340  5064 solver.cpp:228]     Train net output #0: softmax = 3.1431 (* 1 = 3.1431 loss)
I0623 12:29:07.106345  5064 solver.cpp:473] Iteration 6910, lr = 0.0001
I0623 12:29:08.126452  5064 solver.cpp:213] Iteration 6920, loss = 3.24465
I0623 12:29:08.126471  5064 solver.cpp:228]     Train net output #0: softmax = 3.24465 (* 1 = 3.24465 loss)
I0623 12:29:08.126477  5064 solver.cpp:473] Iteration 6920, lr = 0.0001
I0623 12:29:09.147088  5064 solver.cpp:213] Iteration 6930, loss = 3.30373
I0623 12:29:09.147109  5064 solver.cpp:228]     Train net output #0: softmax = 3.30373 (* 1 = 3.30373 loss)
I0623 12:29:09.147114  5064 solver.cpp:473] Iteration 6930, lr = 0.0001
I0623 12:29:10.167951  5064 solver.cpp:213] Iteration 6940, loss = 3.50873
I0623 12:29:10.167970  5064 solver.cpp:228]     Train net output #0: softmax = 3.50873 (* 1 = 3.50873 loss)
I0623 12:29:10.167975  5064 solver.cpp:473] Iteration 6940, lr = 0.0001
I0623 12:29:11.189445  5064 solver.cpp:213] Iteration 6950, loss = 3.31962
I0623 12:29:11.189468  5064 solver.cpp:228]     Train net output #0: softmax = 3.31962 (* 1 = 3.31962 loss)
I0623 12:29:11.189602  5064 solver.cpp:473] Iteration 6950, lr = 0.0001
I0623 12:29:12.207558  5064 solver.cpp:213] Iteration 6960, loss = 3.32319
I0623 12:29:12.207578  5064 solver.cpp:228]     Train net output #0: softmax = 3.32319 (* 1 = 3.32319 loss)
I0623 12:29:12.207583  5064 solver.cpp:473] Iteration 6960, lr = 0.0001
I0623 12:29:13.228237  5064 solver.cpp:213] Iteration 6970, loss = 3.11192
I0623 12:29:13.228257  5064 solver.cpp:228]     Train net output #0: softmax = 3.11192 (* 1 = 3.11192 loss)
I0623 12:29:13.228262  5064 solver.cpp:473] Iteration 6970, lr = 0.0001
I0623 12:29:14.248884  5064 solver.cpp:213] Iteration 6980, loss = 3.39753
I0623 12:29:14.248904  5064 solver.cpp:228]     Train net output #0: softmax = 3.39753 (* 1 = 3.39753 loss)
I0623 12:29:14.248909  5064 solver.cpp:473] Iteration 6980, lr = 0.0001
I0623 12:29:15.269574  5064 solver.cpp:213] Iteration 6990, loss = 3.15654
I0623 12:29:15.269594  5064 solver.cpp:228]     Train net output #0: softmax = 3.15654 (* 1 = 3.15654 loss)
I0623 12:29:15.269599  5064 solver.cpp:473] Iteration 6990, lr = 0.0001
I0623 12:29:16.219516  5064 solver.cpp:362] Snapshotting to snapshots/16-06-21_16h05m31s_0_11_pretrainClassification_iter_7000.caffemodel
I0623 12:29:16.220636  5064 solver.cpp:370] Snapshotting solver state to snapshots/16-06-21_16h05m31s_0_11_pretrainClassification_iter_7000.solverstate
I0623 12:29:16.221253  5064 solver.cpp:291] Iteration 7000, Testing net (#0)
I0623 12:29:16.382364  5064 solver.cpp:342]     Test net output #0: accuracy_top_1 = 0.209375
I0623 12:29:16.382382  5064 solver.cpp:342]     Test net output #1: accuracy_top_5 = 0.475
I0623 12:29:16.382388  5064 solver.cpp:342]     Test net output #2: softmax = 3.3102 (* 1 = 3.3102 loss)
I0623 12:29:16.453004  5064 solver.cpp:213] Iteration 7000, loss = 3.14098
I0623 12:29:16.453018  5064 solver.cpp:228]     Train net output #0: softmax = 3.14098 (* 1 = 3.14098 loss)
I0623 12:29:16.453023  5064 solver.cpp:473] Iteration 7000, lr = 0.0001
I0623 12:29:17.473253  5064 solver.cpp:213] Iteration 7010, loss = 3.37438
I0623 12:29:17.473268  5064 solver.cpp:228]     Train net output #0: softmax = 3.37438 (* 1 = 3.37438 loss)
I0623 12:29:17.473273  5064 solver.cpp:473] Iteration 7010, lr = 0.0001
I0623 12:29:18.493088  5064 solver.cpp:213] Iteration 7020, loss = 3.37599
I0623 12:29:18.493106  5064 solver.cpp:228]     Train net output #0: softmax = 3.37599 (* 1 = 3.37599 loss)
I0623 12:29:18.493111  5064 solver.cpp:473] Iteration 7020, lr = 0.0001
I0623 12:29:19.513700  5064 solver.cpp:213] Iteration 7030, loss = 3.35965
I0623 12:29:19.513720  5064 solver.cpp:228]     Train net output #0: softmax = 3.35965 (* 1 = 3.35965 loss)
I0623 12:29:19.513723  5064 solver.cpp:473] Iteration 7030, lr = 0.0001
I0623 12:29:20.533511  5064 solver.cpp:213] Iteration 7040, loss = 3.06995
I0623 12:29:20.533529  5064 solver.cpp:228]     Train net output #0: softmax = 3.06995 (* 1 = 3.06995 loss)
I0623 12:29:20.533535  5064 solver.cpp:473] Iteration 7040, lr = 0.0001
I0623 12:29:21.554618  5064 solver.cpp:213] Iteration 7050, loss = 3.33045
I0623 12:29:21.554639  5064 solver.cpp:228]     Train net output #0: softmax = 3.33045 (* 1 = 3.33045 loss)
I0623 12:29:21.554644  5064 solver.cpp:473] Iteration 7050, lr = 0.0001
I0623 12:29:22.575202  5064 solver.cpp:213] Iteration 7060, loss = 3.22628
I0623 12:29:22.575232  5064 solver.cpp:228]     Train net output #0: softmax = 3.22628 (* 1 = 3.22628 loss)
I0623 12:29:22.575238  5064 solver.cpp:473] Iteration 7060, lr = 0.0001
I0623 12:29:23.596019  5064 solver.cpp:213] Iteration 7070, loss = 3.23572
I0623 12:29:23.596040  5064 solver.cpp:228]     Train net output #0: softmax = 3.23572 (* 1 = 3.23572 loss)
I0623 12:29:23.596045  5064 solver.cpp:473] Iteration 7070, lr = 0.0001
I0623 12:29:24.616062  5064 solver.cpp:213] Iteration 7080, loss = 3.23456
I0623 12:29:24.616080  5064 solver.cpp:228]     Train net output #0: softmax = 3.23456 (* 1 = 3.23456 loss)
I0623 12:29:24.616086  5064 solver.cpp:473] Iteration 7080, lr = 0.0001
I0623 12:29:25.637045  5064 solver.cpp:213] Iteration 7090, loss = 3.49771
I0623 12:29:25.637068  5064 solver.cpp:228]     Train net output #0: softmax = 3.49771 (* 1 = 3.49771 loss)
I0623 12:29:25.637073  5064 solver.cpp:473] Iteration 7090, lr = 0.0001
I0623 12:29:26.657344  5064 solver.cpp:213] Iteration 7100, loss = 3.33717
I0623 12:29:26.657374  5064 solver.cpp:228]     Train net output #0: softmax = 3.33717 (* 1 = 3.33717 loss)
I0623 12:29:26.657532  5064 solver.cpp:473] Iteration 7100, lr = 0.0001
I0623 12:29:27.677947  5064 solver.cpp:213] Iteration 7110, loss = 3.21899
I0623 12:29:27.677970  5064 solver.cpp:228]     Train net output #0: softmax = 3.21899 (* 1 = 3.21899 loss)
I0623 12:29:27.677976  5064 solver.cpp:473] Iteration 7110, lr = 0.0001
I0623 12:29:28.698746  5064 solver.cpp:213] Iteration 7120, loss = 3.30645
I0623 12:29:28.698768  5064 solver.cpp:228]     Train net output #0: softmax = 3.30645 (* 1 = 3.30645 loss)
I0623 12:29:28.698774  5064 solver.cpp:473] Iteration 7120, lr = 0.0001
I0623 12:29:29.719104  5064 solver.cpp:213] Iteration 7130, loss = 3.25263
I0623 12:29:29.719125  5064 solver.cpp:228]     Train net output #0: softmax = 3.25263 (* 1 = 3.25263 loss)
I0623 12:29:29.719130  5064 solver.cpp:473] Iteration 7130, lr = 0.0001
I0623 12:29:30.740084  5064 solver.cpp:213] Iteration 7140, loss = 3.2358
I0623 12:29:30.740106  5064 solver.cpp:228]     Train net output #0: softmax = 3.2358 (* 1 = 3.2358 loss)
I0623 12:29:30.740113  5064 solver.cpp:473] Iteration 7140, lr = 0.0001
I0623 12:29:31.760953  5064 solver.cpp:213] Iteration 7150, loss = 3.48799
I0623 12:29:31.760977  5064 solver.cpp:228]     Train net output #0: softmax = 3.48799 (* 1 = 3.48799 loss)
I0623 12:29:31.761103  5064 solver.cpp:473] Iteration 7150, lr = 0.0001
I0623 12:29:32.781052  5064 solver.cpp:213] Iteration 7160, loss = 3.24168
I0623 12:29:32.781070  5064 solver.cpp:228]     Train net output #0: softmax = 3.24168 (* 1 = 3.24168 loss)
I0623 12:29:32.781075  5064 solver.cpp:473] Iteration 7160, lr = 0.0001
I0623 12:29:33.800835  5064 solver.cpp:213] Iteration 7170, loss = 3.19609
I0623 12:29:33.800853  5064 solver.cpp:228]     Train net output #0: softmax = 3.19609 (* 1 = 3.19609 loss)
I0623 12:29:33.800858  5064 solver.cpp:473] Iteration 7170, lr = 0.0001
I0623 12:29:34.821317  5064 solver.cpp:213] Iteration 7180, loss = 3.39506
I0623 12:29:34.821336  5064 solver.cpp:228]     Train net output #0: softmax = 3.39506 (* 1 = 3.39506 loss)
I0623 12:29:34.821341  5064 solver.cpp:473] Iteration 7180, lr = 0.0001
I0623 12:29:35.836463  5064 solver.cpp:213] Iteration 7190, loss = 3.37456
I0623 12:29:35.836483  5064 solver.cpp:228]     Train net output #0: softmax = 3.37456 (* 1 = 3.37456 loss)
I0623 12:29:35.836488  5064 solver.cpp:473] Iteration 7190, lr = 0.0001
I0623 12:29:36.857209  5064 solver.cpp:213] Iteration 7200, loss = 3.18221
I0623 12:29:36.857375  5064 solver.cpp:228]     Train net output #0: softmax = 3.18221 (* 1 = 3.18221 loss)
I0623 12:29:36.857383  5064 solver.cpp:473] Iteration 7200, lr = 0.0001
I0623 12:29:37.877622  5064 solver.cpp:213] Iteration 7210, loss = 3.1769
I0623 12:29:37.877641  5064 solver.cpp:228]     Train net output #0: softmax = 3.1769 (* 1 = 3.1769 loss)
I0623 12:29:37.877646  5064 solver.cpp:473] Iteration 7210, lr = 0.0001
I0623 12:29:38.897969  5064 solver.cpp:213] Iteration 7220, loss = 3.45937
I0623 12:29:38.897989  5064 solver.cpp:228]     Train net output #0: softmax = 3.45937 (* 1 = 3.45937 loss)
I0623 12:29:38.897994  5064 solver.cpp:473] Iteration 7220, lr = 0.0001
I0623 12:29:39.918884  5064 solver.cpp:213] Iteration 7230, loss = 3.21373
I0623 12:29:39.918901  5064 solver.cpp:228]     Train net output #0: softmax = 3.21373 (* 1 = 3.21373 loss)
I0623 12:29:39.918906  5064 solver.cpp:473] Iteration 7230, lr = 0.0001
I0623 12:29:40.939571  5064 solver.cpp:213] Iteration 7240, loss = 3.35668
I0623 12:29:40.939592  5064 solver.cpp:228]     Train net output #0: softmax = 3.35668 (* 1 = 3.35668 loss)
I0623 12:29:40.939597  5064 solver.cpp:473] Iteration 7240, lr = 0.0001
I0623 12:29:41.960170  5064 solver.cpp:213] Iteration 7250, loss = 3.17582
I0623 12:29:41.960193  5064 solver.cpp:228]     Train net output #0: softmax = 3.17582 (* 1 = 3.17582 loss)
I0623 12:29:41.960319  5064 solver.cpp:473] Iteration 7250, lr = 0.0001
I0623 12:29:42.980479  5064 solver.cpp:213] Iteration 7260, loss = 3.32268
I0623 12:29:42.980499  5064 solver.cpp:228]     Train net output #0: softmax = 3.32268 (* 1 = 3.32268 loss)
I0623 12:29:42.980504  5064 solver.cpp:473] Iteration 7260, lr = 0.0001
I0623 12:29:44.000922  5064 solver.cpp:213] Iteration 7270, loss = 3.56883
I0623 12:29:44.000942  5064 solver.cpp:228]     Train net output #0: softmax = 3.56883 (* 1 = 3.56883 loss)
I0623 12:29:44.000947  5064 solver.cpp:473] Iteration 7270, lr = 0.0001
I0623 12:29:45.021639  5064 solver.cpp:213] Iteration 7280, loss = 3.09632
I0623 12:29:45.021658  5064 solver.cpp:228]     Train net output #0: softmax = 3.09632 (* 1 = 3.09632 loss)
I0623 12:29:45.021663  5064 solver.cpp:473] Iteration 7280, lr = 0.0001
I0623 12:29:46.042042  5064 solver.cpp:213] Iteration 7290, loss = 3.1729
I0623 12:29:46.042062  5064 solver.cpp:228]     Train net output #0: softmax = 3.1729 (* 1 = 3.1729 loss)
I0623 12:29:46.042068  5064 solver.cpp:473] Iteration 7290, lr = 0.0001
I0623 12:29:47.063278  5064 solver.cpp:213] Iteration 7300, loss = 3.06499
I0623 12:29:47.063302  5064 solver.cpp:228]     Train net output #0: softmax = 3.06499 (* 1 = 3.06499 loss)
I0623 12:29:47.063313  5064 solver.cpp:473] Iteration 7300, lr = 0.0001
I0623 12:29:48.083811  5064 solver.cpp:213] Iteration 7310, loss = 3.12771
I0623 12:29:48.083832  5064 solver.cpp:228]     Train net output #0: softmax = 3.12771 (* 1 = 3.12771 loss)
I0623 12:29:48.083837  5064 solver.cpp:473] Iteration 7310, lr = 0.0001
I0623 12:29:49.103745  5064 solver.cpp:213] Iteration 7320, loss = 3.16229
I0623 12:29:49.103765  5064 solver.cpp:228]     Train net output #0: softmax = 3.16229 (* 1 = 3.16229 loss)
I0623 12:29:49.103770  5064 solver.cpp:473] Iteration 7320, lr = 0.0001
I0623 12:29:50.124258  5064 solver.cpp:213] Iteration 7330, loss = 3.18984
I0623 12:29:50.124276  5064 solver.cpp:228]     Train net output #0: softmax = 3.18984 (* 1 = 3.18984 loss)
I0623 12:29:50.124281  5064 solver.cpp:473] Iteration 7330, lr = 0.0001
I0623 12:29:51.144585  5064 solver.cpp:213] Iteration 7340, loss = 3.28634
I0623 12:29:51.144603  5064 solver.cpp:228]     Train net output #0: softmax = 3.28634 (* 1 = 3.28634 loss)
I0623 12:29:51.144608  5064 solver.cpp:473] Iteration 7340, lr = 0.0001
I0623 12:29:52.164523  5064 solver.cpp:213] Iteration 7350, loss = 3.3045
I0623 12:29:52.164546  5064 solver.cpp:228]     Train net output #0: softmax = 3.3045 (* 1 = 3.3045 loss)
I0623 12:29:52.164695  5064 solver.cpp:473] Iteration 7350, lr = 0.0001
I0623 12:29:53.184442  5064 solver.cpp:213] Iteration 7360, loss = 3.00969
I0623 12:29:53.184461  5064 solver.cpp:228]     Train net output #0: softmax = 3.00969 (* 1 = 3.00969 loss)
I0623 12:29:53.184484  5064 solver.cpp:473] Iteration 7360, lr = 0.0001
I0623 12:29:54.205548  5064 solver.cpp:213] Iteration 7370, loss = 3.3962
I0623 12:29:54.205567  5064 solver.cpp:228]     Train net output #0: softmax = 3.3962 (* 1 = 3.3962 loss)
I0623 12:29:54.205572  5064 solver.cpp:473] Iteration 7370, lr = 0.0001
I0623 12:29:55.225368  5064 solver.cpp:213] Iteration 7380, loss = 3.16314
I0623 12:29:55.225386  5064 solver.cpp:228]     Train net output #0: softmax = 3.16314 (* 1 = 3.16314 loss)
I0623 12:29:55.225391  5064 solver.cpp:473] Iteration 7380, lr = 0.0001
I0623 12:29:56.245939  5064 solver.cpp:213] Iteration 7390, loss = 3.23778
I0623 12:29:56.245959  5064 solver.cpp:228]     Train net output #0: softmax = 3.23778 (* 1 = 3.23778 loss)
I0623 12:29:56.245964  5064 solver.cpp:473] Iteration 7390, lr = 0.0001
I0623 12:29:57.266808  5064 solver.cpp:213] Iteration 7400, loss = 3.30724
I0623 12:29:57.266832  5064 solver.cpp:228]     Train net output #0: softmax = 3.30724 (* 1 = 3.30724 loss)
I0623 12:29:57.266968  5064 solver.cpp:473] Iteration 7400, lr = 0.0001
I0623 12:29:58.287469  5064 solver.cpp:213] Iteration 7410, loss = 3.34193
I0623 12:29:58.287489  5064 solver.cpp:228]     Train net output #0: softmax = 3.34193 (* 1 = 3.34193 loss)
I0623 12:29:58.287494  5064 solver.cpp:473] Iteration 7410, lr = 0.0001
I0623 12:29:59.307760  5064 solver.cpp:213] Iteration 7420, loss = 3.41611
I0623 12:29:59.307778  5064 solver.cpp:228]     Train net output #0: softmax = 3.41611 (* 1 = 3.41611 loss)
I0623 12:29:59.307783  5064 solver.cpp:473] Iteration 7420, lr = 0.0001
I0623 12:30:00.328501  5064 solver.cpp:213] Iteration 7430, loss = 3.11029
I0623 12:30:00.328521  5064 solver.cpp:228]     Train net output #0: softmax = 3.11029 (* 1 = 3.11029 loss)
I0623 12:30:00.328526  5064 solver.cpp:473] Iteration 7430, lr = 0.0001
I0623 12:30:01.348973  5064 solver.cpp:213] Iteration 7440, loss = 3.33843
I0623 12:30:01.348994  5064 solver.cpp:228]     Train net output #0: softmax = 3.33843 (* 1 = 3.33843 loss)
I0623 12:30:01.348999  5064 solver.cpp:473] Iteration 7440, lr = 0.0001
I0623 12:30:02.369879  5064 solver.cpp:213] Iteration 7450, loss = 3.37454
I0623 12:30:02.369901  5064 solver.cpp:228]     Train net output #0: softmax = 3.37454 (* 1 = 3.37454 loss)
I0623 12:30:02.369906  5064 solver.cpp:473] Iteration 7450, lr = 0.0001
I0623 12:30:03.390291  5064 solver.cpp:213] Iteration 7460, loss = 3.24301
I0623 12:30:03.390311  5064 solver.cpp:228]     Train net output #0: softmax = 3.24301 (* 1 = 3.24301 loss)
I0623 12:30:03.390316  5064 solver.cpp:473] Iteration 7460, lr = 0.0001
I0623 12:30:04.410909  5064 solver.cpp:213] Iteration 7470, loss = 3.10622
I0623 12:30:04.410928  5064 solver.cpp:228]     Train net output #0: softmax = 3.10622 (* 1 = 3.10622 loss)
I0623 12:30:04.410933  5064 solver.cpp:473] Iteration 7470, lr = 0.0001
I0623 12:30:05.432050  5064 solver.cpp:213] Iteration 7480, loss = 3.36922
I0623 12:30:05.432077  5064 solver.cpp:228]     Train net output #0: softmax = 3.36922 (* 1 = 3.36922 loss)
I0623 12:30:05.432082  5064 solver.cpp:473] Iteration 7480, lr = 0.0001
I0623 12:30:06.453589  5064 solver.cpp:213] Iteration 7490, loss = 3.45976
I0623 12:30:06.453610  5064 solver.cpp:228]     Train net output #0: softmax = 3.45976 (* 1 = 3.45976 loss)
I0623 12:30:06.453615  5064 solver.cpp:473] Iteration 7490, lr = 0.0001
I0623 12:30:07.472918  5064 solver.cpp:213] Iteration 7500, loss = 3.34563
I0623 12:30:07.473088  5064 solver.cpp:228]     Train net output #0: softmax = 3.34563 (* 1 = 3.34563 loss)
I0623 12:30:07.473094  5064 solver.cpp:473] Iteration 7500, lr = 0.0001
I0623 12:30:08.493659  5064 solver.cpp:213] Iteration 7510, loss = 3.1731
I0623 12:30:08.493681  5064 solver.cpp:228]     Train net output #0: softmax = 3.1731 (* 1 = 3.1731 loss)
I0623 12:30:08.493686  5064 solver.cpp:473] Iteration 7510, lr = 0.0001
I0623 12:30:09.514454  5064 solver.cpp:213] Iteration 7520, loss = 3.18659
I0623 12:30:09.514477  5064 solver.cpp:228]     Train net output #0: softmax = 3.18659 (* 1 = 3.18659 loss)
I0623 12:30:09.514482  5064 solver.cpp:473] Iteration 7520, lr = 0.0001
I0623 12:30:10.535392  5064 solver.cpp:213] Iteration 7530, loss = 3.1585
I0623 12:30:10.535413  5064 solver.cpp:228]     Train net output #0: softmax = 3.1585 (* 1 = 3.1585 loss)
I0623 12:30:10.535418  5064 solver.cpp:473] Iteration 7530, lr = 0.0001
I0623 12:30:11.556100  5064 solver.cpp:213] Iteration 7540, loss = 3.37809
I0623 12:30:11.556120  5064 solver.cpp:228]     Train net output #0: softmax = 3.37809 (* 1 = 3.37809 loss)
I0623 12:30:11.556125  5064 solver.cpp:473] Iteration 7540, lr = 0.0001
I0623 12:30:12.576483  5064 solver.cpp:213] Iteration 7550, loss = 3.18331
I0623 12:30:12.576506  5064 solver.cpp:228]     Train net output #0: softmax = 3.18331 (* 1 = 3.18331 loss)
I0623 12:30:12.576658  5064 solver.cpp:473] Iteration 7550, lr = 0.0001
I0623 12:30:13.597338  5064 solver.cpp:213] Iteration 7560, loss = 3.28335
I0623 12:30:13.597357  5064 solver.cpp:228]     Train net output #0: softmax = 3.28335 (* 1 = 3.28335 loss)
I0623 12:30:13.597362  5064 solver.cpp:473] Iteration 7560, lr = 0.0001
I0623 12:30:14.617357  5064 solver.cpp:213] Iteration 7570, loss = 3.38816
I0623 12:30:14.617377  5064 solver.cpp:228]     Train net output #0: softmax = 3.38816 (* 1 = 3.38816 loss)
I0623 12:30:14.617382  5064 solver.cpp:473] Iteration 7570, lr = 0.0001
I0623 12:30:15.637987  5064 solver.cpp:213] Iteration 7580, loss = 3.37665
I0623 12:30:15.638007  5064 solver.cpp:228]     Train net output #0: softmax = 3.37665 (* 1 = 3.37665 loss)
I0623 12:30:15.638012  5064 solver.cpp:473] Iteration 7580, lr = 0.0001
I0623 12:30:16.658713  5064 solver.cpp:213] Iteration 7590, loss = 3.23797
I0623 12:30:16.658735  5064 solver.cpp:228]     Train net output #0: softmax = 3.23797 (* 1 = 3.23797 loss)
I0623 12:30:16.658740  5064 solver.cpp:473] Iteration 7590, lr = 0.0001
I0623 12:30:17.679307  5064 solver.cpp:213] Iteration 7600, loss = 3.02109
I0623 12:30:17.679332  5064 solver.cpp:228]     Train net output #0: softmax = 3.02109 (* 1 = 3.02109 loss)
I0623 12:30:17.679458  5064 solver.cpp:473] Iteration 7600, lr = 0.0001
I0623 12:30:18.699545  5064 solver.cpp:213] Iteration 7610, loss = 3.36334
I0623 12:30:18.699563  5064 solver.cpp:228]     Train net output #0: softmax = 3.36334 (* 1 = 3.36334 loss)
I0623 12:30:18.699568  5064 solver.cpp:473] Iteration 7610, lr = 0.0001
I0623 12:30:19.719760  5064 solver.cpp:213] Iteration 7620, loss = 3.03729
I0623 12:30:19.719779  5064 solver.cpp:228]     Train net output #0: softmax = 3.03729 (* 1 = 3.03729 loss)
I0623 12:30:19.719784  5064 solver.cpp:473] Iteration 7620, lr = 0.0001
I0623 12:30:20.740312  5064 solver.cpp:213] Iteration 7630, loss = 3.32458
I0623 12:30:20.740332  5064 solver.cpp:228]     Train net output #0: softmax = 3.32458 (* 1 = 3.32458 loss)
I0623 12:30:20.740337  5064 solver.cpp:473] Iteration 7630, lr = 0.0001
I0623 12:30:21.761268  5064 solver.cpp:213] Iteration 7640, loss = 3.22869
I0623 12:30:21.761297  5064 solver.cpp:228]     Train net output #0: softmax = 3.22869 (* 1 = 3.22869 loss)
I0623 12:30:21.761303  5064 solver.cpp:473] Iteration 7640, lr = 0.0001
I0623 12:30:22.781967  5064 solver.cpp:213] Iteration 7650, loss = 3.15194
I0623 12:30:22.781993  5064 solver.cpp:228]     Train net output #0: softmax = 3.15194 (* 1 = 3.15194 loss)
I0623 12:30:22.782119  5064 solver.cpp:473] Iteration 7650, lr = 0.0001
I0623 12:30:23.803009  5064 solver.cpp:213] Iteration 7660, loss = 3.51846
I0623 12:30:23.803030  5064 solver.cpp:228]     Train net output #0: softmax = 3.51846 (* 1 = 3.51846 loss)
I0623 12:30:23.803053  5064 solver.cpp:473] Iteration 7660, lr = 0.0001
I0623 12:30:24.823839  5064 solver.cpp:213] Iteration 7670, loss = 3.11497
I0623 12:30:24.823858  5064 solver.cpp:228]     Train net output #0: softmax = 3.11497 (* 1 = 3.11497 loss)
I0623 12:30:24.823863  5064 solver.cpp:473] Iteration 7670, lr = 0.0001
I0623 12:30:25.844923  5064 solver.cpp:213] Iteration 7680, loss = 3.25759
I0623 12:30:25.844944  5064 solver.cpp:228]     Train net output #0: softmax = 3.25759 (* 1 = 3.25759 loss)
I0623 12:30:25.844949  5064 solver.cpp:473] Iteration 7680, lr = 0.0001
I0623 12:30:26.864964  5064 solver.cpp:213] Iteration 7690, loss = 3.51638
I0623 12:30:26.864982  5064 solver.cpp:228]     Train net output #0: softmax = 3.51638 (* 1 = 3.51638 loss)
I0623 12:30:26.864987  5064 solver.cpp:473] Iteration 7690, lr = 0.0001
I0623 12:30:27.884963  5064 solver.cpp:213] Iteration 7700, loss = 3.25998
I0623 12:30:27.884989  5064 solver.cpp:228]     Train net output #0: softmax = 3.25998 (* 1 = 3.25998 loss)
I0623 12:30:27.885121  5064 solver.cpp:473] Iteration 7700, lr = 0.0001
I0623 12:30:28.905824  5064 solver.cpp:213] Iteration 7710, loss = 3.42055
I0623 12:30:28.905848  5064 solver.cpp:228]     Train net output #0: softmax = 3.42055 (* 1 = 3.42055 loss)
I0623 12:30:28.905853  5064 solver.cpp:473] Iteration 7710, lr = 0.0001
I0623 12:30:29.926815  5064 solver.cpp:213] Iteration 7720, loss = 3.15196
I0623 12:30:29.926837  5064 solver.cpp:228]     Train net output #0: softmax = 3.15196 (* 1 = 3.15196 loss)
I0623 12:30:29.926842  5064 solver.cpp:473] Iteration 7720, lr = 0.0001
I0623 12:30:30.947875  5064 solver.cpp:213] Iteration 7730, loss = 3.46309
I0623 12:30:30.947897  5064 solver.cpp:228]     Train net output #0: softmax = 3.46309 (* 1 = 3.46309 loss)
I0623 12:30:30.947902  5064 solver.cpp:473] Iteration 7730, lr = 0.0001
I0623 12:30:31.968024  5064 solver.cpp:213] Iteration 7740, loss = 3.42242
I0623 12:30:31.968044  5064 solver.cpp:228]     Train net output #0: softmax = 3.42242 (* 1 = 3.42242 loss)
I0623 12:30:31.968049  5064 solver.cpp:473] Iteration 7740, lr = 0.0001
I0623 12:30:32.988639  5064 solver.cpp:213] Iteration 7750, loss = 3.37377
I0623 12:30:32.988662  5064 solver.cpp:228]     Train net output #0: softmax = 3.37377 (* 1 = 3.37377 loss)
I0623 12:30:32.988788  5064 solver.cpp:473] Iteration 7750, lr = 0.0001
I0623 12:30:34.009382  5064 solver.cpp:213] Iteration 7760, loss = 3.21451
I0623 12:30:34.009402  5064 solver.cpp:228]     Train net output #0: softmax = 3.21451 (* 1 = 3.21451 loss)
I0623 12:30:34.009407  5064 solver.cpp:473] Iteration 7760, lr = 0.0001
I0623 12:30:35.030473  5064 solver.cpp:213] Iteration 7770, loss = 3.23415
I0623 12:30:35.030493  5064 solver.cpp:228]     Train net output #0: softmax = 3.23415 (* 1 = 3.23415 loss)
I0623 12:30:35.030498  5064 solver.cpp:473] Iteration 7770, lr = 0.0001
I0623 12:30:36.050773  5064 solver.cpp:213] Iteration 7780, loss = 3.30487
I0623 12:30:36.050793  5064 solver.cpp:228]     Train net output #0: softmax = 3.30487 (* 1 = 3.30487 loss)
I0623 12:30:36.050798  5064 solver.cpp:473] Iteration 7780, lr = 0.0001
I0623 12:30:37.071984  5064 solver.cpp:213] Iteration 7790, loss = 3.4081
I0623 12:30:37.072005  5064 solver.cpp:228]     Train net output #0: softmax = 3.4081 (* 1 = 3.4081 loss)
I0623 12:30:37.072010  5064 solver.cpp:473] Iteration 7790, lr = 0.0001
I0623 12:30:38.092543  5064 solver.cpp:213] Iteration 7800, loss = 3.41313
I0623 12:30:38.092746  5064 solver.cpp:228]     Train net output #0: softmax = 3.41313 (* 1 = 3.41313 loss)
I0623 12:30:38.092752  5064 solver.cpp:473] Iteration 7800, lr = 0.0001
I0623 12:30:39.112859  5064 solver.cpp:213] Iteration 7810, loss = 3.35521
I0623 12:30:39.112876  5064 solver.cpp:228]     Train net output #0: softmax = 3.35521 (* 1 = 3.35521 loss)
I0623 12:30:39.112881  5064 solver.cpp:473] Iteration 7810, lr = 0.0001
I0623 12:30:40.133633  5064 solver.cpp:213] Iteration 7820, loss = 3.26086
I0623 12:30:40.133652  5064 solver.cpp:228]     Train net output #0: softmax = 3.26086 (* 1 = 3.26086 loss)
I0623 12:30:40.133657  5064 solver.cpp:473] Iteration 7820, lr = 0.0001
I0623 12:30:41.154295  5064 solver.cpp:213] Iteration 7830, loss = 3.16749
I0623 12:30:41.154316  5064 solver.cpp:228]     Train net output #0: softmax = 3.16749 (* 1 = 3.16749 loss)
I0623 12:30:41.154321  5064 solver.cpp:473] Iteration 7830, lr = 0.0001
I0623 12:30:42.174965  5064 solver.cpp:213] Iteration 7840, loss = 3.40771
I0623 12:30:42.174985  5064 solver.cpp:228]     Train net output #0: softmax = 3.40771 (* 1 = 3.40771 loss)
I0623 12:30:42.174990  5064 solver.cpp:473] Iteration 7840, lr = 0.0001
I0623 12:30:43.194952  5064 solver.cpp:213] Iteration 7850, loss = 3.44857
I0623 12:30:43.194970  5064 solver.cpp:228]     Train net output #0: softmax = 3.44857 (* 1 = 3.44857 loss)
I0623 12:30:43.195102  5064 solver.cpp:473] Iteration 7850, lr = 0.0001
I0623 12:30:44.214854  5064 solver.cpp:213] Iteration 7860, loss = 3.15926
I0623 12:30:44.214870  5064 solver.cpp:228]     Train net output #0: softmax = 3.15926 (* 1 = 3.15926 loss)
I0623 12:30:44.214874  5064 solver.cpp:473] Iteration 7860, lr = 0.0001
I0623 12:30:45.235437  5064 solver.cpp:213] Iteration 7870, loss = 3.50324
I0623 12:30:45.235458  5064 solver.cpp:228]     Train net output #0: softmax = 3.50324 (* 1 = 3.50324 loss)
I0623 12:30:45.235465  5064 solver.cpp:473] Iteration 7870, lr = 0.0001
I0623 12:30:46.256093  5064 solver.cpp:213] Iteration 7880, loss = 3.24554
I0623 12:30:46.256111  5064 solver.cpp:228]     Train net output #0: softmax = 3.24554 (* 1 = 3.24554 loss)
I0623 12:30:46.256116  5064 solver.cpp:473] Iteration 7880, lr = 0.0001
I0623 12:30:47.276814  5064 solver.cpp:213] Iteration 7890, loss = 3.3972
I0623 12:30:47.276834  5064 solver.cpp:228]     Train net output #0: softmax = 3.3972 (* 1 = 3.3972 loss)
I0623 12:30:47.276839  5064 solver.cpp:473] Iteration 7890, lr = 0.0001
I0623 12:30:48.297304  5064 solver.cpp:213] Iteration 7900, loss = 3.16402
I0623 12:30:48.297329  5064 solver.cpp:228]     Train net output #0: softmax = 3.16402 (* 1 = 3.16402 loss)
I0623 12:30:48.297451  5064 solver.cpp:473] Iteration 7900, lr = 0.0001
I0623 12:30:49.310194  5064 solver.cpp:213] Iteration 7910, loss = 3.4089
I0623 12:30:49.310216  5064 solver.cpp:228]     Train net output #0: softmax = 3.4089 (* 1 = 3.4089 loss)
I0623 12:30:49.310223  5064 solver.cpp:473] Iteration 7910, lr = 0.0001
I0623 12:30:50.321918  5064 solver.cpp:213] Iteration 7920, loss = 3.1018
I0623 12:30:50.321939  5064 solver.cpp:228]     Train net output #0: softmax = 3.1018 (* 1 = 3.1018 loss)
I0623 12:30:50.321945  5064 solver.cpp:473] Iteration 7920, lr = 0.0001
I0623 12:30:51.341363  5064 solver.cpp:213] Iteration 7930, loss = 3.62346
I0623 12:30:51.341383  5064 solver.cpp:228]     Train net output #0: softmax = 3.62346 (* 1 = 3.62346 loss)
I0623 12:30:51.341389  5064 solver.cpp:473] Iteration 7930, lr = 0.0001
I0623 12:30:52.361903  5064 solver.cpp:213] Iteration 7940, loss = 3.01205
I0623 12:30:52.361923  5064 solver.cpp:228]     Train net output #0: softmax = 3.01205 (* 1 = 3.01205 loss)
I0623 12:30:52.361928  5064 solver.cpp:473] Iteration 7940, lr = 0.0001
I0623 12:30:53.383119  5064 solver.cpp:213] Iteration 7950, loss = 3.26858
I0623 12:30:53.383142  5064 solver.cpp:228]     Train net output #0: softmax = 3.26858 (* 1 = 3.26858 loss)
I0623 12:30:53.383147  5064 solver.cpp:473] Iteration 7950, lr = 0.0001
I0623 12:30:54.403882  5064 solver.cpp:213] Iteration 7960, loss = 3.2633
I0623 12:30:54.403903  5064 solver.cpp:228]     Train net output #0: softmax = 3.2633 (* 1 = 3.2633 loss)
I0623 12:30:54.403928  5064 solver.cpp:473] Iteration 7960, lr = 0.0001
I0623 12:30:55.420593  5064 solver.cpp:213] Iteration 7970, loss = 3.08265
I0623 12:30:55.420614  5064 solver.cpp:228]     Train net output #0: softmax = 3.08265 (* 1 = 3.08265 loss)
I0623 12:30:55.420620  5064 solver.cpp:473] Iteration 7970, lr = 0.0001
I0623 12:30:56.438755  5064 solver.cpp:213] Iteration 7980, loss = 3.24737
I0623 12:30:56.438773  5064 solver.cpp:228]     Train net output #0: softmax = 3.24737 (* 1 = 3.24737 loss)
I0623 12:30:56.438778  5064 solver.cpp:473] Iteration 7980, lr = 0.0001
I0623 12:30:57.459065  5064 solver.cpp:213] Iteration 7990, loss = 3.33905
I0623 12:30:57.459086  5064 solver.cpp:228]     Train net output #0: softmax = 3.33905 (* 1 = 3.33905 loss)
I0623 12:30:57.459092  5064 solver.cpp:473] Iteration 7990, lr = 0.0001
I0623 12:30:58.409653  5064 solver.cpp:362] Snapshotting to snapshots/16-06-21_16h05m31s_0_11_pretrainClassification_iter_8000.caffemodel
I0623 12:30:58.410750  5064 solver.cpp:370] Snapshotting solver state to snapshots/16-06-21_16h05m31s_0_11_pretrainClassification_iter_8000.solverstate
I0623 12:30:58.411362  5064 solver.cpp:291] Iteration 8000, Testing net (#0)
I0623 12:30:58.572304  5064 solver.cpp:342]     Test net output #0: accuracy_top_1 = 0.209375
I0623 12:30:58.572320  5064 solver.cpp:342]     Test net output #1: accuracy_top_5 = 0.484375
I0623 12:30:58.572327  5064 solver.cpp:342]     Test net output #2: softmax = 3.24398 (* 1 = 3.24398 loss)
I0623 12:30:58.643021  5064 solver.cpp:213] Iteration 8000, loss = 3.48383
I0623 12:30:58.643035  5064 solver.cpp:228]     Train net output #0: softmax = 3.48383 (* 1 = 3.48383 loss)
I0623 12:30:58.643040  5064 solver.cpp:473] Iteration 8000, lr = 0.0001
I0623 12:30:59.663733  5064 solver.cpp:213] Iteration 8010, loss = 3.14874
I0623 12:30:59.663753  5064 solver.cpp:228]     Train net output #0: softmax = 3.14874 (* 1 = 3.14874 loss)
I0623 12:30:59.663758  5064 solver.cpp:473] Iteration 8010, lr = 0.0001
I0623 12:31:00.683744  5064 solver.cpp:213] Iteration 8020, loss = 3.0533
I0623 12:31:00.683765  5064 solver.cpp:228]     Train net output #0: softmax = 3.0533 (* 1 = 3.0533 loss)
I0623 12:31:00.683770  5064 solver.cpp:473] Iteration 8020, lr = 0.0001
I0623 12:31:01.703963  5064 solver.cpp:213] Iteration 8030, loss = 3.17553
I0623 12:31:01.703984  5064 solver.cpp:228]     Train net output #0: softmax = 3.17553 (* 1 = 3.17553 loss)
I0623 12:31:01.703990  5064 solver.cpp:473] Iteration 8030, lr = 0.0001
I0623 12:31:02.724638  5064 solver.cpp:213] Iteration 8040, loss = 3.32928
I0623 12:31:02.724660  5064 solver.cpp:228]     Train net output #0: softmax = 3.32928 (* 1 = 3.32928 loss)
I0623 12:31:02.724665  5064 solver.cpp:473] Iteration 8040, lr = 0.0001
I0623 12:31:03.744997  5064 solver.cpp:213] Iteration 8050, loss = 3.21481
I0623 12:31:03.745023  5064 solver.cpp:228]     Train net output #0: softmax = 3.21481 (* 1 = 3.21481 loss)
I0623 12:31:03.745033  5064 solver.cpp:473] Iteration 8050, lr = 0.0001
I0623 12:31:04.765295  5064 solver.cpp:213] Iteration 8060, loss = 2.83564
I0623 12:31:04.765314  5064 solver.cpp:228]     Train net output #0: softmax = 2.83564 (* 1 = 2.83564 loss)
I0623 12:31:04.765321  5064 solver.cpp:473] Iteration 8060, lr = 0.0001
I0623 12:31:05.786227  5064 solver.cpp:213] Iteration 8070, loss = 3.33523
I0623 12:31:05.786247  5064 solver.cpp:228]     Train net output #0: softmax = 3.33523 (* 1 = 3.33523 loss)
I0623 12:31:05.786252  5064 solver.cpp:473] Iteration 8070, lr = 0.0001
I0623 12:31:06.806773  5064 solver.cpp:213] Iteration 8080, loss = 3.32898
I0623 12:31:06.806793  5064 solver.cpp:228]     Train net output #0: softmax = 3.32898 (* 1 = 3.32898 loss)
I0623 12:31:06.806799  5064 solver.cpp:473] Iteration 8080, lr = 0.0001
I0623 12:31:07.827441  5064 solver.cpp:213] Iteration 8090, loss = 3.33276
I0623 12:31:07.827463  5064 solver.cpp:228]     Train net output #0: softmax = 3.33276 (* 1 = 3.33276 loss)
I0623 12:31:07.827468  5064 solver.cpp:473] Iteration 8090, lr = 0.0001
I0623 12:31:08.848323  5064 solver.cpp:213] Iteration 8100, loss = 3.26946
I0623 12:31:08.848491  5064 solver.cpp:228]     Train net output #0: softmax = 3.26946 (* 1 = 3.26946 loss)
I0623 12:31:08.848498  5064 solver.cpp:473] Iteration 8100, lr = 0.0001
I0623 12:31:09.868500  5064 solver.cpp:213] Iteration 8110, loss = 3.31412
I0623 12:31:09.868520  5064 solver.cpp:228]     Train net output #0: softmax = 3.31412 (* 1 = 3.31412 loss)
I0623 12:31:09.868525  5064 solver.cpp:473] Iteration 8110, lr = 0.0001
I0623 12:31:10.889466  5064 solver.cpp:213] Iteration 8120, loss = 3.16306
I0623 12:31:10.889487  5064 solver.cpp:228]     Train net output #0: softmax = 3.16306 (* 1 = 3.16306 loss)
I0623 12:31:10.889492  5064 solver.cpp:473] Iteration 8120, lr = 0.0001
I0623 12:31:11.909788  5064 solver.cpp:213] Iteration 8130, loss = 3.16245
I0623 12:31:11.909807  5064 solver.cpp:228]     Train net output #0: softmax = 3.16245 (* 1 = 3.16245 loss)
I0623 12:31:11.909812  5064 solver.cpp:473] Iteration 8130, lr = 0.0001
I0623 12:31:12.930090  5064 solver.cpp:213] Iteration 8140, loss = 3.34928
I0623 12:31:12.930110  5064 solver.cpp:228]     Train net output #0: softmax = 3.34928 (* 1 = 3.34928 loss)
I0623 12:31:12.930116  5064 solver.cpp:473] Iteration 8140, lr = 0.0001
I0623 12:31:13.951180  5064 solver.cpp:213] Iteration 8150, loss = 3.2157
I0623 12:31:13.951206  5064 solver.cpp:228]     Train net output #0: softmax = 3.2157 (* 1 = 3.2157 loss)
I0623 12:31:13.951362  5064 solver.cpp:473] Iteration 8150, lr = 0.0001
I0623 12:31:14.971681  5064 solver.cpp:213] Iteration 8160, loss = 3.32427
I0623 12:31:14.971703  5064 solver.cpp:228]     Train net output #0: softmax = 3.32427 (* 1 = 3.32427 loss)
I0623 12:31:14.971709  5064 solver.cpp:473] Iteration 8160, lr = 0.0001
I0623 12:31:15.991842  5064 solver.cpp:213] Iteration 8170, loss = 3.38835
I0623 12:31:15.991863  5064 solver.cpp:228]     Train net output #0: softmax = 3.38835 (* 1 = 3.38835 loss)
I0623 12:31:15.991868  5064 solver.cpp:473] Iteration 8170, lr = 0.0001
I0623 12:31:17.012280  5064 solver.cpp:213] Iteration 8180, loss = 3.2659
I0623 12:31:17.012300  5064 solver.cpp:228]     Train net output #0: softmax = 3.2659 (* 1 = 3.2659 loss)
I0623 12:31:17.012305  5064 solver.cpp:473] Iteration 8180, lr = 0.0001
I0623 12:31:18.032840  5064 solver.cpp:213] Iteration 8190, loss = 3.33701
I0623 12:31:18.032861  5064 solver.cpp:228]     Train net output #0: softmax = 3.33701 (* 1 = 3.33701 loss)
I0623 12:31:18.032866  5064 solver.cpp:473] Iteration 8190, lr = 0.0001
I0623 12:31:19.053460  5064 solver.cpp:213] Iteration 8200, loss = 3.45565
I0623 12:31:19.053494  5064 solver.cpp:228]     Train net output #0: softmax = 3.45565 (* 1 = 3.45565 loss)
I0623 12:31:19.053503  5064 solver.cpp:473] Iteration 8200, lr = 0.0001
I0623 12:31:20.074136  5064 solver.cpp:213] Iteration 8210, loss = 3.23008
I0623 12:31:20.074157  5064 solver.cpp:228]     Train net output #0: softmax = 3.23008 (* 1 = 3.23008 loss)
I0623 12:31:20.074163  5064 solver.cpp:473] Iteration 8210, lr = 0.0001
I0623 12:31:21.094271  5064 solver.cpp:213] Iteration 8220, loss = 3.09517
I0623 12:31:21.094291  5064 solver.cpp:228]     Train net output #0: softmax = 3.09517 (* 1 = 3.09517 loss)
I0623 12:31:21.094296  5064 solver.cpp:473] Iteration 8220, lr = 0.0001
I0623 12:31:22.114133  5064 solver.cpp:213] Iteration 8230, loss = 3.3467
I0623 12:31:22.114153  5064 solver.cpp:228]     Train net output #0: softmax = 3.3467 (* 1 = 3.3467 loss)
I0623 12:31:22.114158  5064 solver.cpp:473] Iteration 8230, lr = 0.0001
I0623 12:31:23.134755  5064 solver.cpp:213] Iteration 8240, loss = 3.44664
I0623 12:31:23.134775  5064 solver.cpp:228]     Train net output #0: softmax = 3.44664 (* 1 = 3.44664 loss)
I0623 12:31:23.134781  5064 solver.cpp:473] Iteration 8240, lr = 0.0001
I0623 12:31:24.155248  5064 solver.cpp:213] Iteration 8250, loss = 3.28021
I0623 12:31:24.155290  5064 solver.cpp:228]     Train net output #0: softmax = 3.28021 (* 1 = 3.28021 loss)
I0623 12:31:24.155298  5064 solver.cpp:473] Iteration 8250, lr = 0.0001
I0623 12:31:25.175907  5064 solver.cpp:213] Iteration 8260, loss = 3.47206
I0623 12:31:25.175930  5064 solver.cpp:228]     Train net output #0: softmax = 3.47206 (* 1 = 3.47206 loss)
I0623 12:31:25.175951  5064 solver.cpp:473] Iteration 8260, lr = 0.0001
I0623 12:31:26.196421  5064 solver.cpp:213] Iteration 8270, loss = 3.41609
I0623 12:31:26.196441  5064 solver.cpp:228]     Train net output #0: softmax = 3.41609 (* 1 = 3.41609 loss)
I0623 12:31:26.196446  5064 solver.cpp:473] Iteration 8270, lr = 0.0001
I0623 12:31:27.216931  5064 solver.cpp:213] Iteration 8280, loss = 3.3192
I0623 12:31:27.216954  5064 solver.cpp:228]     Train net output #0: softmax = 3.3192 (* 1 = 3.3192 loss)
I0623 12:31:27.216959  5064 solver.cpp:473] Iteration 8280, lr = 0.0001
I0623 12:31:28.236711  5064 solver.cpp:213] Iteration 8290, loss = 3.20427
I0623 12:31:28.236732  5064 solver.cpp:228]     Train net output #0: softmax = 3.20427 (* 1 = 3.20427 loss)
I0623 12:31:28.236737  5064 solver.cpp:473] Iteration 8290, lr = 0.0001
I0623 12:31:29.257524  5064 solver.cpp:213] Iteration 8300, loss = 3.58026
I0623 12:31:29.257557  5064 solver.cpp:228]     Train net output #0: softmax = 3.58026 (* 1 = 3.58026 loss)
I0623 12:31:29.257565  5064 solver.cpp:473] Iteration 8300, lr = 0.0001
I0623 12:31:30.278003  5064 solver.cpp:213] Iteration 8310, loss = 3.45447
I0623 12:31:30.278023  5064 solver.cpp:228]     Train net output #0: softmax = 3.45447 (* 1 = 3.45447 loss)
I0623 12:31:30.278028  5064 solver.cpp:473] Iteration 8310, lr = 0.0001
I0623 12:31:31.298652  5064 solver.cpp:213] Iteration 8320, loss = 3.43535
I0623 12:31:31.298674  5064 solver.cpp:228]     Train net output #0: softmax = 3.43535 (* 1 = 3.43535 loss)
I0623 12:31:31.298679  5064 solver.cpp:473] Iteration 8320, lr = 0.0001
I0623 12:31:32.318965  5064 solver.cpp:213] Iteration 8330, loss = 3.30589
I0623 12:31:32.318989  5064 solver.cpp:228]     Train net output #0: softmax = 3.30589 (* 1 = 3.30589 loss)
I0623 12:31:32.318994  5064 solver.cpp:473] Iteration 8330, lr = 0.0001
I0623 12:31:33.339380  5064 solver.cpp:213] Iteration 8340, loss = 2.92267
I0623 12:31:33.339403  5064 solver.cpp:228]     Train net output #0: softmax = 2.92267 (* 1 = 2.92267 loss)
I0623 12:31:33.339408  5064 solver.cpp:473] Iteration 8340, lr = 0.0001
I0623 12:31:34.359364  5064 solver.cpp:213] Iteration 8350, loss = 3.16135
I0623 12:31:34.359385  5064 solver.cpp:228]     Train net output #0: softmax = 3.16135 (* 1 = 3.16135 loss)
I0623 12:31:34.359390  5064 solver.cpp:473] Iteration 8350, lr = 0.0001
I0623 12:31:35.380084  5064 solver.cpp:213] Iteration 8360, loss = 3.15809
I0623 12:31:35.380100  5064 solver.cpp:228]     Train net output #0: softmax = 3.15809 (* 1 = 3.15809 loss)
I0623 12:31:35.380105  5064 solver.cpp:473] Iteration 8360, lr = 0.0001
I0623 12:31:36.400492  5064 solver.cpp:213] Iteration 8370, loss = 3.37534
I0623 12:31:36.400509  5064 solver.cpp:228]     Train net output #0: softmax = 3.37534 (* 1 = 3.37534 loss)
I0623 12:31:36.400514  5064 solver.cpp:473] Iteration 8370, lr = 0.0001
I0623 12:31:37.420970  5064 solver.cpp:213] Iteration 8380, loss = 3.29213
I0623 12:31:37.420991  5064 solver.cpp:228]     Train net output #0: softmax = 3.29213 (* 1 = 3.29213 loss)
I0623 12:31:37.420996  5064 solver.cpp:473] Iteration 8380, lr = 0.0001
I0623 12:31:38.441359  5064 solver.cpp:213] Iteration 8390, loss = 3.34357
I0623 12:31:38.441380  5064 solver.cpp:228]     Train net output #0: softmax = 3.34357 (* 1 = 3.34357 loss)
I0623 12:31:38.441385  5064 solver.cpp:473] Iteration 8390, lr = 0.0001
I0623 12:31:39.461699  5064 solver.cpp:213] Iteration 8400, loss = 2.99863
I0623 12:31:39.461752  5064 solver.cpp:228]     Train net output #0: softmax = 2.99863 (* 1 = 2.99863 loss)
I0623 12:31:39.461758  5064 solver.cpp:473] Iteration 8400, lr = 0.0001
I0623 12:31:40.481832  5064 solver.cpp:213] Iteration 8410, loss = 2.97689
I0623 12:31:40.481856  5064 solver.cpp:228]     Train net output #0: softmax = 2.97689 (* 1 = 2.97689 loss)
I0623 12:31:40.481861  5064 solver.cpp:473] Iteration 8410, lr = 0.0001
I0623 12:31:41.502149  5064 solver.cpp:213] Iteration 8420, loss = 3.10457
I0623 12:31:41.502168  5064 solver.cpp:228]     Train net output #0: softmax = 3.10457 (* 1 = 3.10457 loss)
I0623 12:31:41.502173  5064 solver.cpp:473] Iteration 8420, lr = 0.0001
I0623 12:31:42.522599  5064 solver.cpp:213] Iteration 8430, loss = 3.51825
I0623 12:31:42.522622  5064 solver.cpp:228]     Train net output #0: softmax = 3.51825 (* 1 = 3.51825 loss)
I0623 12:31:42.522627  5064 solver.cpp:473] Iteration 8430, lr = 0.0001
I0623 12:31:43.543356  5064 solver.cpp:213] Iteration 8440, loss = 3.24179
I0623 12:31:43.543377  5064 solver.cpp:228]     Train net output #0: softmax = 3.24179 (* 1 = 3.24179 loss)
I0623 12:31:43.543382  5064 solver.cpp:473] Iteration 8440, lr = 0.0001
I0623 12:31:44.563392  5064 solver.cpp:213] Iteration 8450, loss = 3.01782
I0623 12:31:44.563424  5064 solver.cpp:228]     Train net output #0: softmax = 3.01782 (* 1 = 3.01782 loss)
I0623 12:31:44.563433  5064 solver.cpp:473] Iteration 8450, lr = 0.0001
I0623 12:31:45.584094  5064 solver.cpp:213] Iteration 8460, loss = 3.288
I0623 12:31:45.584115  5064 solver.cpp:228]     Train net output #0: softmax = 3.288 (* 1 = 3.288 loss)
I0623 12:31:45.584120  5064 solver.cpp:473] Iteration 8460, lr = 0.0001
I0623 12:31:46.604635  5064 solver.cpp:213] Iteration 8470, loss = 3.18146
I0623 12:31:46.604655  5064 solver.cpp:228]     Train net output #0: softmax = 3.18146 (* 1 = 3.18146 loss)
I0623 12:31:46.604660  5064 solver.cpp:473] Iteration 8470, lr = 0.0001
I0623 12:31:47.625231  5064 solver.cpp:213] Iteration 8480, loss = 3.16721
I0623 12:31:47.625249  5064 solver.cpp:228]     Train net output #0: softmax = 3.16721 (* 1 = 3.16721 loss)
I0623 12:31:47.625254  5064 solver.cpp:473] Iteration 8480, lr = 0.0001
I0623 12:31:48.645871  5064 solver.cpp:213] Iteration 8490, loss = 3.0556
I0623 12:31:48.645890  5064 solver.cpp:228]     Train net output #0: softmax = 3.0556 (* 1 = 3.0556 loss)
I0623 12:31:48.645895  5064 solver.cpp:473] Iteration 8490, lr = 0.0001
I0623 12:31:49.666518  5064 solver.cpp:213] Iteration 8500, loss = 3.27718
I0623 12:31:49.666550  5064 solver.cpp:228]     Train net output #0: softmax = 3.27718 (* 1 = 3.27718 loss)
I0623 12:31:49.666558  5064 solver.cpp:473] Iteration 8500, lr = 0.0001
I0623 12:31:50.686985  5064 solver.cpp:213] Iteration 8510, loss = 3.53382
I0623 12:31:50.687006  5064 solver.cpp:228]     Train net output #0: softmax = 3.53382 (* 1 = 3.53382 loss)
I0623 12:31:50.687011  5064 solver.cpp:473] Iteration 8510, lr = 0.0001
I0623 12:31:51.707715  5064 solver.cpp:213] Iteration 8520, loss = 3.29041
I0623 12:31:51.707734  5064 solver.cpp:228]     Train net output #0: softmax = 3.29041 (* 1 = 3.29041 loss)
I0623 12:31:51.707739  5064 solver.cpp:473] Iteration 8520, lr = 0.0001
I0623 12:31:52.727742  5064 solver.cpp:213] Iteration 8530, loss = 3.26554
I0623 12:31:52.727761  5064 solver.cpp:228]     Train net output #0: softmax = 3.26554 (* 1 = 3.26554 loss)
I0623 12:31:52.727766  5064 solver.cpp:473] Iteration 8530, lr = 0.0001
I0623 12:31:53.748294  5064 solver.cpp:213] Iteration 8540, loss = 3.12476
I0623 12:31:53.748317  5064 solver.cpp:228]     Train net output #0: softmax = 3.12476 (* 1 = 3.12476 loss)
I0623 12:31:53.748322  5064 solver.cpp:473] Iteration 8540, lr = 0.0001
I0623 12:31:54.768610  5064 solver.cpp:213] Iteration 8550, loss = 3.20237
I0623 12:31:54.768641  5064 solver.cpp:228]     Train net output #0: softmax = 3.20237 (* 1 = 3.20237 loss)
I0623 12:31:54.768651  5064 solver.cpp:473] Iteration 8550, lr = 0.0001
I0623 12:31:55.789198  5064 solver.cpp:213] Iteration 8560, loss = 3.33817
I0623 12:31:55.789221  5064 solver.cpp:228]     Train net output #0: softmax = 3.33817 (* 1 = 3.33817 loss)
I0623 12:31:55.789242  5064 solver.cpp:473] Iteration 8560, lr = 0.0001
I0623 12:31:56.809661  5064 solver.cpp:213] Iteration 8570, loss = 3.16161
I0623 12:31:56.809680  5064 solver.cpp:228]     Train net output #0: softmax = 3.16161 (* 1 = 3.16161 loss)
I0623 12:31:56.809686  5064 solver.cpp:473] Iteration 8570, lr = 0.0001
I0623 12:31:57.830412  5064 solver.cpp:213] Iteration 8580, loss = 3.43213
I0623 12:31:57.830441  5064 solver.cpp:228]     Train net output #0: softmax = 3.43213 (* 1 = 3.43213 loss)
I0623 12:31:57.830447  5064 solver.cpp:473] Iteration 8580, lr = 0.0001
I0623 12:31:58.850448  5064 solver.cpp:213] Iteration 8590, loss = 3.2722
I0623 12:31:58.850469  5064 solver.cpp:228]     Train net output #0: softmax = 3.2722 (* 1 = 3.2722 loss)
I0623 12:31:58.850474  5064 solver.cpp:473] Iteration 8590, lr = 0.0001
I0623 12:31:59.870890  5064 solver.cpp:213] Iteration 8600, loss = 3.07514
I0623 12:31:59.870911  5064 solver.cpp:228]     Train net output #0: softmax = 3.07514 (* 1 = 3.07514 loss)
I0623 12:31:59.870918  5064 solver.cpp:473] Iteration 8600, lr = 0.0001
I0623 12:32:00.891597  5064 solver.cpp:213] Iteration 8610, loss = 3.24156
I0623 12:32:00.891613  5064 solver.cpp:228]     Train net output #0: softmax = 3.24156 (* 1 = 3.24156 loss)
I0623 12:32:00.891618  5064 solver.cpp:473] Iteration 8610, lr = 0.0001
I0623 12:32:01.912065  5064 solver.cpp:213] Iteration 8620, loss = 3.23464
I0623 12:32:01.912084  5064 solver.cpp:228]     Train net output #0: softmax = 3.23464 (* 1 = 3.23464 loss)
I0623 12:32:01.912089  5064 solver.cpp:473] Iteration 8620, lr = 0.0001
I0623 12:32:02.932253  5064 solver.cpp:213] Iteration 8630, loss = 3.414
I0623 12:32:02.932271  5064 solver.cpp:228]     Train net output #0: softmax = 3.414 (* 1 = 3.414 loss)
I0623 12:32:02.932276  5064 solver.cpp:473] Iteration 8630, lr = 0.0001
I0623 12:32:03.952713  5064 solver.cpp:213] Iteration 8640, loss = 3.18082
I0623 12:32:03.952733  5064 solver.cpp:228]     Train net output #0: softmax = 3.18082 (* 1 = 3.18082 loss)
I0623 12:32:03.952738  5064 solver.cpp:473] Iteration 8640, lr = 0.0001
I0623 12:32:04.973206  5064 solver.cpp:213] Iteration 8650, loss = 3.25611
I0623 12:32:04.973223  5064 solver.cpp:228]     Train net output #0: softmax = 3.25611 (* 1 = 3.25611 loss)
I0623 12:32:04.973228  5064 solver.cpp:473] Iteration 8650, lr = 0.0001
I0623 12:32:05.993614  5064 solver.cpp:213] Iteration 8660, loss = 3.17746
I0623 12:32:05.993634  5064 solver.cpp:228]     Train net output #0: softmax = 3.17746 (* 1 = 3.17746 loss)
I0623 12:32:05.993639  5064 solver.cpp:473] Iteration 8660, lr = 0.0001
I0623 12:32:07.014196  5064 solver.cpp:213] Iteration 8670, loss = 3.40492
I0623 12:32:07.014215  5064 solver.cpp:228]     Train net output #0: softmax = 3.40492 (* 1 = 3.40492 loss)
I0623 12:32:07.014220  5064 solver.cpp:473] Iteration 8670, lr = 0.0001
I0623 12:32:08.034497  5064 solver.cpp:213] Iteration 8680, loss = 3.02463
I0623 12:32:08.034515  5064 solver.cpp:228]     Train net output #0: softmax = 3.02463 (* 1 = 3.02463 loss)
I0623 12:32:08.034520  5064 solver.cpp:473] Iteration 8680, lr = 0.0001
I0623 12:32:09.054935  5064 solver.cpp:213] Iteration 8690, loss = 3.43672
I0623 12:32:09.054955  5064 solver.cpp:228]     Train net output #0: softmax = 3.43672 (* 1 = 3.43672 loss)
I0623 12:32:09.054960  5064 solver.cpp:473] Iteration 8690, lr = 0.0001
I0623 12:32:10.075014  5064 solver.cpp:213] Iteration 8700, loss = 3.36355
I0623 12:32:10.075073  5064 solver.cpp:228]     Train net output #0: softmax = 3.36355 (* 1 = 3.36355 loss)
I0623 12:32:10.075078  5064 solver.cpp:473] Iteration 8700, lr = 0.0001
I0623 12:32:11.095507  5064 solver.cpp:213] Iteration 8710, loss = 3.18634
I0623 12:32:11.095526  5064 solver.cpp:228]     Train net output #0: softmax = 3.18634 (* 1 = 3.18634 loss)
I0623 12:32:11.095531  5064 solver.cpp:473] Iteration 8710, lr = 0.0001
I0623 12:32:12.115860  5064 solver.cpp:213] Iteration 8720, loss = 3.38935
I0623 12:32:12.115881  5064 solver.cpp:228]     Train net output #0: softmax = 3.38935 (* 1 = 3.38935 loss)
I0623 12:32:12.115886  5064 solver.cpp:473] Iteration 8720, lr = 0.0001
I0623 12:32:13.136451  5064 solver.cpp:213] Iteration 8730, loss = 2.92507
I0623 12:32:13.136471  5064 solver.cpp:228]     Train net output #0: softmax = 2.92507 (* 1 = 2.92507 loss)
I0623 12:32:13.136476  5064 solver.cpp:473] Iteration 8730, lr = 0.0001
I0623 12:32:14.156774  5064 solver.cpp:213] Iteration 8740, loss = 3.23213
I0623 12:32:14.156802  5064 solver.cpp:228]     Train net output #0: softmax = 3.23213 (* 1 = 3.23213 loss)
I0623 12:32:14.156808  5064 solver.cpp:473] Iteration 8740, lr = 0.0001
I0623 12:32:15.177170  5064 solver.cpp:213] Iteration 8750, loss = 3.18022
I0623 12:32:15.177191  5064 solver.cpp:228]     Train net output #0: softmax = 3.18022 (* 1 = 3.18022 loss)
I0623 12:32:15.177196  5064 solver.cpp:473] Iteration 8750, lr = 0.0001
I0623 12:32:16.197016  5064 solver.cpp:213] Iteration 8760, loss = 3.16914
I0623 12:32:16.197033  5064 solver.cpp:228]     Train net output #0: softmax = 3.16914 (* 1 = 3.16914 loss)
I0623 12:32:16.197038  5064 solver.cpp:473] Iteration 8760, lr = 0.0001
I0623 12:32:17.217502  5064 solver.cpp:213] Iteration 8770, loss = 3.10455
I0623 12:32:17.217525  5064 solver.cpp:228]     Train net output #0: softmax = 3.10455 (* 1 = 3.10455 loss)
I0623 12:32:17.217530  5064 solver.cpp:473] Iteration 8770, lr = 0.0001
I0623 12:32:18.237522  5064 solver.cpp:213] Iteration 8780, loss = 3.19514
I0623 12:32:18.237545  5064 solver.cpp:228]     Train net output #0: softmax = 3.19514 (* 1 = 3.19514 loss)
I0623 12:32:18.237550  5064 solver.cpp:473] Iteration 8780, lr = 0.0001
I0623 12:32:19.257917  5064 solver.cpp:213] Iteration 8790, loss = 3.1194
I0623 12:32:19.257941  5064 solver.cpp:228]     Train net output #0: softmax = 3.1194 (* 1 = 3.1194 loss)
I0623 12:32:19.257947  5064 solver.cpp:473] Iteration 8790, lr = 0.0001
I0623 12:32:20.276993  5064 solver.cpp:213] Iteration 8800, loss = 3.30722
I0623 12:32:20.277015  5064 solver.cpp:228]     Train net output #0: softmax = 3.30722 (* 1 = 3.30722 loss)
I0623 12:32:20.277021  5064 solver.cpp:473] Iteration 8800, lr = 0.0001
I0623 12:32:21.296280  5064 solver.cpp:213] Iteration 8810, loss = 3.15095
I0623 12:32:21.296301  5064 solver.cpp:228]     Train net output #0: softmax = 3.15095 (* 1 = 3.15095 loss)
I0623 12:32:21.296306  5064 solver.cpp:473] Iteration 8810, lr = 0.0001
I0623 12:32:22.316419  5064 solver.cpp:213] Iteration 8820, loss = 3.4742
I0623 12:32:22.316445  5064 solver.cpp:228]     Train net output #0: softmax = 3.4742 (* 1 = 3.4742 loss)
I0623 12:32:22.316450  5064 solver.cpp:473] Iteration 8820, lr = 0.0001
I0623 12:32:23.336205  5064 solver.cpp:213] Iteration 8830, loss = 3.27402
I0623 12:32:23.336227  5064 solver.cpp:228]     Train net output #0: softmax = 3.27402 (* 1 = 3.27402 loss)
I0623 12:32:23.336232  5064 solver.cpp:473] Iteration 8830, lr = 0.0001
I0623 12:32:24.356539  5064 solver.cpp:213] Iteration 8840, loss = 3.03265
I0623 12:32:24.356559  5064 solver.cpp:228]     Train net output #0: softmax = 3.03265 (* 1 = 3.03265 loss)
I0623 12:32:24.356564  5064 solver.cpp:473] Iteration 8840, lr = 0.0001
I0623 12:32:25.377197  5064 solver.cpp:213] Iteration 8850, loss = 3.29049
I0623 12:32:25.377221  5064 solver.cpp:228]     Train net output #0: softmax = 3.29049 (* 1 = 3.29049 loss)
I0623 12:32:25.377228  5064 solver.cpp:473] Iteration 8850, lr = 0.0001
I0623 12:32:26.397507  5064 solver.cpp:213] Iteration 8860, loss = 3.28146
I0623 12:32:26.397531  5064 solver.cpp:228]     Train net output #0: softmax = 3.28146 (* 1 = 3.28146 loss)
I0623 12:32:26.397552  5064 solver.cpp:473] Iteration 8860, lr = 0.0001
I0623 12:32:27.417551  5064 solver.cpp:213] Iteration 8870, loss = 3.02121
I0623 12:32:27.417573  5064 solver.cpp:228]     Train net output #0: softmax = 3.02121 (* 1 = 3.02121 loss)
I0623 12:32:27.417579  5064 solver.cpp:473] Iteration 8870, lr = 0.0001
I0623 12:32:28.437927  5064 solver.cpp:213] Iteration 8880, loss = 3.12292
I0623 12:32:28.437947  5064 solver.cpp:228]     Train net output #0: softmax = 3.12292 (* 1 = 3.12292 loss)
I0623 12:32:28.437952  5064 solver.cpp:473] Iteration 8880, lr = 0.0001
I0623 12:32:29.458096  5064 solver.cpp:213] Iteration 8890, loss = 3.30849
I0623 12:32:29.458118  5064 solver.cpp:228]     Train net output #0: softmax = 3.30849 (* 1 = 3.30849 loss)
I0623 12:32:29.458123  5064 solver.cpp:473] Iteration 8890, lr = 0.0001
I0623 12:32:30.478471  5064 solver.cpp:213] Iteration 8900, loss = 3.42531
I0623 12:32:30.478495  5064 solver.cpp:228]     Train net output #0: softmax = 3.42531 (* 1 = 3.42531 loss)
I0623 12:32:30.478503  5064 solver.cpp:473] Iteration 8900, lr = 0.0001
I0623 12:32:31.498602  5064 solver.cpp:213] Iteration 8910, loss = 3.62088
I0623 12:32:31.498622  5064 solver.cpp:228]     Train net output #0: softmax = 3.62088 (* 1 = 3.62088 loss)
I0623 12:32:31.498626  5064 solver.cpp:473] Iteration 8910, lr = 0.0001
I0623 12:32:32.518826  5064 solver.cpp:213] Iteration 8920, loss = 3.43631
I0623 12:32:32.518846  5064 solver.cpp:228]     Train net output #0: softmax = 3.43631 (* 1 = 3.43631 loss)
I0623 12:32:32.518851  5064 solver.cpp:473] Iteration 8920, lr = 0.0001
I0623 12:32:33.539785  5064 solver.cpp:213] Iteration 8930, loss = 3.25615
I0623 12:32:33.539808  5064 solver.cpp:228]     Train net output #0: softmax = 3.25615 (* 1 = 3.25615 loss)
I0623 12:32:33.539813  5064 solver.cpp:473] Iteration 8930, lr = 0.0001
I0623 12:32:34.560226  5064 solver.cpp:213] Iteration 8940, loss = 3.30043
I0623 12:32:34.560245  5064 solver.cpp:228]     Train net output #0: softmax = 3.30043 (* 1 = 3.30043 loss)
I0623 12:32:34.560250  5064 solver.cpp:473] Iteration 8940, lr = 0.0001
I0623 12:32:35.578474  5064 solver.cpp:213] Iteration 8950, loss = 3.3374
I0623 12:32:35.578496  5064 solver.cpp:228]     Train net output #0: softmax = 3.3374 (* 1 = 3.3374 loss)
I0623 12:32:35.578501  5064 solver.cpp:473] Iteration 8950, lr = 0.0001
I0623 12:32:36.598543  5064 solver.cpp:213] Iteration 8960, loss = 3.14849
I0623 12:32:36.598562  5064 solver.cpp:228]     Train net output #0: softmax = 3.14849 (* 1 = 3.14849 loss)
I0623 12:32:36.598567  5064 solver.cpp:473] Iteration 8960, lr = 0.0001
I0623 12:32:37.618607  5064 solver.cpp:213] Iteration 8970, loss = 3.37403
I0623 12:32:37.618628  5064 solver.cpp:228]     Train net output #0: softmax = 3.37403 (* 1 = 3.37403 loss)
I0623 12:32:37.618633  5064 solver.cpp:473] Iteration 8970, lr = 0.0001
I0623 12:32:38.639232  5064 solver.cpp:213] Iteration 8980, loss = 3.14622
I0623 12:32:38.639256  5064 solver.cpp:228]     Train net output #0: softmax = 3.14622 (* 1 = 3.14622 loss)
I0623 12:32:38.639261  5064 solver.cpp:473] Iteration 8980, lr = 0.0001
I0623 12:32:39.658920  5064 solver.cpp:213] Iteration 8990, loss = 3.24137
I0623 12:32:39.658939  5064 solver.cpp:228]     Train net output #0: softmax = 3.24137 (* 1 = 3.24137 loss)
I0623 12:32:39.658944  5064 solver.cpp:473] Iteration 8990, lr = 0.0001
I0623 12:32:40.607424  5064 solver.cpp:362] Snapshotting to snapshots/16-06-21_16h05m31s_0_11_pretrainClassification_iter_9000.caffemodel
I0623 12:32:40.608713  5064 solver.cpp:370] Snapshotting solver state to snapshots/16-06-21_16h05m31s_0_11_pretrainClassification_iter_9000.solverstate
I0623 12:32:40.609356  5064 solver.cpp:291] Iteration 9000, Testing net (#0)
I0623 12:32:40.770462  5064 solver.cpp:342]     Test net output #0: accuracy_top_1 = 0.214062
I0623 12:32:40.770478  5064 solver.cpp:342]     Test net output #1: accuracy_top_5 = 0.514063
I0623 12:32:40.770485  5064 solver.cpp:342]     Test net output #2: softmax = 3.18309 (* 1 = 3.18309 loss)
I0623 12:32:40.841316  5064 solver.cpp:213] Iteration 9000, loss = 3.05411
I0623 12:32:40.841331  5064 solver.cpp:228]     Train net output #0: softmax = 3.05411 (* 1 = 3.05411 loss)
I0623 12:32:40.841336  5064 solver.cpp:473] Iteration 9000, lr = 0.0001
I0623 12:32:41.862337  5064 solver.cpp:213] Iteration 9010, loss = 3.32976
I0623 12:32:41.862360  5064 solver.cpp:228]     Train net output #0: softmax = 3.32976 (* 1 = 3.32976 loss)
I0623 12:32:41.862365  5064 solver.cpp:473] Iteration 9010, lr = 0.0001
I0623 12:32:42.883121  5064 solver.cpp:213] Iteration 9020, loss = 3.30383
I0623 12:32:42.883139  5064 solver.cpp:228]     Train net output #0: softmax = 3.30383 (* 1 = 3.30383 loss)
I0623 12:32:42.883144  5064 solver.cpp:473] Iteration 9020, lr = 0.0001
I0623 12:32:43.903493  5064 solver.cpp:213] Iteration 9030, loss = 3.06588
I0623 12:32:43.903512  5064 solver.cpp:228]     Train net output #0: softmax = 3.06588 (* 1 = 3.06588 loss)
I0623 12:32:43.903517  5064 solver.cpp:473] Iteration 9030, lr = 0.0001
I0623 12:32:44.923542  5064 solver.cpp:213] Iteration 9040, loss = 3.07181
I0623 12:32:44.923562  5064 solver.cpp:228]     Train net output #0: softmax = 3.07181 (* 1 = 3.07181 loss)
I0623 12:32:44.923568  5064 solver.cpp:473] Iteration 9040, lr = 0.0001
I0623 12:32:45.944521  5064 solver.cpp:213] Iteration 9050, loss = 3.09444
I0623 12:32:45.944547  5064 solver.cpp:228]     Train net output #0: softmax = 3.09444 (* 1 = 3.09444 loss)
I0623 12:32:45.944560  5064 solver.cpp:473] Iteration 9050, lr = 0.0001
I0623 12:32:46.964932  5064 solver.cpp:213] Iteration 9060, loss = 3.3745
I0623 12:32:46.964954  5064 solver.cpp:228]     Train net output #0: softmax = 3.3745 (* 1 = 3.3745 loss)
I0623 12:32:46.964961  5064 solver.cpp:473] Iteration 9060, lr = 0.0001
I0623 12:32:47.985396  5064 solver.cpp:213] Iteration 9070, loss = 3.30662
I0623 12:32:47.985416  5064 solver.cpp:228]     Train net output #0: softmax = 3.30662 (* 1 = 3.30662 loss)
I0623 12:32:47.985421  5064 solver.cpp:473] Iteration 9070, lr = 0.0001
I0623 12:32:49.006211  5064 solver.cpp:213] Iteration 9080, loss = 3.38958
I0623 12:32:49.006232  5064 solver.cpp:228]     Train net output #0: softmax = 3.38958 (* 1 = 3.38958 loss)
I0623 12:32:49.006237  5064 solver.cpp:473] Iteration 9080, lr = 0.0001
I0623 12:32:50.026424  5064 solver.cpp:213] Iteration 9090, loss = 2.89566
I0623 12:32:50.026444  5064 solver.cpp:228]     Train net output #0: softmax = 2.89566 (* 1 = 2.89566 loss)
I0623 12:32:50.026449  5064 solver.cpp:473] Iteration 9090, lr = 0.0001
I0623 12:32:51.047142  5064 solver.cpp:213] Iteration 9100, loss = 2.98645
I0623 12:32:51.047165  5064 solver.cpp:228]     Train net output #0: softmax = 2.98645 (* 1 = 2.98645 loss)
I0623 12:32:51.047288  5064 solver.cpp:473] Iteration 9100, lr = 0.0001
I0623 12:32:52.067394  5064 solver.cpp:213] Iteration 9110, loss = 3.3115
I0623 12:32:52.067414  5064 solver.cpp:228]     Train net output #0: softmax = 3.3115 (* 1 = 3.3115 loss)
I0623 12:32:52.067420  5064 solver.cpp:473] Iteration 9110, lr = 0.0001
I0623 12:32:53.087115  5064 solver.cpp:213] Iteration 9120, loss = 3.26919
I0623 12:32:53.087133  5064 solver.cpp:228]     Train net output #0: softmax = 3.26919 (* 1 = 3.26919 loss)
I0623 12:32:53.087138  5064 solver.cpp:473] Iteration 9120, lr = 0.0001
I0623 12:32:54.107853  5064 solver.cpp:213] Iteration 9130, loss = 3.26072
I0623 12:32:54.107873  5064 solver.cpp:228]     Train net output #0: softmax = 3.26072 (* 1 = 3.26072 loss)
I0623 12:32:54.107878  5064 solver.cpp:473] Iteration 9130, lr = 0.0001
I0623 12:32:55.128036  5064 solver.cpp:213] Iteration 9140, loss = 3.19935
I0623 12:32:55.128072  5064 solver.cpp:228]     Train net output #0: softmax = 3.19935 (* 1 = 3.19935 loss)
I0623 12:32:55.128077  5064 solver.cpp:473] Iteration 9140, lr = 0.0001
I0623 12:32:56.147506  5064 solver.cpp:213] Iteration 9150, loss = 3.24918
I0623 12:32:56.147528  5064 solver.cpp:228]     Train net output #0: softmax = 3.24918 (* 1 = 3.24918 loss)
I0623 12:32:56.147680  5064 solver.cpp:473] Iteration 9150, lr = 0.0001
I0623 12:32:57.168213  5064 solver.cpp:213] Iteration 9160, loss = 3.22363
I0623 12:32:57.168233  5064 solver.cpp:228]     Train net output #0: softmax = 3.22363 (* 1 = 3.22363 loss)
I0623 12:32:57.168238  5064 solver.cpp:473] Iteration 9160, lr = 0.0001
I0623 12:32:58.188846  5064 solver.cpp:213] Iteration 9170, loss = 3.14834
I0623 12:32:58.188868  5064 solver.cpp:228]     Train net output #0: softmax = 3.14834 (* 1 = 3.14834 loss)
I0623 12:32:58.188874  5064 solver.cpp:473] Iteration 9170, lr = 0.0001
I0623 12:32:59.208999  5064 solver.cpp:213] Iteration 9180, loss = 3.22781
I0623 12:32:59.209019  5064 solver.cpp:228]     Train net output #0: softmax = 3.22781 (* 1 = 3.22781 loss)
I0623 12:32:59.209024  5064 solver.cpp:473] Iteration 9180, lr = 0.0001
I0623 12:33:00.229456  5064 solver.cpp:213] Iteration 9190, loss = 3.24863
I0623 12:33:00.229476  5064 solver.cpp:228]     Train net output #0: softmax = 3.24863 (* 1 = 3.24863 loss)
I0623 12:33:00.229481  5064 solver.cpp:473] Iteration 9190, lr = 0.0001
I0623 12:33:01.250363  5064 solver.cpp:213] Iteration 9200, loss = 3.03097
I0623 12:33:01.250388  5064 solver.cpp:228]     Train net output #0: softmax = 3.03097 (* 1 = 3.03097 loss)
I0623 12:33:01.250511  5064 solver.cpp:473] Iteration 9200, lr = 0.0001
I0623 12:33:02.271236  5064 solver.cpp:213] Iteration 9210, loss = 3.4429
I0623 12:33:02.271260  5064 solver.cpp:228]     Train net output #0: softmax = 3.4429 (* 1 = 3.4429 loss)
I0623 12:33:02.271265  5064 solver.cpp:473] Iteration 9210, lr = 0.0001
I0623 12:33:03.291110  5064 solver.cpp:213] Iteration 9220, loss = 3.08676
I0623 12:33:03.291129  5064 solver.cpp:228]     Train net output #0: softmax = 3.08676 (* 1 = 3.08676 loss)
I0623 12:33:03.291134  5064 solver.cpp:473] Iteration 9220, lr = 0.0001
I0623 12:33:04.311635  5064 solver.cpp:213] Iteration 9230, loss = 3.05095
I0623 12:33:04.311652  5064 solver.cpp:228]     Train net output #0: softmax = 3.05095 (* 1 = 3.05095 loss)
I0623 12:33:04.311657  5064 solver.cpp:473] Iteration 9230, lr = 0.0001
I0623 12:33:05.331848  5064 solver.cpp:213] Iteration 9240, loss = 3.10362
I0623 12:33:05.331868  5064 solver.cpp:228]     Train net output #0: softmax = 3.10362 (* 1 = 3.10362 loss)
I0623 12:33:05.331873  5064 solver.cpp:473] Iteration 9240, lr = 0.0001
I0623 12:33:06.352521  5064 solver.cpp:213] Iteration 9250, loss = 3.17387
I0623 12:33:06.352545  5064 solver.cpp:228]     Train net output #0: softmax = 3.17387 (* 1 = 3.17387 loss)
I0623 12:33:06.352550  5064 solver.cpp:473] Iteration 9250, lr = 0.0001
I0623 12:33:07.373283  5064 solver.cpp:213] Iteration 9260, loss = 2.90139
I0623 12:33:07.373303  5064 solver.cpp:228]     Train net output #0: softmax = 2.90139 (* 1 = 2.90139 loss)
I0623 12:33:07.373309  5064 solver.cpp:473] Iteration 9260, lr = 0.0001
I0623 12:33:08.393664  5064 solver.cpp:213] Iteration 9270, loss = 3.15704
I0623 12:33:08.393684  5064 solver.cpp:228]     Train net output #0: softmax = 3.15704 (* 1 = 3.15704 loss)
I0623 12:33:08.393689  5064 solver.cpp:473] Iteration 9270, lr = 0.0001
I0623 12:33:09.414338  5064 solver.cpp:213] Iteration 9280, loss = 3.17156
I0623 12:33:09.414356  5064 solver.cpp:228]     Train net output #0: softmax = 3.17156 (* 1 = 3.17156 loss)
I0623 12:33:09.414361  5064 solver.cpp:473] Iteration 9280, lr = 0.0001
I0623 12:33:10.434959  5064 solver.cpp:213] Iteration 9290, loss = 3.12207
I0623 12:33:10.434980  5064 solver.cpp:228]     Train net output #0: softmax = 3.12207 (* 1 = 3.12207 loss)
I0623 12:33:10.434985  5064 solver.cpp:473] Iteration 9290, lr = 0.0001
I0623 12:33:11.454805  5064 solver.cpp:213] Iteration 9300, loss = 3.2941
I0623 12:33:11.454975  5064 solver.cpp:228]     Train net output #0: softmax = 3.2941 (* 1 = 3.2941 loss)
I0623 12:33:11.454982  5064 solver.cpp:473] Iteration 9300, lr = 0.0001
I0623 12:33:12.475481  5064 solver.cpp:213] Iteration 9310, loss = 3.23928
I0623 12:33:12.475500  5064 solver.cpp:228]     Train net output #0: softmax = 3.23928 (* 1 = 3.23928 loss)
I0623 12:33:12.475505  5064 solver.cpp:473] Iteration 9310, lr = 0.0001
I0623 12:33:13.496361  5064 solver.cpp:213] Iteration 9320, loss = 3.06129
I0623 12:33:13.496381  5064 solver.cpp:228]     Train net output #0: softmax = 3.06129 (* 1 = 3.06129 loss)
I0623 12:33:13.496386  5064 solver.cpp:473] Iteration 9320, lr = 0.0001
I0623 12:33:14.516841  5064 solver.cpp:213] Iteration 9330, loss = 3.054
I0623 12:33:14.516861  5064 solver.cpp:228]     Train net output #0: softmax = 3.054 (* 1 = 3.054 loss)
I0623 12:33:14.516866  5064 solver.cpp:473] Iteration 9330, lr = 0.0001
I0623 12:33:15.536757  5064 solver.cpp:213] Iteration 9340, loss = 3.35507
I0623 12:33:15.536774  5064 solver.cpp:228]     Train net output #0: softmax = 3.35507 (* 1 = 3.35507 loss)
I0623 12:33:15.536779  5064 solver.cpp:473] Iteration 9340, lr = 0.0001
I0623 12:33:16.557132  5064 solver.cpp:213] Iteration 9350, loss = 3.26335
I0623 12:33:16.557155  5064 solver.cpp:228]     Train net output #0: softmax = 3.26335 (* 1 = 3.26335 loss)
I0623 12:33:16.557338  5064 solver.cpp:473] Iteration 9350, lr = 0.0001
I0623 12:33:17.577668  5064 solver.cpp:213] Iteration 9360, loss = 3.21136
I0623 12:33:17.577688  5064 solver.cpp:228]     Train net output #0: softmax = 3.21136 (* 1 = 3.21136 loss)
I0623 12:33:17.577693  5064 solver.cpp:473] Iteration 9360, lr = 0.0001
I0623 12:33:18.598036  5064 solver.cpp:213] Iteration 9370, loss = 3.4038
I0623 12:33:18.598057  5064 solver.cpp:228]     Train net output #0: softmax = 3.4038 (* 1 = 3.4038 loss)
I0623 12:33:18.598062  5064 solver.cpp:473] Iteration 9370, lr = 0.0001
I0623 12:33:19.617607  5064 solver.cpp:213] Iteration 9380, loss = 3.40979
I0623 12:33:19.617626  5064 solver.cpp:228]     Train net output #0: softmax = 3.40979 (* 1 = 3.40979 loss)
I0623 12:33:19.617631  5064 solver.cpp:473] Iteration 9380, lr = 0.0001
I0623 12:33:20.637953  5064 solver.cpp:213] Iteration 9390, loss = 3.34187
I0623 12:33:20.637972  5064 solver.cpp:228]     Train net output #0: softmax = 3.34187 (* 1 = 3.34187 loss)
I0623 12:33:20.637977  5064 solver.cpp:473] Iteration 9390, lr = 0.0001
I0623 12:33:21.659235  5064 solver.cpp:213] Iteration 9400, loss = 3.1419
I0623 12:33:21.659260  5064 solver.cpp:228]     Train net output #0: softmax = 3.1419 (* 1 = 3.1419 loss)
I0623 12:33:21.659381  5064 solver.cpp:473] Iteration 9400, lr = 0.0001
I0623 12:33:22.679904  5064 solver.cpp:213] Iteration 9410, loss = 3.17453
I0623 12:33:22.679924  5064 solver.cpp:228]     Train net output #0: softmax = 3.17453 (* 1 = 3.17453 loss)
I0623 12:33:22.679929  5064 solver.cpp:473] Iteration 9410, lr = 0.0001
I0623 12:33:23.700080  5064 solver.cpp:213] Iteration 9420, loss = 3.22624
I0623 12:33:23.700099  5064 solver.cpp:228]     Train net output #0: softmax = 3.22624 (* 1 = 3.22624 loss)
I0623 12:33:23.700104  5064 solver.cpp:473] Iteration 9420, lr = 0.0001
I0623 12:33:24.720201  5064 solver.cpp:213] Iteration 9430, loss = 3.12944
I0623 12:33:24.720232  5064 solver.cpp:228]     Train net output #0: softmax = 3.12944 (* 1 = 3.12944 loss)
I0623 12:33:24.720237  5064 solver.cpp:473] Iteration 9430, lr = 0.0001
I0623 12:33:25.740854  5064 solver.cpp:213] Iteration 9440, loss = 2.92738
I0623 12:33:25.740875  5064 solver.cpp:228]     Train net output #0: softmax = 2.92738 (* 1 = 2.92738 loss)
I0623 12:33:25.740880  5064 solver.cpp:473] Iteration 9440, lr = 0.0001
I0623 12:33:26.761941  5064 solver.cpp:213] Iteration 9450, loss = 3.00157
I0623 12:33:26.761966  5064 solver.cpp:228]     Train net output #0: softmax = 3.00157 (* 1 = 3.00157 loss)
I0623 12:33:26.762095  5064 solver.cpp:473] Iteration 9450, lr = 0.0001
I0623 12:33:27.782269  5064 solver.cpp:213] Iteration 9460, loss = 3.13414
I0623 12:33:27.782290  5064 solver.cpp:228]     Train net output #0: softmax = 3.13414 (* 1 = 3.13414 loss)
I0623 12:33:27.782315  5064 solver.cpp:473] Iteration 9460, lr = 0.0001
I0623 12:33:28.802731  5064 solver.cpp:213] Iteration 9470, loss = 3.46097
I0623 12:33:28.802752  5064 solver.cpp:228]     Train net output #0: softmax = 3.46097 (* 1 = 3.46097 loss)
I0623 12:33:28.802757  5064 solver.cpp:473] Iteration 9470, lr = 0.0001
I0623 12:33:29.823560  5064 solver.cpp:213] Iteration 9480, loss = 3.09478
I0623 12:33:29.823585  5064 solver.cpp:228]     Train net output #0: softmax = 3.09478 (* 1 = 3.09478 loss)
I0623 12:33:29.823590  5064 solver.cpp:473] Iteration 9480, lr = 0.0001
I0623 12:33:30.843812  5064 solver.cpp:213] Iteration 9490, loss = 2.96724
I0623 12:33:30.843832  5064 solver.cpp:228]     Train net output #0: softmax = 2.96724 (* 1 = 2.96724 loss)
I0623 12:33:30.843838  5064 solver.cpp:473] Iteration 9490, lr = 0.0001
I0623 12:33:31.864508  5064 solver.cpp:213] Iteration 9500, loss = 3.4292
I0623 12:33:31.864532  5064 solver.cpp:228]     Train net output #0: softmax = 3.4292 (* 1 = 3.4292 loss)
I0623 12:33:31.864686  5064 solver.cpp:473] Iteration 9500, lr = 0.0001
I0623 12:33:32.885403  5064 solver.cpp:213] Iteration 9510, loss = 3.31544
I0623 12:33:32.885426  5064 solver.cpp:228]     Train net output #0: softmax = 3.31544 (* 1 = 3.31544 loss)
I0623 12:33:32.885431  5064 solver.cpp:473] Iteration 9510, lr = 0.0001
I0623 12:33:33.906401  5064 solver.cpp:213] Iteration 9520, loss = 3.20803
I0623 12:33:33.906427  5064 solver.cpp:228]     Train net output #0: softmax = 3.20803 (* 1 = 3.20803 loss)
I0623 12:33:33.906432  5064 solver.cpp:473] Iteration 9520, lr = 0.0001
I0623 12:33:34.927937  5064 solver.cpp:213] Iteration 9530, loss = 3.16075
I0623 12:33:34.927954  5064 solver.cpp:228]     Train net output #0: softmax = 3.16075 (* 1 = 3.16075 loss)
I0623 12:33:34.927959  5064 solver.cpp:473] Iteration 9530, lr = 0.0001
I0623 12:33:35.949250  5064 solver.cpp:213] Iteration 9540, loss = 3.19664
I0623 12:33:35.949267  5064 solver.cpp:228]     Train net output #0: softmax = 3.19664 (* 1 = 3.19664 loss)
I0623 12:33:35.949272  5064 solver.cpp:473] Iteration 9540, lr = 0.0001
I0623 12:33:36.969720  5064 solver.cpp:213] Iteration 9550, loss = 3.41126
I0623 12:33:36.969750  5064 solver.cpp:228]     Train net output #0: softmax = 3.41126 (* 1 = 3.41126 loss)
I0623 12:33:36.969874  5064 solver.cpp:473] Iteration 9550, lr = 0.0001
I0623 12:33:37.990790  5064 solver.cpp:213] Iteration 9560, loss = 3.23088
I0623 12:33:37.990806  5064 solver.cpp:228]     Train net output #0: softmax = 3.23088 (* 1 = 3.23088 loss)
I0623 12:33:37.990813  5064 solver.cpp:473] Iteration 9560, lr = 0.0001
I0623 12:33:39.012004  5064 solver.cpp:213] Iteration 9570, loss = 3.10166
I0623 12:33:39.012022  5064 solver.cpp:228]     Train net output #0: softmax = 3.10166 (* 1 = 3.10166 loss)
I0623 12:33:39.012027  5064 solver.cpp:473] Iteration 9570, lr = 0.0001
I0623 12:33:40.032011  5064 solver.cpp:213] Iteration 9580, loss = 3.13637
I0623 12:33:40.032027  5064 solver.cpp:228]     Train net output #0: softmax = 3.13637 (* 1 = 3.13637 loss)
I0623 12:33:40.032032  5064 solver.cpp:473] Iteration 9580, lr = 0.0001
I0623 12:33:41.052793  5064 solver.cpp:213] Iteration 9590, loss = 3.10661
I0623 12:33:41.052815  5064 solver.cpp:228]     Train net output #0: softmax = 3.10661 (* 1 = 3.10661 loss)
I0623 12:33:41.052821  5064 solver.cpp:473] Iteration 9590, lr = 0.0001
I0623 12:33:42.073459  5064 solver.cpp:213] Iteration 9600, loss = 3.13702
I0623 12:33:42.073632  5064 solver.cpp:228]     Train net output #0: softmax = 3.13702 (* 1 = 3.13702 loss)
I0623 12:33:42.073639  5064 solver.cpp:473] Iteration 9600, lr = 0.0001
I0623 12:33:43.094236  5064 solver.cpp:213] Iteration 9610, loss = 3.13431
I0623 12:33:43.094256  5064 solver.cpp:228]     Train net output #0: softmax = 3.13431 (* 1 = 3.13431 loss)
I0623 12:33:43.094261  5064 solver.cpp:473] Iteration 9610, lr = 0.0001
I0623 12:33:44.114831  5064 solver.cpp:213] Iteration 9620, loss = 3.09668
I0623 12:33:44.114850  5064 solver.cpp:228]     Train net output #0: softmax = 3.09668 (* 1 = 3.09668 loss)
I0623 12:33:44.114856  5064 solver.cpp:473] Iteration 9620, lr = 0.0001
I0623 12:33:45.136122  5064 solver.cpp:213] Iteration 9630, loss = 3.12915
I0623 12:33:45.136142  5064 solver.cpp:228]     Train net output #0: softmax = 3.12915 (* 1 = 3.12915 loss)
I0623 12:33:45.136147  5064 solver.cpp:473] Iteration 9630, lr = 0.0001
I0623 12:33:46.156833  5064 solver.cpp:213] Iteration 9640, loss = 3.06848
I0623 12:33:46.156854  5064 solver.cpp:228]     Train net output #0: softmax = 3.06848 (* 1 = 3.06848 loss)
I0623 12:33:46.156859  5064 solver.cpp:473] Iteration 9640, lr = 0.0001
I0623 12:33:47.177554  5064 solver.cpp:213] Iteration 9650, loss = 3.15608
I0623 12:33:47.177579  5064 solver.cpp:228]     Train net output #0: softmax = 3.15608 (* 1 = 3.15608 loss)
I0623 12:33:47.177712  5064 solver.cpp:473] Iteration 9650, lr = 0.0001
I0623 12:33:48.197754  5064 solver.cpp:213] Iteration 9660, loss = 3.17403
I0623 12:33:48.197775  5064 solver.cpp:228]     Train net output #0: softmax = 3.17403 (* 1 = 3.17403 loss)
I0623 12:33:48.197782  5064 solver.cpp:473] Iteration 9660, lr = 0.0001
I0623 12:33:49.218367  5064 solver.cpp:213] Iteration 9670, loss = 3.02563
I0623 12:33:49.218386  5064 solver.cpp:228]     Train net output #0: softmax = 3.02563 (* 1 = 3.02563 loss)
I0623 12:33:49.218391  5064 solver.cpp:473] Iteration 9670, lr = 0.0001
I0623 12:33:50.239081  5064 solver.cpp:213] Iteration 9680, loss = 3.25542
I0623 12:33:50.239105  5064 solver.cpp:228]     Train net output #0: softmax = 3.25542 (* 1 = 3.25542 loss)
I0623 12:33:50.239110  5064 solver.cpp:473] Iteration 9680, lr = 0.0001
I0623 12:33:51.259685  5064 solver.cpp:213] Iteration 9690, loss = 3.16464
I0623 12:33:51.259701  5064 solver.cpp:228]     Train net output #0: softmax = 3.16464 (* 1 = 3.16464 loss)
I0623 12:33:51.259706  5064 solver.cpp:473] Iteration 9690, lr = 0.0001
I0623 12:33:52.280575  5064 solver.cpp:213] Iteration 9700, loss = 3.35372
I0623 12:33:52.280597  5064 solver.cpp:228]     Train net output #0: softmax = 3.35372 (* 1 = 3.35372 loss)
I0623 12:33:52.280717  5064 solver.cpp:473] Iteration 9700, lr = 0.0001
I0623 12:33:53.301086  5064 solver.cpp:213] Iteration 9710, loss = 3.17718
I0623 12:33:53.301103  5064 solver.cpp:228]     Train net output #0: softmax = 3.17718 (* 1 = 3.17718 loss)
I0623 12:33:53.301108  5064 solver.cpp:473] Iteration 9710, lr = 0.0001
I0623 12:33:54.321890  5064 solver.cpp:213] Iteration 9720, loss = 3.08541
I0623 12:33:54.321907  5064 solver.cpp:228]     Train net output #0: softmax = 3.08541 (* 1 = 3.08541 loss)
I0623 12:33:54.321913  5064 solver.cpp:473] Iteration 9720, lr = 0.0001
I0623 12:33:55.342653  5064 solver.cpp:213] Iteration 9730, loss = 3.12561
I0623 12:33:55.342671  5064 solver.cpp:228]     Train net output #0: softmax = 3.12561 (* 1 = 3.12561 loss)
I0623 12:33:55.342676  5064 solver.cpp:473] Iteration 9730, lr = 0.0001
I0623 12:33:56.363015  5064 solver.cpp:213] Iteration 9740, loss = 3.08011
I0623 12:33:56.363034  5064 solver.cpp:228]     Train net output #0: softmax = 3.08011 (* 1 = 3.08011 loss)
I0623 12:33:56.363039  5064 solver.cpp:473] Iteration 9740, lr = 0.0001
I0623 12:33:57.383563  5064 solver.cpp:213] Iteration 9750, loss = 3.27573
I0623 12:33:57.383584  5064 solver.cpp:228]     Train net output #0: softmax = 3.27573 (* 1 = 3.27573 loss)
I0623 12:33:57.383590  5064 solver.cpp:473] Iteration 9750, lr = 0.0001
I0623 12:33:58.403914  5064 solver.cpp:213] Iteration 9760, loss = 3.17051
I0623 12:33:58.403934  5064 solver.cpp:228]     Train net output #0: softmax = 3.17051 (* 1 = 3.17051 loss)
I0623 12:33:58.403956  5064 solver.cpp:473] Iteration 9760, lr = 0.0001
I0623 12:33:59.424700  5064 solver.cpp:213] Iteration 9770, loss = 3.51246
I0623 12:33:59.424718  5064 solver.cpp:228]     Train net output #0: softmax = 3.51246 (* 1 = 3.51246 loss)
I0623 12:33:59.424723  5064 solver.cpp:473] Iteration 9770, lr = 0.0001
I0623 12:34:00.445226  5064 solver.cpp:213] Iteration 9780, loss = 3.29089
I0623 12:34:00.445245  5064 solver.cpp:228]     Train net output #0: softmax = 3.29089 (* 1 = 3.29089 loss)
I0623 12:34:00.445250  5064 solver.cpp:473] Iteration 9780, lr = 0.0001
I0623 12:34:01.465868  5064 solver.cpp:213] Iteration 9790, loss = 3.08665
I0623 12:34:01.465890  5064 solver.cpp:228]     Train net output #0: softmax = 3.08665 (* 1 = 3.08665 loss)
I0623 12:34:01.465895  5064 solver.cpp:473] Iteration 9790, lr = 0.0001
I0623 12:34:02.486521  5064 solver.cpp:213] Iteration 9800, loss = 3.07396
I0623 12:34:02.486546  5064 solver.cpp:228]     Train net output #0: softmax = 3.07396 (* 1 = 3.07396 loss)
I0623 12:34:02.486706  5064 solver.cpp:473] Iteration 9800, lr = 0.0001
I0623 12:34:03.506944  5064 solver.cpp:213] Iteration 9810, loss = 3.24598
I0623 12:34:03.506963  5064 solver.cpp:228]     Train net output #0: softmax = 3.24598 (* 1 = 3.24598 loss)
I0623 12:34:03.506968  5064 solver.cpp:473] Iteration 9810, lr = 0.0001
I0623 12:34:04.527928  5064 solver.cpp:213] Iteration 9820, loss = 2.83926
I0623 12:34:04.527948  5064 solver.cpp:228]     Train net output #0: softmax = 2.83926 (* 1 = 2.83926 loss)
I0623 12:34:04.527953  5064 solver.cpp:473] Iteration 9820, lr = 0.0001
I0623 12:34:05.548851  5064 solver.cpp:213] Iteration 9830, loss = 3.00444
I0623 12:34:05.548873  5064 solver.cpp:228]     Train net output #0: softmax = 3.00444 (* 1 = 3.00444 loss)
I0623 12:34:05.548878  5064 solver.cpp:473] Iteration 9830, lr = 0.0001
I0623 12:34:06.569820  5064 solver.cpp:213] Iteration 9840, loss = 2.93783
I0623 12:34:06.569847  5064 solver.cpp:228]     Train net output #0: softmax = 2.93783 (* 1 = 2.93783 loss)
I0623 12:34:06.569854  5064 solver.cpp:473] Iteration 9840, lr = 0.0001
I0623 12:34:07.590394  5064 solver.cpp:213] Iteration 9850, loss = 2.95073
I0623 12:34:07.590420  5064 solver.cpp:228]     Train net output #0: softmax = 2.95073 (* 1 = 2.95073 loss)
I0623 12:34:07.590580  5064 solver.cpp:473] Iteration 9850, lr = 0.0001
I0623 12:34:08.611297  5064 solver.cpp:213] Iteration 9860, loss = 3.33182
I0623 12:34:08.611318  5064 solver.cpp:228]     Train net output #0: softmax = 3.33182 (* 1 = 3.33182 loss)
I0623 12:34:08.611325  5064 solver.cpp:473] Iteration 9860, lr = 0.0001
I0623 12:34:09.631518  5064 solver.cpp:213] Iteration 9870, loss = 3.27346
I0623 12:34:09.631538  5064 solver.cpp:228]     Train net output #0: softmax = 3.27346 (* 1 = 3.27346 loss)
I0623 12:34:09.631544  5064 solver.cpp:473] Iteration 9870, lr = 0.0001
I0623 12:34:10.652006  5064 solver.cpp:213] Iteration 9880, loss = 3.24632
I0623 12:34:10.652029  5064 solver.cpp:228]     Train net output #0: softmax = 3.24632 (* 1 = 3.24632 loss)
I0623 12:34:10.652034  5064 solver.cpp:473] Iteration 9880, lr = 0.0001
I0623 12:34:11.672124  5064 solver.cpp:213] Iteration 9890, loss = 3.26277
I0623 12:34:11.672144  5064 solver.cpp:228]     Train net output #0: softmax = 3.26277 (* 1 = 3.26277 loss)
I0623 12:34:11.672150  5064 solver.cpp:473] Iteration 9890, lr = 0.0001
I0623 12:34:12.692544  5064 solver.cpp:213] Iteration 9900, loss = 3.09794
I0623 12:34:12.692725  5064 solver.cpp:228]     Train net output #0: softmax = 3.09794 (* 1 = 3.09794 loss)
I0623 12:34:12.692733  5064 solver.cpp:473] Iteration 9900, lr = 0.0001
I0623 12:34:13.713456  5064 solver.cpp:213] Iteration 9910, loss = 3.22569
I0623 12:34:13.713476  5064 solver.cpp:228]     Train net output #0: softmax = 3.22569 (* 1 = 3.22569 loss)
I0623 12:34:13.713481  5064 solver.cpp:473] Iteration 9910, lr = 0.0001
I0623 12:34:14.733639  5064 solver.cpp:213] Iteration 9920, loss = 3.19932
I0623 12:34:14.733659  5064 solver.cpp:228]     Train net output #0: softmax = 3.19932 (* 1 = 3.19932 loss)
I0623 12:34:14.733664  5064 solver.cpp:473] Iteration 9920, lr = 0.0001
I0623 12:34:15.753517  5064 solver.cpp:213] Iteration 9930, loss = 3.18832
I0623 12:34:15.753535  5064 solver.cpp:228]     Train net output #0: softmax = 3.18832 (* 1 = 3.18832 loss)
I0623 12:34:15.753540  5064 solver.cpp:473] Iteration 9930, lr = 0.0001
I0623 12:34:16.773932  5064 solver.cpp:213] Iteration 9940, loss = 3.27566
I0623 12:34:16.773952  5064 solver.cpp:228]     Train net output #0: softmax = 3.27566 (* 1 = 3.27566 loss)
I0623 12:34:16.773958  5064 solver.cpp:473] Iteration 9940, lr = 0.0001
I0623 12:34:17.794700  5064 solver.cpp:213] Iteration 9950, loss = 3.29766
I0623 12:34:17.794725  5064 solver.cpp:228]     Train net output #0: softmax = 3.29766 (* 1 = 3.29766 loss)
I0623 12:34:17.794870  5064 solver.cpp:473] Iteration 9950, lr = 0.0001
I0623 12:34:18.815523  5064 solver.cpp:213] Iteration 9960, loss = 3.14816
I0623 12:34:18.815542  5064 solver.cpp:228]     Train net output #0: softmax = 3.14816 (* 1 = 3.14816 loss)
I0623 12:34:18.815547  5064 solver.cpp:473] Iteration 9960, lr = 0.0001
I0623 12:34:19.835532  5064 solver.cpp:213] Iteration 9970, loss = 3.07651
I0623 12:34:19.835551  5064 solver.cpp:228]     Train net output #0: softmax = 3.07651 (* 1 = 3.07651 loss)
I0623 12:34:19.835556  5064 solver.cpp:473] Iteration 9970, lr = 0.0001
I0623 12:34:20.855940  5064 solver.cpp:213] Iteration 9980, loss = 2.9944
I0623 12:34:20.855963  5064 solver.cpp:228]     Train net output #0: softmax = 2.9944 (* 1 = 2.9944 loss)
I0623 12:34:20.855968  5064 solver.cpp:473] Iteration 9980, lr = 0.0001
I0623 12:34:21.876821  5064 solver.cpp:213] Iteration 9990, loss = 3.20419
I0623 12:34:21.876840  5064 solver.cpp:228]     Train net output #0: softmax = 3.20419 (* 1 = 3.20419 loss)
I0623 12:34:21.876845  5064 solver.cpp:473] Iteration 9990, lr = 0.0001
I0623 12:34:22.827169  5064 solver.cpp:362] Snapshotting to snapshots/16-06-21_16h05m31s_0_11_pretrainClassification_iter_10000.caffemodel
I0623 12:34:22.828402  5064 solver.cpp:370] Snapshotting solver state to snapshots/16-06-21_16h05m31s_0_11_pretrainClassification_iter_10000.solverstate
I0623 12:34:22.829010  5064 solver.cpp:291] Iteration 10000, Testing net (#0)
I0623 12:34:22.990089  5064 solver.cpp:342]     Test net output #0: accuracy_top_1 = 0.232812
I0623 12:34:22.990108  5064 solver.cpp:342]     Test net output #1: accuracy_top_5 = 0.509375
I0623 12:34:22.990114  5064 solver.cpp:342]     Test net output #2: softmax = 3.18837 (* 1 = 3.18837 loss)
I0623 12:34:23.061002  5064 solver.cpp:213] Iteration 10000, loss = 3.24337
I0623 12:34:23.061017  5064 solver.cpp:228]     Train net output #0: softmax = 3.24337 (* 1 = 3.24337 loss)
I0623 12:34:23.061023  5064 solver.cpp:473] Iteration 10000, lr = 0.0001
I0623 12:34:24.081742  5064 solver.cpp:213] Iteration 10010, loss = 3.07474
I0623 12:34:24.081761  5064 solver.cpp:228]     Train net output #0: softmax = 3.07474 (* 1 = 3.07474 loss)
I0623 12:34:24.081766  5064 solver.cpp:473] Iteration 10010, lr = 0.0001
I0623 12:34:25.102401  5064 solver.cpp:213] Iteration 10020, loss = 3.09341
I0623 12:34:25.102419  5064 solver.cpp:228]     Train net output #0: softmax = 3.09341 (* 1 = 3.09341 loss)
I0623 12:34:25.102424  5064 solver.cpp:473] Iteration 10020, lr = 0.0001
I0623 12:34:26.123338  5064 solver.cpp:213] Iteration 10030, loss = 3.2425
I0623 12:34:26.123361  5064 solver.cpp:228]     Train net output #0: softmax = 3.2425 (* 1 = 3.2425 loss)
I0623 12:34:26.123366  5064 solver.cpp:473] Iteration 10030, lr = 0.0001
I0623 12:34:27.143625  5064 solver.cpp:213] Iteration 10040, loss = 3.13445
I0623 12:34:27.143645  5064 solver.cpp:228]     Train net output #0: softmax = 3.13445 (* 1 = 3.13445 loss)
I0623 12:34:27.143649  5064 solver.cpp:473] Iteration 10040, lr = 0.0001
I0623 12:34:28.163750  5064 solver.cpp:213] Iteration 10050, loss = 3.30068
I0623 12:34:28.163779  5064 solver.cpp:228]     Train net output #0: softmax = 3.30068 (* 1 = 3.30068 loss)
I0623 12:34:28.163789  5064 solver.cpp:473] Iteration 10050, lr = 0.0001
I0623 12:34:29.183877  5064 solver.cpp:213] Iteration 10060, loss = 3.06188
I0623 12:34:29.183897  5064 solver.cpp:228]     Train net output #0: softmax = 3.06188 (* 1 = 3.06188 loss)
I0623 12:34:29.183903  5064 solver.cpp:473] Iteration 10060, lr = 0.0001
I0623 12:34:30.203436  5064 solver.cpp:213] Iteration 10070, loss = 3.16235
I0623 12:34:30.203456  5064 solver.cpp:228]     Train net output #0: softmax = 3.16235 (* 1 = 3.16235 loss)
I0623 12:34:30.203462  5064 solver.cpp:473] Iteration 10070, lr = 0.0001
I0623 12:34:31.222903  5064 solver.cpp:213] Iteration 10080, loss = 3.2831
I0623 12:34:31.222921  5064 solver.cpp:228]     Train net output #0: softmax = 3.2831 (* 1 = 3.2831 loss)
I0623 12:34:31.222926  5064 solver.cpp:473] Iteration 10080, lr = 0.0001
I0623 12:34:32.242812  5064 solver.cpp:213] Iteration 10090, loss = 3.30406
I0623 12:34:32.242835  5064 solver.cpp:228]     Train net output #0: softmax = 3.30406 (* 1 = 3.30406 loss)
I0623 12:34:32.242840  5064 solver.cpp:473] Iteration 10090, lr = 0.0001
I0623 12:34:33.263512  5064 solver.cpp:213] Iteration 10100, loss = 3.25346
I0623 12:34:33.263537  5064 solver.cpp:228]     Train net output #0: softmax = 3.25346 (* 1 = 3.25346 loss)
I0623 12:34:33.263659  5064 solver.cpp:473] Iteration 10100, lr = 0.0001
I0623 12:34:34.283843  5064 solver.cpp:213] Iteration 10110, loss = 3.1507
I0623 12:34:34.283864  5064 solver.cpp:228]     Train net output #0: softmax = 3.1507 (* 1 = 3.1507 loss)
I0623 12:34:34.283869  5064 solver.cpp:473] Iteration 10110, lr = 0.0001
I0623 12:34:35.304241  5064 solver.cpp:213] Iteration 10120, loss = 3.02128
I0623 12:34:35.304260  5064 solver.cpp:228]     Train net output #0: softmax = 3.02128 (* 1 = 3.02128 loss)
I0623 12:34:35.304265  5064 solver.cpp:473] Iteration 10120, lr = 0.0001
I0623 12:34:36.325484  5064 solver.cpp:213] Iteration 10130, loss = 3.065
I0623 12:34:36.325505  5064 solver.cpp:228]     Train net output #0: softmax = 3.065 (* 1 = 3.065 loss)
I0623 12:34:36.325515  5064 solver.cpp:473] Iteration 10130, lr = 0.0001
I0623 12:34:37.346174  5064 solver.cpp:213] Iteration 10140, loss = 3.18506
I0623 12:34:37.346194  5064 solver.cpp:228]     Train net output #0: softmax = 3.18506 (* 1 = 3.18506 loss)
I0623 12:34:37.346199  5064 solver.cpp:473] Iteration 10140, lr = 0.0001
I0623 12:34:38.367261  5064 solver.cpp:213] Iteration 10150, loss = 3.18804
I0623 12:34:38.367282  5064 solver.cpp:228]     Train net output #0: softmax = 3.18804 (* 1 = 3.18804 loss)
I0623 12:34:38.367287  5064 solver.cpp:473] Iteration 10150, lr = 0.0001
I0623 12:34:39.387686  5064 solver.cpp:213] Iteration 10160, loss = 3.52212
I0623 12:34:39.387707  5064 solver.cpp:228]     Train net output #0: softmax = 3.52212 (* 1 = 3.52212 loss)
I0623 12:34:39.387713  5064 solver.cpp:473] Iteration 10160, lr = 0.0001
I0623 12:34:40.407851  5064 solver.cpp:213] Iteration 10170, loss = 3.18605
I0623 12:34:40.407871  5064 solver.cpp:228]     Train net output #0: softmax = 3.18605 (* 1 = 3.18605 loss)
I0623 12:34:40.407877  5064 solver.cpp:473] Iteration 10170, lr = 0.0001
I0623 12:34:41.428190  5064 solver.cpp:213] Iteration 10180, loss = 3.43574
I0623 12:34:41.428210  5064 solver.cpp:228]     Train net output #0: softmax = 3.43574 (* 1 = 3.43574 loss)
I0623 12:34:41.428215  5064 solver.cpp:473] Iteration 10180, lr = 0.0001
I0623 12:34:42.448494  5064 solver.cpp:213] Iteration 10190, loss = 3.19576
I0623 12:34:42.448515  5064 solver.cpp:228]     Train net output #0: softmax = 3.19576 (* 1 = 3.19576 loss)
I0623 12:34:42.448520  5064 solver.cpp:473] Iteration 10190, lr = 0.0001
I0623 12:34:43.468646  5064 solver.cpp:213] Iteration 10200, loss = 3.23224
I0623 12:34:43.468813  5064 solver.cpp:228]     Train net output #0: softmax = 3.23224 (* 1 = 3.23224 loss)
I0623 12:34:43.468821  5064 solver.cpp:473] Iteration 10200, lr = 0.0001
I0623 12:34:44.489117  5064 solver.cpp:213] Iteration 10210, loss = 3.17044
I0623 12:34:44.489137  5064 solver.cpp:228]     Train net output #0: softmax = 3.17044 (* 1 = 3.17044 loss)
I0623 12:34:44.489142  5064 solver.cpp:473] Iteration 10210, lr = 0.0001
I0623 12:34:45.509871  5064 solver.cpp:213] Iteration 10220, loss = 3.01216
I0623 12:34:45.509891  5064 solver.cpp:228]     Train net output #0: softmax = 3.01216 (* 1 = 3.01216 loss)
I0623 12:34:45.509896  5064 solver.cpp:473] Iteration 10220, lr = 0.0001
I0623 12:34:46.530864  5064 solver.cpp:213] Iteration 10230, loss = 3.31401
I0623 12:34:46.530881  5064 solver.cpp:228]     Train net output #0: softmax = 3.31401 (* 1 = 3.31401 loss)
I0623 12:34:46.530886  5064 solver.cpp:473] Iteration 10230, lr = 0.0001
I0623 12:34:47.550917  5064 solver.cpp:213] Iteration 10240, loss = 3.11944
I0623 12:34:47.550936  5064 solver.cpp:228]     Train net output #0: softmax = 3.11944 (* 1 = 3.11944 loss)
I0623 12:34:47.550941  5064 solver.cpp:473] Iteration 10240, lr = 0.0001
I0623 12:34:48.571321  5064 solver.cpp:213] Iteration 10250, loss = 3.08843
I0623 12:34:48.571346  5064 solver.cpp:228]     Train net output #0: softmax = 3.08843 (* 1 = 3.08843 loss)
I0623 12:34:48.571506  5064 solver.cpp:473] Iteration 10250, lr = 0.0001
I0623 12:34:49.592185  5064 solver.cpp:213] Iteration 10260, loss = 3.17087
I0623 12:34:49.592206  5064 solver.cpp:228]     Train net output #0: softmax = 3.17087 (* 1 = 3.17087 loss)
I0623 12:34:49.592211  5064 solver.cpp:473] Iteration 10260, lr = 0.0001
I0623 12:34:50.613015  5064 solver.cpp:213] Iteration 10270, loss = 3.36065
I0623 12:34:50.613039  5064 solver.cpp:228]     Train net output #0: softmax = 3.36065 (* 1 = 3.36065 loss)
I0623 12:34:50.613044  5064 solver.cpp:473] Iteration 10270, lr = 0.0001
I0623 12:34:51.633329  5064 solver.cpp:213] Iteration 10280, loss = 3.24818
I0623 12:34:51.633350  5064 solver.cpp:228]     Train net output #0: softmax = 3.24818 (* 1 = 3.24818 loss)
I0623 12:34:51.633355  5064 solver.cpp:473] Iteration 10280, lr = 0.0001
I0623 12:34:52.653911  5064 solver.cpp:213] Iteration 10290, loss = 3.21863
I0623 12:34:52.653931  5064 solver.cpp:228]     Train net output #0: softmax = 3.21863 (* 1 = 3.21863 loss)
I0623 12:34:52.653944  5064 solver.cpp:473] Iteration 10290, lr = 0.0001
I0623 12:34:53.674998  5064 solver.cpp:213] Iteration 10300, loss = 3.19953
I0623 12:34:53.675024  5064 solver.cpp:228]     Train net output #0: softmax = 3.19953 (* 1 = 3.19953 loss)
I0623 12:34:53.675145  5064 solver.cpp:473] Iteration 10300, lr = 0.0001
I0623 12:34:54.695698  5064 solver.cpp:213] Iteration 10310, loss = 3.31342
I0623 12:34:54.695719  5064 solver.cpp:228]     Train net output #0: softmax = 3.31342 (* 1 = 3.31342 loss)
I0623 12:34:54.695724  5064 solver.cpp:473] Iteration 10310, lr = 0.0001
I0623 12:34:55.715839  5064 solver.cpp:213] Iteration 10320, loss = 2.9789
I0623 12:34:55.715860  5064 solver.cpp:228]     Train net output #0: softmax = 2.9789 (* 1 = 2.9789 loss)
I0623 12:34:55.715865  5064 solver.cpp:473] Iteration 10320, lr = 0.0001
I0623 12:34:56.736840  5064 solver.cpp:213] Iteration 10330, loss = 3.00412
I0623 12:34:56.736862  5064 solver.cpp:228]     Train net output #0: softmax = 3.00412 (* 1 = 3.00412 loss)
I0623 12:34:56.736868  5064 solver.cpp:473] Iteration 10330, lr = 0.0001
I0623 12:34:57.757583  5064 solver.cpp:213] Iteration 10340, loss = 3.4716
I0623 12:34:57.757606  5064 solver.cpp:228]     Train net output #0: softmax = 3.4716 (* 1 = 3.4716 loss)
I0623 12:34:57.757611  5064 solver.cpp:473] Iteration 10340, lr = 0.0001
I0623 12:34:58.778700  5064 solver.cpp:213] Iteration 10350, loss = 3.32926
I0623 12:34:58.778728  5064 solver.cpp:228]     Train net output #0: softmax = 3.32926 (* 1 = 3.32926 loss)
I0623 12:34:58.778878  5064 solver.cpp:473] Iteration 10350, lr = 0.0001
I0623 12:34:59.798550  5064 solver.cpp:213] Iteration 10360, loss = 3.04306
I0623 12:34:59.798586  5064 solver.cpp:228]     Train net output #0: softmax = 3.04306 (* 1 = 3.04306 loss)
I0623 12:34:59.798593  5064 solver.cpp:473] Iteration 10360, lr = 0.0001
I0623 12:35:00.818624  5064 solver.cpp:213] Iteration 10370, loss = 3.20608
I0623 12:35:00.818645  5064 solver.cpp:228]     Train net output #0: softmax = 3.20608 (* 1 = 3.20608 loss)
I0623 12:35:00.818650  5064 solver.cpp:473] Iteration 10370, lr = 0.0001
I0623 12:35:01.839131  5064 solver.cpp:213] Iteration 10380, loss = 3.33936
I0623 12:35:01.839153  5064 solver.cpp:228]     Train net output #0: softmax = 3.33936 (* 1 = 3.33936 loss)
I0623 12:35:01.839157  5064 solver.cpp:473] Iteration 10380, lr = 0.0001
I0623 12:35:02.860059  5064 solver.cpp:213] Iteration 10390, loss = 3.42407
I0623 12:35:02.860081  5064 solver.cpp:228]     Train net output #0: softmax = 3.42407 (* 1 = 3.42407 loss)
I0623 12:35:02.860086  5064 solver.cpp:473] Iteration 10390, lr = 0.0001
I0623 12:35:03.880525  5064 solver.cpp:213] Iteration 10400, loss = 3.10826
I0623 12:35:03.880549  5064 solver.cpp:228]     Train net output #0: softmax = 3.10826 (* 1 = 3.10826 loss)
I0623 12:35:03.880684  5064 solver.cpp:473] Iteration 10400, lr = 0.0001
I0623 12:35:04.901231  5064 solver.cpp:213] Iteration 10410, loss = 3.34766
I0623 12:35:04.901252  5064 solver.cpp:228]     Train net output #0: softmax = 3.34766 (* 1 = 3.34766 loss)
I0623 12:35:04.901258  5064 solver.cpp:473] Iteration 10410, lr = 0.0001
I0623 12:35:05.922071  5064 solver.cpp:213] Iteration 10420, loss = 3.41319
I0623 12:35:05.922094  5064 solver.cpp:228]     Train net output #0: softmax = 3.41319 (* 1 = 3.41319 loss)
I0623 12:35:05.922101  5064 solver.cpp:473] Iteration 10420, lr = 0.0001
I0623 12:35:06.942868  5064 solver.cpp:213] Iteration 10430, loss = 3.38244
I0623 12:35:06.942890  5064 solver.cpp:228]     Train net output #0: softmax = 3.38244 (* 1 = 3.38244 loss)
I0623 12:35:06.942896  5064 solver.cpp:473] Iteration 10430, lr = 0.0001
I0623 12:35:07.963544  5064 solver.cpp:213] Iteration 10440, loss = 3.4228
I0623 12:35:07.963565  5064 solver.cpp:228]     Train net output #0: softmax = 3.4228 (* 1 = 3.4228 loss)
I0623 12:35:07.963570  5064 solver.cpp:473] Iteration 10440, lr = 0.0001
I0623 12:35:08.984211  5064 solver.cpp:213] Iteration 10450, loss = 3.26565
I0623 12:35:08.984235  5064 solver.cpp:228]     Train net output #0: softmax = 3.26565 (* 1 = 3.26565 loss)
I0623 12:35:08.984365  5064 solver.cpp:473] Iteration 10450, lr = 0.0001
I0623 12:35:10.005216  5064 solver.cpp:213] Iteration 10460, loss = 2.98717
I0623 12:35:10.005237  5064 solver.cpp:228]     Train net output #0: softmax = 2.98717 (* 1 = 2.98717 loss)
I0623 12:35:10.005242  5064 solver.cpp:473] Iteration 10460, lr = 0.0001
I0623 12:35:11.025231  5064 solver.cpp:213] Iteration 10470, loss = 3.24809
I0623 12:35:11.025249  5064 solver.cpp:228]     Train net output #0: softmax = 3.24809 (* 1 = 3.24809 loss)
I0623 12:35:11.025255  5064 solver.cpp:473] Iteration 10470, lr = 0.0001
I0623 12:35:12.046205  5064 solver.cpp:213] Iteration 10480, loss = 2.93498
I0623 12:35:12.046224  5064 solver.cpp:228]     Train net output #0: softmax = 2.93498 (* 1 = 2.93498 loss)
I0623 12:35:12.046229  5064 solver.cpp:473] Iteration 10480, lr = 0.0001
I0623 12:35:13.067042  5064 solver.cpp:213] Iteration 10490, loss = 3.20274
I0623 12:35:13.067064  5064 solver.cpp:228]     Train net output #0: softmax = 3.20274 (* 1 = 3.20274 loss)
I0623 12:35:13.067070  5064 solver.cpp:473] Iteration 10490, lr = 0.0001
I0623 12:35:14.087551  5064 solver.cpp:213] Iteration 10500, loss = 3.20258
I0623 12:35:14.087745  5064 solver.cpp:228]     Train net output #0: softmax = 3.20258 (* 1 = 3.20258 loss)
I0623 12:35:14.087752  5064 solver.cpp:473] Iteration 10500, lr = 0.0001
I0623 12:35:15.107730  5064 solver.cpp:213] Iteration 10510, loss = 3.11066
I0623 12:35:15.107751  5064 solver.cpp:228]     Train net output #0: softmax = 3.11066 (* 1 = 3.11066 loss)
I0623 12:35:15.107756  5064 solver.cpp:473] Iteration 10510, lr = 0.0001
I0623 12:35:16.127964  5064 solver.cpp:213] Iteration 10520, loss = 3.08947
I0623 12:35:16.127981  5064 solver.cpp:228]     Train net output #0: softmax = 3.08947 (* 1 = 3.08947 loss)
I0623 12:35:16.127986  5064 solver.cpp:473] Iteration 10520, lr = 0.0001
I0623 12:35:17.148839  5064 solver.cpp:213] Iteration 10530, loss = 3.2009
I0623 12:35:17.148856  5064 solver.cpp:228]     Train net output #0: softmax = 3.2009 (* 1 = 3.2009 loss)
I0623 12:35:17.148862  5064 solver.cpp:473] Iteration 10530, lr = 0.0001
I0623 12:35:18.169708  5064 solver.cpp:213] Iteration 10540, loss = 3.27357
I0623 12:35:18.169728  5064 solver.cpp:228]     Train net output #0: softmax = 3.27357 (* 1 = 3.27357 loss)
I0623 12:35:18.169733  5064 solver.cpp:473] Iteration 10540, lr = 0.0001
I0623 12:35:19.189707  5064 solver.cpp:213] Iteration 10550, loss = 3.27663
I0623 12:35:19.189728  5064 solver.cpp:228]     Train net output #0: softmax = 3.27663 (* 1 = 3.27663 loss)
I0623 12:35:19.189739  5064 solver.cpp:473] Iteration 10550, lr = 0.0001
I0623 12:35:20.210530  5064 solver.cpp:213] Iteration 10560, loss = 3.11782
I0623 12:35:20.210549  5064 solver.cpp:228]     Train net output #0: softmax = 3.11782 (* 1 = 3.11782 loss)
I0623 12:35:20.210554  5064 solver.cpp:473] Iteration 10560, lr = 0.0001
I0623 12:35:21.231873  5064 solver.cpp:213] Iteration 10570, loss = 3.39776
I0623 12:35:21.231892  5064 solver.cpp:228]     Train net output #0: softmax = 3.39776 (* 1 = 3.39776 loss)
I0623 12:35:21.231897  5064 solver.cpp:473] Iteration 10570, lr = 0.0001
I0623 12:35:22.253150  5064 solver.cpp:213] Iteration 10580, loss = 3.0804
I0623 12:35:22.253173  5064 solver.cpp:228]     Train net output #0: softmax = 3.0804 (* 1 = 3.0804 loss)
I0623 12:35:22.253178  5064 solver.cpp:473] Iteration 10580, lr = 0.0001
I0623 12:35:23.273463  5064 solver.cpp:213] Iteration 10590, loss = 3.20875
I0623 12:35:23.273485  5064 solver.cpp:228]     Train net output #0: softmax = 3.20875 (* 1 = 3.20875 loss)
I0623 12:35:23.273490  5064 solver.cpp:473] Iteration 10590, lr = 0.0001
I0623 12:35:24.294069  5064 solver.cpp:213] Iteration 10600, loss = 3.08444
I0623 12:35:24.294095  5064 solver.cpp:228]     Train net output #0: softmax = 3.08444 (* 1 = 3.08444 loss)
I0623 12:35:24.294220  5064 solver.cpp:473] Iteration 10600, lr = 0.0001
I0623 12:35:25.314918  5064 solver.cpp:213] Iteration 10610, loss = 3.10715
I0623 12:35:25.314935  5064 solver.cpp:228]     Train net output #0: softmax = 3.10715 (* 1 = 3.10715 loss)
I0623 12:35:25.314947  5064 solver.cpp:473] Iteration 10610, lr = 0.0001
I0623 12:35:26.335203  5064 solver.cpp:213] Iteration 10620, loss = 3.08203
I0623 12:35:26.335233  5064 solver.cpp:228]     Train net output #0: softmax = 3.08203 (* 1 = 3.08203 loss)
I0623 12:35:26.335239  5064 solver.cpp:473] Iteration 10620, lr = 0.0001
I0623 12:35:27.355828  5064 solver.cpp:213] Iteration 10630, loss = 3.31089
I0623 12:35:27.355849  5064 solver.cpp:228]     Train net output #0: softmax = 3.31089 (* 1 = 3.31089 loss)
I0623 12:35:27.355854  5064 solver.cpp:473] Iteration 10630, lr = 0.0001
I0623 12:35:28.375728  5064 solver.cpp:213] Iteration 10640, loss = 3.01626
I0623 12:35:28.375747  5064 solver.cpp:228]     Train net output #0: softmax = 3.01626 (* 1 = 3.01626 loss)
I0623 12:35:28.375753  5064 solver.cpp:473] Iteration 10640, lr = 0.0001
I0623 12:35:29.396703  5064 solver.cpp:213] Iteration 10650, loss = 2.96367
I0623 12:35:29.396723  5064 solver.cpp:228]     Train net output #0: softmax = 2.96367 (* 1 = 2.96367 loss)
I0623 12:35:29.396728  5064 solver.cpp:473] Iteration 10650, lr = 0.0001
I0623 12:35:30.417006  5064 solver.cpp:213] Iteration 10660, loss = 3.11984
I0623 12:35:30.417047  5064 solver.cpp:228]     Train net output #0: softmax = 3.11984 (* 1 = 3.11984 loss)
I0623 12:35:30.417052  5064 solver.cpp:473] Iteration 10660, lr = 0.0001
I0623 12:35:31.435966  5064 solver.cpp:213] Iteration 10670, loss = 3.23867
I0623 12:35:31.435984  5064 solver.cpp:228]     Train net output #0: softmax = 3.23867 (* 1 = 3.23867 loss)
I0623 12:35:31.435991  5064 solver.cpp:473] Iteration 10670, lr = 0.0001
I0623 12:35:32.456465  5064 solver.cpp:213] Iteration 10680, loss = 3.2481
I0623 12:35:32.456485  5064 solver.cpp:228]     Train net output #0: softmax = 3.2481 (* 1 = 3.2481 loss)
I0623 12:35:32.456490  5064 solver.cpp:473] Iteration 10680, lr = 0.0001
I0623 12:35:33.477108  5064 solver.cpp:213] Iteration 10690, loss = 3.27238
I0623 12:35:33.477128  5064 solver.cpp:228]     Train net output #0: softmax = 3.27238 (* 1 = 3.27238 loss)
I0623 12:35:33.477134  5064 solver.cpp:473] Iteration 10690, lr = 0.0001
I0623 12:35:34.497586  5064 solver.cpp:213] Iteration 10700, loss = 3.22989
I0623 12:35:34.497608  5064 solver.cpp:228]     Train net output #0: softmax = 3.22989 (* 1 = 3.22989 loss)
I0623 12:35:34.497769  5064 solver.cpp:473] Iteration 10700, lr = 0.0001
I0623 12:35:35.517868  5064 solver.cpp:213] Iteration 10710, loss = 2.92869
I0623 12:35:35.517894  5064 solver.cpp:228]     Train net output #0: softmax = 2.92869 (* 1 = 2.92869 loss)
I0623 12:35:35.517899  5064 solver.cpp:473] Iteration 10710, lr = 0.0001
I0623 12:35:36.538123  5064 solver.cpp:213] Iteration 10720, loss = 3.2101
I0623 12:35:36.538143  5064 solver.cpp:228]     Train net output #0: softmax = 3.2101 (* 1 = 3.2101 loss)
I0623 12:35:36.538148  5064 solver.cpp:473] Iteration 10720, lr = 0.0001
I0623 12:35:37.558302  5064 solver.cpp:213] Iteration 10730, loss = 3.11067
I0623 12:35:37.558322  5064 solver.cpp:228]     Train net output #0: softmax = 3.11067 (* 1 = 3.11067 loss)
I0623 12:35:37.558327  5064 solver.cpp:473] Iteration 10730, lr = 0.0001
I0623 12:35:38.578583  5064 solver.cpp:213] Iteration 10740, loss = 3.35777
I0623 12:35:38.578603  5064 solver.cpp:228]     Train net output #0: softmax = 3.35777 (* 1 = 3.35777 loss)
I0623 12:35:38.578608  5064 solver.cpp:473] Iteration 10740, lr = 0.0001
I0623 12:35:39.598568  5064 solver.cpp:213] Iteration 10750, loss = 3.09707
I0623 12:35:39.598590  5064 solver.cpp:228]     Train net output #0: softmax = 3.09707 (* 1 = 3.09707 loss)
I0623 12:35:39.598711  5064 solver.cpp:473] Iteration 10750, lr = 0.0001
I0623 12:35:40.618604  5064 solver.cpp:213] Iteration 10760, loss = 3.10027
I0623 12:35:40.618623  5064 solver.cpp:228]     Train net output #0: softmax = 3.10027 (* 1 = 3.10027 loss)
I0623 12:35:40.618628  5064 solver.cpp:473] Iteration 10760, lr = 0.0001
I0623 12:35:41.638881  5064 solver.cpp:213] Iteration 10770, loss = 3.22368
I0623 12:35:41.638909  5064 solver.cpp:228]     Train net output #0: softmax = 3.22368 (* 1 = 3.22368 loss)
I0623 12:35:41.638921  5064 solver.cpp:473] Iteration 10770, lr = 0.0001
I0623 12:35:42.659818  5064 solver.cpp:213] Iteration 10780, loss = 3.22278
I0623 12:35:42.659835  5064 solver.cpp:228]     Train net output #0: softmax = 3.22278 (* 1 = 3.22278 loss)
I0623 12:35:42.659840  5064 solver.cpp:473] Iteration 10780, lr = 0.0001
I0623 12:35:43.680994  5064 solver.cpp:213] Iteration 10790, loss = 3.25652
I0623 12:35:43.681010  5064 solver.cpp:228]     Train net output #0: softmax = 3.25652 (* 1 = 3.25652 loss)
I0623 12:35:43.681015  5064 solver.cpp:473] Iteration 10790, lr = 0.0001
I0623 12:35:44.701656  5064 solver.cpp:213] Iteration 10800, loss = 3.2119
I0623 12:35:44.701830  5064 solver.cpp:228]     Train net output #0: softmax = 3.2119 (* 1 = 3.2119 loss)
I0623 12:35:44.701838  5064 solver.cpp:473] Iteration 10800, lr = 0.0001
I0623 12:35:45.722265  5064 solver.cpp:213] Iteration 10810, loss = 3.2764
I0623 12:35:45.722283  5064 solver.cpp:228]     Train net output #0: softmax = 3.2764 (* 1 = 3.2764 loss)
I0623 12:35:45.722288  5064 solver.cpp:473] Iteration 10810, lr = 0.0001
I0623 12:35:46.743139  5064 solver.cpp:213] Iteration 10820, loss = 3.4253
I0623 12:35:46.743160  5064 solver.cpp:228]     Train net output #0: softmax = 3.4253 (* 1 = 3.4253 loss)
I0623 12:35:46.743165  5064 solver.cpp:473] Iteration 10820, lr = 0.0001
I0623 12:35:47.763298  5064 solver.cpp:213] Iteration 10830, loss = 3.11061
I0623 12:35:47.763316  5064 solver.cpp:228]     Train net output #0: softmax = 3.11061 (* 1 = 3.11061 loss)
I0623 12:35:47.763321  5064 solver.cpp:473] Iteration 10830, lr = 0.0001
I0623 12:35:48.784057  5064 solver.cpp:213] Iteration 10840, loss = 3.27351
I0623 12:35:48.784080  5064 solver.cpp:228]     Train net output #0: softmax = 3.27351 (* 1 = 3.27351 loss)
I0623 12:35:48.784085  5064 solver.cpp:473] Iteration 10840, lr = 0.0001
I0623 12:35:49.804131  5064 solver.cpp:213] Iteration 10850, loss = 3.17351
I0623 12:35:49.804157  5064 solver.cpp:228]     Train net output #0: softmax = 3.17351 (* 1 = 3.17351 loss)
I0623 12:35:49.804317  5064 solver.cpp:473] Iteration 10850, lr = 0.0001
I0623 12:35:50.824594  5064 solver.cpp:213] Iteration 10860, loss = 2.80466
I0623 12:35:50.824612  5064 solver.cpp:228]     Train net output #0: softmax = 2.80466 (* 1 = 2.80466 loss)
I0623 12:35:50.824616  5064 solver.cpp:473] Iteration 10860, lr = 0.0001
I0623 12:35:51.844545  5064 solver.cpp:213] Iteration 10870, loss = 3.00125
I0623 12:35:51.844561  5064 solver.cpp:228]     Train net output #0: softmax = 3.00125 (* 1 = 3.00125 loss)
I0623 12:35:51.844566  5064 solver.cpp:473] Iteration 10870, lr = 0.0001
I0623 12:35:52.864790  5064 solver.cpp:213] Iteration 10880, loss = 3.13551
I0623 12:35:52.864809  5064 solver.cpp:228]     Train net output #0: softmax = 3.13551 (* 1 = 3.13551 loss)
I0623 12:35:52.864815  5064 solver.cpp:473] Iteration 10880, lr = 0.0001
I0623 12:35:53.885138  5064 solver.cpp:213] Iteration 10890, loss = 3.35312
I0623 12:35:53.885157  5064 solver.cpp:228]     Train net output #0: softmax = 3.35312 (* 1 = 3.35312 loss)
I0623 12:35:53.885162  5064 solver.cpp:473] Iteration 10890, lr = 0.0001
I0623 12:35:54.905390  5064 solver.cpp:213] Iteration 10900, loss = 3.20288
I0623 12:35:54.905417  5064 solver.cpp:228]     Train net output #0: softmax = 3.20288 (* 1 = 3.20288 loss)
I0623 12:35:54.905544  5064 solver.cpp:473] Iteration 10900, lr = 0.0001
I0623 12:35:55.925309  5064 solver.cpp:213] Iteration 10910, loss = 3.18633
I0623 12:35:55.925326  5064 solver.cpp:228]     Train net output #0: softmax = 3.18633 (* 1 = 3.18633 loss)
I0623 12:35:55.925331  5064 solver.cpp:473] Iteration 10910, lr = 0.0001
I0623 12:35:56.946389  5064 solver.cpp:213] Iteration 10920, loss = 2.86205
I0623 12:35:56.946409  5064 solver.cpp:228]     Train net output #0: softmax = 2.86205 (* 1 = 2.86205 loss)
I0623 12:35:56.946414  5064 solver.cpp:473] Iteration 10920, lr = 0.0001
I0623 12:35:57.966902  5064 solver.cpp:213] Iteration 10930, loss = 3.36447
I0623 12:35:57.966920  5064 solver.cpp:228]     Train net output #0: softmax = 3.36447 (* 1 = 3.36447 loss)
I0623 12:35:57.966933  5064 solver.cpp:473] Iteration 10930, lr = 0.0001
I0623 12:35:58.987190  5064 solver.cpp:213] Iteration 10940, loss = 3.01537
I0623 12:35:58.987208  5064 solver.cpp:228]     Train net output #0: softmax = 3.01537 (* 1 = 3.01537 loss)
I0623 12:35:58.987215  5064 solver.cpp:473] Iteration 10940, lr = 0.0001
I0623 12:36:00.007421  5064 solver.cpp:213] Iteration 10950, loss = 3.1232
I0623 12:36:00.007441  5064 solver.cpp:228]     Train net output #0: softmax = 3.1232 (* 1 = 3.1232 loss)
I0623 12:36:00.007570  5064 solver.cpp:473] Iteration 10950, lr = 0.0001
I0623 12:36:01.027990  5064 solver.cpp:213] Iteration 10960, loss = 3.1505
I0623 12:36:01.028023  5064 solver.cpp:228]     Train net output #0: softmax = 3.1505 (* 1 = 3.1505 loss)
I0623 12:36:01.028028  5064 solver.cpp:473] Iteration 10960, lr = 0.0001
I0623 12:36:02.048835  5064 solver.cpp:213] Iteration 10970, loss = 3.15732
I0623 12:36:02.048856  5064 solver.cpp:228]     Train net output #0: softmax = 3.15732 (* 1 = 3.15732 loss)
I0623 12:36:02.048861  5064 solver.cpp:473] Iteration 10970, lr = 0.0001
I0623 12:36:03.069368  5064 solver.cpp:213] Iteration 10980, loss = 3.38894
I0623 12:36:03.069388  5064 solver.cpp:228]     Train net output #0: softmax = 3.38894 (* 1 = 3.38894 loss)
I0623 12:36:03.069394  5064 solver.cpp:473] Iteration 10980, lr = 0.0001
I0623 12:36:04.089392  5064 solver.cpp:213] Iteration 10990, loss = 2.96815
I0623 12:36:04.089411  5064 solver.cpp:228]     Train net output #0: softmax = 2.96815 (* 1 = 2.96815 loss)
I0623 12:36:04.089416  5064 solver.cpp:473] Iteration 10990, lr = 0.0001
I0623 12:36:05.040287  5064 solver.cpp:362] Snapshotting to snapshots/16-06-21_16h05m31s_0_11_pretrainClassification_iter_11000.caffemodel
I0623 12:36:05.041331  5064 solver.cpp:370] Snapshotting solver state to snapshots/16-06-21_16h05m31s_0_11_pretrainClassification_iter_11000.solverstate
I0623 12:36:05.041934  5064 solver.cpp:291] Iteration 11000, Testing net (#0)
I0623 12:36:05.202970  5064 solver.cpp:342]     Test net output #0: accuracy_top_1 = 0.240625
I0623 12:36:05.202986  5064 solver.cpp:342]     Test net output #1: accuracy_top_5 = 0.525
I0623 12:36:05.202992  5064 solver.cpp:342]     Test net output #2: softmax = 3.13767 (* 1 = 3.13767 loss)
I0623 12:36:05.273512  5064 solver.cpp:213] Iteration 11000, loss = 3.01443
I0623 12:36:05.273527  5064 solver.cpp:228]     Train net output #0: softmax = 3.01443 (* 1 = 3.01443 loss)
I0623 12:36:05.273532  5064 solver.cpp:473] Iteration 11000, lr = 0.0001
I0623 12:36:06.294031  5064 solver.cpp:213] Iteration 11010, loss = 3.09536
I0623 12:36:06.294050  5064 solver.cpp:228]     Train net output #0: softmax = 3.09536 (* 1 = 3.09536 loss)
I0623 12:36:06.294055  5064 solver.cpp:473] Iteration 11010, lr = 0.0001
I0623 12:36:07.314412  5064 solver.cpp:213] Iteration 11020, loss = 3.06279
I0623 12:36:07.314429  5064 solver.cpp:228]     Train net output #0: softmax = 3.06279 (* 1 = 3.06279 loss)
I0623 12:36:07.314435  5064 solver.cpp:473] Iteration 11020, lr = 0.0001
I0623 12:36:08.335147  5064 solver.cpp:213] Iteration 11030, loss = 3.14907
I0623 12:36:08.335167  5064 solver.cpp:228]     Train net output #0: softmax = 3.14907 (* 1 = 3.14907 loss)
I0623 12:36:08.335173  5064 solver.cpp:473] Iteration 11030, lr = 0.0001
I0623 12:36:09.355927  5064 solver.cpp:213] Iteration 11040, loss = 2.8969
I0623 12:36:09.355947  5064 solver.cpp:228]     Train net output #0: softmax = 2.8969 (* 1 = 2.8969 loss)
I0623 12:36:09.355952  5064 solver.cpp:473] Iteration 11040, lr = 0.0001
I0623 12:36:10.376215  5064 solver.cpp:213] Iteration 11050, loss = 3.0664
I0623 12:36:10.376236  5064 solver.cpp:228]     Train net output #0: softmax = 3.0664 (* 1 = 3.0664 loss)
I0623 12:36:10.376241  5064 solver.cpp:473] Iteration 11050, lr = 0.0001
I0623 12:36:11.396263  5064 solver.cpp:213] Iteration 11060, loss = 3.25466
I0623 12:36:11.396286  5064 solver.cpp:228]     Train net output #0: softmax = 3.25466 (* 1 = 3.25466 loss)
I0623 12:36:11.396289  5064 solver.cpp:473] Iteration 11060, lr = 0.0001
I0623 12:36:12.416616  5064 solver.cpp:213] Iteration 11070, loss = 3.36369
I0623 12:36:12.416636  5064 solver.cpp:228]     Train net output #0: softmax = 3.36369 (* 1 = 3.36369 loss)
I0623 12:36:12.416641  5064 solver.cpp:473] Iteration 11070, lr = 0.0001
I0623 12:36:13.437588  5064 solver.cpp:213] Iteration 11080, loss = 3.2688
I0623 12:36:13.437610  5064 solver.cpp:228]     Train net output #0: softmax = 3.2688 (* 1 = 3.2688 loss)
I0623 12:36:13.437615  5064 solver.cpp:473] Iteration 11080, lr = 0.0001
I0623 12:36:14.458127  5064 solver.cpp:213] Iteration 11090, loss = 3.28724
I0623 12:36:14.458148  5064 solver.cpp:228]     Train net output #0: softmax = 3.28724 (* 1 = 3.28724 loss)
I0623 12:36:14.458170  5064 solver.cpp:473] Iteration 11090, lr = 0.0001
I0623 12:36:15.478757  5064 solver.cpp:213] Iteration 11100, loss = 2.86327
I0623 12:36:15.478932  5064 solver.cpp:228]     Train net output #0: softmax = 2.86327 (* 1 = 2.86327 loss)
I0623 12:36:15.478940  5064 solver.cpp:473] Iteration 11100, lr = 0.0001
I0623 12:36:16.498651  5064 solver.cpp:213] Iteration 11110, loss = 3.24877
I0623 12:36:16.498670  5064 solver.cpp:228]     Train net output #0: softmax = 3.24877 (* 1 = 3.24877 loss)
I0623 12:36:16.498675  5064 solver.cpp:473] Iteration 11110, lr = 0.0001
I0623 12:36:17.519434  5064 solver.cpp:213] Iteration 11120, loss = 3.09958
I0623 12:36:17.519454  5064 solver.cpp:228]     Train net output #0: softmax = 3.09958 (* 1 = 3.09958 loss)
I0623 12:36:17.519459  5064 solver.cpp:473] Iteration 11120, lr = 0.0001
I0623 12:36:18.540724  5064 solver.cpp:213] Iteration 11130, loss = 2.98664
I0623 12:36:18.540747  5064 solver.cpp:228]     Train net output #0: softmax = 2.98664 (* 1 = 2.98664 loss)
I0623 12:36:18.540752  5064 solver.cpp:473] Iteration 11130, lr = 0.0001
I0623 12:36:19.561381  5064 solver.cpp:213] Iteration 11140, loss = 3.17793
I0623 12:36:19.561403  5064 solver.cpp:228]     Train net output #0: softmax = 3.17793 (* 1 = 3.17793 loss)
I0623 12:36:19.561408  5064 solver.cpp:473] Iteration 11140, lr = 0.0001
I0623 12:36:20.581681  5064 solver.cpp:213] Iteration 11150, loss = 2.90819
I0623 12:36:20.581703  5064 solver.cpp:228]     Train net output #0: softmax = 2.90819 (* 1 = 2.90819 loss)
I0623 12:36:20.581878  5064 solver.cpp:473] Iteration 11150, lr = 0.0001
I0623 12:36:21.602197  5064 solver.cpp:213] Iteration 11160, loss = 3.22365
I0623 12:36:21.602215  5064 solver.cpp:228]     Train net output #0: softmax = 3.22365 (* 1 = 3.22365 loss)
I0623 12:36:21.602219  5064 solver.cpp:473] Iteration 11160, lr = 0.0001
I0623 12:36:22.622882  5064 solver.cpp:213] Iteration 11170, loss = 2.95606
I0623 12:36:22.622901  5064 solver.cpp:228]     Train net output #0: softmax = 2.95606 (* 1 = 2.95606 loss)
I0623 12:36:22.622907  5064 solver.cpp:473] Iteration 11170, lr = 0.0001
I0623 12:36:23.642602  5064 solver.cpp:213] Iteration 11180, loss = 3.22502
I0623 12:36:23.642622  5064 solver.cpp:228]     Train net output #0: softmax = 3.22502 (* 1 = 3.22502 loss)
I0623 12:36:23.642627  5064 solver.cpp:473] Iteration 11180, lr = 0.0001
I0623 12:36:24.662891  5064 solver.cpp:213] Iteration 11190, loss = 3.05954
I0623 12:36:24.662910  5064 solver.cpp:228]     Train net output #0: softmax = 3.05954 (* 1 = 3.05954 loss)
I0623 12:36:24.662916  5064 solver.cpp:473] Iteration 11190, lr = 0.0001
I0623 12:36:25.683651  5064 solver.cpp:213] Iteration 11200, loss = 3.20141
I0623 12:36:25.683676  5064 solver.cpp:228]     Train net output #0: softmax = 3.20141 (* 1 = 3.20141 loss)
I0623 12:36:25.683800  5064 solver.cpp:473] Iteration 11200, lr = 0.0001
I0623 12:36:26.703655  5064 solver.cpp:213] Iteration 11210, loss = 3.12372
I0623 12:36:26.703672  5064 solver.cpp:228]     Train net output #0: softmax = 3.12372 (* 1 = 3.12372 loss)
I0623 12:36:26.703677  5064 solver.cpp:473] Iteration 11210, lr = 0.0001
I0623 12:36:27.723606  5064 solver.cpp:213] Iteration 11220, loss = 3.06459
I0623 12:36:27.723624  5064 solver.cpp:228]     Train net output #0: softmax = 3.06459 (* 1 = 3.06459 loss)
I0623 12:36:27.723629  5064 solver.cpp:473] Iteration 11220, lr = 0.0001
I0623 12:36:28.744422  5064 solver.cpp:213] Iteration 11230, loss = 3.22127
I0623 12:36:28.744441  5064 solver.cpp:228]     Train net output #0: softmax = 3.22127 (* 1 = 3.22127 loss)
I0623 12:36:28.744446  5064 solver.cpp:473] Iteration 11230, lr = 0.0001
I0623 12:36:29.765022  5064 solver.cpp:213] Iteration 11240, loss = 3.1545
I0623 12:36:29.765040  5064 solver.cpp:228]     Train net output #0: softmax = 3.1545 (* 1 = 3.1545 loss)
I0623 12:36:29.765045  5064 solver.cpp:473] Iteration 11240, lr = 0.0001
I0623 12:36:30.785516  5064 solver.cpp:213] Iteration 11250, loss = 3.10205
I0623 12:36:30.785537  5064 solver.cpp:228]     Train net output #0: softmax = 3.10205 (* 1 = 3.10205 loss)
I0623 12:36:30.785665  5064 solver.cpp:473] Iteration 11250, lr = 0.0001
I0623 12:36:31.805785  5064 solver.cpp:213] Iteration 11260, loss = 3.07375
I0623 12:36:31.805824  5064 solver.cpp:228]     Train net output #0: softmax = 3.07375 (* 1 = 3.07375 loss)
I0623 12:36:31.805830  5064 solver.cpp:473] Iteration 11260, lr = 0.0001
I0623 12:36:32.826491  5064 solver.cpp:213] Iteration 11270, loss = 3.25728
I0623 12:36:32.826510  5064 solver.cpp:228]     Train net output #0: softmax = 3.25728 (* 1 = 3.25728 loss)
I0623 12:36:32.826515  5064 solver.cpp:473] Iteration 11270, lr = 0.0001
I0623 12:36:33.847183  5064 solver.cpp:213] Iteration 11280, loss = 3.27065
I0623 12:36:33.847203  5064 solver.cpp:228]     Train net output #0: softmax = 3.27065 (* 1 = 3.27065 loss)
I0623 12:36:33.847208  5064 solver.cpp:473] Iteration 11280, lr = 0.0001
I0623 12:36:34.868551  5064 solver.cpp:213] Iteration 11290, loss = 3.21674
I0623 12:36:34.868572  5064 solver.cpp:228]     Train net output #0: softmax = 3.21674 (* 1 = 3.21674 loss)
I0623 12:36:34.868577  5064 solver.cpp:473] Iteration 11290, lr = 0.0001
I0623 12:36:35.888968  5064 solver.cpp:213] Iteration 11300, loss = 3.4586
I0623 12:36:35.888988  5064 solver.cpp:228]     Train net output #0: softmax = 3.4586 (* 1 = 3.4586 loss)
I0623 12:36:35.888993  5064 solver.cpp:473] Iteration 11300, lr = 0.0001
I0623 12:36:36.909323  5064 solver.cpp:213] Iteration 11310, loss = 3.06919
I0623 12:36:36.909344  5064 solver.cpp:228]     Train net output #0: softmax = 3.06919 (* 1 = 3.06919 loss)
I0623 12:36:36.909349  5064 solver.cpp:473] Iteration 11310, lr = 0.0001
I0623 12:36:37.929160  5064 solver.cpp:213] Iteration 11320, loss = 2.96055
I0623 12:36:37.929177  5064 solver.cpp:228]     Train net output #0: softmax = 2.96055 (* 1 = 2.96055 loss)
I0623 12:36:37.929183  5064 solver.cpp:473] Iteration 11320, lr = 0.0001
I0623 12:36:38.949602  5064 solver.cpp:213] Iteration 11330, loss = 3.11322
I0623 12:36:38.949623  5064 solver.cpp:228]     Train net output #0: softmax = 3.11322 (* 1 = 3.11322 loss)
I0623 12:36:38.949628  5064 solver.cpp:473] Iteration 11330, lr = 0.0001
I0623 12:36:39.969127  5064 solver.cpp:213] Iteration 11340, loss = 3.00597
I0623 12:36:39.969144  5064 solver.cpp:228]     Train net output #0: softmax = 3.00597 (* 1 = 3.00597 loss)
I0623 12:36:39.969149  5064 solver.cpp:473] Iteration 11340, lr = 0.0001
I0623 12:36:40.989989  5064 solver.cpp:213] Iteration 11350, loss = 3.06662
I0623 12:36:40.990013  5064 solver.cpp:228]     Train net output #0: softmax = 3.06662 (* 1 = 3.06662 loss)
I0623 12:36:40.990025  5064 solver.cpp:473] Iteration 11350, lr = 0.0001
I0623 12:36:42.010941  5064 solver.cpp:213] Iteration 11360, loss = 3.08612
I0623 12:36:42.010958  5064 solver.cpp:228]     Train net output #0: softmax = 3.08612 (* 1 = 3.08612 loss)
I0623 12:36:42.010963  5064 solver.cpp:473] Iteration 11360, lr = 0.0001
I0623 12:36:43.032018  5064 solver.cpp:213] Iteration 11370, loss = 3.33842
I0623 12:36:43.032038  5064 solver.cpp:228]     Train net output #0: softmax = 3.33842 (* 1 = 3.33842 loss)
I0623 12:36:43.032044  5064 solver.cpp:473] Iteration 11370, lr = 0.0001
I0623 12:36:44.051674  5064 solver.cpp:213] Iteration 11380, loss = 3.26178
I0623 12:36:44.051695  5064 solver.cpp:228]     Train net output #0: softmax = 3.26178 (* 1 = 3.26178 loss)
I0623 12:36:44.051700  5064 solver.cpp:473] Iteration 11380, lr = 0.0001
I0623 12:36:45.072860  5064 solver.cpp:213] Iteration 11390, loss = 2.96145
I0623 12:36:45.072881  5064 solver.cpp:228]     Train net output #0: softmax = 2.96145 (* 1 = 2.96145 loss)
I0623 12:36:45.072886  5064 solver.cpp:473] Iteration 11390, lr = 0.0001
I0623 12:36:46.093044  5064 solver.cpp:213] Iteration 11400, loss = 3.01959
I0623 12:36:46.093214  5064 solver.cpp:228]     Train net output #0: softmax = 3.01959 (* 1 = 3.01959 loss)
I0623 12:36:46.093221  5064 solver.cpp:473] Iteration 11400, lr = 0.0001
I0623 12:36:47.112697  5064 solver.cpp:213] Iteration 11410, loss = 2.90017
I0623 12:36:47.112720  5064 solver.cpp:228]     Train net output #0: softmax = 2.90017 (* 1 = 2.90017 loss)
I0623 12:36:47.112727  5064 solver.cpp:473] Iteration 11410, lr = 0.0001
I0623 12:36:48.133419  5064 solver.cpp:213] Iteration 11420, loss = 3.03234
I0623 12:36:48.133437  5064 solver.cpp:228]     Train net output #0: softmax = 3.03234 (* 1 = 3.03234 loss)
I0623 12:36:48.133442  5064 solver.cpp:473] Iteration 11420, lr = 0.0001
I0623 12:36:49.154038  5064 solver.cpp:213] Iteration 11430, loss = 3.00621
I0623 12:36:49.154055  5064 solver.cpp:228]     Train net output #0: softmax = 3.00621 (* 1 = 3.00621 loss)
I0623 12:36:49.154059  5064 solver.cpp:473] Iteration 11430, lr = 0.0001
I0623 12:36:50.175032  5064 solver.cpp:213] Iteration 11440, loss = 3.23435
I0623 12:36:50.175048  5064 solver.cpp:228]     Train net output #0: softmax = 3.23435 (* 1 = 3.23435 loss)
I0623 12:36:50.175053  5064 solver.cpp:473] Iteration 11440, lr = 0.0001
I0623 12:36:51.196321  5064 solver.cpp:213] Iteration 11450, loss = 3.29904
I0623 12:36:51.196342  5064 solver.cpp:228]     Train net output #0: softmax = 3.29904 (* 1 = 3.29904 loss)
I0623 12:36:51.196487  5064 solver.cpp:473] Iteration 11450, lr = 0.0001
I0623 12:36:52.217088  5064 solver.cpp:213] Iteration 11460, loss = 3.26307
I0623 12:36:52.217105  5064 solver.cpp:228]     Train net output #0: softmax = 3.26307 (* 1 = 3.26307 loss)
I0623 12:36:52.217110  5064 solver.cpp:473] Iteration 11460, lr = 0.0001
I0623 12:36:53.237604  5064 solver.cpp:213] Iteration 11470, loss = 2.96646
I0623 12:36:53.237622  5064 solver.cpp:228]     Train net output #0: softmax = 2.96646 (* 1 = 2.96646 loss)
I0623 12:36:53.237627  5064 solver.cpp:473] Iteration 11470, lr = 0.0001
I0623 12:36:54.258461  5064 solver.cpp:213] Iteration 11480, loss = 3.30206
I0623 12:36:54.258486  5064 solver.cpp:228]     Train net output #0: softmax = 3.30206 (* 1 = 3.30206 loss)
I0623 12:36:54.258491  5064 solver.cpp:473] Iteration 11480, lr = 0.0001
I0623 12:36:55.279047  5064 solver.cpp:213] Iteration 11490, loss = 2.98661
I0623 12:36:55.279067  5064 solver.cpp:228]     Train net output #0: softmax = 2.98661 (* 1 = 2.98661 loss)
I0623 12:36:55.279072  5064 solver.cpp:473] Iteration 11490, lr = 0.0001
I0623 12:36:56.299543  5064 solver.cpp:213] Iteration 11500, loss = 3.22311
I0623 12:36:56.299567  5064 solver.cpp:228]     Train net output #0: softmax = 3.22311 (* 1 = 3.22311 loss)
I0623 12:36:56.299691  5064 solver.cpp:473] Iteration 11500, lr = 0.0001
I0623 12:36:57.319983  5064 solver.cpp:213] Iteration 11510, loss = 3.16893
I0623 12:36:57.320003  5064 solver.cpp:228]     Train net output #0: softmax = 3.16893 (* 1 = 3.16893 loss)
I0623 12:36:57.320008  5064 solver.cpp:473] Iteration 11510, lr = 0.0001
I0623 12:36:58.340728  5064 solver.cpp:213] Iteration 11520, loss = 3.21905
I0623 12:36:58.340751  5064 solver.cpp:228]     Train net output #0: softmax = 3.21905 (* 1 = 3.21905 loss)
I0623 12:36:58.340757  5064 solver.cpp:473] Iteration 11520, lr = 0.0001
I0623 12:36:59.360859  5064 solver.cpp:213] Iteration 11530, loss = 3.09421
I0623 12:36:59.360879  5064 solver.cpp:228]     Train net output #0: softmax = 3.09421 (* 1 = 3.09421 loss)
I0623 12:36:59.360884  5064 solver.cpp:473] Iteration 11530, lr = 0.0001
I0623 12:37:00.381489  5064 solver.cpp:213] Iteration 11540, loss = 2.98324
I0623 12:37:00.381508  5064 solver.cpp:228]     Train net output #0: softmax = 2.98324 (* 1 = 2.98324 loss)
I0623 12:37:00.381515  5064 solver.cpp:473] Iteration 11540, lr = 0.0001
I0623 12:37:01.401509  5064 solver.cpp:213] Iteration 11550, loss = 3.15774
I0623 12:37:01.401530  5064 solver.cpp:228]     Train net output #0: softmax = 3.15774 (* 1 = 3.15774 loss)
I0623 12:37:01.401535  5064 solver.cpp:473] Iteration 11550, lr = 0.0001
I0623 12:37:02.422029  5064 solver.cpp:213] Iteration 11560, loss = 2.83084
I0623 12:37:02.422065  5064 solver.cpp:228]     Train net output #0: softmax = 2.83084 (* 1 = 2.83084 loss)
I0623 12:37:02.422071  5064 solver.cpp:473] Iteration 11560, lr = 0.0001
I0623 12:37:03.442631  5064 solver.cpp:213] Iteration 11570, loss = 3.13754
I0623 12:37:03.442649  5064 solver.cpp:228]     Train net output #0: softmax = 3.13754 (* 1 = 3.13754 loss)
I0623 12:37:03.442656  5064 solver.cpp:473] Iteration 11570, lr = 0.0001
I0623 12:37:04.463368  5064 solver.cpp:213] Iteration 11580, loss = 3.23051
I0623 12:37:04.463387  5064 solver.cpp:228]     Train net output #0: softmax = 3.23051 (* 1 = 3.23051 loss)
I0623 12:37:04.463392  5064 solver.cpp:473] Iteration 11580, lr = 0.0001
I0623 12:37:05.484230  5064 solver.cpp:213] Iteration 11590, loss = 3.24542
I0623 12:37:05.484251  5064 solver.cpp:228]     Train net output #0: softmax = 3.24542 (* 1 = 3.24542 loss)
I0623 12:37:05.484256  5064 solver.cpp:473] Iteration 11590, lr = 0.0001
I0623 12:37:06.504869  5064 solver.cpp:213] Iteration 11600, loss = 3.15024
I0623 12:37:06.504895  5064 solver.cpp:228]     Train net output #0: softmax = 3.15024 (* 1 = 3.15024 loss)
I0623 12:37:06.505038  5064 solver.cpp:473] Iteration 11600, lr = 0.0001
I0623 12:37:07.525225  5064 solver.cpp:213] Iteration 11610, loss = 3.11556
I0623 12:37:07.525243  5064 solver.cpp:228]     Train net output #0: softmax = 3.11556 (* 1 = 3.11556 loss)
I0623 12:37:07.525249  5064 solver.cpp:473] Iteration 11610, lr = 0.0001
I0623 12:37:08.545949  5064 solver.cpp:213] Iteration 11620, loss = 2.80861
I0623 12:37:08.545971  5064 solver.cpp:228]     Train net output #0: softmax = 2.80861 (* 1 = 2.80861 loss)
I0623 12:37:08.545976  5064 solver.cpp:473] Iteration 11620, lr = 0.0001
I0623 12:37:09.565492  5064 solver.cpp:213] Iteration 11630, loss = 2.96195
I0623 12:37:09.565516  5064 solver.cpp:228]     Train net output #0: softmax = 2.96195 (* 1 = 2.96195 loss)
I0623 12:37:09.565522  5064 solver.cpp:473] Iteration 11630, lr = 0.0001
I0623 12:37:10.586072  5064 solver.cpp:213] Iteration 11640, loss = 3.11597
I0623 12:37:10.586092  5064 solver.cpp:228]     Train net output #0: softmax = 3.11597 (* 1 = 3.11597 loss)
I0623 12:37:10.586097  5064 solver.cpp:473] Iteration 11640, lr = 0.0001
I0623 12:37:11.606485  5064 solver.cpp:213] Iteration 11650, loss = 2.9939
I0623 12:37:11.606508  5064 solver.cpp:228]     Train net output #0: softmax = 2.9939 (* 1 = 2.9939 loss)
I0623 12:37:11.606623  5064 solver.cpp:473] Iteration 11650, lr = 0.0001
I0623 12:37:12.627346  5064 solver.cpp:213] Iteration 11660, loss = 3.13657
I0623 12:37:12.627365  5064 solver.cpp:228]     Train net output #0: softmax = 3.13657 (* 1 = 3.13657 loss)
I0623 12:37:12.627372  5064 solver.cpp:473] Iteration 11660, lr = 0.0001
I0623 12:37:13.647657  5064 solver.cpp:213] Iteration 11670, loss = 3.19699
I0623 12:37:13.647675  5064 solver.cpp:228]     Train net output #0: softmax = 3.19699 (* 1 = 3.19699 loss)
I0623 12:37:13.647681  5064 solver.cpp:473] Iteration 11670, lr = 0.0001
I0623 12:37:14.668474  5064 solver.cpp:213] Iteration 11680, loss = 3.09028
I0623 12:37:14.668496  5064 solver.cpp:228]     Train net output #0: softmax = 3.09028 (* 1 = 3.09028 loss)
I0623 12:37:14.668501  5064 solver.cpp:473] Iteration 11680, lr = 0.0001
I0623 12:37:15.689203  5064 solver.cpp:213] Iteration 11690, loss = 3.11664
I0623 12:37:15.689221  5064 solver.cpp:228]     Train net output #0: softmax = 3.11664 (* 1 = 3.11664 loss)
I0623 12:37:15.689226  5064 solver.cpp:473] Iteration 11690, lr = 0.0001
I0623 12:37:16.709686  5064 solver.cpp:213] Iteration 11700, loss = 3.09238
I0623 12:37:16.709856  5064 solver.cpp:228]     Train net output #0: softmax = 3.09238 (* 1 = 3.09238 loss)
I0623 12:37:16.709869  5064 solver.cpp:473] Iteration 11700, lr = 0.0001
I0623 12:37:17.730293  5064 solver.cpp:213] Iteration 11710, loss = 2.99931
I0623 12:37:17.730312  5064 solver.cpp:228]     Train net output #0: softmax = 2.99931 (* 1 = 2.99931 loss)
I0623 12:37:17.730317  5064 solver.cpp:473] Iteration 11710, lr = 0.0001
I0623 12:37:18.750954  5064 solver.cpp:213] Iteration 11720, loss = 3.19826
I0623 12:37:18.750977  5064 solver.cpp:228]     Train net output #0: softmax = 3.19826 (* 1 = 3.19826 loss)
I0623 12:37:18.750982  5064 solver.cpp:473] Iteration 11720, lr = 0.0001
I0623 12:37:19.771478  5064 solver.cpp:213] Iteration 11730, loss = 3.01034
I0623 12:37:19.771497  5064 solver.cpp:228]     Train net output #0: softmax = 3.01034 (* 1 = 3.01034 loss)
I0623 12:37:19.771502  5064 solver.cpp:473] Iteration 11730, lr = 0.0001
I0623 12:37:20.791828  5064 solver.cpp:213] Iteration 11740, loss = 2.8694
I0623 12:37:20.791851  5064 solver.cpp:228]     Train net output #0: softmax = 2.8694 (* 1 = 2.8694 loss)
I0623 12:37:20.791857  5064 solver.cpp:473] Iteration 11740, lr = 0.0001
I0623 12:37:21.812465  5064 solver.cpp:213] Iteration 11750, loss = 3.12118
I0623 12:37:21.812487  5064 solver.cpp:228]     Train net output #0: softmax = 3.12118 (* 1 = 3.12118 loss)
I0623 12:37:21.812610  5064 solver.cpp:473] Iteration 11750, lr = 0.0001
I0623 12:37:22.833477  5064 solver.cpp:213] Iteration 11760, loss = 3.01081
I0623 12:37:22.833498  5064 solver.cpp:228]     Train net output #0: softmax = 3.01081 (* 1 = 3.01081 loss)
I0623 12:37:22.833503  5064 solver.cpp:473] Iteration 11760, lr = 0.0001
I0623 12:37:23.853740  5064 solver.cpp:213] Iteration 11770, loss = 3.30089
I0623 12:37:23.853760  5064 solver.cpp:228]     Train net output #0: softmax = 3.30089 (* 1 = 3.30089 loss)
I0623 12:37:23.853765  5064 solver.cpp:473] Iteration 11770, lr = 0.0001
I0623 12:37:24.874202  5064 solver.cpp:213] Iteration 11780, loss = 3.13802
I0623 12:37:24.874222  5064 solver.cpp:228]     Train net output #0: softmax = 3.13802 (* 1 = 3.13802 loss)
I0623 12:37:24.874228  5064 solver.cpp:473] Iteration 11780, lr = 0.0001
I0623 12:37:25.894760  5064 solver.cpp:213] Iteration 11790, loss = 2.97924
I0623 12:37:25.894779  5064 solver.cpp:228]     Train net output #0: softmax = 2.97924 (* 1 = 2.97924 loss)
I0623 12:37:25.894784  5064 solver.cpp:473] Iteration 11790, lr = 0.0001
I0623 12:37:26.915916  5064 solver.cpp:213] Iteration 11800, loss = 2.69385
I0623 12:37:26.915952  5064 solver.cpp:228]     Train net output #0: softmax = 2.69385 (* 1 = 2.69385 loss)
I0623 12:37:26.916107  5064 solver.cpp:473] Iteration 11800, lr = 0.0001
I0623 12:37:27.935761  5064 solver.cpp:213] Iteration 11810, loss = 3.05409
I0623 12:37:27.935782  5064 solver.cpp:228]     Train net output #0: softmax = 3.05409 (* 1 = 3.05409 loss)
I0623 12:37:27.935787  5064 solver.cpp:473] Iteration 11810, lr = 0.0001
I0623 12:37:28.956923  5064 solver.cpp:213] Iteration 11820, loss = 3.03195
I0623 12:37:28.956944  5064 solver.cpp:228]     Train net output #0: softmax = 3.03195 (* 1 = 3.03195 loss)
I0623 12:37:28.956950  5064 solver.cpp:473] Iteration 11820, lr = 0.0001
I0623 12:37:29.978091  5064 solver.cpp:213] Iteration 11830, loss = 3.00106
I0623 12:37:29.978111  5064 solver.cpp:228]     Train net output #0: softmax = 3.00106 (* 1 = 3.00106 loss)
I0623 12:37:29.978116  5064 solver.cpp:473] Iteration 11830, lr = 0.0001
I0623 12:37:30.999238  5064 solver.cpp:213] Iteration 11840, loss = 3.10945
I0623 12:37:30.999258  5064 solver.cpp:228]     Train net output #0: softmax = 3.10945 (* 1 = 3.10945 loss)
I0623 12:37:30.999264  5064 solver.cpp:473] Iteration 11840, lr = 0.0001
I0623 12:37:32.019675  5064 solver.cpp:213] Iteration 11850, loss = 3.00985
I0623 12:37:32.019704  5064 solver.cpp:228]     Train net output #0: softmax = 3.00985 (* 1 = 3.00985 loss)
I0623 12:37:32.019827  5064 solver.cpp:473] Iteration 11850, lr = 0.0001
I0623 12:37:33.040796  5064 solver.cpp:213] Iteration 11860, loss = 3.08109
I0623 12:37:33.040833  5064 solver.cpp:228]     Train net output #0: softmax = 3.08109 (* 1 = 3.08109 loss)
I0623 12:37:33.040841  5064 solver.cpp:473] Iteration 11860, lr = 0.0001
I0623 12:37:34.061507  5064 solver.cpp:213] Iteration 11870, loss = 3.18862
I0623 12:37:34.061527  5064 solver.cpp:228]     Train net output #0: softmax = 3.18862 (* 1 = 3.18862 loss)
I0623 12:37:34.061532  5064 solver.cpp:473] Iteration 11870, lr = 0.0001
I0623 12:37:35.082116  5064 solver.cpp:213] Iteration 11880, loss = 2.64315
I0623 12:37:35.082141  5064 solver.cpp:228]     Train net output #0: softmax = 2.64315 (* 1 = 2.64315 loss)
I0623 12:37:35.082149  5064 solver.cpp:473] Iteration 11880, lr = 0.0001
I0623 12:37:36.102530  5064 solver.cpp:213] Iteration 11890, loss = 3.0583
I0623 12:37:36.102550  5064 solver.cpp:228]     Train net output #0: softmax = 3.0583 (* 1 = 3.0583 loss)
I0623 12:37:36.102555  5064 solver.cpp:473] Iteration 11890, lr = 0.0001
I0623 12:37:37.123265  5064 solver.cpp:213] Iteration 11900, loss = 3.01751
I0623 12:37:37.123291  5064 solver.cpp:228]     Train net output #0: softmax = 3.01751 (* 1 = 3.01751 loss)
I0623 12:37:37.123430  5064 solver.cpp:473] Iteration 11900, lr = 0.0001
I0623 12:37:38.144096  5064 solver.cpp:213] Iteration 11910, loss = 3.33252
I0623 12:37:38.144117  5064 solver.cpp:228]     Train net output #0: softmax = 3.33252 (* 1 = 3.33252 loss)
I0623 12:37:38.144124  5064 solver.cpp:473] Iteration 11910, lr = 0.0001
I0623 12:37:39.164919  5064 solver.cpp:213] Iteration 11920, loss = 3.27057
I0623 12:37:39.164943  5064 solver.cpp:228]     Train net output #0: softmax = 3.27057 (* 1 = 3.27057 loss)
I0623 12:37:39.164948  5064 solver.cpp:473] Iteration 11920, lr = 0.0001
I0623 12:37:40.186458  5064 solver.cpp:213] Iteration 11930, loss = 3.14322
I0623 12:37:40.186480  5064 solver.cpp:228]     Train net output #0: softmax = 3.14322 (* 1 = 3.14322 loss)
I0623 12:37:40.186486  5064 solver.cpp:473] Iteration 11930, lr = 0.0001
I0623 12:37:41.207473  5064 solver.cpp:213] Iteration 11940, loss = 3.0288
I0623 12:37:41.207492  5064 solver.cpp:228]     Train net output #0: softmax = 3.0288 (* 1 = 3.0288 loss)
I0623 12:37:41.207497  5064 solver.cpp:473] Iteration 11940, lr = 0.0001
I0623 12:37:42.228746  5064 solver.cpp:213] Iteration 11950, loss = 2.99153
I0623 12:37:42.228772  5064 solver.cpp:228]     Train net output #0: softmax = 2.99153 (* 1 = 2.99153 loss)
I0623 12:37:42.228895  5064 solver.cpp:473] Iteration 11950, lr = 0.0001
I0623 12:37:43.249399  5064 solver.cpp:213] Iteration 11960, loss = 3.19278
I0623 12:37:43.249419  5064 solver.cpp:228]     Train net output #0: softmax = 3.19278 (* 1 = 3.19278 loss)
I0623 12:37:43.249424  5064 solver.cpp:473] Iteration 11960, lr = 0.0001
I0623 12:37:44.270478  5064 solver.cpp:213] Iteration 11970, loss = 2.93684
I0623 12:37:44.270509  5064 solver.cpp:228]     Train net output #0: softmax = 2.93684 (* 1 = 2.93684 loss)
I0623 12:37:44.270514  5064 solver.cpp:473] Iteration 11970, lr = 0.0001
I0623 12:37:45.291476  5064 solver.cpp:213] Iteration 11980, loss = 3.17524
I0623 12:37:45.291501  5064 solver.cpp:228]     Train net output #0: softmax = 3.17524 (* 1 = 3.17524 loss)
I0623 12:37:45.291506  5064 solver.cpp:473] Iteration 11980, lr = 0.0001
I0623 12:37:46.312396  5064 solver.cpp:213] Iteration 11990, loss = 3.08722
I0623 12:37:46.312414  5064 solver.cpp:228]     Train net output #0: softmax = 3.08722 (* 1 = 3.08722 loss)
I0623 12:37:46.312420  5064 solver.cpp:473] Iteration 11990, lr = 0.0001
I0623 12:37:47.261597  5064 solver.cpp:362] Snapshotting to snapshots/16-06-21_16h05m31s_0_11_pretrainClassification_iter_12000.caffemodel
I0623 12:37:47.262796  5064 solver.cpp:370] Snapshotting solver state to snapshots/16-06-21_16h05m31s_0_11_pretrainClassification_iter_12000.solverstate
I0623 12:37:47.263389  5064 solver.cpp:291] Iteration 12000, Testing net (#0)
I0623 12:37:47.424608  5064 solver.cpp:342]     Test net output #0: accuracy_top_1 = 0.240625
I0623 12:37:47.424625  5064 solver.cpp:342]     Test net output #1: accuracy_top_5 = 0.504687
I0623 12:37:47.424633  5064 solver.cpp:342]     Test net output #2: softmax = 3.22326 (* 1 = 3.22326 loss)
I0623 12:37:47.495323  5064 solver.cpp:213] Iteration 12000, loss = 2.69976
I0623 12:37:47.495338  5064 solver.cpp:228]     Train net output #0: softmax = 2.69976 (* 1 = 2.69976 loss)
I0623 12:37:47.495343  5064 solver.cpp:473] Iteration 12000, lr = 0.0001
I0623 12:37:48.515439  5064 solver.cpp:213] Iteration 12010, loss = 2.93582
I0623 12:37:48.515455  5064 solver.cpp:228]     Train net output #0: softmax = 2.93582 (* 1 = 2.93582 loss)
I0623 12:37:48.515460  5064 solver.cpp:473] Iteration 12010, lr = 0.0001
I0623 12:37:49.535619  5064 solver.cpp:213] Iteration 12020, loss = 3.2408
I0623 12:37:49.535639  5064 solver.cpp:228]     Train net output #0: softmax = 3.2408 (* 1 = 3.2408 loss)
I0623 12:37:49.535643  5064 solver.cpp:473] Iteration 12020, lr = 0.0001
I0623 12:37:50.556463  5064 solver.cpp:213] Iteration 12030, loss = 3.11946
I0623 12:37:50.556483  5064 solver.cpp:228]     Train net output #0: softmax = 3.11946 (* 1 = 3.11946 loss)
I0623 12:37:50.556488  5064 solver.cpp:473] Iteration 12030, lr = 0.0001
I0623 12:37:51.576184  5064 solver.cpp:213] Iteration 12040, loss = 2.88291
I0623 12:37:51.576201  5064 solver.cpp:228]     Train net output #0: softmax = 2.88291 (* 1 = 2.88291 loss)
I0623 12:37:51.576206  5064 solver.cpp:473] Iteration 12040, lr = 0.0001
I0623 12:37:52.597055  5064 solver.cpp:213] Iteration 12050, loss = 3.1119
I0623 12:37:52.597081  5064 solver.cpp:228]     Train net output #0: softmax = 3.1119 (* 1 = 3.1119 loss)
I0623 12:37:52.597087  5064 solver.cpp:473] Iteration 12050, lr = 0.0001
I0623 12:37:53.617854  5064 solver.cpp:213] Iteration 12060, loss = 3.0622
I0623 12:37:53.617874  5064 solver.cpp:228]     Train net output #0: softmax = 3.0622 (* 1 = 3.0622 loss)
I0623 12:37:53.617879  5064 solver.cpp:473] Iteration 12060, lr = 0.0001
I0623 12:37:54.638427  5064 solver.cpp:213] Iteration 12070, loss = 2.97173
I0623 12:37:54.638447  5064 solver.cpp:228]     Train net output #0: softmax = 2.97173 (* 1 = 2.97173 loss)
I0623 12:37:54.638451  5064 solver.cpp:473] Iteration 12070, lr = 0.0001
I0623 12:37:55.659565  5064 solver.cpp:213] Iteration 12080, loss = 3.05336
I0623 12:37:55.659584  5064 solver.cpp:228]     Train net output #0: softmax = 3.05336 (* 1 = 3.05336 loss)
I0623 12:37:55.659590  5064 solver.cpp:473] Iteration 12080, lr = 0.0001
I0623 12:37:56.679682  5064 solver.cpp:213] Iteration 12090, loss = 3.18108
I0623 12:37:56.679699  5064 solver.cpp:228]     Train net output #0: softmax = 3.18108 (* 1 = 3.18108 loss)
I0623 12:37:56.679704  5064 solver.cpp:473] Iteration 12090, lr = 0.0001
I0623 12:37:57.700351  5064 solver.cpp:213] Iteration 12100, loss = 3.26271
I0623 12:37:57.700382  5064 solver.cpp:228]     Train net output #0: softmax = 3.26271 (* 1 = 3.26271 loss)
I0623 12:37:57.700531  5064 solver.cpp:473] Iteration 12100, lr = 0.0001
I0623 12:37:58.720816  5064 solver.cpp:213] Iteration 12110, loss = 3.27701
I0623 12:37:58.720836  5064 solver.cpp:228]     Train net output #0: softmax = 3.27701 (* 1 = 3.27701 loss)
I0623 12:37:58.720842  5064 solver.cpp:473] Iteration 12110, lr = 0.0001
I0623 12:37:59.740324  5064 solver.cpp:213] Iteration 12120, loss = 2.97874
I0623 12:37:59.740344  5064 solver.cpp:228]     Train net output #0: softmax = 2.97874 (* 1 = 2.97874 loss)
I0623 12:37:59.740348  5064 solver.cpp:473] Iteration 12120, lr = 0.0001
I0623 12:38:00.761589  5064 solver.cpp:213] Iteration 12130, loss = 2.94608
I0623 12:38:00.761607  5064 solver.cpp:228]     Train net output #0: softmax = 2.94608 (* 1 = 2.94608 loss)
I0623 12:38:00.761613  5064 solver.cpp:473] Iteration 12130, lr = 0.0001
I0623 12:38:01.782093  5064 solver.cpp:213] Iteration 12140, loss = 3.36973
I0623 12:38:01.782114  5064 solver.cpp:228]     Train net output #0: softmax = 3.36973 (* 1 = 3.36973 loss)
I0623 12:38:01.782119  5064 solver.cpp:473] Iteration 12140, lr = 0.0001
I0623 12:38:02.803105  5064 solver.cpp:213] Iteration 12150, loss = 2.93596
I0623 12:38:02.803134  5064 solver.cpp:228]     Train net output #0: softmax = 2.93596 (* 1 = 2.93596 loss)
I0623 12:38:02.803282  5064 solver.cpp:473] Iteration 12150, lr = 0.0001
I0623 12:38:03.823626  5064 solver.cpp:213] Iteration 12160, loss = 3.0893
I0623 12:38:03.823647  5064 solver.cpp:228]     Train net output #0: softmax = 3.0893 (* 1 = 3.0893 loss)
I0623 12:38:03.823652  5064 solver.cpp:473] Iteration 12160, lr = 0.0001
I0623 12:38:04.844920  5064 solver.cpp:213] Iteration 12170, loss = 3.19917
I0623 12:38:04.844941  5064 solver.cpp:228]     Train net output #0: softmax = 3.19917 (* 1 = 3.19917 loss)
I0623 12:38:04.844947  5064 solver.cpp:473] Iteration 12170, lr = 0.0001
I0623 12:38:05.865746  5064 solver.cpp:213] Iteration 12180, loss = 3.04626
I0623 12:38:05.865768  5064 solver.cpp:228]     Train net output #0: softmax = 3.04626 (* 1 = 3.04626 loss)
I0623 12:38:05.865773  5064 solver.cpp:473] Iteration 12180, lr = 0.0001
I0623 12:38:06.886586  5064 solver.cpp:213] Iteration 12190, loss = 3.07158
I0623 12:38:06.886607  5064 solver.cpp:228]     Train net output #0: softmax = 3.07158 (* 1 = 3.07158 loss)
I0623 12:38:06.886613  5064 solver.cpp:473] Iteration 12190, lr = 0.0001
I0623 12:38:07.908104  5064 solver.cpp:213] Iteration 12200, loss = 3.07133
I0623 12:38:07.908136  5064 solver.cpp:228]     Train net output #0: softmax = 3.07133 (* 1 = 3.07133 loss)
I0623 12:38:07.908262  5064 solver.cpp:473] Iteration 12200, lr = 0.0001
I0623 12:38:08.928369  5064 solver.cpp:213] Iteration 12210, loss = 3.07414
I0623 12:38:08.928390  5064 solver.cpp:228]     Train net output #0: softmax = 3.07414 (* 1 = 3.07414 loss)
I0623 12:38:08.928395  5064 solver.cpp:473] Iteration 12210, lr = 0.0001
I0623 12:38:09.949018  5064 solver.cpp:213] Iteration 12220, loss = 2.93692
I0623 12:38:09.949036  5064 solver.cpp:228]     Train net output #0: softmax = 2.93692 (* 1 = 2.93692 loss)
I0623 12:38:09.949041  5064 solver.cpp:473] Iteration 12220, lr = 0.0001
I0623 12:38:10.969455  5064 solver.cpp:213] Iteration 12230, loss = 3.02659
I0623 12:38:10.969473  5064 solver.cpp:228]     Train net output #0: softmax = 3.02659 (* 1 = 3.02659 loss)
I0623 12:38:10.969478  5064 solver.cpp:473] Iteration 12230, lr = 0.0001
I0623 12:38:11.989137  5064 solver.cpp:213] Iteration 12240, loss = 2.76355
I0623 12:38:11.989157  5064 solver.cpp:228]     Train net output #0: softmax = 2.76355 (* 1 = 2.76355 loss)
I0623 12:38:11.989163  5064 solver.cpp:473] Iteration 12240, lr = 0.0001
I0623 12:38:13.009726  5064 solver.cpp:213] Iteration 12250, loss = 3.03817
I0623 12:38:13.009749  5064 solver.cpp:228]     Train net output #0: softmax = 3.03817 (* 1 = 3.03817 loss)
I0623 12:38:13.009882  5064 solver.cpp:473] Iteration 12250, lr = 0.0001
I0623 12:38:14.031518  5064 solver.cpp:213] Iteration 12260, loss = 3.18852
I0623 12:38:14.031538  5064 solver.cpp:228]     Train net output #0: softmax = 3.18852 (* 1 = 3.18852 loss)
I0623 12:38:14.031543  5064 solver.cpp:473] Iteration 12260, lr = 0.0001
I0623 12:38:15.052239  5064 solver.cpp:213] Iteration 12270, loss = 3.04284
I0623 12:38:15.052261  5064 solver.cpp:228]     Train net output #0: softmax = 3.04284 (* 1 = 3.04284 loss)
I0623 12:38:15.052266  5064 solver.cpp:473] Iteration 12270, lr = 0.0001
I0623 12:38:16.072247  5064 solver.cpp:213] Iteration 12280, loss = 3.11361
I0623 12:38:16.072266  5064 solver.cpp:228]     Train net output #0: softmax = 3.11361 (* 1 = 3.11361 loss)
I0623 12:38:16.072271  5064 solver.cpp:473] Iteration 12280, lr = 0.0001
I0623 12:38:17.093513  5064 solver.cpp:213] Iteration 12290, loss = 3.15403
I0623 12:38:17.093534  5064 solver.cpp:228]     Train net output #0: softmax = 3.15403 (* 1 = 3.15403 loss)
I0623 12:38:17.093539  5064 solver.cpp:473] Iteration 12290, lr = 0.0001
I0623 12:38:18.114713  5064 solver.cpp:213] Iteration 12300, loss = 3.20376
I0623 12:38:18.114900  5064 solver.cpp:228]     Train net output #0: softmax = 3.20376 (* 1 = 3.20376 loss)
I0623 12:38:18.114908  5064 solver.cpp:473] Iteration 12300, lr = 0.0001
I0623 12:38:19.135675  5064 solver.cpp:213] Iteration 12310, loss = 3.15779
I0623 12:38:19.135695  5064 solver.cpp:228]     Train net output #0: softmax = 3.15779 (* 1 = 3.15779 loss)
I0623 12:38:19.135700  5064 solver.cpp:473] Iteration 12310, lr = 0.0001
I0623 12:38:20.156136  5064 solver.cpp:213] Iteration 12320, loss = 2.91539
I0623 12:38:20.156157  5064 solver.cpp:228]     Train net output #0: softmax = 2.91539 (* 1 = 2.91539 loss)
I0623 12:38:20.156162  5064 solver.cpp:473] Iteration 12320, lr = 0.0001
I0623 12:38:21.177000  5064 solver.cpp:213] Iteration 12330, loss = 2.99286
I0623 12:38:21.177021  5064 solver.cpp:228]     Train net output #0: softmax = 2.99286 (* 1 = 2.99286 loss)
I0623 12:38:21.177026  5064 solver.cpp:473] Iteration 12330, lr = 0.0001
I0623 12:38:22.197345  5064 solver.cpp:213] Iteration 12340, loss = 2.9077
I0623 12:38:22.197363  5064 solver.cpp:228]     Train net output #0: softmax = 2.9077 (* 1 = 2.9077 loss)
I0623 12:38:22.197368  5064 solver.cpp:473] Iteration 12340, lr = 0.0001
I0623 12:38:23.217735  5064 solver.cpp:213] Iteration 12350, loss = 3.07027
I0623 12:38:23.217759  5064 solver.cpp:228]     Train net output #0: softmax = 3.07027 (* 1 = 3.07027 loss)
I0623 12:38:23.217910  5064 solver.cpp:473] Iteration 12350, lr = 0.0001
I0623 12:38:24.238477  5064 solver.cpp:213] Iteration 12360, loss = 3.0198
I0623 12:38:24.238497  5064 solver.cpp:228]     Train net output #0: softmax = 3.0198 (* 1 = 3.0198 loss)
I0623 12:38:24.238502  5064 solver.cpp:473] Iteration 12360, lr = 0.0001
I0623 12:38:25.259377  5064 solver.cpp:213] Iteration 12370, loss = 3.22628
I0623 12:38:25.259397  5064 solver.cpp:228]     Train net output #0: softmax = 3.22628 (* 1 = 3.22628 loss)
I0623 12:38:25.259402  5064 solver.cpp:473] Iteration 12370, lr = 0.0001
I0623 12:38:26.279997  5064 solver.cpp:213] Iteration 12380, loss = 3.07421
I0623 12:38:26.280017  5064 solver.cpp:228]     Train net output #0: softmax = 3.07421 (* 1 = 3.07421 loss)
I0623 12:38:26.280022  5064 solver.cpp:473] Iteration 12380, lr = 0.0001
I0623 12:38:27.300364  5064 solver.cpp:213] Iteration 12390, loss = 2.94035
I0623 12:38:27.300387  5064 solver.cpp:228]     Train net output #0: softmax = 2.94035 (* 1 = 2.94035 loss)
I0623 12:38:27.300392  5064 solver.cpp:473] Iteration 12390, lr = 0.0001
I0623 12:38:28.321216  5064 solver.cpp:213] Iteration 12400, loss = 3.16626
I0623 12:38:28.321238  5064 solver.cpp:228]     Train net output #0: softmax = 3.16626 (* 1 = 3.16626 loss)
I0623 12:38:28.321249  5064 solver.cpp:473] Iteration 12400, lr = 0.0001
I0623 12:38:29.342239  5064 solver.cpp:213] Iteration 12410, loss = 3.14112
I0623 12:38:29.342258  5064 solver.cpp:228]     Train net output #0: softmax = 3.14112 (* 1 = 3.14112 loss)
I0623 12:38:29.342263  5064 solver.cpp:473] Iteration 12410, lr = 0.0001
I0623 12:38:30.363303  5064 solver.cpp:213] Iteration 12420, loss = 2.98066
I0623 12:38:30.363323  5064 solver.cpp:228]     Train net output #0: softmax = 2.98066 (* 1 = 2.98066 loss)
I0623 12:38:30.363329  5064 solver.cpp:473] Iteration 12420, lr = 0.0001
I0623 12:38:31.383306  5064 solver.cpp:213] Iteration 12430, loss = 2.905
I0623 12:38:31.383325  5064 solver.cpp:228]     Train net output #0: softmax = 2.905 (* 1 = 2.905 loss)
I0623 12:38:31.383330  5064 solver.cpp:473] Iteration 12430, lr = 0.0001
I0623 12:38:32.403857  5064 solver.cpp:213] Iteration 12440, loss = 3.26687
I0623 12:38:32.403878  5064 solver.cpp:228]     Train net output #0: softmax = 3.26687 (* 1 = 3.26687 loss)
I0623 12:38:32.403883  5064 solver.cpp:473] Iteration 12440, lr = 0.0001
I0623 12:38:33.424275  5064 solver.cpp:213] Iteration 12450, loss = 3.0987
I0623 12:38:33.424296  5064 solver.cpp:228]     Train net output #0: softmax = 3.0987 (* 1 = 3.0987 loss)
I0623 12:38:33.424301  5064 solver.cpp:473] Iteration 12450, lr = 0.0001
I0623 12:38:34.444723  5064 solver.cpp:213] Iteration 12460, loss = 2.87932
I0623 12:38:34.444759  5064 solver.cpp:228]     Train net output #0: softmax = 2.87932 (* 1 = 2.87932 loss)
I0623 12:38:34.444764  5064 solver.cpp:473] Iteration 12460, lr = 0.0001
I0623 12:38:35.465813  5064 solver.cpp:213] Iteration 12470, loss = 2.90865
I0623 12:38:35.465834  5064 solver.cpp:228]     Train net output #0: softmax = 2.90865 (* 1 = 2.90865 loss)
I0623 12:38:35.465840  5064 solver.cpp:473] Iteration 12470, lr = 0.0001
I0623 12:38:36.486368  5064 solver.cpp:213] Iteration 12480, loss = 2.98822
I0623 12:38:36.486389  5064 solver.cpp:228]     Train net output #0: softmax = 2.98822 (* 1 = 2.98822 loss)
I0623 12:38:36.486394  5064 solver.cpp:473] Iteration 12480, lr = 0.0001
I0623 12:38:37.507330  5064 solver.cpp:213] Iteration 12490, loss = 3.06326
I0623 12:38:37.507351  5064 solver.cpp:228]     Train net output #0: softmax = 3.06326 (* 1 = 3.06326 loss)
I0623 12:38:37.507357  5064 solver.cpp:473] Iteration 12490, lr = 0.0001
I0623 12:38:38.527958  5064 solver.cpp:213] Iteration 12500, loss = 3.08933
I0623 12:38:38.527986  5064 solver.cpp:228]     Train net output #0: softmax = 3.08933 (* 1 = 3.08933 loss)
I0623 12:38:38.528115  5064 solver.cpp:473] Iteration 12500, lr = 0.0001
I0623 12:38:39.548810  5064 solver.cpp:213] Iteration 12510, loss = 3.04012
I0623 12:38:39.548830  5064 solver.cpp:228]     Train net output #0: softmax = 3.04012 (* 1 = 3.04012 loss)
I0623 12:38:39.548836  5064 solver.cpp:473] Iteration 12510, lr = 0.0001
I0623 12:38:40.569586  5064 solver.cpp:213] Iteration 12520, loss = 3.10814
I0623 12:38:40.569604  5064 solver.cpp:228]     Train net output #0: softmax = 3.10814 (* 1 = 3.10814 loss)
I0623 12:38:40.569609  5064 solver.cpp:473] Iteration 12520, lr = 0.0001
I0623 12:38:41.590801  5064 solver.cpp:213] Iteration 12530, loss = 3.29286
I0623 12:38:41.590819  5064 solver.cpp:228]     Train net output #0: softmax = 3.29286 (* 1 = 3.29286 loss)
I0623 12:38:41.590824  5064 solver.cpp:473] Iteration 12530, lr = 0.0001
I0623 12:38:42.611253  5064 solver.cpp:213] Iteration 12540, loss = 2.9703
I0623 12:38:42.611274  5064 solver.cpp:228]     Train net output #0: softmax = 2.9703 (* 1 = 2.9703 loss)
I0623 12:38:42.611279  5064 solver.cpp:473] Iteration 12540, lr = 0.0001
I0623 12:38:43.631692  5064 solver.cpp:213] Iteration 12550, loss = 3.03783
I0623 12:38:43.631718  5064 solver.cpp:228]     Train net output #0: softmax = 3.03783 (* 1 = 3.03783 loss)
I0623 12:38:43.631849  5064 solver.cpp:473] Iteration 12550, lr = 0.0001
I0623 12:38:44.652071  5064 solver.cpp:213] Iteration 12560, loss = 3.19818
I0623 12:38:44.652091  5064 solver.cpp:228]     Train net output #0: softmax = 3.19818 (* 1 = 3.19818 loss)
I0623 12:38:44.652096  5064 solver.cpp:473] Iteration 12560, lr = 0.0001
I0623 12:38:45.673337  5064 solver.cpp:213] Iteration 12570, loss = 3.16258
I0623 12:38:45.673359  5064 solver.cpp:228]     Train net output #0: softmax = 3.16258 (* 1 = 3.16258 loss)
I0623 12:38:45.673364  5064 solver.cpp:473] Iteration 12570, lr = 0.0001
I0623 12:38:46.693603  5064 solver.cpp:213] Iteration 12580, loss = 3.08119
I0623 12:38:46.693626  5064 solver.cpp:228]     Train net output #0: softmax = 3.08119 (* 1 = 3.08119 loss)
I0623 12:38:46.693631  5064 solver.cpp:473] Iteration 12580, lr = 0.0001
I0623 12:38:47.714192  5064 solver.cpp:213] Iteration 12590, loss = 3.04776
I0623 12:38:47.714215  5064 solver.cpp:228]     Train net output #0: softmax = 3.04776 (* 1 = 3.04776 loss)
I0623 12:38:47.714220  5064 solver.cpp:473] Iteration 12590, lr = 0.0001
I0623 12:38:48.735244  5064 solver.cpp:213] Iteration 12600, loss = 2.99723
I0623 12:38:48.735416  5064 solver.cpp:228]     Train net output #0: softmax = 2.99723 (* 1 = 2.99723 loss)
I0623 12:38:48.735424  5064 solver.cpp:473] Iteration 12600, lr = 0.0001
I0623 12:38:49.755318  5064 solver.cpp:213] Iteration 12610, loss = 3.07338
I0623 12:38:49.755336  5064 solver.cpp:228]     Train net output #0: softmax = 3.07338 (* 1 = 3.07338 loss)
I0623 12:38:49.755342  5064 solver.cpp:473] Iteration 12610, lr = 0.0001
I0623 12:38:50.776350  5064 solver.cpp:213] Iteration 12620, loss = 3.19664
I0623 12:38:50.776371  5064 solver.cpp:228]     Train net output #0: softmax = 3.19664 (* 1 = 3.19664 loss)
I0623 12:38:50.776376  5064 solver.cpp:473] Iteration 12620, lr = 0.0001
I0623 12:38:51.796871  5064 solver.cpp:213] Iteration 12630, loss = 3.07944
I0623 12:38:51.796890  5064 solver.cpp:228]     Train net output #0: softmax = 3.07944 (* 1 = 3.07944 loss)
I0623 12:38:51.796895  5064 solver.cpp:473] Iteration 12630, lr = 0.0001
I0623 12:38:52.817912  5064 solver.cpp:213] Iteration 12640, loss = 3.31392
I0623 12:38:52.817935  5064 solver.cpp:228]     Train net output #0: softmax = 3.31392 (* 1 = 3.31392 loss)
I0623 12:38:52.817940  5064 solver.cpp:473] Iteration 12640, lr = 0.0001
I0623 12:38:53.838734  5064 solver.cpp:213] Iteration 12650, loss = 3.19426
I0623 12:38:53.838757  5064 solver.cpp:228]     Train net output #0: softmax = 3.19426 (* 1 = 3.19426 loss)
I0623 12:38:53.838893  5064 solver.cpp:473] Iteration 12650, lr = 0.0001
I0623 12:38:54.859406  5064 solver.cpp:213] Iteration 12660, loss = 3.30449
I0623 12:38:54.859431  5064 solver.cpp:228]     Train net output #0: softmax = 3.30449 (* 1 = 3.30449 loss)
I0623 12:38:54.859436  5064 solver.cpp:473] Iteration 12660, lr = 0.0001
I0623 12:38:55.879632  5064 solver.cpp:213] Iteration 12670, loss = 3.17251
I0623 12:38:55.879653  5064 solver.cpp:228]     Train net output #0: softmax = 3.17251 (* 1 = 3.17251 loss)
I0623 12:38:55.879658  5064 solver.cpp:473] Iteration 12670, lr = 0.0001
I0623 12:38:56.900843  5064 solver.cpp:213] Iteration 12680, loss = 3.35809
I0623 12:38:56.900866  5064 solver.cpp:228]     Train net output #0: softmax = 3.35809 (* 1 = 3.35809 loss)
I0623 12:38:56.900871  5064 solver.cpp:473] Iteration 12680, lr = 0.0001
I0623 12:38:57.921598  5064 solver.cpp:213] Iteration 12690, loss = 3.28521
I0623 12:38:57.921617  5064 solver.cpp:228]     Train net output #0: softmax = 3.28521 (* 1 = 3.28521 loss)
I0623 12:38:57.921622  5064 solver.cpp:473] Iteration 12690, lr = 0.0001
I0623 12:38:58.942447  5064 solver.cpp:213] Iteration 12700, loss = 3.02371
I0623 12:38:58.942473  5064 solver.cpp:228]     Train net output #0: softmax = 3.02371 (* 1 = 3.02371 loss)
I0623 12:38:58.942597  5064 solver.cpp:473] Iteration 12700, lr = 0.0001
I0623 12:38:59.963045  5064 solver.cpp:213] Iteration 12710, loss = 2.69868
I0623 12:38:59.963063  5064 solver.cpp:228]     Train net output #0: softmax = 2.69868 (* 1 = 2.69868 loss)
I0623 12:38:59.963068  5064 solver.cpp:473] Iteration 12710, lr = 0.0001
I0623 12:39:00.984136  5064 solver.cpp:213] Iteration 12720, loss = 2.86217
I0623 12:39:00.984158  5064 solver.cpp:228]     Train net output #0: softmax = 2.86217 (* 1 = 2.86217 loss)
I0623 12:39:00.984163  5064 solver.cpp:473] Iteration 12720, lr = 0.0001
I0623 12:39:02.004608  5064 solver.cpp:213] Iteration 12730, loss = 2.8832
I0623 12:39:02.004627  5064 solver.cpp:228]     Train net output #0: softmax = 2.8832 (* 1 = 2.8832 loss)
I0623 12:39:02.004633  5064 solver.cpp:473] Iteration 12730, lr = 0.0001
I0623 12:39:03.024922  5064 solver.cpp:213] Iteration 12740, loss = 2.97033
I0623 12:39:03.024945  5064 solver.cpp:228]     Train net output #0: softmax = 2.97033 (* 1 = 2.97033 loss)
I0623 12:39:03.024950  5064 solver.cpp:473] Iteration 12740, lr = 0.0001
I0623 12:39:04.045722  5064 solver.cpp:213] Iteration 12750, loss = 2.96182
I0623 12:39:04.045748  5064 solver.cpp:228]     Train net output #0: softmax = 2.96182 (* 1 = 2.96182 loss)
I0623 12:39:04.045898  5064 solver.cpp:473] Iteration 12750, lr = 0.0001
I0623 12:39:05.066370  5064 solver.cpp:213] Iteration 12760, loss = 2.96888
I0623 12:39:05.066409  5064 solver.cpp:228]     Train net output #0: softmax = 2.96888 (* 1 = 2.96888 loss)
I0623 12:39:05.066414  5064 solver.cpp:473] Iteration 12760, lr = 0.0001
I0623 12:39:06.087069  5064 solver.cpp:213] Iteration 12770, loss = 2.89816
I0623 12:39:06.087091  5064 solver.cpp:228]     Train net output #0: softmax = 2.89816 (* 1 = 2.89816 loss)
I0623 12:39:06.087096  5064 solver.cpp:473] Iteration 12770, lr = 0.0001
I0623 12:39:07.108142  5064 solver.cpp:213] Iteration 12780, loss = 3.11294
I0623 12:39:07.108163  5064 solver.cpp:228]     Train net output #0: softmax = 3.11294 (* 1 = 3.11294 loss)
I0623 12:39:07.108170  5064 solver.cpp:473] Iteration 12780, lr = 0.0001
I0623 12:39:08.128693  5064 solver.cpp:213] Iteration 12790, loss = 3.22168
I0623 12:39:08.128711  5064 solver.cpp:228]     Train net output #0: softmax = 3.22168 (* 1 = 3.22168 loss)
I0623 12:39:08.128717  5064 solver.cpp:473] Iteration 12790, lr = 0.0001
I0623 12:39:09.149757  5064 solver.cpp:213] Iteration 12800, loss = 3.09667
I0623 12:39:09.149785  5064 solver.cpp:228]     Train net output #0: softmax = 3.09667 (* 1 = 3.09667 loss)
I0623 12:39:09.149940  5064 solver.cpp:473] Iteration 12800, lr = 0.0001
I0623 12:39:10.169771  5064 solver.cpp:213] Iteration 12810, loss = 2.85444
I0623 12:39:10.169791  5064 solver.cpp:228]     Train net output #0: softmax = 2.85444 (* 1 = 2.85444 loss)
I0623 12:39:10.169796  5064 solver.cpp:473] Iteration 12810, lr = 0.0001
I0623 12:39:11.189656  5064 solver.cpp:213] Iteration 12820, loss = 3.11876
I0623 12:39:11.189676  5064 solver.cpp:228]     Train net output #0: softmax = 3.11876 (* 1 = 3.11876 loss)
I0623 12:39:11.189680  5064 solver.cpp:473] Iteration 12820, lr = 0.0001
I0623 12:39:12.210213  5064 solver.cpp:213] Iteration 12830, loss = 3.01928
I0623 12:39:12.210234  5064 solver.cpp:228]     Train net output #0: softmax = 3.01928 (* 1 = 3.01928 loss)
I0623 12:39:12.210240  5064 solver.cpp:473] Iteration 12830, lr = 0.0001
I0623 12:39:13.230573  5064 solver.cpp:213] Iteration 12840, loss = 3.50553
I0623 12:39:13.230595  5064 solver.cpp:228]     Train net output #0: softmax = 3.50553 (* 1 = 3.50553 loss)
I0623 12:39:13.230602  5064 solver.cpp:473] Iteration 12840, lr = 0.0001
I0623 12:39:14.251423  5064 solver.cpp:213] Iteration 12850, loss = 2.98221
I0623 12:39:14.251449  5064 solver.cpp:228]     Train net output #0: softmax = 2.98221 (* 1 = 2.98221 loss)
I0623 12:39:14.251575  5064 solver.cpp:473] Iteration 12850, lr = 0.0001
I0623 12:39:15.272195  5064 solver.cpp:213] Iteration 12860, loss = 3.01913
I0623 12:39:15.272218  5064 solver.cpp:228]     Train net output #0: softmax = 3.01913 (* 1 = 3.01913 loss)
I0623 12:39:15.272223  5064 solver.cpp:473] Iteration 12860, lr = 0.0001
I0623 12:39:16.293454  5064 solver.cpp:213] Iteration 12870, loss = 3.12516
I0623 12:39:16.293475  5064 solver.cpp:228]     Train net output #0: softmax = 3.12516 (* 1 = 3.12516 loss)
I0623 12:39:16.293480  5064 solver.cpp:473] Iteration 12870, lr = 0.0001
I0623 12:39:17.314029  5064 solver.cpp:213] Iteration 12880, loss = 3.21909
I0623 12:39:17.314050  5064 solver.cpp:228]     Train net output #0: softmax = 3.21909 (* 1 = 3.21909 loss)
I0623 12:39:17.314055  5064 solver.cpp:473] Iteration 12880, lr = 0.0001
I0623 12:39:18.334585  5064 solver.cpp:213] Iteration 12890, loss = 3.11874
I0623 12:39:18.334604  5064 solver.cpp:228]     Train net output #0: softmax = 3.11874 (* 1 = 3.11874 loss)
I0623 12:39:18.334609  5064 solver.cpp:473] Iteration 12890, lr = 0.0001
I0623 12:39:19.354575  5064 solver.cpp:213] Iteration 12900, loss = 3.12587
I0623 12:39:19.354621  5064 solver.cpp:228]     Train net output #0: softmax = 3.12587 (* 1 = 3.12587 loss)
I0623 12:39:19.354627  5064 solver.cpp:473] Iteration 12900, lr = 0.0001
I0623 12:39:20.374940  5064 solver.cpp:213] Iteration 12910, loss = 3.03489
I0623 12:39:20.374959  5064 solver.cpp:228]     Train net output #0: softmax = 3.03489 (* 1 = 3.03489 loss)
I0623 12:39:20.374965  5064 solver.cpp:473] Iteration 12910, lr = 0.0001
I0623 12:39:21.395406  5064 solver.cpp:213] Iteration 12920, loss = 3.0501
I0623 12:39:21.395423  5064 solver.cpp:228]     Train net output #0: softmax = 3.0501 (* 1 = 3.0501 loss)
I0623 12:39:21.395428  5064 solver.cpp:473] Iteration 12920, lr = 0.0001
I0623 12:39:22.415959  5064 solver.cpp:213] Iteration 12930, loss = 3.10987
I0623 12:39:22.415978  5064 solver.cpp:228]     Train net output #0: softmax = 3.10987 (* 1 = 3.10987 loss)
I0623 12:39:22.415984  5064 solver.cpp:473] Iteration 12930, lr = 0.0001
I0623 12:39:23.436341  5064 solver.cpp:213] Iteration 12940, loss = 3.3497
I0623 12:39:23.436360  5064 solver.cpp:228]     Train net output #0: softmax = 3.3497 (* 1 = 3.3497 loss)
I0623 12:39:23.436365  5064 solver.cpp:473] Iteration 12940, lr = 0.0001
I0623 12:39:24.457165  5064 solver.cpp:213] Iteration 12950, loss = 3.25055
I0623 12:39:24.457190  5064 solver.cpp:228]     Train net output #0: softmax = 3.25055 (* 1 = 3.25055 loss)
I0623 12:39:24.457325  5064 solver.cpp:473] Iteration 12950, lr = 0.0001
I0623 12:39:25.478080  5064 solver.cpp:213] Iteration 12960, loss = 2.98688
I0623 12:39:25.478101  5064 solver.cpp:228]     Train net output #0: softmax = 2.98688 (* 1 = 2.98688 loss)
I0623 12:39:25.478106  5064 solver.cpp:473] Iteration 12960, lr = 0.0001
I0623 12:39:26.498850  5064 solver.cpp:213] Iteration 12970, loss = 3.01926
I0623 12:39:26.498868  5064 solver.cpp:228]     Train net output #0: softmax = 3.01926 (* 1 = 3.01926 loss)
I0623 12:39:26.498874  5064 solver.cpp:473] Iteration 12970, lr = 0.0001
I0623 12:39:27.519131  5064 solver.cpp:213] Iteration 12980, loss = 2.94433
I0623 12:39:27.519165  5064 solver.cpp:228]     Train net output #0: softmax = 2.94433 (* 1 = 2.94433 loss)
I0623 12:39:27.519170  5064 solver.cpp:473] Iteration 12980, lr = 0.0001
I0623 12:39:28.539743  5064 solver.cpp:213] Iteration 12990, loss = 2.76856
I0623 12:39:28.539765  5064 solver.cpp:228]     Train net output #0: softmax = 2.76856 (* 1 = 2.76856 loss)
I0623 12:39:28.539770  5064 solver.cpp:473] Iteration 12990, lr = 0.0001
I0623 12:39:29.490105  5064 solver.cpp:362] Snapshotting to snapshots/16-06-21_16h05m31s_0_11_pretrainClassification_iter_13000.caffemodel
I0623 12:39:29.491155  5064 solver.cpp:370] Snapshotting solver state to snapshots/16-06-21_16h05m31s_0_11_pretrainClassification_iter_13000.solverstate
I0623 12:39:29.491757  5064 solver.cpp:291] Iteration 13000, Testing net (#0)
I0623 12:39:29.652755  5064 solver.cpp:342]     Test net output #0: accuracy_top_1 = 0.278125
I0623 12:39:29.652772  5064 solver.cpp:342]     Test net output #1: accuracy_top_5 = 0.579687
I0623 12:39:29.652778  5064 solver.cpp:342]     Test net output #2: softmax = 2.91866 (* 1 = 2.91866 loss)
I0623 12:39:29.723414  5064 solver.cpp:213] Iteration 13000, loss = 3.01603
I0623 12:39:29.723429  5064 solver.cpp:228]     Train net output #0: softmax = 3.01603 (* 1 = 3.01603 loss)
I0623 12:39:29.723434  5064 solver.cpp:473] Iteration 13000, lr = 0.0001
I0623 12:39:30.743891  5064 solver.cpp:213] Iteration 13010, loss = 3.15202
I0623 12:39:30.743912  5064 solver.cpp:228]     Train net output #0: softmax = 3.15202 (* 1 = 3.15202 loss)
I0623 12:39:30.743918  5064 solver.cpp:473] Iteration 13010, lr = 0.0001
I0623 12:39:31.764677  5064 solver.cpp:213] Iteration 13020, loss = 2.89331
I0623 12:39:31.764695  5064 solver.cpp:228]     Train net output #0: softmax = 2.89331 (* 1 = 2.89331 loss)
I0623 12:39:31.764700  5064 solver.cpp:473] Iteration 13020, lr = 0.0001
I0623 12:39:32.785293  5064 solver.cpp:213] Iteration 13030, loss = 3.01719
I0623 12:39:32.785313  5064 solver.cpp:228]     Train net output #0: softmax = 3.01719 (* 1 = 3.01719 loss)
I0623 12:39:32.785337  5064 solver.cpp:473] Iteration 13030, lr = 0.0001
I0623 12:39:33.805886  5064 solver.cpp:213] Iteration 13040, loss = 3.03321
I0623 12:39:33.805902  5064 solver.cpp:228]     Train net output #0: softmax = 3.03321 (* 1 = 3.03321 loss)
I0623 12:39:33.805907  5064 solver.cpp:473] Iteration 13040, lr = 0.0001
I0623 12:39:34.826855  5064 solver.cpp:213] Iteration 13050, loss = 3.19826
I0623 12:39:34.826877  5064 solver.cpp:228]     Train net output #0: softmax = 3.19826 (* 1 = 3.19826 loss)
I0623 12:39:34.826889  5064 solver.cpp:473] Iteration 13050, lr = 0.0001
I0623 12:39:35.847187  5064 solver.cpp:213] Iteration 13060, loss = 2.94279
I0623 12:39:35.847208  5064 solver.cpp:228]     Train net output #0: softmax = 2.94279 (* 1 = 2.94279 loss)
I0623 12:39:35.847213  5064 solver.cpp:473] Iteration 13060, lr = 0.0001
I0623 12:39:36.868384  5064 solver.cpp:213] Iteration 13070, loss = 2.95116
I0623 12:39:36.868404  5064 solver.cpp:228]     Train net output #0: softmax = 2.95116 (* 1 = 2.95116 loss)
I0623 12:39:36.868410  5064 solver.cpp:473] Iteration 13070, lr = 0.0001
I0623 12:39:37.889904  5064 solver.cpp:213] Iteration 13080, loss = 3.3767
I0623 12:39:37.889926  5064 solver.cpp:228]     Train net output #0: softmax = 3.3767 (* 1 = 3.3767 loss)
I0623 12:39:37.889932  5064 solver.cpp:473] Iteration 13080, lr = 0.0001
I0623 12:39:38.910851  5064 solver.cpp:213] Iteration 13090, loss = 3.24848
I0623 12:39:38.910879  5064 solver.cpp:228]     Train net output #0: softmax = 3.24848 (* 1 = 3.24848 loss)
I0623 12:39:38.910886  5064 solver.cpp:473] Iteration 13090, lr = 0.0001
I0623 12:39:39.931329  5064 solver.cpp:213] Iteration 13100, loss = 2.83567
I0623 12:39:39.931352  5064 solver.cpp:228]     Train net output #0: softmax = 2.83567 (* 1 = 2.83567 loss)
I0623 12:39:39.931501  5064 solver.cpp:473] Iteration 13100, lr = 0.0001
I0623 12:39:40.952055  5064 solver.cpp:213] Iteration 13110, loss = 2.93468
I0623 12:39:40.952076  5064 solver.cpp:228]     Train net output #0: softmax = 2.93468 (* 1 = 2.93468 loss)
I0623 12:39:40.952081  5064 solver.cpp:473] Iteration 13110, lr = 0.0001
I0623 12:39:41.973125  5064 solver.cpp:213] Iteration 13120, loss = 2.99352
I0623 12:39:41.973145  5064 solver.cpp:228]     Train net output #0: softmax = 2.99352 (* 1 = 2.99352 loss)
I0623 12:39:41.973151  5064 solver.cpp:473] Iteration 13120, lr = 0.0001
I0623 12:39:42.994246  5064 solver.cpp:213] Iteration 13130, loss = 3.24119
I0623 12:39:42.994266  5064 solver.cpp:228]     Train net output #0: softmax = 3.24119 (* 1 = 3.24119 loss)
I0623 12:39:42.994271  5064 solver.cpp:473] Iteration 13130, lr = 0.0001
I0623 12:39:44.014624  5064 solver.cpp:213] Iteration 13140, loss = 3.02753
I0623 12:39:44.014642  5064 solver.cpp:228]     Train net output #0: softmax = 3.02753 (* 1 = 3.02753 loss)
I0623 12:39:44.014647  5064 solver.cpp:473] Iteration 13140, lr = 0.0001
I0623 12:39:45.035363  5064 solver.cpp:213] Iteration 13150, loss = 2.81019
I0623 12:39:45.035387  5064 solver.cpp:228]     Train net output #0: softmax = 2.81019 (* 1 = 2.81019 loss)
I0623 12:39:45.035509  5064 solver.cpp:473] Iteration 13150, lr = 0.0001
I0623 12:39:46.055881  5064 solver.cpp:213] Iteration 13160, loss = 2.82525
I0623 12:39:46.055902  5064 solver.cpp:228]     Train net output #0: softmax = 2.82525 (* 1 = 2.82525 loss)
I0623 12:39:46.055907  5064 solver.cpp:473] Iteration 13160, lr = 0.0001
I0623 12:39:47.076601  5064 solver.cpp:213] Iteration 13170, loss = 2.99156
I0623 12:39:47.076625  5064 solver.cpp:228]     Train net output #0: softmax = 2.99156 (* 1 = 2.99156 loss)
I0623 12:39:47.076632  5064 solver.cpp:473] Iteration 13170, lr = 0.0001
I0623 12:39:48.096119  5064 solver.cpp:213] Iteration 13180, loss = 2.99778
I0623 12:39:48.096140  5064 solver.cpp:228]     Train net output #0: softmax = 2.99778 (* 1 = 2.99778 loss)
I0623 12:39:48.096146  5064 solver.cpp:473] Iteration 13180, lr = 0.0001
I0623 12:39:49.117210  5064 solver.cpp:213] Iteration 13190, loss = 3.19736
I0623 12:39:49.117229  5064 solver.cpp:228]     Train net output #0: softmax = 3.19736 (* 1 = 3.19736 loss)
I0623 12:39:49.117251  5064 solver.cpp:473] Iteration 13190, lr = 0.0001
I0623 12:39:50.137812  5064 solver.cpp:213] Iteration 13200, loss = 3.0664
I0623 12:39:50.137986  5064 solver.cpp:228]     Train net output #0: softmax = 3.0664 (* 1 = 3.0664 loss)
I0623 12:39:50.137994  5064 solver.cpp:473] Iteration 13200, lr = 0.0001
I0623 12:39:51.158962  5064 solver.cpp:213] Iteration 13210, loss = 3.01146
I0623 12:39:51.158983  5064 solver.cpp:228]     Train net output #0: softmax = 3.01146 (* 1 = 3.01146 loss)
I0623 12:39:51.158988  5064 solver.cpp:473] Iteration 13210, lr = 0.0001
I0623 12:39:52.180096  5064 solver.cpp:213] Iteration 13220, loss = 2.86883
I0623 12:39:52.180121  5064 solver.cpp:228]     Train net output #0: softmax = 2.86883 (* 1 = 2.86883 loss)
I0623 12:39:52.180126  5064 solver.cpp:473] Iteration 13220, lr = 0.0001
I0623 12:39:53.200436  5064 solver.cpp:213] Iteration 13230, loss = 3.12113
I0623 12:39:53.200454  5064 solver.cpp:228]     Train net output #0: softmax = 3.12113 (* 1 = 3.12113 loss)
I0623 12:39:53.200460  5064 solver.cpp:473] Iteration 13230, lr = 0.0001
I0623 12:39:54.220481  5064 solver.cpp:213] Iteration 13240, loss = 2.82864
I0623 12:39:54.220501  5064 solver.cpp:228]     Train net output #0: softmax = 2.82864 (* 1 = 2.82864 loss)
I0623 12:39:54.220506  5064 solver.cpp:473] Iteration 13240, lr = 0.0001
I0623 12:39:55.241278  5064 solver.cpp:213] Iteration 13250, loss = 2.89858
I0623 12:39:55.241467  5064 solver.cpp:228]     Train net output #0: softmax = 2.89858 (* 1 = 2.89858 loss)
I0623 12:39:55.241475  5064 solver.cpp:473] Iteration 13250, lr = 0.0001
I0623 12:39:56.261277  5064 solver.cpp:213] Iteration 13260, loss = 3.05831
I0623 12:39:56.261296  5064 solver.cpp:228]     Train net output #0: softmax = 3.05831 (* 1 = 3.05831 loss)
I0623 12:39:56.261301  5064 solver.cpp:473] Iteration 13260, lr = 0.0001
I0623 12:39:57.281942  5064 solver.cpp:213] Iteration 13270, loss = 3.12653
I0623 12:39:57.281967  5064 solver.cpp:228]     Train net output #0: softmax = 3.12653 (* 1 = 3.12653 loss)
I0623 12:39:57.281972  5064 solver.cpp:473] Iteration 13270, lr = 0.0001
I0623 12:39:58.302297  5064 solver.cpp:213] Iteration 13280, loss = 3.10747
I0623 12:39:58.302317  5064 solver.cpp:228]     Train net output #0: softmax = 3.10747 (* 1 = 3.10747 loss)
I0623 12:39:58.302322  5064 solver.cpp:473] Iteration 13280, lr = 0.0001
I0623 12:39:59.321856  5064 solver.cpp:213] Iteration 13290, loss = 2.80673
I0623 12:39:59.321874  5064 solver.cpp:228]     Train net output #0: softmax = 2.80673 (* 1 = 2.80673 loss)
I0623 12:39:59.321880  5064 solver.cpp:473] Iteration 13290, lr = 0.0001
I0623 12:40:00.342509  5064 solver.cpp:213] Iteration 13300, loss = 3.01392
I0623 12:40:00.342530  5064 solver.cpp:228]     Train net output #0: softmax = 3.01392 (* 1 = 3.01392 loss)
I0623 12:40:00.342535  5064 solver.cpp:473] Iteration 13300, lr = 0.0001
I0623 12:40:01.362836  5064 solver.cpp:213] Iteration 13310, loss = 2.99233
I0623 12:40:01.362859  5064 solver.cpp:228]     Train net output #0: softmax = 2.99233 (* 1 = 2.99233 loss)
I0623 12:40:01.362864  5064 solver.cpp:473] Iteration 13310, lr = 0.0001
I0623 12:40:02.383955  5064 solver.cpp:213] Iteration 13320, loss = 2.94564
I0623 12:40:02.383975  5064 solver.cpp:228]     Train net output #0: softmax = 2.94564 (* 1 = 2.94564 loss)
I0623 12:40:02.383980  5064 solver.cpp:473] Iteration 13320, lr = 0.0001
I0623 12:40:03.404407  5064 solver.cpp:213] Iteration 13330, loss = 2.89635
I0623 12:40:03.404427  5064 solver.cpp:228]     Train net output #0: softmax = 2.89635 (* 1 = 2.89635 loss)
I0623 12:40:03.404431  5064 solver.cpp:473] Iteration 13330, lr = 0.0001
I0623 12:40:04.424648  5064 solver.cpp:213] Iteration 13340, loss = 3.20405
I0623 12:40:04.424667  5064 solver.cpp:228]     Train net output #0: softmax = 3.20405 (* 1 = 3.20405 loss)
I0623 12:40:04.424672  5064 solver.cpp:473] Iteration 13340, lr = 0.0001
I0623 12:40:05.445212  5064 solver.cpp:213] Iteration 13350, loss = 3.0514
I0623 12:40:05.445235  5064 solver.cpp:228]     Train net output #0: softmax = 3.0514 (* 1 = 3.0514 loss)
I0623 12:40:05.445387  5064 solver.cpp:473] Iteration 13350, lr = 0.0001
I0623 12:40:06.465724  5064 solver.cpp:213] Iteration 13360, loss = 2.86681
I0623 12:40:06.465762  5064 solver.cpp:228]     Train net output #0: softmax = 2.86681 (* 1 = 2.86681 loss)
I0623 12:40:06.465769  5064 solver.cpp:473] Iteration 13360, lr = 0.0001
I0623 12:40:07.485795  5064 solver.cpp:213] Iteration 13370, loss = 3.12089
I0623 12:40:07.485816  5064 solver.cpp:228]     Train net output #0: softmax = 3.12089 (* 1 = 3.12089 loss)
I0623 12:40:07.485821  5064 solver.cpp:473] Iteration 13370, lr = 0.0001
I0623 12:40:08.506300  5064 solver.cpp:213] Iteration 13380, loss = 3.05907
I0623 12:40:08.506319  5064 solver.cpp:228]     Train net output #0: softmax = 3.05907 (* 1 = 3.05907 loss)
I0623 12:40:08.506325  5064 solver.cpp:473] Iteration 13380, lr = 0.0001
I0623 12:40:09.526154  5064 solver.cpp:213] Iteration 13390, loss = 2.91126
I0623 12:40:09.526173  5064 solver.cpp:228]     Train net output #0: softmax = 2.91126 (* 1 = 2.91126 loss)
I0623 12:40:09.526178  5064 solver.cpp:473] Iteration 13390, lr = 0.0001
I0623 12:40:10.546486  5064 solver.cpp:213] Iteration 13400, loss = 3.29509
I0623 12:40:10.546512  5064 solver.cpp:228]     Train net output #0: softmax = 3.29509 (* 1 = 3.29509 loss)
I0623 12:40:10.546643  5064 solver.cpp:473] Iteration 13400, lr = 0.0001
I0623 12:40:11.567289  5064 solver.cpp:213] Iteration 13410, loss = 2.98169
I0623 12:40:11.567313  5064 solver.cpp:228]     Train net output #0: softmax = 2.98169 (* 1 = 2.98169 loss)
I0623 12:40:11.567319  5064 solver.cpp:473] Iteration 13410, lr = 0.0001
I0623 12:40:12.587901  5064 solver.cpp:213] Iteration 13420, loss = 3.08703
I0623 12:40:12.587921  5064 solver.cpp:228]     Train net output #0: softmax = 3.08703 (* 1 = 3.08703 loss)
I0623 12:40:12.587926  5064 solver.cpp:473] Iteration 13420, lr = 0.0001
I0623 12:40:13.608307  5064 solver.cpp:213] Iteration 13430, loss = 3.06501
I0623 12:40:13.608326  5064 solver.cpp:228]     Train net output #0: softmax = 3.06501 (* 1 = 3.06501 loss)
I0623 12:40:13.608331  5064 solver.cpp:473] Iteration 13430, lr = 0.0001
I0623 12:40:14.629340  5064 solver.cpp:213] Iteration 13440, loss = 3.16907
I0623 12:40:14.629360  5064 solver.cpp:228]     Train net output #0: softmax = 3.16907 (* 1 = 3.16907 loss)
I0623 12:40:14.629365  5064 solver.cpp:473] Iteration 13440, lr = 0.0001
I0623 12:40:15.649154  5064 solver.cpp:213] Iteration 13450, loss = 2.91029
I0623 12:40:15.649176  5064 solver.cpp:228]     Train net output #0: softmax = 2.91029 (* 1 = 2.91029 loss)
I0623 12:40:15.649324  5064 solver.cpp:473] Iteration 13450, lr = 0.0001
I0623 12:40:16.669975  5064 solver.cpp:213] Iteration 13460, loss = 2.82996
I0623 12:40:16.669997  5064 solver.cpp:228]     Train net output #0: softmax = 2.82996 (* 1 = 2.82996 loss)
I0623 12:40:16.670002  5064 solver.cpp:473] Iteration 13460, lr = 0.0001
I0623 12:40:17.690244  5064 solver.cpp:213] Iteration 13470, loss = 3.22414
I0623 12:40:17.690264  5064 solver.cpp:228]     Train net output #0: softmax = 3.22414 (* 1 = 3.22414 loss)
I0623 12:40:17.690269  5064 solver.cpp:473] Iteration 13470, lr = 0.0001
I0623 12:40:18.710522  5064 solver.cpp:213] Iteration 13480, loss = 3.03433
I0623 12:40:18.710542  5064 solver.cpp:228]     Train net output #0: softmax = 3.03433 (* 1 = 3.03433 loss)
I0623 12:40:18.710547  5064 solver.cpp:473] Iteration 13480, lr = 0.0001
I0623 12:40:19.731298  5064 solver.cpp:213] Iteration 13490, loss = 3.12946
I0623 12:40:19.731319  5064 solver.cpp:228]     Train net output #0: softmax = 3.12946 (* 1 = 3.12946 loss)
I0623 12:40:19.731325  5064 solver.cpp:473] Iteration 13490, lr = 0.0001
I0623 12:40:20.752113  5064 solver.cpp:213] Iteration 13500, loss = 2.82988
I0623 12:40:20.752287  5064 solver.cpp:228]     Train net output #0: softmax = 2.82988 (* 1 = 2.82988 loss)
I0623 12:40:20.752295  5064 solver.cpp:473] Iteration 13500, lr = 0.0001
I0623 12:40:21.772821  5064 solver.cpp:213] Iteration 13510, loss = 2.99825
I0623 12:40:21.772840  5064 solver.cpp:228]     Train net output #0: softmax = 2.99825 (* 1 = 2.99825 loss)
I0623 12:40:21.772846  5064 solver.cpp:473] Iteration 13510, lr = 0.0001
I0623 12:40:22.794126  5064 solver.cpp:213] Iteration 13520, loss = 3.35391
I0623 12:40:22.794147  5064 solver.cpp:228]     Train net output #0: softmax = 3.35391 (* 1 = 3.35391 loss)
I0623 12:40:22.794152  5064 solver.cpp:473] Iteration 13520, lr = 0.0001
I0623 12:40:23.813555  5064 solver.cpp:213] Iteration 13530, loss = 2.76247
I0623 12:40:23.813572  5064 solver.cpp:228]     Train net output #0: softmax = 2.76247 (* 1 = 2.76247 loss)
I0623 12:40:23.813577  5064 solver.cpp:473] Iteration 13530, lr = 0.0001
I0623 12:40:24.834556  5064 solver.cpp:213] Iteration 13540, loss = 2.87268
I0623 12:40:24.834576  5064 solver.cpp:228]     Train net output #0: softmax = 2.87268 (* 1 = 2.87268 loss)
I0623 12:40:24.834581  5064 solver.cpp:473] Iteration 13540, lr = 0.0001
I0623 12:40:25.855648  5064 solver.cpp:213] Iteration 13550, loss = 2.77736
I0623 12:40:25.855674  5064 solver.cpp:228]     Train net output #0: softmax = 2.77736 (* 1 = 2.77736 loss)
I0623 12:40:25.855809  5064 solver.cpp:473] Iteration 13550, lr = 0.0001
I0623 12:40:26.876590  5064 solver.cpp:213] Iteration 13560, loss = 2.88518
I0623 12:40:26.876610  5064 solver.cpp:228]     Train net output #0: softmax = 2.88518 (* 1 = 2.88518 loss)
I0623 12:40:26.876616  5064 solver.cpp:473] Iteration 13560, lr = 0.0001
I0623 12:40:27.895850  5064 solver.cpp:213] Iteration 13570, loss = 2.82096
I0623 12:40:27.895875  5064 solver.cpp:228]     Train net output #0: softmax = 2.82096 (* 1 = 2.82096 loss)
I0623 12:40:27.895880  5064 solver.cpp:473] Iteration 13570, lr = 0.0001
I0623 12:40:28.916308  5064 solver.cpp:213] Iteration 13580, loss = 2.91082
I0623 12:40:28.916331  5064 solver.cpp:228]     Train net output #0: softmax = 2.91082 (* 1 = 2.91082 loss)
I0623 12:40:28.916335  5064 solver.cpp:473] Iteration 13580, lr = 0.0001
I0623 12:40:29.937001  5064 solver.cpp:213] Iteration 13590, loss = 3.09942
I0623 12:40:29.937021  5064 solver.cpp:228]     Train net output #0: softmax = 3.09942 (* 1 = 3.09942 loss)
I0623 12:40:29.937027  5064 solver.cpp:473] Iteration 13590, lr = 0.0001
I0623 12:40:30.957168  5064 solver.cpp:213] Iteration 13600, loss = 2.96341
I0623 12:40:30.957191  5064 solver.cpp:228]     Train net output #0: softmax = 2.96341 (* 1 = 2.96341 loss)
I0623 12:40:30.957319  5064 solver.cpp:473] Iteration 13600, lr = 0.0001
I0623 12:40:31.976975  5064 solver.cpp:213] Iteration 13610, loss = 2.82731
I0623 12:40:31.976995  5064 solver.cpp:228]     Train net output #0: softmax = 2.82731 (* 1 = 2.82731 loss)
I0623 12:40:31.977000  5064 solver.cpp:473] Iteration 13610, lr = 0.0001
I0623 12:40:32.997777  5064 solver.cpp:213] Iteration 13620, loss = 3.13035
I0623 12:40:32.997797  5064 solver.cpp:228]     Train net output #0: softmax = 3.13035 (* 1 = 3.13035 loss)
I0623 12:40:32.997802  5064 solver.cpp:473] Iteration 13620, lr = 0.0001
I0623 12:40:34.018568  5064 solver.cpp:213] Iteration 13630, loss = 2.90392
I0623 12:40:34.018591  5064 solver.cpp:228]     Train net output #0: softmax = 2.90392 (* 1 = 2.90392 loss)
I0623 12:40:34.018596  5064 solver.cpp:473] Iteration 13630, lr = 0.0001
I0623 12:40:35.039198  5064 solver.cpp:213] Iteration 13640, loss = 2.97272
I0623 12:40:35.039218  5064 solver.cpp:228]     Train net output #0: softmax = 2.97272 (* 1 = 2.97272 loss)
I0623 12:40:35.039223  5064 solver.cpp:473] Iteration 13640, lr = 0.0001
I0623 12:40:36.059707  5064 solver.cpp:213] Iteration 13650, loss = 3.02819
I0623 12:40:36.059729  5064 solver.cpp:228]     Train net output #0: softmax = 3.02819 (* 1 = 3.02819 loss)
I0623 12:40:36.059862  5064 solver.cpp:473] Iteration 13650, lr = 0.0001
I0623 12:40:37.081037  5064 solver.cpp:213] Iteration 13660, loss = 3.03258
I0623 12:40:37.081071  5064 solver.cpp:228]     Train net output #0: softmax = 3.03258 (* 1 = 3.03258 loss)
I0623 12:40:37.081076  5064 solver.cpp:473] Iteration 13660, lr = 0.0001
I0623 12:40:38.101577  5064 solver.cpp:213] Iteration 13670, loss = 3.0857
I0623 12:40:38.101598  5064 solver.cpp:228]     Train net output #0: softmax = 3.0857 (* 1 = 3.0857 loss)
I0623 12:40:38.101603  5064 solver.cpp:473] Iteration 13670, lr = 0.0001
I0623 12:40:39.121747  5064 solver.cpp:213] Iteration 13680, loss = 2.75681
I0623 12:40:39.121772  5064 solver.cpp:228]     Train net output #0: softmax = 2.75681 (* 1 = 2.75681 loss)
I0623 12:40:39.121778  5064 solver.cpp:473] Iteration 13680, lr = 0.0001
I0623 12:40:40.142323  5064 solver.cpp:213] Iteration 13690, loss = 3.09857
I0623 12:40:40.142348  5064 solver.cpp:228]     Train net output #0: softmax = 3.09857 (* 1 = 3.09857 loss)
I0623 12:40:40.142354  5064 solver.cpp:473] Iteration 13690, lr = 0.0001
I0623 12:40:41.163054  5064 solver.cpp:213] Iteration 13700, loss = 3.12052
I0623 12:40:41.163077  5064 solver.cpp:228]     Train net output #0: softmax = 3.12052 (* 1 = 3.12052 loss)
I0623 12:40:41.163235  5064 solver.cpp:473] Iteration 13700, lr = 0.0001
I0623 12:40:42.184026  5064 solver.cpp:213] Iteration 13710, loss = 2.96684
I0623 12:40:42.184048  5064 solver.cpp:228]     Train net output #0: softmax = 2.96684 (* 1 = 2.96684 loss)
I0623 12:40:42.184053  5064 solver.cpp:473] Iteration 13710, lr = 0.0001
I0623 12:40:43.203276  5064 solver.cpp:213] Iteration 13720, loss = 2.83125
I0623 12:40:43.203299  5064 solver.cpp:228]     Train net output #0: softmax = 2.83125 (* 1 = 2.83125 loss)
I0623 12:40:43.203305  5064 solver.cpp:473] Iteration 13720, lr = 0.0001
I0623 12:40:44.224295  5064 solver.cpp:213] Iteration 13730, loss = 3.14781
I0623 12:40:44.224318  5064 solver.cpp:228]     Train net output #0: softmax = 3.14781 (* 1 = 3.14781 loss)
I0623 12:40:44.224323  5064 solver.cpp:473] Iteration 13730, lr = 0.0001
I0623 12:40:45.244530  5064 solver.cpp:213] Iteration 13740, loss = 3.08225
I0623 12:40:45.244550  5064 solver.cpp:228]     Train net output #0: softmax = 3.08225 (* 1 = 3.08225 loss)
I0623 12:40:45.244555  5064 solver.cpp:473] Iteration 13740, lr = 0.0001
I0623 12:40:46.265120  5064 solver.cpp:213] Iteration 13750, loss = 2.94672
I0623 12:40:46.265148  5064 solver.cpp:228]     Train net output #0: softmax = 2.94672 (* 1 = 2.94672 loss)
I0623 12:40:46.265300  5064 solver.cpp:473] Iteration 13750, lr = 0.0001
I0623 12:40:47.285514  5064 solver.cpp:213] Iteration 13760, loss = 2.84377
I0623 12:40:47.285537  5064 solver.cpp:228]     Train net output #0: softmax = 2.84377 (* 1 = 2.84377 loss)
I0623 12:40:47.285542  5064 solver.cpp:473] Iteration 13760, lr = 0.0001
I0623 12:40:48.306411  5064 solver.cpp:213] Iteration 13770, loss = 3.03643
I0623 12:40:48.306430  5064 solver.cpp:228]     Train net output #0: softmax = 3.03643 (* 1 = 3.03643 loss)
I0623 12:40:48.306435  5064 solver.cpp:473] Iteration 13770, lr = 0.0001
I0623 12:40:49.327033  5064 solver.cpp:213] Iteration 13780, loss = 2.8653
I0623 12:40:49.327052  5064 solver.cpp:228]     Train net output #0: softmax = 2.8653 (* 1 = 2.8653 loss)
I0623 12:40:49.327057  5064 solver.cpp:473] Iteration 13780, lr = 0.0001
I0623 12:40:50.347157  5064 solver.cpp:213] Iteration 13790, loss = 3.14503
I0623 12:40:50.347174  5064 solver.cpp:228]     Train net output #0: softmax = 3.14503 (* 1 = 3.14503 loss)
I0623 12:40:50.347179  5064 solver.cpp:473] Iteration 13790, lr = 0.0001
I0623 12:40:51.367969  5064 solver.cpp:213] Iteration 13800, loss = 2.89
I0623 12:40:51.368012  5064 solver.cpp:228]     Train net output #0: softmax = 2.89 (* 1 = 2.89 loss)
I0623 12:40:51.368017  5064 solver.cpp:473] Iteration 13800, lr = 0.0001
I0623 12:40:52.389001  5064 solver.cpp:213] Iteration 13810, loss = 3.06597
I0623 12:40:52.389024  5064 solver.cpp:228]     Train net output #0: softmax = 3.06597 (* 1 = 3.06597 loss)
I0623 12:40:52.389029  5064 solver.cpp:473] Iteration 13810, lr = 0.0001
I0623 12:40:53.409765  5064 solver.cpp:213] Iteration 13820, loss = 3.06169
I0623 12:40:53.409783  5064 solver.cpp:228]     Train net output #0: softmax = 3.06169 (* 1 = 3.06169 loss)
I0623 12:40:53.409788  5064 solver.cpp:473] Iteration 13820, lr = 0.0001
I0623 12:40:54.430013  5064 solver.cpp:213] Iteration 13830, loss = 3.17002
I0623 12:40:54.430033  5064 solver.cpp:228]     Train net output #0: softmax = 3.17002 (* 1 = 3.17002 loss)
I0623 12:40:54.430038  5064 solver.cpp:473] Iteration 13830, lr = 0.0001
I0623 12:40:55.449103  5064 solver.cpp:213] Iteration 13840, loss = 2.92383
I0623 12:40:55.449122  5064 solver.cpp:228]     Train net output #0: softmax = 2.92383 (* 1 = 2.92383 loss)
I0623 12:40:55.449127  5064 solver.cpp:473] Iteration 13840, lr = 0.0001
I0623 12:40:56.469480  5064 solver.cpp:213] Iteration 13850, loss = 2.86405
I0623 12:40:56.469506  5064 solver.cpp:228]     Train net output #0: softmax = 2.86405 (* 1 = 2.86405 loss)
I0623 12:40:56.469647  5064 solver.cpp:473] Iteration 13850, lr = 0.0001
I0623 12:40:57.489689  5064 solver.cpp:213] Iteration 13860, loss = 3.11976
I0623 12:40:57.489711  5064 solver.cpp:228]     Train net output #0: softmax = 3.11976 (* 1 = 3.11976 loss)
I0623 12:40:57.489717  5064 solver.cpp:473] Iteration 13860, lr = 0.0001
I0623 12:40:58.510473  5064 solver.cpp:213] Iteration 13870, loss = 2.77823
I0623 12:40:58.510491  5064 solver.cpp:228]     Train net output #0: softmax = 2.77823 (* 1 = 2.77823 loss)
I0623 12:40:58.510496  5064 solver.cpp:473] Iteration 13870, lr = 0.0001
I0623 12:40:59.529003  5064 solver.cpp:213] Iteration 13880, loss = 3.11583
I0623 12:40:59.529021  5064 solver.cpp:228]     Train net output #0: softmax = 3.11583 (* 1 = 3.11583 loss)
I0623 12:40:59.529026  5064 solver.cpp:473] Iteration 13880, lr = 0.0001
I0623 12:41:00.549121  5064 solver.cpp:213] Iteration 13890, loss = 2.92171
I0623 12:41:00.549141  5064 solver.cpp:228]     Train net output #0: softmax = 2.92171 (* 1 = 2.92171 loss)
I0623 12:41:00.549146  5064 solver.cpp:473] Iteration 13890, lr = 0.0001
I0623 12:41:01.569634  5064 solver.cpp:213] Iteration 13900, loss = 2.82641
I0623 12:41:01.569659  5064 solver.cpp:228]     Train net output #0: softmax = 2.82641 (* 1 = 2.82641 loss)
I0623 12:41:01.569782  5064 solver.cpp:473] Iteration 13900, lr = 0.0001
I0623 12:41:02.590131  5064 solver.cpp:213] Iteration 13910, loss = 3.19503
I0623 12:41:02.590150  5064 solver.cpp:228]     Train net output #0: softmax = 3.19503 (* 1 = 3.19503 loss)
I0623 12:41:02.590155  5064 solver.cpp:473] Iteration 13910, lr = 0.0001
I0623 12:41:03.610275  5064 solver.cpp:213] Iteration 13920, loss = 2.86947
I0623 12:41:03.610294  5064 solver.cpp:228]     Train net output #0: softmax = 2.86947 (* 1 = 2.86947 loss)
I0623 12:41:03.610301  5064 solver.cpp:473] Iteration 13920, lr = 0.0001
I0623 12:41:04.630039  5064 solver.cpp:213] Iteration 13930, loss = 3.08346
I0623 12:41:04.630056  5064 solver.cpp:228]     Train net output #0: softmax = 3.08346 (* 1 = 3.08346 loss)
I0623 12:41:04.630061  5064 solver.cpp:473] Iteration 13930, lr = 0.0001
I0623 12:41:05.650080  5064 solver.cpp:213] Iteration 13940, loss = 3.15163
I0623 12:41:05.650102  5064 solver.cpp:228]     Train net output #0: softmax = 3.15163 (* 1 = 3.15163 loss)
I0623 12:41:05.650107  5064 solver.cpp:473] Iteration 13940, lr = 0.0001
I0623 12:41:06.670876  5064 solver.cpp:213] Iteration 13950, loss = 3.00395
I0623 12:41:06.670899  5064 solver.cpp:228]     Train net output #0: softmax = 3.00395 (* 1 = 3.00395 loss)
I0623 12:41:06.671023  5064 solver.cpp:473] Iteration 13950, lr = 0.0001
I0623 12:41:07.690703  5064 solver.cpp:213] Iteration 13960, loss = 3.24819
I0623 12:41:07.690742  5064 solver.cpp:228]     Train net output #0: softmax = 3.24819 (* 1 = 3.24819 loss)
I0623 12:41:07.690748  5064 solver.cpp:473] Iteration 13960, lr = 0.0001
I0623 12:41:08.711238  5064 solver.cpp:213] Iteration 13970, loss = 2.85635
I0623 12:41:08.711259  5064 solver.cpp:228]     Train net output #0: softmax = 2.85635 (* 1 = 2.85635 loss)
I0623 12:41:08.711264  5064 solver.cpp:473] Iteration 13970, lr = 0.0001
I0623 12:41:09.731624  5064 solver.cpp:213] Iteration 13980, loss = 3.13186
I0623 12:41:09.731642  5064 solver.cpp:228]     Train net output #0: softmax = 3.13186 (* 1 = 3.13186 loss)
I0623 12:41:09.731647  5064 solver.cpp:473] Iteration 13980, lr = 0.0001
I0623 12:41:10.752672  5064 solver.cpp:213] Iteration 13990, loss = 3.14587
I0623 12:41:10.752691  5064 solver.cpp:228]     Train net output #0: softmax = 3.14587 (* 1 = 3.14587 loss)
I0623 12:41:10.752696  5064 solver.cpp:473] Iteration 13990, lr = 0.0001
I0623 12:41:11.702065  5064 solver.cpp:362] Snapshotting to snapshots/16-06-21_16h05m31s_0_11_pretrainClassification_iter_14000.caffemodel
I0623 12:41:11.703127  5064 solver.cpp:370] Snapshotting solver state to snapshots/16-06-21_16h05m31s_0_11_pretrainClassification_iter_14000.solverstate
I0623 12:41:11.703748  5064 solver.cpp:291] Iteration 14000, Testing net (#0)
I0623 12:41:11.864907  5064 solver.cpp:342]     Test net output #0: accuracy_top_1 = 0.265625
I0623 12:41:11.864923  5064 solver.cpp:342]     Test net output #1: accuracy_top_5 = 0.589063
I0623 12:41:11.864930  5064 solver.cpp:342]     Test net output #2: softmax = 2.97725 (* 1 = 2.97725 loss)
I0623 12:41:11.935561  5064 solver.cpp:213] Iteration 14000, loss = 2.9165
I0623 12:41:11.935576  5064 solver.cpp:228]     Train net output #0: softmax = 2.9165 (* 1 = 2.9165 loss)
I0623 12:41:11.935581  5064 solver.cpp:473] Iteration 14000, lr = 0.0001
I0623 12:41:12.956256  5064 solver.cpp:213] Iteration 14010, loss = 2.89792
I0623 12:41:12.956274  5064 solver.cpp:228]     Train net output #0: softmax = 2.89792 (* 1 = 2.89792 loss)
I0623 12:41:12.956279  5064 solver.cpp:473] Iteration 14010, lr = 0.0001
I0623 12:41:13.976917  5064 solver.cpp:213] Iteration 14020, loss = 2.9583
I0623 12:41:13.976948  5064 solver.cpp:228]     Train net output #0: softmax = 2.9583 (* 1 = 2.9583 loss)
I0623 12:41:13.976953  5064 solver.cpp:473] Iteration 14020, lr = 0.0001
I0623 12:41:14.997509  5064 solver.cpp:213] Iteration 14030, loss = 3.18523
I0623 12:41:14.997527  5064 solver.cpp:228]     Train net output #0: softmax = 3.18523 (* 1 = 3.18523 loss)
I0623 12:41:14.997532  5064 solver.cpp:473] Iteration 14030, lr = 0.0001
I0623 12:41:16.016944  5064 solver.cpp:213] Iteration 14040, loss = 3.05981
I0623 12:41:16.016963  5064 solver.cpp:228]     Train net output #0: softmax = 3.05981 (* 1 = 3.05981 loss)
I0623 12:41:16.016968  5064 solver.cpp:473] Iteration 14040, lr = 0.0001
I0623 12:41:17.036891  5064 solver.cpp:213] Iteration 14050, loss = 3.00006
I0623 12:41:17.036912  5064 solver.cpp:228]     Train net output #0: softmax = 3.00006 (* 1 = 3.00006 loss)
I0623 12:41:17.037101  5064 solver.cpp:473] Iteration 14050, lr = 0.0001
I0623 12:41:18.057499  5064 solver.cpp:213] Iteration 14060, loss = 3.05714
I0623 12:41:18.057519  5064 solver.cpp:228]     Train net output #0: softmax = 3.05714 (* 1 = 3.05714 loss)
I0623 12:41:18.057524  5064 solver.cpp:473] Iteration 14060, lr = 0.0001
I0623 12:41:19.077970  5064 solver.cpp:213] Iteration 14070, loss = 2.93844
I0623 12:41:19.077996  5064 solver.cpp:228]     Train net output #0: softmax = 2.93844 (* 1 = 2.93844 loss)
I0623 12:41:19.078002  5064 solver.cpp:473] Iteration 14070, lr = 0.0001
I0623 12:41:20.097378  5064 solver.cpp:213] Iteration 14080, loss = 2.81291
I0623 12:41:20.097399  5064 solver.cpp:228]     Train net output #0: softmax = 2.81291 (* 1 = 2.81291 loss)
I0623 12:41:20.097405  5064 solver.cpp:473] Iteration 14080, lr = 0.0001
I0623 12:41:21.118440  5064 solver.cpp:213] Iteration 14090, loss = 3.03789
I0623 12:41:21.118459  5064 solver.cpp:228]     Train net output #0: softmax = 3.03789 (* 1 = 3.03789 loss)
I0623 12:41:21.118482  5064 solver.cpp:473] Iteration 14090, lr = 0.0001
I0623 12:41:22.138826  5064 solver.cpp:213] Iteration 14100, loss = 3.1441
I0623 12:41:22.138994  5064 solver.cpp:228]     Train net output #0: softmax = 3.1441 (* 1 = 3.1441 loss)
I0623 12:41:22.139001  5064 solver.cpp:473] Iteration 14100, lr = 0.0001
I0623 12:41:23.158531  5064 solver.cpp:213] Iteration 14110, loss = 3.04946
I0623 12:41:23.158556  5064 solver.cpp:228]     Train net output #0: softmax = 3.04946 (* 1 = 3.04946 loss)
I0623 12:41:23.158563  5064 solver.cpp:473] Iteration 14110, lr = 0.0001
I0623 12:41:24.178789  5064 solver.cpp:213] Iteration 14120, loss = 3.12506
I0623 12:41:24.178810  5064 solver.cpp:228]     Train net output #0: softmax = 3.12506 (* 1 = 3.12506 loss)
I0623 12:41:24.178815  5064 solver.cpp:473] Iteration 14120, lr = 0.0001
I0623 12:41:25.199653  5064 solver.cpp:213] Iteration 14130, loss = 3.02239
I0623 12:41:25.199676  5064 solver.cpp:228]     Train net output #0: softmax = 3.02239 (* 1 = 3.02239 loss)
I0623 12:41:25.199681  5064 solver.cpp:473] Iteration 14130, lr = 0.0001
I0623 12:41:26.219383  5064 solver.cpp:213] Iteration 14140, loss = 3.14229
I0623 12:41:26.219401  5064 solver.cpp:228]     Train net output #0: softmax = 3.14229 (* 1 = 3.14229 loss)
I0623 12:41:26.219406  5064 solver.cpp:473] Iteration 14140, lr = 0.0001
I0623 12:41:27.240253  5064 solver.cpp:213] Iteration 14150, loss = 3.02623
I0623 12:41:27.240277  5064 solver.cpp:228]     Train net output #0: softmax = 3.02623 (* 1 = 3.02623 loss)
I0623 12:41:27.240401  5064 solver.cpp:473] Iteration 14150, lr = 0.0001
I0623 12:41:28.260540  5064 solver.cpp:213] Iteration 14160, loss = 3.18619
I0623 12:41:28.260560  5064 solver.cpp:228]     Train net output #0: softmax = 3.18619 (* 1 = 3.18619 loss)
I0623 12:41:28.260565  5064 solver.cpp:473] Iteration 14160, lr = 0.0001
I0623 12:41:29.281913  5064 solver.cpp:213] Iteration 14170, loss = 2.78461
I0623 12:41:29.281944  5064 solver.cpp:228]     Train net output #0: softmax = 2.78461 (* 1 = 2.78461 loss)
I0623 12:41:29.281950  5064 solver.cpp:473] Iteration 14170, lr = 0.0001
I0623 12:41:30.302788  5064 solver.cpp:213] Iteration 14180, loss = 3.44597
I0623 12:41:30.302817  5064 solver.cpp:228]     Train net output #0: softmax = 3.44597 (* 1 = 3.44597 loss)
I0623 12:41:30.302824  5064 solver.cpp:473] Iteration 14180, lr = 0.0001
I0623 12:41:31.321586  5064 solver.cpp:213] Iteration 14190, loss = 2.82557
I0623 12:41:31.321609  5064 solver.cpp:228]     Train net output #0: softmax = 2.82557 (* 1 = 2.82557 loss)
I0623 12:41:31.321614  5064 solver.cpp:473] Iteration 14190, lr = 0.0001
I0623 12:41:32.342718  5064 solver.cpp:213] Iteration 14200, loss = 3.09141
I0623 12:41:32.342741  5064 solver.cpp:228]     Train net output #0: softmax = 3.09141 (* 1 = 3.09141 loss)
I0623 12:41:32.342746  5064 solver.cpp:473] Iteration 14200, lr = 0.0001
I0623 12:41:33.363754  5064 solver.cpp:213] Iteration 14210, loss = 2.86921
I0623 12:41:33.363776  5064 solver.cpp:228]     Train net output #0: softmax = 2.86921 (* 1 = 2.86921 loss)
I0623 12:41:33.363781  5064 solver.cpp:473] Iteration 14210, lr = 0.0001
I0623 12:41:34.384379  5064 solver.cpp:213] Iteration 14220, loss = 2.77209
I0623 12:41:34.384400  5064 solver.cpp:228]     Train net output #0: softmax = 2.77209 (* 1 = 2.77209 loss)
I0623 12:41:34.384405  5064 solver.cpp:473] Iteration 14220, lr = 0.0001
I0623 12:41:35.404247  5064 solver.cpp:213] Iteration 14230, loss = 3.03279
I0623 12:41:35.404270  5064 solver.cpp:228]     Train net output #0: softmax = 3.03279 (* 1 = 3.03279 loss)
I0623 12:41:35.404275  5064 solver.cpp:473] Iteration 14230, lr = 0.0001
I0623 12:41:36.424772  5064 solver.cpp:213] Iteration 14240, loss = 3.13422
I0623 12:41:36.424793  5064 solver.cpp:228]     Train net output #0: softmax = 3.13422 (* 1 = 3.13422 loss)
I0623 12:41:36.424798  5064 solver.cpp:473] Iteration 14240, lr = 0.0001
I0623 12:41:37.445592  5064 solver.cpp:213] Iteration 14250, loss = 3.20827
I0623 12:41:37.445618  5064 solver.cpp:228]     Train net output #0: softmax = 3.20827 (* 1 = 3.20827 loss)
I0623 12:41:37.445745  5064 solver.cpp:473] Iteration 14250, lr = 0.0001
I0623 12:41:38.466133  5064 solver.cpp:213] Iteration 14260, loss = 2.82257
I0623 12:41:38.466168  5064 solver.cpp:228]     Train net output #0: softmax = 2.82257 (* 1 = 2.82257 loss)
I0623 12:41:38.466173  5064 solver.cpp:473] Iteration 14260, lr = 0.0001
I0623 12:41:39.486670  5064 solver.cpp:213] Iteration 14270, loss = 2.87986
I0623 12:41:39.486690  5064 solver.cpp:228]     Train net output #0: softmax = 2.87986 (* 1 = 2.87986 loss)
I0623 12:41:39.486695  5064 solver.cpp:473] Iteration 14270, lr = 0.0001
I0623 12:41:40.507477  5064 solver.cpp:213] Iteration 14280, loss = 2.81327
I0623 12:41:40.507498  5064 solver.cpp:228]     Train net output #0: softmax = 2.81327 (* 1 = 2.81327 loss)
I0623 12:41:40.507503  5064 solver.cpp:473] Iteration 14280, lr = 0.0001
I0623 12:41:41.527631  5064 solver.cpp:213] Iteration 14290, loss = 3.261
I0623 12:41:41.527649  5064 solver.cpp:228]     Train net output #0: softmax = 3.261 (* 1 = 3.261 loss)
I0623 12:41:41.527654  5064 solver.cpp:473] Iteration 14290, lr = 0.0001
I0623 12:41:42.548244  5064 solver.cpp:213] Iteration 14300, loss = 3.05043
I0623 12:41:42.548267  5064 solver.cpp:228]     Train net output #0: softmax = 3.05043 (* 1 = 3.05043 loss)
I0623 12:41:42.548401  5064 solver.cpp:473] Iteration 14300, lr = 0.0001
I0623 12:41:43.569367  5064 solver.cpp:213] Iteration 14310, loss = 2.7093
I0623 12:41:43.569388  5064 solver.cpp:228]     Train net output #0: softmax = 2.7093 (* 1 = 2.7093 loss)
I0623 12:41:43.569393  5064 solver.cpp:473] Iteration 14310, lr = 0.0001
I0623 12:41:44.589731  5064 solver.cpp:213] Iteration 14320, loss = 3.11129
I0623 12:41:44.589750  5064 solver.cpp:228]     Train net output #0: softmax = 3.11129 (* 1 = 3.11129 loss)
I0623 12:41:44.589754  5064 solver.cpp:473] Iteration 14320, lr = 0.0001
I0623 12:41:45.610662  5064 solver.cpp:213] Iteration 14330, loss = 3.06586
I0623 12:41:45.610683  5064 solver.cpp:228]     Train net output #0: softmax = 3.06586 (* 1 = 3.06586 loss)
I0623 12:41:45.610689  5064 solver.cpp:473] Iteration 14330, lr = 0.0001
I0623 12:41:46.631553  5064 solver.cpp:213] Iteration 14340, loss = 3.05154
I0623 12:41:46.631579  5064 solver.cpp:228]     Train net output #0: softmax = 3.05154 (* 1 = 3.05154 loss)
I0623 12:41:46.631585  5064 solver.cpp:473] Iteration 14340, lr = 0.0001
I0623 12:41:47.651911  5064 solver.cpp:213] Iteration 14350, loss = 3.0723
I0623 12:41:47.651935  5064 solver.cpp:228]     Train net output #0: softmax = 3.0723 (* 1 = 3.0723 loss)
I0623 12:41:47.652076  5064 solver.cpp:473] Iteration 14350, lr = 0.0001
I0623 12:41:48.672482  5064 solver.cpp:213] Iteration 14360, loss = 2.9325
I0623 12:41:48.672502  5064 solver.cpp:228]     Train net output #0: softmax = 2.9325 (* 1 = 2.9325 loss)
I0623 12:41:48.672508  5064 solver.cpp:473] Iteration 14360, lr = 0.0001
I0623 12:41:49.693022  5064 solver.cpp:213] Iteration 14370, loss = 2.88857
I0623 12:41:49.693042  5064 solver.cpp:228]     Train net output #0: softmax = 2.88857 (* 1 = 2.88857 loss)
I0623 12:41:49.693047  5064 solver.cpp:473] Iteration 14370, lr = 0.0001
I0623 12:41:50.714074  5064 solver.cpp:213] Iteration 14380, loss = 2.79824
I0623 12:41:50.714093  5064 solver.cpp:228]     Train net output #0: softmax = 2.79824 (* 1 = 2.79824 loss)
I0623 12:41:50.714097  5064 solver.cpp:473] Iteration 14380, lr = 0.0001
I0623 12:41:51.732866  5064 solver.cpp:213] Iteration 14390, loss = 3.14648
I0623 12:41:51.732885  5064 solver.cpp:228]     Train net output #0: softmax = 3.14648 (* 1 = 3.14648 loss)
I0623 12:41:51.732892  5064 solver.cpp:473] Iteration 14390, lr = 0.0001
I0623 12:41:52.753772  5064 solver.cpp:213] Iteration 14400, loss = 2.93568
I0623 12:41:52.753974  5064 solver.cpp:228]     Train net output #0: softmax = 2.93568 (* 1 = 2.93568 loss)
I0623 12:41:52.753983  5064 solver.cpp:473] Iteration 14400, lr = 0.0001
I0623 12:41:53.774865  5064 solver.cpp:213] Iteration 14410, loss = 2.98401
I0623 12:41:53.774885  5064 solver.cpp:228]     Train net output #0: softmax = 2.98401 (* 1 = 2.98401 loss)
I0623 12:41:53.774890  5064 solver.cpp:473] Iteration 14410, lr = 0.0001
I0623 12:41:54.795325  5064 solver.cpp:213] Iteration 14420, loss = 3.1019
I0623 12:41:54.795346  5064 solver.cpp:228]     Train net output #0: softmax = 3.1019 (* 1 = 3.1019 loss)
I0623 12:41:54.795351  5064 solver.cpp:473] Iteration 14420, lr = 0.0001
I0623 12:41:55.815953  5064 solver.cpp:213] Iteration 14430, loss = 3.04882
I0623 12:41:55.815973  5064 solver.cpp:228]     Train net output #0: softmax = 3.04882 (* 1 = 3.04882 loss)
I0623 12:41:55.815979  5064 solver.cpp:473] Iteration 14430, lr = 0.0001
I0623 12:41:56.836655  5064 solver.cpp:213] Iteration 14440, loss = 3.12609
I0623 12:41:56.836683  5064 solver.cpp:228]     Train net output #0: softmax = 3.12609 (* 1 = 3.12609 loss)
I0623 12:41:56.836688  5064 solver.cpp:473] Iteration 14440, lr = 0.0001
I0623 12:41:57.857636  5064 solver.cpp:213] Iteration 14450, loss = 3.14483
I0623 12:41:57.857657  5064 solver.cpp:228]     Train net output #0: softmax = 3.14483 (* 1 = 3.14483 loss)
I0623 12:41:57.857792  5064 solver.cpp:473] Iteration 14450, lr = 0.0001
I0623 12:41:58.878451  5064 solver.cpp:213] Iteration 14460, loss = 2.86346
I0623 12:41:58.878468  5064 solver.cpp:228]     Train net output #0: softmax = 2.86346 (* 1 = 2.86346 loss)
I0623 12:41:58.878473  5064 solver.cpp:473] Iteration 14460, lr = 0.0001
I0623 12:41:59.899158  5064 solver.cpp:213] Iteration 14470, loss = 2.88954
I0623 12:41:59.899174  5064 solver.cpp:228]     Train net output #0: softmax = 2.88954 (* 1 = 2.88954 loss)
I0623 12:41:59.899179  5064 solver.cpp:473] Iteration 14470, lr = 0.0001
I0623 12:42:00.920315  5064 solver.cpp:213] Iteration 14480, loss = 3.19445
I0623 12:42:00.920334  5064 solver.cpp:228]     Train net output #0: softmax = 3.19445 (* 1 = 3.19445 loss)
I0623 12:42:00.920339  5064 solver.cpp:473] Iteration 14480, lr = 0.0001
I0623 12:42:01.941490  5064 solver.cpp:213] Iteration 14490, loss = 3.24168
I0623 12:42:01.941514  5064 solver.cpp:228]     Train net output #0: softmax = 3.24168 (* 1 = 3.24168 loss)
I0623 12:42:01.941519  5064 solver.cpp:473] Iteration 14490, lr = 0.0001
I0623 12:42:02.962308  5064 solver.cpp:213] Iteration 14500, loss = 3.11365
I0623 12:42:02.962471  5064 solver.cpp:228]     Train net output #0: softmax = 3.11365 (* 1 = 3.11365 loss)
I0623 12:42:02.962478  5064 solver.cpp:473] Iteration 14500, lr = 0.0001
I0623 12:42:03.980290  5064 solver.cpp:213] Iteration 14510, loss = 3.20057
I0623 12:42:03.980309  5064 solver.cpp:228]     Train net output #0: softmax = 3.20057 (* 1 = 3.20057 loss)
I0623 12:42:03.980314  5064 solver.cpp:473] Iteration 14510, lr = 0.0001
I0623 12:42:05.000936  5064 solver.cpp:213] Iteration 14520, loss = 3.11012
I0623 12:42:05.000953  5064 solver.cpp:228]     Train net output #0: softmax = 3.11012 (* 1 = 3.11012 loss)
I0623 12:42:05.000958  5064 solver.cpp:473] Iteration 14520, lr = 0.0001
I0623 12:42:06.020990  5064 solver.cpp:213] Iteration 14530, loss = 3.03711
I0623 12:42:06.021011  5064 solver.cpp:228]     Train net output #0: softmax = 3.03711 (* 1 = 3.03711 loss)
I0623 12:42:06.021016  5064 solver.cpp:473] Iteration 14530, lr = 0.0001
I0623 12:42:07.041657  5064 solver.cpp:213] Iteration 14540, loss = 2.98577
I0623 12:42:07.041677  5064 solver.cpp:228]     Train net output #0: softmax = 2.98577 (* 1 = 2.98577 loss)
I0623 12:42:07.041682  5064 solver.cpp:473] Iteration 14540, lr = 0.0001
I0623 12:42:08.062136  5064 solver.cpp:213] Iteration 14550, loss = 3.25978
I0623 12:42:08.062158  5064 solver.cpp:228]     Train net output #0: softmax = 3.25978 (* 1 = 3.25978 loss)
I0623 12:42:08.062280  5064 solver.cpp:473] Iteration 14550, lr = 0.0001
I0623 12:42:09.083647  5064 solver.cpp:213] Iteration 14560, loss = 3.27302
I0623 12:42:09.083685  5064 solver.cpp:228]     Train net output #0: softmax = 3.27302 (* 1 = 3.27302 loss)
I0623 12:42:09.083690  5064 solver.cpp:473] Iteration 14560, lr = 0.0001
I0623 12:42:10.104607  5064 solver.cpp:213] Iteration 14570, loss = 3.29895
I0623 12:42:10.104629  5064 solver.cpp:228]     Train net output #0: softmax = 3.29895 (* 1 = 3.29895 loss)
I0623 12:42:10.104634  5064 solver.cpp:473] Iteration 14570, lr = 0.0001
I0623 12:42:11.125039  5064 solver.cpp:213] Iteration 14580, loss = 3.11192
I0623 12:42:11.125059  5064 solver.cpp:228]     Train net output #0: softmax = 3.11192 (* 1 = 3.11192 loss)
I0623 12:42:11.125064  5064 solver.cpp:473] Iteration 14580, lr = 0.0001
I0623 12:42:12.145582  5064 solver.cpp:213] Iteration 14590, loss = 2.8108
I0623 12:42:12.145603  5064 solver.cpp:228]     Train net output #0: softmax = 2.8108 (* 1 = 2.8108 loss)
I0623 12:42:12.145608  5064 solver.cpp:473] Iteration 14590, lr = 0.0001
I0623 12:42:13.166447  5064 solver.cpp:213] Iteration 14600, loss = 2.84483
I0623 12:42:13.166473  5064 solver.cpp:228]     Train net output #0: softmax = 2.84483 (* 1 = 2.84483 loss)
I0623 12:42:13.166606  5064 solver.cpp:473] Iteration 14600, lr = 0.0001
I0623 12:42:14.187438  5064 solver.cpp:213] Iteration 14610, loss = 2.93943
I0623 12:42:14.187455  5064 solver.cpp:228]     Train net output #0: softmax = 2.93943 (* 1 = 2.93943 loss)
I0623 12:42:14.187461  5064 solver.cpp:473] Iteration 14610, lr = 0.0001
I0623 12:42:15.207820  5064 solver.cpp:213] Iteration 14620, loss = 3.04594
I0623 12:42:15.207841  5064 solver.cpp:228]     Train net output #0: softmax = 3.04594 (* 1 = 3.04594 loss)
I0623 12:42:15.207847  5064 solver.cpp:473] Iteration 14620, lr = 0.0001
I0623 12:42:16.227881  5064 solver.cpp:213] Iteration 14630, loss = 3.02331
I0623 12:42:16.227901  5064 solver.cpp:228]     Train net output #0: softmax = 3.02331 (* 1 = 3.02331 loss)
I0623 12:42:16.227906  5064 solver.cpp:473] Iteration 14630, lr = 0.0001
I0623 12:42:17.248056  5064 solver.cpp:213] Iteration 14640, loss = 3.14072
I0623 12:42:17.248076  5064 solver.cpp:228]     Train net output #0: softmax = 3.14072 (* 1 = 3.14072 loss)
I0623 12:42:17.248081  5064 solver.cpp:473] Iteration 14640, lr = 0.0001
I0623 12:42:18.269059  5064 solver.cpp:213] Iteration 14650, loss = 2.74917
I0623 12:42:18.269083  5064 solver.cpp:228]     Train net output #0: softmax = 2.74917 (* 1 = 2.74917 loss)
I0623 12:42:18.269207  5064 solver.cpp:473] Iteration 14650, lr = 0.0001
I0623 12:42:19.289407  5064 solver.cpp:213] Iteration 14660, loss = 2.67346
I0623 12:42:19.289436  5064 solver.cpp:228]     Train net output #0: softmax = 2.67346 (* 1 = 2.67346 loss)
I0623 12:42:19.289441  5064 solver.cpp:473] Iteration 14660, lr = 0.0001
I0623 12:42:20.309852  5064 solver.cpp:213] Iteration 14670, loss = 2.9737
I0623 12:42:20.309872  5064 solver.cpp:228]     Train net output #0: softmax = 2.9737 (* 1 = 2.9737 loss)
I0623 12:42:20.309877  5064 solver.cpp:473] Iteration 14670, lr = 0.0001
I0623 12:42:21.330617  5064 solver.cpp:213] Iteration 14680, loss = 3.29341
I0623 12:42:21.330636  5064 solver.cpp:228]     Train net output #0: softmax = 3.29341 (* 1 = 3.29341 loss)
I0623 12:42:21.330641  5064 solver.cpp:473] Iteration 14680, lr = 0.0001
I0623 12:42:22.351732  5064 solver.cpp:213] Iteration 14690, loss = 3.01079
I0623 12:42:22.351753  5064 solver.cpp:228]     Train net output #0: softmax = 3.01079 (* 1 = 3.01079 loss)
I0623 12:42:22.351759  5064 solver.cpp:473] Iteration 14690, lr = 0.0001
I0623 12:42:23.372447  5064 solver.cpp:213] Iteration 14700, loss = 2.75977
I0623 12:42:23.372486  5064 solver.cpp:228]     Train net output #0: softmax = 2.75977 (* 1 = 2.75977 loss)
I0623 12:42:23.372493  5064 solver.cpp:473] Iteration 14700, lr = 0.0001
I0623 12:42:24.392891  5064 solver.cpp:213] Iteration 14710, loss = 3.0569
I0623 12:42:24.392909  5064 solver.cpp:228]     Train net output #0: softmax = 3.0569 (* 1 = 3.0569 loss)
I0623 12:42:24.392915  5064 solver.cpp:473] Iteration 14710, lr = 0.0001
I0623 12:42:25.413781  5064 solver.cpp:213] Iteration 14720, loss = 2.99137
I0623 12:42:25.413802  5064 solver.cpp:228]     Train net output #0: softmax = 2.99137 (* 1 = 2.99137 loss)
I0623 12:42:25.413807  5064 solver.cpp:473] Iteration 14720, lr = 0.0001
I0623 12:42:26.434887  5064 solver.cpp:213] Iteration 14730, loss = 2.90919
I0623 12:42:26.434911  5064 solver.cpp:228]     Train net output #0: softmax = 2.90919 (* 1 = 2.90919 loss)
I0623 12:42:26.434916  5064 solver.cpp:473] Iteration 14730, lr = 0.0001
I0623 12:42:27.455198  5064 solver.cpp:213] Iteration 14740, loss = 2.9795
I0623 12:42:27.455216  5064 solver.cpp:228]     Train net output #0: softmax = 2.9795 (* 1 = 2.9795 loss)
I0623 12:42:27.455221  5064 solver.cpp:473] Iteration 14740, lr = 0.0001
I0623 12:42:28.475980  5064 solver.cpp:213] Iteration 14750, loss = 3.07012
I0623 12:42:28.476001  5064 solver.cpp:228]     Train net output #0: softmax = 3.07012 (* 1 = 3.07012 loss)
I0623 12:42:28.476157  5064 solver.cpp:473] Iteration 14750, lr = 0.0001
I0623 12:42:29.496649  5064 solver.cpp:213] Iteration 14760, loss = 3.10494
I0623 12:42:29.496665  5064 solver.cpp:228]     Train net output #0: softmax = 3.10494 (* 1 = 3.10494 loss)
I0623 12:42:29.496670  5064 solver.cpp:473] Iteration 14760, lr = 0.0001
I0623 12:42:30.516783  5064 solver.cpp:213] Iteration 14770, loss = 2.96742
I0623 12:42:30.516798  5064 solver.cpp:228]     Train net output #0: softmax = 2.96742 (* 1 = 2.96742 loss)
I0623 12:42:30.516803  5064 solver.cpp:473] Iteration 14770, lr = 0.0001
I0623 12:42:31.537958  5064 solver.cpp:213] Iteration 14780, loss = 3.15153
I0623 12:42:31.537977  5064 solver.cpp:228]     Train net output #0: softmax = 3.15153 (* 1 = 3.15153 loss)
I0623 12:42:31.537983  5064 solver.cpp:473] Iteration 14780, lr = 0.0001
I0623 12:42:32.558284  5064 solver.cpp:213] Iteration 14790, loss = 2.96247
I0623 12:42:32.558300  5064 solver.cpp:228]     Train net output #0: softmax = 2.96247 (* 1 = 2.96247 loss)
I0623 12:42:32.558305  5064 solver.cpp:473] Iteration 14790, lr = 0.0001
I0623 12:42:33.578570  5064 solver.cpp:213] Iteration 14800, loss = 2.93577
I0623 12:42:33.578595  5064 solver.cpp:228]     Train net output #0: softmax = 2.93577 (* 1 = 2.93577 loss)
I0623 12:42:33.578723  5064 solver.cpp:473] Iteration 14800, lr = 0.0001
I0623 12:42:34.599313  5064 solver.cpp:213] Iteration 14810, loss = 3.11073
I0623 12:42:34.599330  5064 solver.cpp:228]     Train net output #0: softmax = 3.11073 (* 1 = 3.11073 loss)
I0623 12:42:34.599336  5064 solver.cpp:473] Iteration 14810, lr = 0.0001
I0623 12:42:35.619715  5064 solver.cpp:213] Iteration 14820, loss = 2.89228
I0623 12:42:35.619740  5064 solver.cpp:228]     Train net output #0: softmax = 2.89228 (* 1 = 2.89228 loss)
I0623 12:42:35.619745  5064 solver.cpp:473] Iteration 14820, lr = 0.0001
I0623 12:42:36.640630  5064 solver.cpp:213] Iteration 14830, loss = 3.1533
I0623 12:42:36.640650  5064 solver.cpp:228]     Train net output #0: softmax = 3.1533 (* 1 = 3.1533 loss)
I0623 12:42:36.640655  5064 solver.cpp:473] Iteration 14830, lr = 0.0001
I0623 12:42:37.660565  5064 solver.cpp:213] Iteration 14840, loss = 2.91993
I0623 12:42:37.660583  5064 solver.cpp:228]     Train net output #0: softmax = 2.91993 (* 1 = 2.91993 loss)
I0623 12:42:37.660588  5064 solver.cpp:473] Iteration 14840, lr = 0.0001
I0623 12:42:38.681259  5064 solver.cpp:213] Iteration 14850, loss = 2.85
I0623 12:42:38.681279  5064 solver.cpp:228]     Train net output #0: softmax = 2.85 (* 1 = 2.85 loss)
I0623 12:42:38.681404  5064 solver.cpp:473] Iteration 14850, lr = 0.0001
I0623 12:42:39.701863  5064 solver.cpp:213] Iteration 14860, loss = 3.06563
I0623 12:42:39.701881  5064 solver.cpp:228]     Train net output #0: softmax = 3.06563 (* 1 = 3.06563 loss)
I0623 12:42:39.701902  5064 solver.cpp:473] Iteration 14860, lr = 0.0001
I0623 12:42:40.722291  5064 solver.cpp:213] Iteration 14870, loss = 2.97235
I0623 12:42:40.722316  5064 solver.cpp:228]     Train net output #0: softmax = 2.97235 (* 1 = 2.97235 loss)
I0623 12:42:40.722321  5064 solver.cpp:473] Iteration 14870, lr = 0.0001
I0623 12:42:41.742228  5064 solver.cpp:213] Iteration 14880, loss = 3.15403
I0623 12:42:41.742247  5064 solver.cpp:228]     Train net output #0: softmax = 3.15403 (* 1 = 3.15403 loss)
I0623 12:42:41.742252  5064 solver.cpp:473] Iteration 14880, lr = 0.0001
I0623 12:42:42.763161  5064 solver.cpp:213] Iteration 14890, loss = 2.89267
I0623 12:42:42.763180  5064 solver.cpp:228]     Train net output #0: softmax = 2.89267 (* 1 = 2.89267 loss)
I0623 12:42:42.763186  5064 solver.cpp:473] Iteration 14890, lr = 0.0001
I0623 12:42:43.782711  5064 solver.cpp:213] Iteration 14900, loss = 2.99569
I0623 12:42:43.782730  5064 solver.cpp:228]     Train net output #0: softmax = 2.99569 (* 1 = 2.99569 loss)
I0623 12:42:43.782865  5064 solver.cpp:473] Iteration 14900, lr = 0.0001
I0623 12:42:44.803496  5064 solver.cpp:213] Iteration 14910, loss = 3.00307
I0623 12:42:44.803514  5064 solver.cpp:228]     Train net output #0: softmax = 3.00307 (* 1 = 3.00307 loss)
I0623 12:42:44.803519  5064 solver.cpp:473] Iteration 14910, lr = 0.0001
I0623 12:42:45.824281  5064 solver.cpp:213] Iteration 14920, loss = 3.18113
I0623 12:42:45.824301  5064 solver.cpp:228]     Train net output #0: softmax = 3.18113 (* 1 = 3.18113 loss)
I0623 12:42:45.824306  5064 solver.cpp:473] Iteration 14920, lr = 0.0001
I0623 12:42:46.844483  5064 solver.cpp:213] Iteration 14930, loss = 2.74226
I0623 12:42:46.844504  5064 solver.cpp:228]     Train net output #0: softmax = 2.74226 (* 1 = 2.74226 loss)
I0623 12:42:46.844509  5064 solver.cpp:473] Iteration 14930, lr = 0.0001
I0623 12:42:47.865036  5064 solver.cpp:213] Iteration 14940, loss = 3.14213
I0623 12:42:47.865057  5064 solver.cpp:228]     Train net output #0: softmax = 3.14213 (* 1 = 3.14213 loss)
I0623 12:42:47.865062  5064 solver.cpp:473] Iteration 14940, lr = 0.0001
I0623 12:42:48.885710  5064 solver.cpp:213] Iteration 14950, loss = 3.06751
I0623 12:42:48.885736  5064 solver.cpp:228]     Train net output #0: softmax = 3.06751 (* 1 = 3.06751 loss)
I0623 12:42:48.885859  5064 solver.cpp:473] Iteration 14950, lr = 0.0001
I0623 12:42:49.906255  5064 solver.cpp:213] Iteration 14960, loss = 3.03644
I0623 12:42:49.906275  5064 solver.cpp:228]     Train net output #0: softmax = 3.03644 (* 1 = 3.03644 loss)
I0623 12:42:49.906280  5064 solver.cpp:473] Iteration 14960, lr = 0.0001
I0623 12:42:50.926347  5064 solver.cpp:213] Iteration 14970, loss = 3.13773
I0623 12:42:50.926364  5064 solver.cpp:228]     Train net output #0: softmax = 3.13773 (* 1 = 3.13773 loss)
I0623 12:42:50.926369  5064 solver.cpp:473] Iteration 14970, lr = 0.0001
I0623 12:42:51.946467  5064 solver.cpp:213] Iteration 14980, loss = 2.70746
I0623 12:42:51.946492  5064 solver.cpp:228]     Train net output #0: softmax = 2.70746 (* 1 = 2.70746 loss)
I0623 12:42:51.946498  5064 solver.cpp:473] Iteration 14980, lr = 0.0001
I0623 12:42:52.966567  5064 solver.cpp:213] Iteration 14990, loss = 3.05743
I0623 12:42:52.966588  5064 solver.cpp:228]     Train net output #0: softmax = 3.05743 (* 1 = 3.05743 loss)
I0623 12:42:52.966593  5064 solver.cpp:473] Iteration 14990, lr = 0.0001
I0623 12:42:53.916910  5064 solver.cpp:362] Snapshotting to snapshots/16-06-21_16h05m31s_0_11_pretrainClassification_iter_15000.caffemodel
I0623 12:42:53.918082  5064 solver.cpp:370] Snapshotting solver state to snapshots/16-06-21_16h05m31s_0_11_pretrainClassification_iter_15000.solverstate
I0623 12:42:53.918689  5064 solver.cpp:291] Iteration 15000, Testing net (#0)
I0623 12:42:54.079610  5064 solver.cpp:342]     Test net output #0: accuracy_top_1 = 0.2625
I0623 12:42:54.079625  5064 solver.cpp:342]     Test net output #1: accuracy_top_5 = 0.56875
I0623 12:42:54.079632  5064 solver.cpp:342]     Test net output #2: softmax = 3.01252 (* 1 = 3.01252 loss)
I0623 12:42:54.150284  5064 solver.cpp:213] Iteration 15000, loss = 2.96011
I0623 12:42:54.150298  5064 solver.cpp:228]     Train net output #0: softmax = 2.96011 (* 1 = 2.96011 loss)
I0623 12:42:54.150303  5064 solver.cpp:473] Iteration 15000, lr = 0.0001
I0623 12:42:55.169950  5064 solver.cpp:213] Iteration 15010, loss = 2.95779
I0623 12:42:55.169972  5064 solver.cpp:228]     Train net output #0: softmax = 2.95779 (* 1 = 2.95779 loss)
I0623 12:42:55.169978  5064 solver.cpp:473] Iteration 15010, lr = 0.0001
I0623 12:42:56.189178  5064 solver.cpp:213] Iteration 15020, loss = 2.81891
I0623 12:42:56.189199  5064 solver.cpp:228]     Train net output #0: softmax = 2.81891 (* 1 = 2.81891 loss)
I0623 12:42:56.189204  5064 solver.cpp:473] Iteration 15020, lr = 0.0001
I0623 12:42:57.210396  5064 solver.cpp:213] Iteration 15030, loss = 2.93727
I0623 12:42:57.210414  5064 solver.cpp:228]     Train net output #0: softmax = 2.93727 (* 1 = 2.93727 loss)
I0623 12:42:57.210420  5064 solver.cpp:473] Iteration 15030, lr = 0.0001
I0623 12:42:58.231118  5064 solver.cpp:213] Iteration 15040, loss = 2.78482
I0623 12:42:58.231137  5064 solver.cpp:228]     Train net output #0: softmax = 2.78482 (* 1 = 2.78482 loss)
I0623 12:42:58.231142  5064 solver.cpp:473] Iteration 15040, lr = 0.0001
I0623 12:42:59.251808  5064 solver.cpp:213] Iteration 15050, loss = 2.96582
I0623 12:42:59.251834  5064 solver.cpp:228]     Train net output #0: softmax = 2.96582 (* 1 = 2.96582 loss)
I0623 12:42:59.251845  5064 solver.cpp:473] Iteration 15050, lr = 0.0001
I0623 12:43:00.272186  5064 solver.cpp:213] Iteration 15060, loss = 2.92813
I0623 12:43:00.272208  5064 solver.cpp:228]     Train net output #0: softmax = 2.92813 (* 1 = 2.92813 loss)
I0623 12:43:00.272213  5064 solver.cpp:473] Iteration 15060, lr = 0.0001
I0623 12:43:01.292925  5064 solver.cpp:213] Iteration 15070, loss = 3.19951
I0623 12:43:01.292946  5064 solver.cpp:228]     Train net output #0: softmax = 3.19951 (* 1 = 3.19951 loss)
I0623 12:43:01.292951  5064 solver.cpp:473] Iteration 15070, lr = 0.0001
I0623 12:43:02.313311  5064 solver.cpp:213] Iteration 15080, loss = 3.12556
I0623 12:43:02.313330  5064 solver.cpp:228]     Train net output #0: softmax = 3.12556 (* 1 = 3.12556 loss)
I0623 12:43:02.313336  5064 solver.cpp:473] Iteration 15080, lr = 0.0001
I0623 12:43:03.334064  5064 solver.cpp:213] Iteration 15090, loss = 2.83289
I0623 12:43:03.334084  5064 solver.cpp:228]     Train net output #0: softmax = 2.83289 (* 1 = 2.83289 loss)
I0623 12:43:03.334089  5064 solver.cpp:473] Iteration 15090, lr = 0.0001
I0623 12:43:04.354734  5064 solver.cpp:213] Iteration 15100, loss = 2.9585
I0623 12:43:04.354754  5064 solver.cpp:228]     Train net output #0: softmax = 2.9585 (* 1 = 2.9585 loss)
I0623 12:43:04.354759  5064 solver.cpp:473] Iteration 15100, lr = 0.0001
I0623 12:43:05.375470  5064 solver.cpp:213] Iteration 15110, loss = 3.01049
I0623 12:43:05.375494  5064 solver.cpp:228]     Train net output #0: softmax = 3.01049 (* 1 = 3.01049 loss)
I0623 12:43:05.375505  5064 solver.cpp:473] Iteration 15110, lr = 0.0001
I0623 12:43:06.396014  5064 solver.cpp:213] Iteration 15120, loss = 2.86447
I0623 12:43:06.396035  5064 solver.cpp:228]     Train net output #0: softmax = 2.86447 (* 1 = 2.86447 loss)
I0623 12:43:06.396041  5064 solver.cpp:473] Iteration 15120, lr = 0.0001
I0623 12:43:07.416597  5064 solver.cpp:213] Iteration 15130, loss = 2.77017
I0623 12:43:07.416616  5064 solver.cpp:228]     Train net output #0: softmax = 2.77017 (* 1 = 2.77017 loss)
I0623 12:43:07.416622  5064 solver.cpp:473] Iteration 15130, lr = 0.0001
I0623 12:43:08.437769  5064 solver.cpp:213] Iteration 15140, loss = 3.1599
I0623 12:43:08.437790  5064 solver.cpp:228]     Train net output #0: softmax = 3.1599 (* 1 = 3.1599 loss)
I0623 12:43:08.437795  5064 solver.cpp:473] Iteration 15140, lr = 0.0001
I0623 12:43:09.458566  5064 solver.cpp:213] Iteration 15150, loss = 3.09373
I0623 12:43:09.458587  5064 solver.cpp:228]     Train net output #0: softmax = 3.09373 (* 1 = 3.09373 loss)
I0623 12:43:09.458719  5064 solver.cpp:473] Iteration 15150, lr = 0.0001
I0623 12:43:10.479632  5064 solver.cpp:213] Iteration 15160, loss = 3.55251
I0623 12:43:10.479650  5064 solver.cpp:228]     Train net output #0: softmax = 3.55251 (* 1 = 3.55251 loss)
I0623 12:43:10.479655  5064 solver.cpp:473] Iteration 15160, lr = 0.0001
I0623 12:43:11.500937  5064 solver.cpp:213] Iteration 15170, loss = 3.0972
I0623 12:43:11.500960  5064 solver.cpp:228]     Train net output #0: softmax = 3.0972 (* 1 = 3.0972 loss)
I0623 12:43:11.500965  5064 solver.cpp:473] Iteration 15170, lr = 0.0001
I0623 12:43:12.522243  5064 solver.cpp:213] Iteration 15180, loss = 2.95711
I0623 12:43:12.522264  5064 solver.cpp:228]     Train net output #0: softmax = 2.95711 (* 1 = 2.95711 loss)
I0623 12:43:12.522269  5064 solver.cpp:473] Iteration 15180, lr = 0.0001
I0623 12:43:13.543592  5064 solver.cpp:213] Iteration 15190, loss = 3.00577
I0623 12:43:13.543613  5064 solver.cpp:228]     Train net output #0: softmax = 3.00577 (* 1 = 3.00577 loss)
I0623 12:43:13.543618  5064 solver.cpp:473] Iteration 15190, lr = 0.0001
I0623 12:43:14.564225  5064 solver.cpp:213] Iteration 15200, loss = 3.23901
I0623 12:43:14.564257  5064 solver.cpp:228]     Train net output #0: softmax = 3.23901 (* 1 = 3.23901 loss)
I0623 12:43:14.564415  5064 solver.cpp:473] Iteration 15200, lr = 0.0001
I0623 12:43:15.584872  5064 solver.cpp:213] Iteration 15210, loss = 2.94586
I0623 12:43:15.584892  5064 solver.cpp:228]     Train net output #0: softmax = 2.94586 (* 1 = 2.94586 loss)
I0623 12:43:15.584897  5064 solver.cpp:473] Iteration 15210, lr = 0.0001
I0623 12:43:16.605526  5064 solver.cpp:213] Iteration 15220, loss = 2.9641
I0623 12:43:16.605548  5064 solver.cpp:228]     Train net output #0: softmax = 2.9641 (* 1 = 2.9641 loss)
I0623 12:43:16.605554  5064 solver.cpp:473] Iteration 15220, lr = 0.0001
I0623 12:43:17.626739  5064 solver.cpp:213] Iteration 15230, loss = 2.94512
I0623 12:43:17.626761  5064 solver.cpp:228]     Train net output #0: softmax = 2.94512 (* 1 = 2.94512 loss)
I0623 12:43:17.626766  5064 solver.cpp:473] Iteration 15230, lr = 0.0001
I0623 12:43:18.647954  5064 solver.cpp:213] Iteration 15240, loss = 3.00148
I0623 12:43:18.647974  5064 solver.cpp:228]     Train net output #0: softmax = 3.00148 (* 1 = 3.00148 loss)
I0623 12:43:18.647979  5064 solver.cpp:473] Iteration 15240, lr = 0.0001
I0623 12:43:19.669064  5064 solver.cpp:213] Iteration 15250, loss = 2.79889
I0623 12:43:19.669088  5064 solver.cpp:228]     Train net output #0: softmax = 2.79889 (* 1 = 2.79889 loss)
I0623 12:43:19.669214  5064 solver.cpp:473] Iteration 15250, lr = 0.0001
I0623 12:43:20.690532  5064 solver.cpp:213] Iteration 15260, loss = 2.99776
I0623 12:43:20.690553  5064 solver.cpp:228]     Train net output #0: softmax = 2.99776 (* 1 = 2.99776 loss)
I0623 12:43:20.690558  5064 solver.cpp:473] Iteration 15260, lr = 0.0001
I0623 12:43:21.711840  5064 solver.cpp:213] Iteration 15270, loss = 2.84906
I0623 12:43:21.711863  5064 solver.cpp:228]     Train net output #0: softmax = 2.84906 (* 1 = 2.84906 loss)
I0623 12:43:21.711874  5064 solver.cpp:473] Iteration 15270, lr = 0.0001
I0623 12:43:22.732127  5064 solver.cpp:213] Iteration 15280, loss = 2.76805
I0623 12:43:22.732147  5064 solver.cpp:228]     Train net output #0: softmax = 2.76805 (* 1 = 2.76805 loss)
I0623 12:43:22.732152  5064 solver.cpp:473] Iteration 15280, lr = 0.0001
I0623 12:43:23.752904  5064 solver.cpp:213] Iteration 15290, loss = 2.85443
I0623 12:43:23.752924  5064 solver.cpp:228]     Train net output #0: softmax = 2.85443 (* 1 = 2.85443 loss)
I0623 12:43:23.752929  5064 solver.cpp:473] Iteration 15290, lr = 0.0001
I0623 12:43:24.773332  5064 solver.cpp:213] Iteration 15300, loss = 2.79935
I0623 12:43:24.773495  5064 solver.cpp:228]     Train net output #0: softmax = 2.79935 (* 1 = 2.79935 loss)
I0623 12:43:24.773504  5064 solver.cpp:473] Iteration 15300, lr = 0.0001
I0623 12:43:25.793939  5064 solver.cpp:213] Iteration 15310, loss = 3.09752
I0623 12:43:25.793962  5064 solver.cpp:228]     Train net output #0: softmax = 3.09752 (* 1 = 3.09752 loss)
I0623 12:43:25.793967  5064 solver.cpp:473] Iteration 15310, lr = 0.0001
I0623 12:43:26.814307  5064 solver.cpp:213] Iteration 15320, loss = 3.16334
I0623 12:43:26.814329  5064 solver.cpp:228]     Train net output #0: softmax = 3.16334 (* 1 = 3.16334 loss)
I0623 12:43:26.814335  5064 solver.cpp:473] Iteration 15320, lr = 0.0001
I0623 12:43:27.832919  5064 solver.cpp:213] Iteration 15330, loss = 3.11514
I0623 12:43:27.832940  5064 solver.cpp:228]     Train net output #0: softmax = 3.11514 (* 1 = 3.11514 loss)
I0623 12:43:27.832945  5064 solver.cpp:473] Iteration 15330, lr = 0.0001
I0623 12:43:28.853950  5064 solver.cpp:213] Iteration 15340, loss = 2.59662
I0623 12:43:28.853971  5064 solver.cpp:228]     Train net output #0: softmax = 2.59662 (* 1 = 2.59662 loss)
I0623 12:43:28.853977  5064 solver.cpp:473] Iteration 15340, lr = 0.0001
I0623 12:43:29.875177  5064 solver.cpp:213] Iteration 15350, loss = 2.83806
I0623 12:43:29.875214  5064 solver.cpp:228]     Train net output #0: softmax = 2.83806 (* 1 = 2.83806 loss)
I0623 12:43:29.875363  5064 solver.cpp:473] Iteration 15350, lr = 0.0001
I0623 12:43:30.896178  5064 solver.cpp:213] Iteration 15360, loss = 3.06101
I0623 12:43:30.896195  5064 solver.cpp:228]     Train net output #0: softmax = 3.06101 (* 1 = 3.06101 loss)
I0623 12:43:30.896200  5064 solver.cpp:473] Iteration 15360, lr = 0.0001
I0623 12:43:31.917598  5064 solver.cpp:213] Iteration 15370, loss = 3.09722
I0623 12:43:31.917618  5064 solver.cpp:228]     Train net output #0: softmax = 3.09722 (* 1 = 3.09722 loss)
I0623 12:43:31.917623  5064 solver.cpp:473] Iteration 15370, lr = 0.0001
I0623 12:43:32.938480  5064 solver.cpp:213] Iteration 15380, loss = 2.94529
I0623 12:43:32.938501  5064 solver.cpp:228]     Train net output #0: softmax = 2.94529 (* 1 = 2.94529 loss)
I0623 12:43:32.938506  5064 solver.cpp:473] Iteration 15380, lr = 0.0001
I0623 12:43:33.958817  5064 solver.cpp:213] Iteration 15390, loss = 2.94777
I0623 12:43:33.958834  5064 solver.cpp:228]     Train net output #0: softmax = 2.94777 (* 1 = 2.94777 loss)
I0623 12:43:33.958839  5064 solver.cpp:473] Iteration 15390, lr = 0.0001
I0623 12:43:34.979920  5064 solver.cpp:213] Iteration 15400, loss = 3.07234
I0623 12:43:34.979944  5064 solver.cpp:228]     Train net output #0: softmax = 3.07234 (* 1 = 3.07234 loss)
I0623 12:43:34.980098  5064 solver.cpp:473] Iteration 15400, lr = 0.0001
I0623 12:43:36.000319  5064 solver.cpp:213] Iteration 15410, loss = 2.90999
I0623 12:43:36.000339  5064 solver.cpp:228]     Train net output #0: softmax = 2.90999 (* 1 = 2.90999 loss)
I0623 12:43:36.000345  5064 solver.cpp:473] Iteration 15410, lr = 0.0001
I0623 12:43:37.020561  5064 solver.cpp:213] Iteration 15420, loss = 2.88379
I0623 12:43:37.020583  5064 solver.cpp:228]     Train net output #0: softmax = 2.88379 (* 1 = 2.88379 loss)
I0623 12:43:37.020589  5064 solver.cpp:473] Iteration 15420, lr = 0.0001
I0623 12:43:38.042333  5064 solver.cpp:213] Iteration 15430, loss = 2.89818
I0623 12:43:38.042356  5064 solver.cpp:228]     Train net output #0: softmax = 2.89818 (* 1 = 2.89818 loss)
I0623 12:43:38.042367  5064 solver.cpp:473] Iteration 15430, lr = 0.0001
I0623 12:43:39.063810  5064 solver.cpp:213] Iteration 15440, loss = 2.95566
I0623 12:43:39.063833  5064 solver.cpp:228]     Train net output #0: softmax = 2.95566 (* 1 = 2.95566 loss)
I0623 12:43:39.063838  5064 solver.cpp:473] Iteration 15440, lr = 0.0001
I0623 12:43:40.084060  5064 solver.cpp:213] Iteration 15450, loss = 2.82952
I0623 12:43:40.084084  5064 solver.cpp:228]     Train net output #0: softmax = 2.82952 (* 1 = 2.82952 loss)
I0623 12:43:40.084211  5064 solver.cpp:473] Iteration 15450, lr = 0.0001
I0623 12:43:41.104706  5064 solver.cpp:213] Iteration 15460, loss = 3.27429
I0623 12:43:41.104745  5064 solver.cpp:228]     Train net output #0: softmax = 3.27429 (* 1 = 3.27429 loss)
I0623 12:43:41.104751  5064 solver.cpp:473] Iteration 15460, lr = 0.0001
I0623 12:43:42.125339  5064 solver.cpp:213] Iteration 15470, loss = 2.83078
I0623 12:43:42.125363  5064 solver.cpp:228]     Train net output #0: softmax = 2.83078 (* 1 = 2.83078 loss)
I0623 12:43:42.125368  5064 solver.cpp:473] Iteration 15470, lr = 0.0001
I0623 12:43:43.146139  5064 solver.cpp:213] Iteration 15480, loss = 2.81763
I0623 12:43:43.146160  5064 solver.cpp:228]     Train net output #0: softmax = 2.81763 (* 1 = 2.81763 loss)
I0623 12:43:43.146165  5064 solver.cpp:473] Iteration 15480, lr = 0.0001
I0623 12:43:44.167456  5064 solver.cpp:213] Iteration 15490, loss = 2.85173
I0623 12:43:44.167476  5064 solver.cpp:228]     Train net output #0: softmax = 2.85173 (* 1 = 2.85173 loss)
I0623 12:43:44.167481  5064 solver.cpp:473] Iteration 15490, lr = 0.0001
I0623 12:43:45.188244  5064 solver.cpp:213] Iteration 15500, loss = 2.96712
I0623 12:43:45.188268  5064 solver.cpp:228]     Train net output #0: softmax = 2.96712 (* 1 = 2.96712 loss)
I0623 12:43:45.188405  5064 solver.cpp:473] Iteration 15500, lr = 0.0001
I0623 12:43:46.208668  5064 solver.cpp:213] Iteration 15510, loss = 2.77594
I0623 12:43:46.208689  5064 solver.cpp:228]     Train net output #0: softmax = 2.77594 (* 1 = 2.77594 loss)
I0623 12:43:46.208694  5064 solver.cpp:473] Iteration 15510, lr = 0.0001
I0623 12:43:47.227660  5064 solver.cpp:213] Iteration 15520, loss = 2.87683
I0623 12:43:47.227684  5064 solver.cpp:228]     Train net output #0: softmax = 2.87683 (* 1 = 2.87683 loss)
I0623 12:43:47.227691  5064 solver.cpp:473] Iteration 15520, lr = 0.0001
I0623 12:43:48.247303  5064 solver.cpp:213] Iteration 15530, loss = 2.97374
I0623 12:43:48.247321  5064 solver.cpp:228]     Train net output #0: softmax = 2.97374 (* 1 = 2.97374 loss)
I0623 12:43:48.247326  5064 solver.cpp:473] Iteration 15530, lr = 0.0001
I0623 12:43:49.268726  5064 solver.cpp:213] Iteration 15540, loss = 2.95945
I0623 12:43:49.268746  5064 solver.cpp:228]     Train net output #0: softmax = 2.95945 (* 1 = 2.95945 loss)
I0623 12:43:49.268753  5064 solver.cpp:473] Iteration 15540, lr = 0.0001
I0623 12:43:50.289295  5064 solver.cpp:213] Iteration 15550, loss = 3.13861
I0623 12:43:50.289319  5064 solver.cpp:228]     Train net output #0: softmax = 3.13861 (* 1 = 3.13861 loss)
I0623 12:43:50.289443  5064 solver.cpp:473] Iteration 15550, lr = 0.0001
I0623 12:43:51.310365  5064 solver.cpp:213] Iteration 15560, loss = 3.03534
I0623 12:43:51.310386  5064 solver.cpp:228]     Train net output #0: softmax = 3.03534 (* 1 = 3.03534 loss)
I0623 12:43:51.310391  5064 solver.cpp:473] Iteration 15560, lr = 0.0001
I0623 12:43:52.331537  5064 solver.cpp:213] Iteration 15570, loss = 2.82321
I0623 12:43:52.331557  5064 solver.cpp:228]     Train net output #0: softmax = 2.82321 (* 1 = 2.82321 loss)
I0623 12:43:52.331562  5064 solver.cpp:473] Iteration 15570, lr = 0.0001
I0623 12:43:53.352417  5064 solver.cpp:213] Iteration 15580, loss = 2.89297
I0623 12:43:53.352439  5064 solver.cpp:228]     Train net output #0: softmax = 2.89297 (* 1 = 2.89297 loss)
I0623 12:43:53.352445  5064 solver.cpp:473] Iteration 15580, lr = 0.0001
I0623 12:43:54.373131  5064 solver.cpp:213] Iteration 15590, loss = 3.11955
I0623 12:43:54.373152  5064 solver.cpp:228]     Train net output #0: softmax = 3.11955 (* 1 = 3.11955 loss)
I0623 12:43:54.373164  5064 solver.cpp:473] Iteration 15590, lr = 0.0001
I0623 12:43:55.393940  5064 solver.cpp:213] Iteration 15600, loss = 2.97737
I0623 12:43:55.393983  5064 solver.cpp:228]     Train net output #0: softmax = 2.97737 (* 1 = 2.97737 loss)
I0623 12:43:55.393990  5064 solver.cpp:473] Iteration 15600, lr = 0.0001
I0623 12:43:56.414325  5064 solver.cpp:213] Iteration 15610, loss = 2.78062
I0623 12:43:56.414343  5064 solver.cpp:228]     Train net output #0: softmax = 2.78062 (* 1 = 2.78062 loss)
I0623 12:43:56.414348  5064 solver.cpp:473] Iteration 15610, lr = 0.0001
I0623 12:43:57.435071  5064 solver.cpp:213] Iteration 15620, loss = 3.02123
I0623 12:43:57.435091  5064 solver.cpp:228]     Train net output #0: softmax = 3.02123 (* 1 = 3.02123 loss)
I0623 12:43:57.435096  5064 solver.cpp:473] Iteration 15620, lr = 0.0001
I0623 12:43:58.456065  5064 solver.cpp:213] Iteration 15630, loss = 3.19534
I0623 12:43:58.456086  5064 solver.cpp:228]     Train net output #0: softmax = 3.19534 (* 1 = 3.19534 loss)
I0623 12:43:58.456091  5064 solver.cpp:473] Iteration 15630, lr = 0.0001
I0623 12:43:59.476547  5064 solver.cpp:213] Iteration 15640, loss = 3.07463
I0623 12:43:59.476570  5064 solver.cpp:228]     Train net output #0: softmax = 3.07463 (* 1 = 3.07463 loss)
I0623 12:43:59.476575  5064 solver.cpp:473] Iteration 15640, lr = 0.0001
I0623 12:44:00.496856  5064 solver.cpp:213] Iteration 15650, loss = 2.90171
I0623 12:44:00.496881  5064 solver.cpp:228]     Train net output #0: softmax = 2.90171 (* 1 = 2.90171 loss)
I0623 12:44:00.496891  5064 solver.cpp:473] Iteration 15650, lr = 0.0001
I0623 12:44:01.518061  5064 solver.cpp:213] Iteration 15660, loss = 3.01443
I0623 12:44:01.518082  5064 solver.cpp:228]     Train net output #0: softmax = 3.01443 (* 1 = 3.01443 loss)
I0623 12:44:01.518087  5064 solver.cpp:473] Iteration 15660, lr = 0.0001
I0623 12:44:02.538117  5064 solver.cpp:213] Iteration 15670, loss = 2.96257
I0623 12:44:02.538143  5064 solver.cpp:228]     Train net output #0: softmax = 2.96257 (* 1 = 2.96257 loss)
I0623 12:44:02.538149  5064 solver.cpp:473] Iteration 15670, lr = 0.0001
I0623 12:44:03.558249  5064 solver.cpp:213] Iteration 15680, loss = 2.76764
I0623 12:44:03.558269  5064 solver.cpp:228]     Train net output #0: softmax = 2.76764 (* 1 = 2.76764 loss)
I0623 12:44:03.558275  5064 solver.cpp:473] Iteration 15680, lr = 0.0001
I0623 12:44:04.578709  5064 solver.cpp:213] Iteration 15690, loss = 2.7455
I0623 12:44:04.578734  5064 solver.cpp:228]     Train net output #0: softmax = 2.7455 (* 1 = 2.7455 loss)
I0623 12:44:04.578740  5064 solver.cpp:473] Iteration 15690, lr = 0.0001
I0623 12:44:05.599797  5064 solver.cpp:213] Iteration 15700, loss = 2.7998
I0623 12:44:05.599822  5064 solver.cpp:228]     Train net output #0: softmax = 2.7998 (* 1 = 2.7998 loss)
I0623 12:44:05.599972  5064 solver.cpp:473] Iteration 15700, lr = 0.0001
I0623 12:44:06.620843  5064 solver.cpp:213] Iteration 15710, loss = 2.78403
I0623 12:44:06.620864  5064 solver.cpp:228]     Train net output #0: softmax = 2.78403 (* 1 = 2.78403 loss)
I0623 12:44:06.620870  5064 solver.cpp:473] Iteration 15710, lr = 0.0001
I0623 12:44:07.639183  5064 solver.cpp:213] Iteration 15720, loss = 3.09835
I0623 12:44:07.639200  5064 solver.cpp:228]     Train net output #0: softmax = 3.09835 (* 1 = 3.09835 loss)
I0623 12:44:07.639205  5064 solver.cpp:473] Iteration 15720, lr = 0.0001
I0623 12:44:08.659983  5064 solver.cpp:213] Iteration 15730, loss = 2.78359
I0623 12:44:08.660004  5064 solver.cpp:228]     Train net output #0: softmax = 2.78359 (* 1 = 2.78359 loss)
I0623 12:44:08.660010  5064 solver.cpp:473] Iteration 15730, lr = 0.0001
I0623 12:44:09.681516  5064 solver.cpp:213] Iteration 15740, loss = 2.71244
I0623 12:44:09.681541  5064 solver.cpp:228]     Train net output #0: softmax = 2.71244 (* 1 = 2.71244 loss)
I0623 12:44:09.681547  5064 solver.cpp:473] Iteration 15740, lr = 0.0001
I0623 12:44:10.701359  5064 solver.cpp:213] Iteration 15750, loss = 3.07574
I0623 12:44:10.701395  5064 solver.cpp:228]     Train net output #0: softmax = 3.07574 (* 1 = 3.07574 loss)
I0623 12:44:10.701403  5064 solver.cpp:473] Iteration 15750, lr = 0.0001
I0623 12:44:11.721652  5064 solver.cpp:213] Iteration 15760, loss = 3.11648
I0623 12:44:11.721688  5064 solver.cpp:228]     Train net output #0: softmax = 3.11648 (* 1 = 3.11648 loss)
I0623 12:44:11.721693  5064 solver.cpp:473] Iteration 15760, lr = 0.0001
I0623 12:44:12.742202  5064 solver.cpp:213] Iteration 15770, loss = 2.98743
I0623 12:44:12.742223  5064 solver.cpp:228]     Train net output #0: softmax = 2.98743 (* 1 = 2.98743 loss)
I0623 12:44:12.742228  5064 solver.cpp:473] Iteration 15770, lr = 0.0001
I0623 12:44:13.763249  5064 solver.cpp:213] Iteration 15780, loss = 3.0305
I0623 12:44:13.763269  5064 solver.cpp:228]     Train net output #0: softmax = 3.0305 (* 1 = 3.0305 loss)
I0623 12:44:13.763275  5064 solver.cpp:473] Iteration 15780, lr = 0.0001
I0623 12:44:14.784111  5064 solver.cpp:213] Iteration 15790, loss = 2.9093
I0623 12:44:14.784135  5064 solver.cpp:228]     Train net output #0: softmax = 2.9093 (* 1 = 2.9093 loss)
I0623 12:44:14.784142  5064 solver.cpp:473] Iteration 15790, lr = 0.0001
I0623 12:44:15.803222  5064 solver.cpp:213] Iteration 15800, loss = 3.21635
I0623 12:44:15.803247  5064 solver.cpp:228]     Train net output #0: softmax = 3.21635 (* 1 = 3.21635 loss)
I0623 12:44:15.803385  5064 solver.cpp:473] Iteration 15800, lr = 0.0001
I0623 12:44:16.824306  5064 solver.cpp:213] Iteration 15810, loss = 3.06073
I0623 12:44:16.824326  5064 solver.cpp:228]     Train net output #0: softmax = 3.06073 (* 1 = 3.06073 loss)
I0623 12:44:16.824331  5064 solver.cpp:473] Iteration 15810, lr = 0.0001
I0623 12:44:17.844790  5064 solver.cpp:213] Iteration 15820, loss = 2.83595
I0623 12:44:17.844811  5064 solver.cpp:228]     Train net output #0: softmax = 2.83595 (* 1 = 2.83595 loss)
I0623 12:44:17.844816  5064 solver.cpp:473] Iteration 15820, lr = 0.0001
I0623 12:44:18.865957  5064 solver.cpp:213] Iteration 15830, loss = 2.96155
I0623 12:44:18.865980  5064 solver.cpp:228]     Train net output #0: softmax = 2.96155 (* 1 = 2.96155 loss)
I0623 12:44:18.865985  5064 solver.cpp:473] Iteration 15830, lr = 0.0001
I0623 12:44:19.886667  5064 solver.cpp:213] Iteration 15840, loss = 2.89853
I0623 12:44:19.886693  5064 solver.cpp:228]     Train net output #0: softmax = 2.89853 (* 1 = 2.89853 loss)
I0623 12:44:19.886698  5064 solver.cpp:473] Iteration 15840, lr = 0.0001
I0623 12:44:20.908567  5064 solver.cpp:213] Iteration 15850, loss = 2.7596
I0623 12:44:20.908592  5064 solver.cpp:228]     Train net output #0: softmax = 2.7596 (* 1 = 2.7596 loss)
I0623 12:44:20.908718  5064 solver.cpp:473] Iteration 15850, lr = 0.0001
I0623 12:44:21.929532  5064 solver.cpp:213] Iteration 15860, loss = 2.80128
I0623 12:44:21.929553  5064 solver.cpp:228]     Train net output #0: softmax = 2.80128 (* 1 = 2.80128 loss)
I0623 12:44:21.929558  5064 solver.cpp:473] Iteration 15860, lr = 0.0001
I0623 12:44:22.950004  5064 solver.cpp:213] Iteration 15870, loss = 2.71196
I0623 12:44:22.950026  5064 solver.cpp:228]     Train net output #0: softmax = 2.71196 (* 1 = 2.71196 loss)
I0623 12:44:22.950031  5064 solver.cpp:473] Iteration 15870, lr = 0.0001
I0623 12:44:23.970326  5064 solver.cpp:213] Iteration 15880, loss = 2.89012
I0623 12:44:23.970345  5064 solver.cpp:228]     Train net output #0: softmax = 2.89012 (* 1 = 2.89012 loss)
I0623 12:44:23.970351  5064 solver.cpp:473] Iteration 15880, lr = 0.0001
I0623 12:44:24.991160  5064 solver.cpp:213] Iteration 15890, loss = 2.91889
I0623 12:44:24.991186  5064 solver.cpp:228]     Train net output #0: softmax = 2.91889 (* 1 = 2.91889 loss)
I0623 12:44:24.991191  5064 solver.cpp:473] Iteration 15890, lr = 0.0001
I0623 12:44:26.011706  5064 solver.cpp:213] Iteration 15900, loss = 2.94161
I0623 12:44:26.011875  5064 solver.cpp:228]     Train net output #0: softmax = 2.94161 (* 1 = 2.94161 loss)
I0623 12:44:26.011883  5064 solver.cpp:473] Iteration 15900, lr = 0.0001
I0623 12:44:27.032397  5064 solver.cpp:213] Iteration 15910, loss = 3.00614
I0623 12:44:27.032425  5064 solver.cpp:228]     Train net output #0: softmax = 3.00614 (* 1 = 3.00614 loss)
I0623 12:44:27.032431  5064 solver.cpp:473] Iteration 15910, lr = 0.0001
I0623 12:44:28.050374  5064 solver.cpp:213] Iteration 15920, loss = 2.79647
I0623 12:44:28.050391  5064 solver.cpp:228]     Train net output #0: softmax = 2.79647 (* 1 = 2.79647 loss)
I0623 12:44:28.050396  5064 solver.cpp:473] Iteration 15920, lr = 0.0001
I0623 12:44:29.071303  5064 solver.cpp:213] Iteration 15930, loss = 3.00608
I0623 12:44:29.071326  5064 solver.cpp:228]     Train net output #0: softmax = 3.00608 (* 1 = 3.00608 loss)
I0623 12:44:29.071331  5064 solver.cpp:473] Iteration 15930, lr = 0.0001
I0623 12:44:30.092479  5064 solver.cpp:213] Iteration 15940, loss = 2.92156
I0623 12:44:30.092504  5064 solver.cpp:228]     Train net output #0: softmax = 2.92156 (* 1 = 2.92156 loss)
I0623 12:44:30.092509  5064 solver.cpp:473] Iteration 15940, lr = 0.0001
I0623 12:44:31.113239  5064 solver.cpp:213] Iteration 15950, loss = 3.0489
I0623 12:44:31.113265  5064 solver.cpp:228]     Train net output #0: softmax = 3.0489 (* 1 = 3.0489 loss)
I0623 12:44:31.113394  5064 solver.cpp:473] Iteration 15950, lr = 0.0001
I0623 12:44:32.133800  5064 solver.cpp:213] Iteration 15960, loss = 3.02728
I0623 12:44:32.133817  5064 solver.cpp:228]     Train net output #0: softmax = 3.02728 (* 1 = 3.02728 loss)
I0623 12:44:32.133822  5064 solver.cpp:473] Iteration 15960, lr = 0.0001
I0623 12:44:33.154726  5064 solver.cpp:213] Iteration 15970, loss = 2.90086
I0623 12:44:33.154745  5064 solver.cpp:228]     Train net output #0: softmax = 2.90086 (* 1 = 2.90086 loss)
I0623 12:44:33.154750  5064 solver.cpp:473] Iteration 15970, lr = 0.0001
I0623 12:44:34.175678  5064 solver.cpp:213] Iteration 15980, loss = 3.03858
I0623 12:44:34.175698  5064 solver.cpp:228]     Train net output #0: softmax = 3.03858 (* 1 = 3.03858 loss)
I0623 12:44:34.175704  5064 solver.cpp:473] Iteration 15980, lr = 0.0001
I0623 12:44:35.196568  5064 solver.cpp:213] Iteration 15990, loss = 2.90248
I0623 12:44:35.196591  5064 solver.cpp:228]     Train net output #0: softmax = 2.90248 (* 1 = 2.90248 loss)
I0623 12:44:35.196596  5064 solver.cpp:473] Iteration 15990, lr = 0.0001
I0623 12:44:36.146775  5064 solver.cpp:362] Snapshotting to snapshots/16-06-21_16h05m31s_0_11_pretrainClassification_iter_16000.caffemodel
I0623 12:44:36.147862  5064 solver.cpp:370] Snapshotting solver state to snapshots/16-06-21_16h05m31s_0_11_pretrainClassification_iter_16000.solverstate
I0623 12:44:36.148491  5064 solver.cpp:291] Iteration 16000, Testing net (#0)
I0623 12:44:36.309417  5064 solver.cpp:342]     Test net output #0: accuracy_top_1 = 0.264062
I0623 12:44:36.309432  5064 solver.cpp:342]     Test net output #1: accuracy_top_5 = 0.567187
I0623 12:44:36.309439  5064 solver.cpp:342]     Test net output #2: softmax = 3.02057 (* 1 = 3.02057 loss)
I0623 12:44:36.380085  5064 solver.cpp:213] Iteration 16000, loss = 3.06336
I0623 12:44:36.380100  5064 solver.cpp:228]     Train net output #0: softmax = 3.06336 (* 1 = 3.06336 loss)
I0623 12:44:36.380105  5064 solver.cpp:473] Iteration 16000, lr = 0.0001
I0623 12:44:37.400223  5064 solver.cpp:213] Iteration 16010, loss = 2.92132
I0623 12:44:37.400245  5064 solver.cpp:228]     Train net output #0: softmax = 2.92132 (* 1 = 2.92132 loss)
I0623 12:44:37.400251  5064 solver.cpp:473] Iteration 16010, lr = 0.0001
I0623 12:44:38.421437  5064 solver.cpp:213] Iteration 16020, loss = 3.22418
I0623 12:44:38.421458  5064 solver.cpp:228]     Train net output #0: softmax = 3.22418 (* 1 = 3.22418 loss)
I0623 12:44:38.421464  5064 solver.cpp:473] Iteration 16020, lr = 0.0001
I0623 12:44:39.442172  5064 solver.cpp:213] Iteration 16030, loss = 2.96907
I0623 12:44:39.442190  5064 solver.cpp:228]     Train net output #0: softmax = 2.96907 (* 1 = 2.96907 loss)
I0623 12:44:39.442214  5064 solver.cpp:473] Iteration 16030, lr = 0.0001
I0623 12:44:40.463186  5064 solver.cpp:213] Iteration 16040, loss = 2.80346
I0623 12:44:40.463212  5064 solver.cpp:228]     Train net output #0: softmax = 2.80346 (* 1 = 2.80346 loss)
I0623 12:44:40.463222  5064 solver.cpp:473] Iteration 16040, lr = 0.0001
I0623 12:44:41.484169  5064 solver.cpp:213] Iteration 16050, loss = 2.72503
I0623 12:44:41.484202  5064 solver.cpp:228]     Train net output #0: softmax = 2.72503 (* 1 = 2.72503 loss)
I0623 12:44:41.484207  5064 solver.cpp:473] Iteration 16050, lr = 0.0001
I0623 12:44:42.504793  5064 solver.cpp:213] Iteration 16060, loss = 3.0233
I0623 12:44:42.504814  5064 solver.cpp:228]     Train net output #0: softmax = 3.0233 (* 1 = 3.0233 loss)
I0623 12:44:42.504819  5064 solver.cpp:473] Iteration 16060, lr = 0.0001
I0623 12:44:43.525429  5064 solver.cpp:213] Iteration 16070, loss = 2.73411
I0623 12:44:43.525451  5064 solver.cpp:228]     Train net output #0: softmax = 2.73411 (* 1 = 2.73411 loss)
I0623 12:44:43.525456  5064 solver.cpp:473] Iteration 16070, lr = 0.0001
I0623 12:44:44.545886  5064 solver.cpp:213] Iteration 16080, loss = 2.88579
I0623 12:44:44.545904  5064 solver.cpp:228]     Train net output #0: softmax = 2.88579 (* 1 = 2.88579 loss)
I0623 12:44:44.545909  5064 solver.cpp:473] Iteration 16080, lr = 0.0001
I0623 12:44:45.566906  5064 solver.cpp:213] Iteration 16090, loss = 2.64285
I0623 12:44:45.566929  5064 solver.cpp:228]     Train net output #0: softmax = 2.64285 (* 1 = 2.64285 loss)
I0623 12:44:45.566936  5064 solver.cpp:473] Iteration 16090, lr = 0.0001
I0623 12:44:46.586964  5064 solver.cpp:213] Iteration 16100, loss = 2.78956
I0623 12:44:46.586988  5064 solver.cpp:228]     Train net output #0: softmax = 2.78956 (* 1 = 2.78956 loss)
I0623 12:44:46.587148  5064 solver.cpp:473] Iteration 16100, lr = 0.0001
I0623 12:44:47.605221  5064 solver.cpp:213] Iteration 16110, loss = 3.10371
I0623 12:44:47.605238  5064 solver.cpp:228]     Train net output #0: softmax = 3.10371 (* 1 = 3.10371 loss)
I0623 12:44:47.605243  5064 solver.cpp:473] Iteration 16110, lr = 0.0001
I0623 12:44:48.626077  5064 solver.cpp:213] Iteration 16120, loss = 3.1011
I0623 12:44:48.626099  5064 solver.cpp:228]     Train net output #0: softmax = 3.1011 (* 1 = 3.1011 loss)
I0623 12:44:48.626104  5064 solver.cpp:473] Iteration 16120, lr = 0.0001
I0623 12:44:49.646586  5064 solver.cpp:213] Iteration 16130, loss = 2.99364
I0623 12:44:49.646608  5064 solver.cpp:228]     Train net output #0: softmax = 2.99364 (* 1 = 2.99364 loss)
I0623 12:44:49.646613  5064 solver.cpp:473] Iteration 16130, lr = 0.0001
I0623 12:44:50.666901  5064 solver.cpp:213] Iteration 16140, loss = 2.96188
I0623 12:44:50.666923  5064 solver.cpp:228]     Train net output #0: softmax = 2.96188 (* 1 = 2.96188 loss)
I0623 12:44:50.666927  5064 solver.cpp:473] Iteration 16140, lr = 0.0001
I0623 12:44:51.687640  5064 solver.cpp:213] Iteration 16150, loss = 2.84271
I0623 12:44:51.687670  5064 solver.cpp:228]     Train net output #0: softmax = 2.84271 (* 1 = 2.84271 loss)
I0623 12:44:51.687796  5064 solver.cpp:473] Iteration 16150, lr = 0.0001
I0623 12:44:52.708725  5064 solver.cpp:213] Iteration 16160, loss = 3.03345
I0623 12:44:52.708747  5064 solver.cpp:228]     Train net output #0: softmax = 3.03345 (* 1 = 3.03345 loss)
I0623 12:44:52.708752  5064 solver.cpp:473] Iteration 16160, lr = 0.0001
I0623 12:44:53.729969  5064 solver.cpp:213] Iteration 16170, loss = 3.00996
I0623 12:44:53.729990  5064 solver.cpp:228]     Train net output #0: softmax = 3.00996 (* 1 = 3.00996 loss)
I0623 12:44:53.729995  5064 solver.cpp:473] Iteration 16170, lr = 0.0001
I0623 12:44:54.750625  5064 solver.cpp:213] Iteration 16180, loss = 2.83409
I0623 12:44:54.750645  5064 solver.cpp:228]     Train net output #0: softmax = 2.83409 (* 1 = 2.83409 loss)
I0623 12:44:54.750650  5064 solver.cpp:473] Iteration 16180, lr = 0.0001
I0623 12:44:55.768137  5064 solver.cpp:213] Iteration 16190, loss = 3.02533
I0623 12:44:55.768157  5064 solver.cpp:228]     Train net output #0: softmax = 3.02533 (* 1 = 3.02533 loss)
I0623 12:44:55.768179  5064 solver.cpp:473] Iteration 16190, lr = 0.0001
I0623 12:44:56.788877  5064 solver.cpp:213] Iteration 16200, loss = 3.03445
I0623 12:44:56.789049  5064 solver.cpp:228]     Train net output #0: softmax = 3.03445 (* 1 = 3.03445 loss)
I0623 12:44:56.789060  5064 solver.cpp:473] Iteration 16200, lr = 0.0001
I0623 12:44:57.809506  5064 solver.cpp:213] Iteration 16210, loss = 2.94485
I0623 12:44:57.809528  5064 solver.cpp:228]     Train net output #0: softmax = 2.94485 (* 1 = 2.94485 loss)
I0623 12:44:57.809533  5064 solver.cpp:473] Iteration 16210, lr = 0.0001
I0623 12:44:58.830157  5064 solver.cpp:213] Iteration 16220, loss = 2.83952
I0623 12:44:58.830176  5064 solver.cpp:228]     Train net output #0: softmax = 2.83952 (* 1 = 2.83952 loss)
I0623 12:44:58.830181  5064 solver.cpp:473] Iteration 16220, lr = 0.0001
I0623 12:44:59.851512  5064 solver.cpp:213] Iteration 16230, loss = 2.78747
I0623 12:44:59.851533  5064 solver.cpp:228]     Train net output #0: softmax = 2.78747 (* 1 = 2.78747 loss)
I0623 12:44:59.851539  5064 solver.cpp:473] Iteration 16230, lr = 0.0001
I0623 12:45:00.872074  5064 solver.cpp:213] Iteration 16240, loss = 2.91056
I0623 12:45:00.872095  5064 solver.cpp:228]     Train net output #0: softmax = 2.91056 (* 1 = 2.91056 loss)
I0623 12:45:00.872102  5064 solver.cpp:473] Iteration 16240, lr = 0.0001
I0623 12:45:01.893544  5064 solver.cpp:213] Iteration 16250, loss = 2.9663
I0623 12:45:01.893573  5064 solver.cpp:228]     Train net output #0: softmax = 2.9663 (* 1 = 2.9663 loss)
I0623 12:45:01.893704  5064 solver.cpp:473] Iteration 16250, lr = 0.0001
I0623 12:45:02.913885  5064 solver.cpp:213] Iteration 16260, loss = 2.78089
I0623 12:45:02.913908  5064 solver.cpp:228]     Train net output #0: softmax = 2.78089 (* 1 = 2.78089 loss)
I0623 12:45:02.913913  5064 solver.cpp:473] Iteration 16260, lr = 0.0001
I0623 12:45:03.934203  5064 solver.cpp:213] Iteration 16270, loss = 2.81047
I0623 12:45:03.934221  5064 solver.cpp:228]     Train net output #0: softmax = 2.81047 (* 1 = 2.81047 loss)
I0623 12:45:03.934226  5064 solver.cpp:473] Iteration 16270, lr = 0.0001
I0623 12:45:04.955076  5064 solver.cpp:213] Iteration 16280, loss = 3.02451
I0623 12:45:04.955096  5064 solver.cpp:228]     Train net output #0: softmax = 3.02451 (* 1 = 3.02451 loss)
I0623 12:45:04.955101  5064 solver.cpp:473] Iteration 16280, lr = 0.0001
I0623 12:45:05.976469  5064 solver.cpp:213] Iteration 16290, loss = 2.94276
I0623 12:45:05.976490  5064 solver.cpp:228]     Train net output #0: softmax = 2.94276 (* 1 = 2.94276 loss)
I0623 12:45:05.976495  5064 solver.cpp:473] Iteration 16290, lr = 0.0001
I0623 12:45:06.997310  5064 solver.cpp:213] Iteration 16300, loss = 3.20801
I0623 12:45:06.997334  5064 solver.cpp:228]     Train net output #0: softmax = 3.20801 (* 1 = 3.20801 loss)
I0623 12:45:06.997459  5064 solver.cpp:473] Iteration 16300, lr = 0.0001
I0623 12:45:08.017482  5064 solver.cpp:213] Iteration 16310, loss = 2.8963
I0623 12:45:08.017505  5064 solver.cpp:228]     Train net output #0: softmax = 2.8963 (* 1 = 2.8963 loss)
I0623 12:45:08.017510  5064 solver.cpp:473] Iteration 16310, lr = 0.0001
I0623 12:45:09.038043  5064 solver.cpp:213] Iteration 16320, loss = 3.00801
I0623 12:45:09.038064  5064 solver.cpp:228]     Train net output #0: softmax = 3.00801 (* 1 = 3.00801 loss)
I0623 12:45:09.038069  5064 solver.cpp:473] Iteration 16320, lr = 0.0001
I0623 12:45:10.059145  5064 solver.cpp:213] Iteration 16330, loss = 3.20787
I0623 12:45:10.059164  5064 solver.cpp:228]     Train net output #0: softmax = 3.20787 (* 1 = 3.20787 loss)
I0623 12:45:10.059168  5064 solver.cpp:473] Iteration 16330, lr = 0.0001
I0623 12:45:11.079838  5064 solver.cpp:213] Iteration 16340, loss = 3.14035
I0623 12:45:11.079859  5064 solver.cpp:228]     Train net output #0: softmax = 3.14035 (* 1 = 3.14035 loss)
I0623 12:45:11.079864  5064 solver.cpp:473] Iteration 16340, lr = 0.0001
I0623 12:45:12.100345  5064 solver.cpp:213] Iteration 16350, loss = 3.0069
I0623 12:45:12.100366  5064 solver.cpp:228]     Train net output #0: softmax = 3.0069 (* 1 = 3.0069 loss)
I0623 12:45:12.100373  5064 solver.cpp:473] Iteration 16350, lr = 0.0001
I0623 12:45:13.120565  5064 solver.cpp:213] Iteration 16360, loss = 2.95058
I0623 12:45:13.120602  5064 solver.cpp:228]     Train net output #0: softmax = 2.95058 (* 1 = 2.95058 loss)
I0623 12:45:13.120610  5064 solver.cpp:473] Iteration 16360, lr = 0.0001
I0623 12:45:14.141274  5064 solver.cpp:213] Iteration 16370, loss = 2.93301
I0623 12:45:14.141295  5064 solver.cpp:228]     Train net output #0: softmax = 2.93301 (* 1 = 2.93301 loss)
I0623 12:45:14.141301  5064 solver.cpp:473] Iteration 16370, lr = 0.0001
I0623 12:45:15.161475  5064 solver.cpp:213] Iteration 16380, loss = 2.86114
I0623 12:45:15.161494  5064 solver.cpp:228]     Train net output #0: softmax = 2.86114 (* 1 = 2.86114 loss)
I0623 12:45:15.161499  5064 solver.cpp:473] Iteration 16380, lr = 0.0001
I0623 12:45:16.181893  5064 solver.cpp:213] Iteration 16390, loss = 3.06105
I0623 12:45:16.181910  5064 solver.cpp:228]     Train net output #0: softmax = 3.06105 (* 1 = 3.06105 loss)
I0623 12:45:16.181915  5064 solver.cpp:473] Iteration 16390, lr = 0.0001
I0623 12:45:17.202950  5064 solver.cpp:213] Iteration 16400, loss = 2.95616
I0623 12:45:17.202976  5064 solver.cpp:228]     Train net output #0: softmax = 2.95616 (* 1 = 2.95616 loss)
I0623 12:45:17.203111  5064 solver.cpp:473] Iteration 16400, lr = 0.0001
I0623 12:45:18.224138  5064 solver.cpp:213] Iteration 16410, loss = 3.18269
I0623 12:45:18.224159  5064 solver.cpp:228]     Train net output #0: softmax = 3.18269 (* 1 = 3.18269 loss)
I0623 12:45:18.224164  5064 solver.cpp:473] Iteration 16410, lr = 0.0001
I0623 12:45:19.244561  5064 solver.cpp:213] Iteration 16420, loss = 2.912
I0623 12:45:19.244580  5064 solver.cpp:228]     Train net output #0: softmax = 2.912 (* 1 = 2.912 loss)
I0623 12:45:19.244585  5064 solver.cpp:473] Iteration 16420, lr = 0.0001
I0623 12:45:20.265023  5064 solver.cpp:213] Iteration 16430, loss = 3.26846
I0623 12:45:20.265046  5064 solver.cpp:228]     Train net output #0: softmax = 3.26846 (* 1 = 3.26846 loss)
I0623 12:45:20.265051  5064 solver.cpp:473] Iteration 16430, lr = 0.0001
I0623 12:45:21.286229  5064 solver.cpp:213] Iteration 16440, loss = 2.89422
I0623 12:45:21.286249  5064 solver.cpp:228]     Train net output #0: softmax = 2.89422 (* 1 = 2.89422 loss)
I0623 12:45:21.286254  5064 solver.cpp:473] Iteration 16440, lr = 0.0001
I0623 12:45:22.306718  5064 solver.cpp:213] Iteration 16450, loss = 2.89925
I0623 12:45:22.306740  5064 solver.cpp:228]     Train net output #0: softmax = 2.89925 (* 1 = 2.89925 loss)
I0623 12:45:22.306866  5064 solver.cpp:473] Iteration 16450, lr = 0.0001
I0623 12:45:23.327291  5064 solver.cpp:213] Iteration 16460, loss = 2.90498
I0623 12:45:23.327311  5064 solver.cpp:228]     Train net output #0: softmax = 2.90498 (* 1 = 2.90498 loss)
I0623 12:45:23.327316  5064 solver.cpp:473] Iteration 16460, lr = 0.0001
I0623 12:45:24.347905  5064 solver.cpp:213] Iteration 16470, loss = 2.83032
I0623 12:45:24.347923  5064 solver.cpp:228]     Train net output #0: softmax = 2.83032 (* 1 = 2.83032 loss)
I0623 12:45:24.347929  5064 solver.cpp:473] Iteration 16470, lr = 0.0001
I0623 12:45:25.368553  5064 solver.cpp:213] Iteration 16480, loss = 3.18074
I0623 12:45:25.368577  5064 solver.cpp:228]     Train net output #0: softmax = 3.18074 (* 1 = 3.18074 loss)
I0623 12:45:25.368582  5064 solver.cpp:473] Iteration 16480, lr = 0.0001
I0623 12:45:26.389211  5064 solver.cpp:213] Iteration 16490, loss = 2.94121
I0623 12:45:26.389231  5064 solver.cpp:228]     Train net output #0: softmax = 2.94121 (* 1 = 2.94121 loss)
I0623 12:45:26.389236  5064 solver.cpp:473] Iteration 16490, lr = 0.0001
I0623 12:45:27.409097  5064 solver.cpp:213] Iteration 16500, loss = 2.84678
I0623 12:45:27.409132  5064 solver.cpp:228]     Train net output #0: softmax = 2.84678 (* 1 = 2.84678 loss)
I0623 12:45:27.409138  5064 solver.cpp:473] Iteration 16500, lr = 0.0001
I0623 12:45:28.429159  5064 solver.cpp:213] Iteration 16510, loss = 2.91562
I0623 12:45:28.429179  5064 solver.cpp:228]     Train net output #0: softmax = 2.91562 (* 1 = 2.91562 loss)
I0623 12:45:28.429184  5064 solver.cpp:473] Iteration 16510, lr = 0.0001
I0623 12:45:29.449990  5064 solver.cpp:213] Iteration 16520, loss = 3.10128
I0623 12:45:29.450011  5064 solver.cpp:228]     Train net output #0: softmax = 3.10128 (* 1 = 3.10128 loss)
I0623 12:45:29.450023  5064 solver.cpp:473] Iteration 16520, lr = 0.0001
I0623 12:45:30.471451  5064 solver.cpp:213] Iteration 16530, loss = 2.91973
I0623 12:45:30.471484  5064 solver.cpp:228]     Train net output #0: softmax = 2.91973 (* 1 = 2.91973 loss)
I0623 12:45:30.471489  5064 solver.cpp:473] Iteration 16530, lr = 0.0001
I0623 12:45:31.492960  5064 solver.cpp:213] Iteration 16540, loss = 2.9611
I0623 12:45:31.492986  5064 solver.cpp:228]     Train net output #0: softmax = 2.9611 (* 1 = 2.9611 loss)
I0623 12:45:31.492992  5064 solver.cpp:473] Iteration 16540, lr = 0.0001
I0623 12:45:32.513504  5064 solver.cpp:213] Iteration 16550, loss = 2.90306
I0623 12:45:32.513528  5064 solver.cpp:228]     Train net output #0: softmax = 2.90306 (* 1 = 2.90306 loss)
I0623 12:45:32.513653  5064 solver.cpp:473] Iteration 16550, lr = 0.0001
I0623 12:45:33.534375  5064 solver.cpp:213] Iteration 16560, loss = 3.07033
I0623 12:45:33.534395  5064 solver.cpp:228]     Train net output #0: softmax = 3.07033 (* 1 = 3.07033 loss)
I0623 12:45:33.534400  5064 solver.cpp:473] Iteration 16560, lr = 0.0001
I0623 12:45:34.554913  5064 solver.cpp:213] Iteration 16570, loss = 2.79129
I0623 12:45:34.554934  5064 solver.cpp:228]     Train net output #0: softmax = 2.79129 (* 1 = 2.79129 loss)
I0623 12:45:34.554939  5064 solver.cpp:473] Iteration 16570, lr = 0.0001
I0623 12:45:35.573287  5064 solver.cpp:213] Iteration 16580, loss = 2.80444
I0623 12:45:35.573312  5064 solver.cpp:228]     Train net output #0: softmax = 2.80444 (* 1 = 2.80444 loss)
I0623 12:45:35.573317  5064 solver.cpp:473] Iteration 16580, lr = 0.0001
I0623 12:45:36.593881  5064 solver.cpp:213] Iteration 16590, loss = 3.23653
I0623 12:45:36.593904  5064 solver.cpp:228]     Train net output #0: softmax = 3.23653 (* 1 = 3.23653 loss)
I0623 12:45:36.593909  5064 solver.cpp:473] Iteration 16590, lr = 0.0001
I0623 12:45:37.614428  5064 solver.cpp:213] Iteration 16600, loss = 3.21493
I0623 12:45:37.614455  5064 solver.cpp:228]     Train net output #0: softmax = 3.21493 (* 1 = 3.21493 loss)
I0623 12:45:37.614581  5064 solver.cpp:473] Iteration 16600, lr = 0.0001
I0623 12:45:38.634573  5064 solver.cpp:213] Iteration 16610, loss = 2.87364
I0623 12:45:38.634591  5064 solver.cpp:228]     Train net output #0: softmax = 2.87364 (* 1 = 2.87364 loss)
I0623 12:45:38.634596  5064 solver.cpp:473] Iteration 16610, lr = 0.0001
I0623 12:45:39.655110  5064 solver.cpp:213] Iteration 16620, loss = 2.87732
I0623 12:45:39.655131  5064 solver.cpp:228]     Train net output #0: softmax = 2.87732 (* 1 = 2.87732 loss)
I0623 12:45:39.655138  5064 solver.cpp:473] Iteration 16620, lr = 0.0001
I0623 12:45:40.675462  5064 solver.cpp:213] Iteration 16630, loss = 3.05935
I0623 12:45:40.675483  5064 solver.cpp:228]     Train net output #0: softmax = 3.05935 (* 1 = 3.05935 loss)
I0623 12:45:40.675489  5064 solver.cpp:473] Iteration 16630, lr = 0.0001
I0623 12:45:41.695876  5064 solver.cpp:213] Iteration 16640, loss = 3.28568
I0623 12:45:41.695897  5064 solver.cpp:228]     Train net output #0: softmax = 3.28568 (* 1 = 3.28568 loss)
I0623 12:45:41.695902  5064 solver.cpp:473] Iteration 16640, lr = 0.0001
I0623 12:45:42.717389  5064 solver.cpp:213] Iteration 16650, loss = 2.90209
I0623 12:45:42.717416  5064 solver.cpp:228]     Train net output #0: softmax = 2.90209 (* 1 = 2.90209 loss)
I0623 12:45:42.717537  5064 solver.cpp:473] Iteration 16650, lr = 0.0001
I0623 12:45:43.737865  5064 solver.cpp:213] Iteration 16660, loss = 3.13376
I0623 12:45:43.737901  5064 solver.cpp:228]     Train net output #0: softmax = 3.13376 (* 1 = 3.13376 loss)
I0623 12:45:43.737907  5064 solver.cpp:473] Iteration 16660, lr = 0.0001
I0623 12:45:44.757771  5064 solver.cpp:213] Iteration 16670, loss = 3.17244
I0623 12:45:44.757791  5064 solver.cpp:228]     Train net output #0: softmax = 3.17244 (* 1 = 3.17244 loss)
I0623 12:45:44.757797  5064 solver.cpp:473] Iteration 16670, lr = 0.0001
I0623 12:45:45.778825  5064 solver.cpp:213] Iteration 16680, loss = 3.26466
I0623 12:45:45.778849  5064 solver.cpp:228]     Train net output #0: softmax = 3.26466 (* 1 = 3.26466 loss)
I0623 12:45:45.778861  5064 solver.cpp:473] Iteration 16680, lr = 0.0001
I0623 12:45:46.799911  5064 solver.cpp:213] Iteration 16690, loss = 3.11601
I0623 12:45:46.799932  5064 solver.cpp:228]     Train net output #0: softmax = 3.11601 (* 1 = 3.11601 loss)
I0623 12:45:46.799937  5064 solver.cpp:473] Iteration 16690, lr = 0.0001
I0623 12:45:47.820451  5064 solver.cpp:213] Iteration 16700, loss = 3.12812
I0623 12:45:47.820478  5064 solver.cpp:228]     Train net output #0: softmax = 3.12812 (* 1 = 3.12812 loss)
I0623 12:45:47.820626  5064 solver.cpp:473] Iteration 16700, lr = 0.0001
I0623 12:45:48.841771  5064 solver.cpp:213] Iteration 16710, loss = 2.82725
I0623 12:45:48.841795  5064 solver.cpp:228]     Train net output #0: softmax = 2.82725 (* 1 = 2.82725 loss)
I0623 12:45:48.841800  5064 solver.cpp:473] Iteration 16710, lr = 0.0001
I0623 12:45:49.862824  5064 solver.cpp:213] Iteration 16720, loss = 2.90237
I0623 12:45:49.862846  5064 solver.cpp:228]     Train net output #0: softmax = 2.90237 (* 1 = 2.90237 loss)
I0623 12:45:49.862851  5064 solver.cpp:473] Iteration 16720, lr = 0.0001
I0623 12:45:50.883543  5064 solver.cpp:213] Iteration 16730, loss = 2.72878
I0623 12:45:50.883566  5064 solver.cpp:228]     Train net output #0: softmax = 2.72878 (* 1 = 2.72878 loss)
I0623 12:45:50.883572  5064 solver.cpp:473] Iteration 16730, lr = 0.0001
I0623 12:45:51.904328  5064 solver.cpp:213] Iteration 16740, loss = 3.06042
I0623 12:45:51.904351  5064 solver.cpp:228]     Train net output #0: softmax = 3.06042 (* 1 = 3.06042 loss)
I0623 12:45:51.904356  5064 solver.cpp:473] Iteration 16740, lr = 0.0001
I0623 12:45:52.924906  5064 solver.cpp:213] Iteration 16750, loss = 2.97369
I0623 12:45:52.924931  5064 solver.cpp:228]     Train net output #0: softmax = 2.97369 (* 1 = 2.97369 loss)
I0623 12:45:52.925055  5064 solver.cpp:473] Iteration 16750, lr = 0.0001
I0623 12:45:53.946097  5064 solver.cpp:213] Iteration 16760, loss = 2.8891
I0623 12:45:53.946120  5064 solver.cpp:228]     Train net output #0: softmax = 2.8891 (* 1 = 2.8891 loss)
I0623 12:45:53.946125  5064 solver.cpp:473] Iteration 16760, lr = 0.0001
I0623 12:45:54.967483  5064 solver.cpp:213] Iteration 16770, loss = 2.8122
I0623 12:45:54.967505  5064 solver.cpp:228]     Train net output #0: softmax = 2.8122 (* 1 = 2.8122 loss)
I0623 12:45:54.967510  5064 solver.cpp:473] Iteration 16770, lr = 0.0001
I0623 12:45:55.988732  5064 solver.cpp:213] Iteration 16780, loss = 2.95223
I0623 12:45:55.988755  5064 solver.cpp:228]     Train net output #0: softmax = 2.95223 (* 1 = 2.95223 loss)
I0623 12:45:55.988760  5064 solver.cpp:473] Iteration 16780, lr = 0.0001
I0623 12:45:57.009351  5064 solver.cpp:213] Iteration 16790, loss = 2.91955
I0623 12:45:57.009372  5064 solver.cpp:228]     Train net output #0: softmax = 2.91955 (* 1 = 2.91955 loss)
I0623 12:45:57.009377  5064 solver.cpp:473] Iteration 16790, lr = 0.0001
I0623 12:45:58.030279  5064 solver.cpp:213] Iteration 16800, loss = 3.00512
I0623 12:45:58.030452  5064 solver.cpp:228]     Train net output #0: softmax = 3.00512 (* 1 = 3.00512 loss)
I0623 12:45:58.030460  5064 solver.cpp:473] Iteration 16800, lr = 0.0001
I0623 12:45:59.051014  5064 solver.cpp:213] Iteration 16810, loss = 2.82882
I0623 12:45:59.051034  5064 solver.cpp:228]     Train net output #0: softmax = 2.82882 (* 1 = 2.82882 loss)
I0623 12:45:59.051039  5064 solver.cpp:473] Iteration 16810, lr = 0.0001
I0623 12:46:00.071749  5064 solver.cpp:213] Iteration 16820, loss = 3.24703
I0623 12:46:00.071768  5064 solver.cpp:228]     Train net output #0: softmax = 3.24703 (* 1 = 3.24703 loss)
I0623 12:46:00.071774  5064 solver.cpp:473] Iteration 16820, lr = 0.0001
I0623 12:46:01.092651  5064 solver.cpp:213] Iteration 16830, loss = 2.88389
I0623 12:46:01.092676  5064 solver.cpp:228]     Train net output #0: softmax = 2.88389 (* 1 = 2.88389 loss)
I0623 12:46:01.092681  5064 solver.cpp:473] Iteration 16830, lr = 0.0001
I0623 12:46:02.113406  5064 solver.cpp:213] Iteration 16840, loss = 3.11074
I0623 12:46:02.113426  5064 solver.cpp:228]     Train net output #0: softmax = 3.11074 (* 1 = 3.11074 loss)
I0623 12:46:02.113437  5064 solver.cpp:473] Iteration 16840, lr = 0.0001
I0623 12:46:03.133774  5064 solver.cpp:213] Iteration 16850, loss = 2.93094
I0623 12:46:03.133797  5064 solver.cpp:228]     Train net output #0: softmax = 2.93094 (* 1 = 2.93094 loss)
I0623 12:46:03.133945  5064 solver.cpp:473] Iteration 16850, lr = 0.0001
I0623 12:46:04.154726  5064 solver.cpp:213] Iteration 16860, loss = 2.89927
I0623 12:46:04.154744  5064 solver.cpp:228]     Train net output #0: softmax = 2.89927 (* 1 = 2.89927 loss)
I0623 12:46:04.154749  5064 solver.cpp:473] Iteration 16860, lr = 0.0001
I0623 12:46:05.175709  5064 solver.cpp:213] Iteration 16870, loss = 2.7914
I0623 12:46:05.175729  5064 solver.cpp:228]     Train net output #0: softmax = 2.7914 (* 1 = 2.7914 loss)
I0623 12:46:05.175734  5064 solver.cpp:473] Iteration 16870, lr = 0.0001
I0623 12:46:06.196357  5064 solver.cpp:213] Iteration 16880, loss = 3.20015
I0623 12:46:06.196382  5064 solver.cpp:228]     Train net output #0: softmax = 3.20015 (* 1 = 3.20015 loss)
I0623 12:46:06.196388  5064 solver.cpp:473] Iteration 16880, lr = 0.0001
I0623 12:46:07.217356  5064 solver.cpp:213] Iteration 16890, loss = 2.85424
I0623 12:46:07.217384  5064 solver.cpp:228]     Train net output #0: softmax = 2.85424 (* 1 = 2.85424 loss)
I0623 12:46:07.217389  5064 solver.cpp:473] Iteration 16890, lr = 0.0001
I0623 12:46:08.239015  5064 solver.cpp:213] Iteration 16900, loss = 2.93509
I0623 12:46:08.239039  5064 solver.cpp:228]     Train net output #0: softmax = 2.93509 (* 1 = 2.93509 loss)
I0623 12:46:08.239157  5064 solver.cpp:473] Iteration 16900, lr = 0.0001
I0623 12:46:09.259887  5064 solver.cpp:213] Iteration 16910, loss = 2.88002
I0623 12:46:09.259910  5064 solver.cpp:228]     Train net output #0: softmax = 2.88002 (* 1 = 2.88002 loss)
I0623 12:46:09.259915  5064 solver.cpp:473] Iteration 16910, lr = 0.0001
I0623 12:46:10.281204  5064 solver.cpp:213] Iteration 16920, loss = 3.23129
I0623 12:46:10.281224  5064 solver.cpp:228]     Train net output #0: softmax = 3.23129 (* 1 = 3.23129 loss)
I0623 12:46:10.281229  5064 solver.cpp:473] Iteration 16920, lr = 0.0001
I0623 12:46:11.301928  5064 solver.cpp:213] Iteration 16930, loss = 2.98228
I0623 12:46:11.301949  5064 solver.cpp:228]     Train net output #0: softmax = 2.98228 (* 1 = 2.98228 loss)
I0623 12:46:11.301954  5064 solver.cpp:473] Iteration 16930, lr = 0.0001
I0623 12:46:12.322857  5064 solver.cpp:213] Iteration 16940, loss = 3.0355
I0623 12:46:12.322878  5064 solver.cpp:228]     Train net output #0: softmax = 3.0355 (* 1 = 3.0355 loss)
I0623 12:46:12.322883  5064 solver.cpp:473] Iteration 16940, lr = 0.0001
I0623 12:46:13.344260  5064 solver.cpp:213] Iteration 16950, loss = 3.00702
I0623 12:46:13.344287  5064 solver.cpp:228]     Train net output #0: softmax = 3.00702 (* 1 = 3.00702 loss)
I0623 12:46:13.344293  5064 solver.cpp:473] Iteration 16950, lr = 0.0001
I0623 12:46:14.364863  5064 solver.cpp:213] Iteration 16960, loss = 2.78403
I0623 12:46:14.364902  5064 solver.cpp:228]     Train net output #0: softmax = 2.78403 (* 1 = 2.78403 loss)
I0623 12:46:14.364907  5064 solver.cpp:473] Iteration 16960, lr = 0.0001
I0623 12:46:15.383436  5064 solver.cpp:213] Iteration 16970, loss = 2.96558
I0623 12:46:15.383455  5064 solver.cpp:228]     Train net output #0: softmax = 2.96558 (* 1 = 2.96558 loss)
I0623 12:46:15.383460  5064 solver.cpp:473] Iteration 16970, lr = 0.0001
I0623 12:46:16.404263  5064 solver.cpp:213] Iteration 16980, loss = 2.94422
I0623 12:46:16.404287  5064 solver.cpp:228]     Train net output #0: softmax = 2.94422 (* 1 = 2.94422 loss)
I0623 12:46:16.404292  5064 solver.cpp:473] Iteration 16980, lr = 0.0001
I0623 12:46:17.424965  5064 solver.cpp:213] Iteration 16990, loss = 3.192
I0623 12:46:17.424988  5064 solver.cpp:228]     Train net output #0: softmax = 3.192 (* 1 = 3.192 loss)
I0623 12:46:17.424993  5064 solver.cpp:473] Iteration 16990, lr = 0.0001
I0623 12:46:18.375077  5064 solver.cpp:362] Snapshotting to snapshots/16-06-21_16h05m31s_0_11_pretrainClassification_iter_17000.caffemodel
I0623 12:46:18.376230  5064 solver.cpp:370] Snapshotting solver state to snapshots/16-06-21_16h05m31s_0_11_pretrainClassification_iter_17000.solverstate
I0623 12:46:18.376827  5064 solver.cpp:291] Iteration 17000, Testing net (#0)
I0623 12:46:18.538031  5064 solver.cpp:342]     Test net output #0: accuracy_top_1 = 0.265625
I0623 12:46:18.538048  5064 solver.cpp:342]     Test net output #1: accuracy_top_5 = 0.579687
I0623 12:46:18.538054  5064 solver.cpp:342]     Test net output #2: softmax = 2.99883 (* 1 = 2.99883 loss)
I0623 12:46:18.608700  5064 solver.cpp:213] Iteration 17000, loss = 3.07831
I0623 12:46:18.608716  5064 solver.cpp:228]     Train net output #0: softmax = 3.07831 (* 1 = 3.07831 loss)
I0623 12:46:18.608721  5064 solver.cpp:473] Iteration 17000, lr = 0.0001
I0623 12:46:19.630038  5064 solver.cpp:213] Iteration 17010, loss = 2.94717
I0623 12:46:19.630059  5064 solver.cpp:228]     Train net output #0: softmax = 2.94717 (* 1 = 2.94717 loss)
I0623 12:46:19.630064  5064 solver.cpp:473] Iteration 17010, lr = 0.0001
I0623 12:46:20.650877  5064 solver.cpp:213] Iteration 17020, loss = 2.84046
I0623 12:46:20.650897  5064 solver.cpp:228]     Train net output #0: softmax = 2.84046 (* 1 = 2.84046 loss)
I0623 12:46:20.650902  5064 solver.cpp:473] Iteration 17020, lr = 0.0001
I0623 12:46:21.671797  5064 solver.cpp:213] Iteration 17030, loss = 3.08497
I0623 12:46:21.671820  5064 solver.cpp:228]     Train net output #0: softmax = 3.08497 (* 1 = 3.08497 loss)
I0623 12:46:21.671825  5064 solver.cpp:473] Iteration 17030, lr = 0.0001
I0623 12:46:22.692361  5064 solver.cpp:213] Iteration 17040, loss = 3.02242
I0623 12:46:22.692383  5064 solver.cpp:228]     Train net output #0: softmax = 3.02242 (* 1 = 3.02242 loss)
I0623 12:46:22.692389  5064 solver.cpp:473] Iteration 17040, lr = 0.0001
I0623 12:46:23.713623  5064 solver.cpp:213] Iteration 17050, loss = 2.68353
I0623 12:46:23.713644  5064 solver.cpp:228]     Train net output #0: softmax = 2.68353 (* 1 = 2.68353 loss)
I0623 12:46:23.713654  5064 solver.cpp:473] Iteration 17050, lr = 0.0001
I0623 12:46:24.734529  5064 solver.cpp:213] Iteration 17060, loss = 3.21535
I0623 12:46:24.734547  5064 solver.cpp:228]     Train net output #0: softmax = 3.21535 (* 1 = 3.21535 loss)
I0623 12:46:24.734554  5064 solver.cpp:473] Iteration 17060, lr = 0.0001
I0623 12:46:25.755213  5064 solver.cpp:213] Iteration 17070, loss = 3.23514
I0623 12:46:25.755236  5064 solver.cpp:228]     Train net output #0: softmax = 3.23514 (* 1 = 3.23514 loss)
I0623 12:46:25.755241  5064 solver.cpp:473] Iteration 17070, lr = 0.0001
I0623 12:46:26.775902  5064 solver.cpp:213] Iteration 17080, loss = 2.88222
I0623 12:46:26.775926  5064 solver.cpp:228]     Train net output #0: softmax = 2.88222 (* 1 = 2.88222 loss)
I0623 12:46:26.775931  5064 solver.cpp:473] Iteration 17080, lr = 0.0001
I0623 12:46:27.796633  5064 solver.cpp:213] Iteration 17090, loss = 3.10663
I0623 12:46:27.796655  5064 solver.cpp:228]     Train net output #0: softmax = 3.10663 (* 1 = 3.10663 loss)
I0623 12:46:27.796679  5064 solver.cpp:473] Iteration 17090, lr = 0.0001
I0623 12:46:28.817808  5064 solver.cpp:213] Iteration 17100, loss = 3.05328
I0623 12:46:28.817973  5064 solver.cpp:228]     Train net output #0: softmax = 3.05328 (* 1 = 3.05328 loss)
I0623 12:46:28.817981  5064 solver.cpp:473] Iteration 17100, lr = 0.0001
I0623 12:46:29.838860  5064 solver.cpp:213] Iteration 17110, loss = 2.67481
I0623 12:46:29.838879  5064 solver.cpp:228]     Train net output #0: softmax = 2.67481 (* 1 = 2.67481 loss)
I0623 12:46:29.838884  5064 solver.cpp:473] Iteration 17110, lr = 0.0001
I0623 12:46:30.859546  5064 solver.cpp:213] Iteration 17120, loss = 2.82964
I0623 12:46:30.859566  5064 solver.cpp:228]     Train net output #0: softmax = 2.82964 (* 1 = 2.82964 loss)
I0623 12:46:30.859572  5064 solver.cpp:473] Iteration 17120, lr = 0.0001
I0623 12:46:31.880563  5064 solver.cpp:213] Iteration 17130, loss = 2.87143
I0623 12:46:31.880587  5064 solver.cpp:228]     Train net output #0: softmax = 2.87143 (* 1 = 2.87143 loss)
I0623 12:46:31.880594  5064 solver.cpp:473] Iteration 17130, lr = 0.0001
I0623 12:46:32.901487  5064 solver.cpp:213] Iteration 17140, loss = 3.14104
I0623 12:46:32.901509  5064 solver.cpp:228]     Train net output #0: softmax = 3.14104 (* 1 = 3.14104 loss)
I0623 12:46:32.901515  5064 solver.cpp:473] Iteration 17140, lr = 0.0001
I0623 12:46:33.922300  5064 solver.cpp:213] Iteration 17150, loss = 2.93634
I0623 12:46:33.922324  5064 solver.cpp:228]     Train net output #0: softmax = 2.93634 (* 1 = 2.93634 loss)
I0623 12:46:33.922446  5064 solver.cpp:473] Iteration 17150, lr = 0.0001
I0623 12:46:34.942855  5064 solver.cpp:213] Iteration 17160, loss = 3.11827
I0623 12:46:34.942873  5064 solver.cpp:228]     Train net output #0: softmax = 3.11827 (* 1 = 3.11827 loss)
I0623 12:46:34.942878  5064 solver.cpp:473] Iteration 17160, lr = 0.0001
I0623 12:46:35.963486  5064 solver.cpp:213] Iteration 17170, loss = 2.61668
I0623 12:46:35.963505  5064 solver.cpp:228]     Train net output #0: softmax = 2.61668 (* 1 = 2.61668 loss)
I0623 12:46:35.963510  5064 solver.cpp:473] Iteration 17170, lr = 0.0001
I0623 12:46:36.983906  5064 solver.cpp:213] Iteration 17180, loss = 3.12434
I0623 12:46:36.983928  5064 solver.cpp:228]     Train net output #0: softmax = 3.12434 (* 1 = 3.12434 loss)
I0623 12:46:36.983933  5064 solver.cpp:473] Iteration 17180, lr = 0.0001
I0623 12:46:38.004534  5064 solver.cpp:213] Iteration 17190, loss = 2.86539
I0623 12:46:38.004556  5064 solver.cpp:228]     Train net output #0: softmax = 2.86539 (* 1 = 2.86539 loss)
I0623 12:46:38.004561  5064 solver.cpp:473] Iteration 17190, lr = 0.0001
I0623 12:46:39.025645  5064 solver.cpp:213] Iteration 17200, loss = 2.96457
I0623 12:46:39.025668  5064 solver.cpp:228]     Train net output #0: softmax = 2.96457 (* 1 = 2.96457 loss)
I0623 12:46:39.025795  5064 solver.cpp:473] Iteration 17200, lr = 0.0001
I0623 12:46:40.046567  5064 solver.cpp:213] Iteration 17210, loss = 2.99684
I0623 12:46:40.046583  5064 solver.cpp:228]     Train net output #0: softmax = 2.99684 (* 1 = 2.99684 loss)
I0623 12:46:40.046589  5064 solver.cpp:473] Iteration 17210, lr = 0.0001
I0623 12:46:41.067090  5064 solver.cpp:213] Iteration 17220, loss = 2.92061
I0623 12:46:41.067111  5064 solver.cpp:228]     Train net output #0: softmax = 2.92061 (* 1 = 2.92061 loss)
I0623 12:46:41.067116  5064 solver.cpp:473] Iteration 17220, lr = 0.0001
I0623 12:46:42.087743  5064 solver.cpp:213] Iteration 17230, loss = 3.12427
I0623 12:46:42.087765  5064 solver.cpp:228]     Train net output #0: softmax = 3.12427 (* 1 = 3.12427 loss)
I0623 12:46:42.087771  5064 solver.cpp:473] Iteration 17230, lr = 0.0001
I0623 12:46:43.108274  5064 solver.cpp:213] Iteration 17240, loss = 2.62554
I0623 12:46:43.108294  5064 solver.cpp:228]     Train net output #0: softmax = 2.62554 (* 1 = 2.62554 loss)
I0623 12:46:43.108299  5064 solver.cpp:473] Iteration 17240, lr = 0.0001
I0623 12:46:44.129164  5064 solver.cpp:213] Iteration 17250, loss = 2.84012
I0623 12:46:44.129187  5064 solver.cpp:228]     Train net output #0: softmax = 2.84012 (* 1 = 2.84012 loss)
I0623 12:46:44.129314  5064 solver.cpp:473] Iteration 17250, lr = 0.0001
I0623 12:46:45.150476  5064 solver.cpp:213] Iteration 17260, loss = 2.85691
I0623 12:46:45.150511  5064 solver.cpp:228]     Train net output #0: softmax = 2.85691 (* 1 = 2.85691 loss)
I0623 12:46:45.150516  5064 solver.cpp:473] Iteration 17260, lr = 0.0001
I0623 12:46:46.171517  5064 solver.cpp:213] Iteration 17270, loss = 2.90064
I0623 12:46:46.171540  5064 solver.cpp:228]     Train net output #0: softmax = 2.90064 (* 1 = 2.90064 loss)
I0623 12:46:46.171545  5064 solver.cpp:473] Iteration 17270, lr = 0.0001
I0623 12:46:47.192144  5064 solver.cpp:213] Iteration 17280, loss = 2.77934
I0623 12:46:47.192167  5064 solver.cpp:228]     Train net output #0: softmax = 2.77934 (* 1 = 2.77934 loss)
I0623 12:46:47.192173  5064 solver.cpp:473] Iteration 17280, lr = 0.0001
I0623 12:46:48.212404  5064 solver.cpp:213] Iteration 17290, loss = 2.81563
I0623 12:46:48.212422  5064 solver.cpp:228]     Train net output #0: softmax = 2.81563 (* 1 = 2.81563 loss)
I0623 12:46:48.212433  5064 solver.cpp:473] Iteration 17290, lr = 0.0001
I0623 12:46:49.232944  5064 solver.cpp:213] Iteration 17300, loss = 2.96195
I0623 12:46:49.232966  5064 solver.cpp:228]     Train net output #0: softmax = 2.96195 (* 1 = 2.96195 loss)
I0623 12:46:49.251090  5064 solver.cpp:473] Iteration 17300, lr = 0.0001
I0623 12:46:50.253662  5064 solver.cpp:213] Iteration 17310, loss = 3.07572
I0623 12:46:50.253681  5064 solver.cpp:228]     Train net output #0: softmax = 3.07572 (* 1 = 3.07572 loss)
I0623 12:46:50.253687  5064 solver.cpp:473] Iteration 17310, lr = 0.0001
I0623 12:46:51.273859  5064 solver.cpp:213] Iteration 17320, loss = 3.07446
I0623 12:46:51.273879  5064 solver.cpp:228]     Train net output #0: softmax = 3.07446 (* 1 = 3.07446 loss)
I0623 12:46:51.273883  5064 solver.cpp:473] Iteration 17320, lr = 0.0001
I0623 12:46:52.294684  5064 solver.cpp:213] Iteration 17330, loss = 3.06205
I0623 12:46:52.294706  5064 solver.cpp:228]     Train net output #0: softmax = 3.06205 (* 1 = 3.06205 loss)
I0623 12:46:52.294713  5064 solver.cpp:473] Iteration 17330, lr = 0.0001
I0623 12:46:53.315572  5064 solver.cpp:213] Iteration 17340, loss = 3.1037
I0623 12:46:53.315594  5064 solver.cpp:228]     Train net output #0: softmax = 3.1037 (* 1 = 3.1037 loss)
I0623 12:46:53.315599  5064 solver.cpp:473] Iteration 17340, lr = 0.0001
I0623 12:46:54.336905  5064 solver.cpp:213] Iteration 17350, loss = 2.78875
I0623 12:46:54.336925  5064 solver.cpp:228]     Train net output #0: softmax = 2.78875 (* 1 = 2.78875 loss)
I0623 12:46:54.336930  5064 solver.cpp:473] Iteration 17350, lr = 0.0001
I0623 12:46:55.355201  5064 solver.cpp:213] Iteration 17360, loss = 3.13003
I0623 12:46:55.355219  5064 solver.cpp:228]     Train net output #0: softmax = 3.13003 (* 1 = 3.13003 loss)
I0623 12:46:55.355224  5064 solver.cpp:473] Iteration 17360, lr = 0.0001
I0623 12:46:56.375695  5064 solver.cpp:213] Iteration 17370, loss = 2.90099
I0623 12:46:56.375715  5064 solver.cpp:228]     Train net output #0: softmax = 2.90099 (* 1 = 2.90099 loss)
I0623 12:46:56.375721  5064 solver.cpp:473] Iteration 17370, lr = 0.0001
I0623 12:46:57.396420  5064 solver.cpp:213] Iteration 17380, loss = 2.99748
I0623 12:46:57.396442  5064 solver.cpp:228]     Train net output #0: softmax = 2.99748 (* 1 = 2.99748 loss)
I0623 12:46:57.396447  5064 solver.cpp:473] Iteration 17380, lr = 0.0001
I0623 12:46:58.416613  5064 solver.cpp:213] Iteration 17390, loss = 2.98213
I0623 12:46:58.416633  5064 solver.cpp:228]     Train net output #0: softmax = 2.98213 (* 1 = 2.98213 loss)
I0623 12:46:58.416640  5064 solver.cpp:473] Iteration 17390, lr = 0.0001
I0623 12:46:59.437196  5064 solver.cpp:213] Iteration 17400, loss = 2.72938
I0623 12:46:59.437391  5064 solver.cpp:228]     Train net output #0: softmax = 2.72938 (* 1 = 2.72938 loss)
I0623 12:46:59.437398  5064 solver.cpp:473] Iteration 17400, lr = 0.0001
I0623 12:47:00.457439  5064 solver.cpp:213] Iteration 17410, loss = 3.07692
I0623 12:47:00.457458  5064 solver.cpp:228]     Train net output #0: softmax = 3.07692 (* 1 = 3.07692 loss)
I0623 12:47:00.457463  5064 solver.cpp:473] Iteration 17410, lr = 0.0001
I0623 12:47:01.478737  5064 solver.cpp:213] Iteration 17420, loss = 2.76046
I0623 12:47:01.478760  5064 solver.cpp:228]     Train net output #0: softmax = 2.76046 (* 1 = 2.76046 loss)
I0623 12:47:01.478765  5064 solver.cpp:473] Iteration 17420, lr = 0.0001
I0623 12:47:02.499580  5064 solver.cpp:213] Iteration 17430, loss = 3.07396
I0623 12:47:02.499605  5064 solver.cpp:228]     Train net output #0: softmax = 3.07396 (* 1 = 3.07396 loss)
I0623 12:47:02.499610  5064 solver.cpp:473] Iteration 17430, lr = 0.0001
I0623 12:47:03.520365  5064 solver.cpp:213] Iteration 17440, loss = 2.85277
I0623 12:47:03.520386  5064 solver.cpp:228]     Train net output #0: softmax = 2.85277 (* 1 = 2.85277 loss)
I0623 12:47:03.520392  5064 solver.cpp:473] Iteration 17440, lr = 0.0001
I0623 12:47:04.540820  5064 solver.cpp:213] Iteration 17450, loss = 2.94179
I0623 12:47:04.540843  5064 solver.cpp:228]     Train net output #0: softmax = 2.94179 (* 1 = 2.94179 loss)
I0623 12:47:04.540988  5064 solver.cpp:473] Iteration 17450, lr = 0.0001
I0623 12:47:05.561187  5064 solver.cpp:213] Iteration 17460, loss = 2.83129
I0623 12:47:05.561205  5064 solver.cpp:228]     Train net output #0: softmax = 2.83129 (* 1 = 2.83129 loss)
I0623 12:47:05.561210  5064 solver.cpp:473] Iteration 17460, lr = 0.0001
I0623 12:47:06.581866  5064 solver.cpp:213] Iteration 17470, loss = 2.86347
I0623 12:47:06.581887  5064 solver.cpp:228]     Train net output #0: softmax = 2.86347 (* 1 = 2.86347 loss)
I0623 12:47:06.581892  5064 solver.cpp:473] Iteration 17470, lr = 0.0001
I0623 12:47:07.601382  5064 solver.cpp:213] Iteration 17480, loss = 2.93982
I0623 12:47:07.601404  5064 solver.cpp:228]     Train net output #0: softmax = 2.93982 (* 1 = 2.93982 loss)
I0623 12:47:07.601410  5064 solver.cpp:473] Iteration 17480, lr = 0.0001
I0623 12:47:08.621654  5064 solver.cpp:213] Iteration 17490, loss = 2.90709
I0623 12:47:08.621676  5064 solver.cpp:228]     Train net output #0: softmax = 2.90709 (* 1 = 2.90709 loss)
I0623 12:47:08.621681  5064 solver.cpp:473] Iteration 17490, lr = 0.0001
I0623 12:47:09.642280  5064 solver.cpp:213] Iteration 17500, loss = 2.96161
I0623 12:47:09.642304  5064 solver.cpp:228]     Train net output #0: softmax = 2.96161 (* 1 = 2.96161 loss)
I0623 12:47:09.642429  5064 solver.cpp:473] Iteration 17500, lr = 0.0001
I0623 12:47:10.662969  5064 solver.cpp:213] Iteration 17510, loss = 2.83335
I0623 12:47:10.662989  5064 solver.cpp:228]     Train net output #0: softmax = 2.83335 (* 1 = 2.83335 loss)
I0623 12:47:10.662994  5064 solver.cpp:473] Iteration 17510, lr = 0.0001
I0623 12:47:11.683214  5064 solver.cpp:213] Iteration 17520, loss = 3.11411
I0623 12:47:11.683233  5064 solver.cpp:228]     Train net output #0: softmax = 3.11411 (* 1 = 3.11411 loss)
I0623 12:47:11.683238  5064 solver.cpp:473] Iteration 17520, lr = 0.0001
I0623 12:47:12.703706  5064 solver.cpp:213] Iteration 17530, loss = 2.99951
I0623 12:47:12.703732  5064 solver.cpp:228]     Train net output #0: softmax = 2.99951 (* 1 = 2.99951 loss)
I0623 12:47:12.703737  5064 solver.cpp:473] Iteration 17530, lr = 0.0001
I0623 12:47:13.724125  5064 solver.cpp:213] Iteration 17540, loss = 3.0982
I0623 12:47:13.724145  5064 solver.cpp:228]     Train net output #0: softmax = 3.0982 (* 1 = 3.0982 loss)
I0623 12:47:13.724150  5064 solver.cpp:473] Iteration 17540, lr = 0.0001
I0623 12:47:14.744484  5064 solver.cpp:213] Iteration 17550, loss = 3.16584
I0623 12:47:14.744508  5064 solver.cpp:228]     Train net output #0: softmax = 3.16584 (* 1 = 3.16584 loss)
I0623 12:47:14.744637  5064 solver.cpp:473] Iteration 17550, lr = 0.0001
I0623 12:47:15.765020  5064 solver.cpp:213] Iteration 17560, loss = 2.8481
I0623 12:47:15.765055  5064 solver.cpp:228]     Train net output #0: softmax = 2.8481 (* 1 = 2.8481 loss)
I0623 12:47:15.765060  5064 solver.cpp:473] Iteration 17560, lr = 0.0001
I0623 12:47:16.785570  5064 solver.cpp:213] Iteration 17570, loss = 2.76625
I0623 12:47:16.785589  5064 solver.cpp:228]     Train net output #0: softmax = 2.76625 (* 1 = 2.76625 loss)
I0623 12:47:16.785595  5064 solver.cpp:473] Iteration 17570, lr = 0.0001
I0623 12:47:17.806641  5064 solver.cpp:213] Iteration 17580, loss = 2.912
I0623 12:47:17.806665  5064 solver.cpp:228]     Train net output #0: softmax = 2.912 (* 1 = 2.912 loss)
I0623 12:47:17.806670  5064 solver.cpp:473] Iteration 17580, lr = 0.0001
I0623 12:47:18.827533  5064 solver.cpp:213] Iteration 17590, loss = 2.8592
I0623 12:47:18.827554  5064 solver.cpp:228]     Train net output #0: softmax = 2.8592 (* 1 = 2.8592 loss)
I0623 12:47:18.827559  5064 solver.cpp:473] Iteration 17590, lr = 0.0001
I0623 12:47:19.847857  5064 solver.cpp:213] Iteration 17600, loss = 2.83101
I0623 12:47:19.847882  5064 solver.cpp:228]     Train net output #0: softmax = 2.83101 (* 1 = 2.83101 loss)
I0623 12:47:19.848016  5064 solver.cpp:473] Iteration 17600, lr = 0.0001
I0623 12:47:20.868485  5064 solver.cpp:213] Iteration 17610, loss = 2.8713
I0623 12:47:20.868506  5064 solver.cpp:228]     Train net output #0: softmax = 2.8713 (* 1 = 2.8713 loss)
I0623 12:47:20.868518  5064 solver.cpp:473] Iteration 17610, lr = 0.0001
I0623 12:47:21.889500  5064 solver.cpp:213] Iteration 17620, loss = 3.12049
I0623 12:47:21.889521  5064 solver.cpp:228]     Train net output #0: softmax = 3.12049 (* 1 = 3.12049 loss)
I0623 12:47:21.889526  5064 solver.cpp:473] Iteration 17620, lr = 0.0001
I0623 12:47:22.910528  5064 solver.cpp:213] Iteration 17630, loss = 2.91606
I0623 12:47:22.910553  5064 solver.cpp:228]     Train net output #0: softmax = 2.91606 (* 1 = 2.91606 loss)
I0623 12:47:22.910559  5064 solver.cpp:473] Iteration 17630, lr = 0.0001
I0623 12:47:23.931591  5064 solver.cpp:213] Iteration 17640, loss = 2.77162
I0623 12:47:23.931610  5064 solver.cpp:228]     Train net output #0: softmax = 2.77162 (* 1 = 2.77162 loss)
I0623 12:47:23.931615  5064 solver.cpp:473] Iteration 17640, lr = 0.0001
I0623 12:47:24.953361  5064 solver.cpp:213] Iteration 17650, loss = 2.98167
I0623 12:47:24.953384  5064 solver.cpp:228]     Train net output #0: softmax = 2.98167 (* 1 = 2.98167 loss)
I0623 12:47:24.953512  5064 solver.cpp:473] Iteration 17650, lr = 0.0001
I0623 12:47:25.975064  5064 solver.cpp:213] Iteration 17660, loss = 2.82013
I0623 12:47:25.975087  5064 solver.cpp:228]     Train net output #0: softmax = 2.82013 (* 1 = 2.82013 loss)
I0623 12:47:25.975092  5064 solver.cpp:473] Iteration 17660, lr = 0.0001
I0623 12:47:26.995947  5064 solver.cpp:213] Iteration 17670, loss = 2.7249
I0623 12:47:26.995968  5064 solver.cpp:228]     Train net output #0: softmax = 2.7249 (* 1 = 2.7249 loss)
I0623 12:47:26.995973  5064 solver.cpp:473] Iteration 17670, lr = 0.0001
I0623 12:47:28.017236  5064 solver.cpp:213] Iteration 17680, loss = 2.87108
I0623 12:47:28.017261  5064 solver.cpp:228]     Train net output #0: softmax = 2.87108 (* 1 = 2.87108 loss)
I0623 12:47:28.017266  5064 solver.cpp:473] Iteration 17680, lr = 0.0001
I0623 12:47:29.038259  5064 solver.cpp:213] Iteration 17690, loss = 2.96223
I0623 12:47:29.038280  5064 solver.cpp:228]     Train net output #0: softmax = 2.96223 (* 1 = 2.96223 loss)
I0623 12:47:29.038285  5064 solver.cpp:473] Iteration 17690, lr = 0.0001
I0623 12:47:30.059252  5064 solver.cpp:213] Iteration 17700, loss = 3.07831
I0623 12:47:30.059427  5064 solver.cpp:228]     Train net output #0: softmax = 3.07831 (* 1 = 3.07831 loss)
I0623 12:47:30.059435  5064 solver.cpp:473] Iteration 17700, lr = 0.0001
I0623 12:47:31.079831  5064 solver.cpp:213] Iteration 17710, loss = 3.0466
I0623 12:47:31.079849  5064 solver.cpp:228]     Train net output #0: softmax = 3.0466 (* 1 = 3.0466 loss)
I0623 12:47:31.079854  5064 solver.cpp:473] Iteration 17710, lr = 0.0001
I0623 12:47:32.100942  5064 solver.cpp:213] Iteration 17720, loss = 2.9077
I0623 12:47:32.100971  5064 solver.cpp:228]     Train net output #0: softmax = 2.9077 (* 1 = 2.9077 loss)
I0623 12:47:32.100976  5064 solver.cpp:473] Iteration 17720, lr = 0.0001
I0623 12:47:33.121407  5064 solver.cpp:213] Iteration 17730, loss = 3.10841
I0623 12:47:33.121433  5064 solver.cpp:228]     Train net output #0: softmax = 3.10841 (* 1 = 3.10841 loss)
I0623 12:47:33.121439  5064 solver.cpp:473] Iteration 17730, lr = 0.0001
I0623 12:47:34.142360  5064 solver.cpp:213] Iteration 17740, loss = 2.76855
I0623 12:47:34.142381  5064 solver.cpp:228]     Train net output #0: softmax = 2.76855 (* 1 = 2.76855 loss)
I0623 12:47:34.142386  5064 solver.cpp:473] Iteration 17740, lr = 0.0001
I0623 12:47:35.163277  5064 solver.cpp:213] Iteration 17750, loss = 2.9139
I0623 12:47:35.163303  5064 solver.cpp:228]     Train net output #0: softmax = 2.9139 (* 1 = 2.9139 loss)
I0623 12:47:35.163502  5064 solver.cpp:473] Iteration 17750, lr = 0.0001
I0623 12:47:36.184121  5064 solver.cpp:213] Iteration 17760, loss = 2.95092
I0623 12:47:36.184142  5064 solver.cpp:228]     Train net output #0: softmax = 2.95092 (* 1 = 2.95092 loss)
I0623 12:47:36.184149  5064 solver.cpp:473] Iteration 17760, lr = 0.0001
I0623 12:47:37.205252  5064 solver.cpp:213] Iteration 17770, loss = 2.97222
I0623 12:47:37.205272  5064 solver.cpp:228]     Train net output #0: softmax = 2.97222 (* 1 = 2.97222 loss)
I0623 12:47:37.205286  5064 solver.cpp:473] Iteration 17770, lr = 0.0001
I0623 12:47:38.226660  5064 solver.cpp:213] Iteration 17780, loss = 2.87973
I0623 12:47:38.226686  5064 solver.cpp:228]     Train net output #0: softmax = 2.87973 (* 1 = 2.87973 loss)
I0623 12:47:38.226691  5064 solver.cpp:473] Iteration 17780, lr = 0.0001
I0623 12:47:39.247547  5064 solver.cpp:213] Iteration 17790, loss = 2.8815
I0623 12:47:39.247570  5064 solver.cpp:228]     Train net output #0: softmax = 2.8815 (* 1 = 2.8815 loss)
I0623 12:47:39.247575  5064 solver.cpp:473] Iteration 17790, lr = 0.0001
I0623 12:47:40.267953  5064 solver.cpp:213] Iteration 17800, loss = 3.12436
I0623 12:47:40.267976  5064 solver.cpp:228]     Train net output #0: softmax = 3.12436 (* 1 = 3.12436 loss)
I0623 12:47:40.268102  5064 solver.cpp:473] Iteration 17800, lr = 0.0001
I0623 12:47:41.281034  5064 solver.cpp:213] Iteration 17810, loss = 2.64645
I0623 12:47:41.281060  5064 solver.cpp:228]     Train net output #0: softmax = 2.64645 (* 1 = 2.64645 loss)
I0623 12:47:41.281067  5064 solver.cpp:473] Iteration 17810, lr = 0.0001
I0623 12:47:42.292579  5064 solver.cpp:213] Iteration 17820, loss = 3.01412
I0623 12:47:42.292600  5064 solver.cpp:228]     Train net output #0: softmax = 3.01412 (* 1 = 3.01412 loss)
I0623 12:47:42.292606  5064 solver.cpp:473] Iteration 17820, lr = 0.0001
I0623 12:47:43.310519  5064 solver.cpp:213] Iteration 17830, loss = 2.93868
I0623 12:47:43.310541  5064 solver.cpp:228]     Train net output #0: softmax = 2.93868 (* 1 = 2.93868 loss)
I0623 12:47:43.310546  5064 solver.cpp:473] Iteration 17830, lr = 0.0001
I0623 12:47:44.331264  5064 solver.cpp:213] Iteration 17840, loss = 2.96272
I0623 12:47:44.331287  5064 solver.cpp:228]     Train net output #0: softmax = 2.96272 (* 1 = 2.96272 loss)
I0623 12:47:44.331292  5064 solver.cpp:473] Iteration 17840, lr = 0.0001
I0623 12:47:45.351897  5064 solver.cpp:213] Iteration 17850, loss = 2.89713
I0623 12:47:45.351922  5064 solver.cpp:228]     Train net output #0: softmax = 2.89713 (* 1 = 2.89713 loss)
I0623 12:47:45.351927  5064 solver.cpp:473] Iteration 17850, lr = 0.0001
I0623 12:47:46.372668  5064 solver.cpp:213] Iteration 17860, loss = 2.73786
I0623 12:47:46.372706  5064 solver.cpp:228]     Train net output #0: softmax = 2.73786 (* 1 = 2.73786 loss)
I0623 12:47:46.372711  5064 solver.cpp:473] Iteration 17860, lr = 0.0001
I0623 12:47:47.393950  5064 solver.cpp:213] Iteration 17870, loss = 2.74971
I0623 12:47:47.393970  5064 solver.cpp:228]     Train net output #0: softmax = 2.74971 (* 1 = 2.74971 loss)
I0623 12:47:47.393975  5064 solver.cpp:473] Iteration 17870, lr = 0.0001
I0623 12:47:48.414362  5064 solver.cpp:213] Iteration 17880, loss = 2.82809
I0623 12:47:48.414386  5064 solver.cpp:228]     Train net output #0: softmax = 2.82809 (* 1 = 2.82809 loss)
I0623 12:47:48.414391  5064 solver.cpp:473] Iteration 17880, lr = 0.0001
I0623 12:47:49.435395  5064 solver.cpp:213] Iteration 17890, loss = 2.95342
I0623 12:47:49.435417  5064 solver.cpp:228]     Train net output #0: softmax = 2.95342 (* 1 = 2.95342 loss)
I0623 12:47:49.435422  5064 solver.cpp:473] Iteration 17890, lr = 0.0001
I0623 12:47:50.456089  5064 solver.cpp:213] Iteration 17900, loss = 2.77322
I0623 12:47:50.456109  5064 solver.cpp:228]     Train net output #0: softmax = 2.77322 (* 1 = 2.77322 loss)
I0623 12:47:50.456248  5064 solver.cpp:473] Iteration 17900, lr = 0.0001
I0623 12:47:51.476428  5064 solver.cpp:213] Iteration 17910, loss = 2.9241
I0623 12:47:51.476446  5064 solver.cpp:228]     Train net output #0: softmax = 2.9241 (* 1 = 2.9241 loss)
I0623 12:47:51.476451  5064 solver.cpp:473] Iteration 17910, lr = 0.0001
I0623 12:47:52.497414  5064 solver.cpp:213] Iteration 17920, loss = 3.06512
I0623 12:47:52.497433  5064 solver.cpp:228]     Train net output #0: softmax = 3.06512 (* 1 = 3.06512 loss)
I0623 12:47:52.497438  5064 solver.cpp:473] Iteration 17920, lr = 0.0001
I0623 12:47:53.518117  5064 solver.cpp:213] Iteration 17930, loss = 3.00885
I0623 12:47:53.518142  5064 solver.cpp:228]     Train net output #0: softmax = 3.00885 (* 1 = 3.00885 loss)
I0623 12:47:53.518154  5064 solver.cpp:473] Iteration 17930, lr = 0.0001
I0623 12:47:54.538491  5064 solver.cpp:213] Iteration 17940, loss = 2.94682
I0623 12:47:54.538511  5064 solver.cpp:228]     Train net output #0: softmax = 2.94682 (* 1 = 2.94682 loss)
I0623 12:47:54.538516  5064 solver.cpp:473] Iteration 17940, lr = 0.0001
I0623 12:47:55.558996  5064 solver.cpp:213] Iteration 17950, loss = 3.00023
I0623 12:47:55.559020  5064 solver.cpp:228]     Train net output #0: softmax = 3.00023 (* 1 = 3.00023 loss)
I0623 12:47:55.559142  5064 solver.cpp:473] Iteration 17950, lr = 0.0001
I0623 12:47:56.579479  5064 solver.cpp:213] Iteration 17960, loss = 2.79999
I0623 12:47:56.579499  5064 solver.cpp:228]     Train net output #0: softmax = 2.79999 (* 1 = 2.79999 loss)
I0623 12:47:56.579504  5064 solver.cpp:473] Iteration 17960, lr = 0.0001
I0623 12:47:57.600332  5064 solver.cpp:213] Iteration 17970, loss = 3.04607
I0623 12:47:57.600353  5064 solver.cpp:228]     Train net output #0: softmax = 3.04607 (* 1 = 3.04607 loss)
I0623 12:47:57.600358  5064 solver.cpp:473] Iteration 17970, lr = 0.0001
I0623 12:47:58.621352  5064 solver.cpp:213] Iteration 17980, loss = 2.75836
I0623 12:47:58.621373  5064 solver.cpp:228]     Train net output #0: softmax = 2.75836 (* 1 = 2.75836 loss)
I0623 12:47:58.621378  5064 solver.cpp:473] Iteration 17980, lr = 0.0001
I0623 12:47:59.641050  5064 solver.cpp:213] Iteration 17990, loss = 2.81641
I0623 12:47:59.641073  5064 solver.cpp:228]     Train net output #0: softmax = 2.81641 (* 1 = 2.81641 loss)
I0623 12:47:59.641078  5064 solver.cpp:473] Iteration 17990, lr = 0.0001
I0623 12:48:00.591434  5064 solver.cpp:362] Snapshotting to snapshots/16-06-21_16h05m31s_0_11_pretrainClassification_iter_18000.caffemodel
I0623 12:48:00.592630  5064 solver.cpp:370] Snapshotting solver state to snapshots/16-06-21_16h05m31s_0_11_pretrainClassification_iter_18000.solverstate
I0623 12:48:00.593226  5064 solver.cpp:291] Iteration 18000, Testing net (#0)
I0623 12:48:00.754112  5064 solver.cpp:342]     Test net output #0: accuracy_top_1 = 0.2875
I0623 12:48:00.754128  5064 solver.cpp:342]     Test net output #1: accuracy_top_5 = 0.565625
I0623 12:48:00.754135  5064 solver.cpp:342]     Test net output #2: softmax = 2.9113 (* 1 = 2.9113 loss)
I0623 12:48:00.824893  5064 solver.cpp:213] Iteration 18000, loss = 2.9515
I0623 12:48:00.824908  5064 solver.cpp:228]     Train net output #0: softmax = 2.9515 (* 1 = 2.9515 loss)
I0623 12:48:00.824913  5064 solver.cpp:473] Iteration 18000, lr = 0.0001
I0623 12:48:01.845803  5064 solver.cpp:213] Iteration 18010, loss = 2.84702
I0623 12:48:01.845824  5064 solver.cpp:228]     Train net output #0: softmax = 2.84702 (* 1 = 2.84702 loss)
I0623 12:48:01.845830  5064 solver.cpp:473] Iteration 18010, lr = 0.0001
I0623 12:48:02.866197  5064 solver.cpp:213] Iteration 18020, loss = 2.88056
I0623 12:48:02.866217  5064 solver.cpp:228]     Train net output #0: softmax = 2.88056 (* 1 = 2.88056 loss)
I0623 12:48:02.866222  5064 solver.cpp:473] Iteration 18020, lr = 0.0001
I0623 12:48:03.886385  5064 solver.cpp:213] Iteration 18030, loss = 2.87212
I0623 12:48:03.886407  5064 solver.cpp:228]     Train net output #0: softmax = 2.87212 (* 1 = 2.87212 loss)
I0623 12:48:03.886412  5064 solver.cpp:473] Iteration 18030, lr = 0.0001
I0623 12:48:04.907181  5064 solver.cpp:213] Iteration 18040, loss = 2.73641
I0623 12:48:04.907204  5064 solver.cpp:228]     Train net output #0: softmax = 2.73641 (* 1 = 2.73641 loss)
I0623 12:48:04.907210  5064 solver.cpp:473] Iteration 18040, lr = 0.0001
I0623 12:48:05.928206  5064 solver.cpp:213] Iteration 18050, loss = 2.61773
I0623 12:48:05.928232  5064 solver.cpp:228]     Train net output #0: softmax = 2.61773 (* 1 = 2.61773 loss)
I0623 12:48:05.928246  5064 solver.cpp:473] Iteration 18050, lr = 0.0001
I0623 12:48:06.949187  5064 solver.cpp:213] Iteration 18060, loss = 2.90539
I0623 12:48:06.949208  5064 solver.cpp:228]     Train net output #0: softmax = 2.90539 (* 1 = 2.90539 loss)
I0623 12:48:06.949214  5064 solver.cpp:473] Iteration 18060, lr = 0.0001
I0623 12:48:07.969446  5064 solver.cpp:213] Iteration 18070, loss = 2.92331
I0623 12:48:07.969465  5064 solver.cpp:228]     Train net output #0: softmax = 2.92331 (* 1 = 2.92331 loss)
I0623 12:48:07.969470  5064 solver.cpp:473] Iteration 18070, lr = 0.0001
I0623 12:48:08.990478  5064 solver.cpp:213] Iteration 18080, loss = 2.89258
I0623 12:48:08.990500  5064 solver.cpp:228]     Train net output #0: softmax = 2.89258 (* 1 = 2.89258 loss)
I0623 12:48:08.990505  5064 solver.cpp:473] Iteration 18080, lr = 0.0001
I0623 12:48:10.010746  5064 solver.cpp:213] Iteration 18090, loss = 2.92956
I0623 12:48:10.010771  5064 solver.cpp:228]     Train net output #0: softmax = 2.92956 (* 1 = 2.92956 loss)
I0623 12:48:10.010776  5064 solver.cpp:473] Iteration 18090, lr = 0.0001
I0623 12:48:11.031556  5064 solver.cpp:213] Iteration 18100, loss = 2.84622
I0623 12:48:11.031582  5064 solver.cpp:228]     Train net output #0: softmax = 2.84622 (* 1 = 2.84622 loss)
I0623 12:48:11.031710  5064 solver.cpp:473] Iteration 18100, lr = 0.0001
I0623 12:48:12.052268  5064 solver.cpp:213] Iteration 18110, loss = 3.05391
I0623 12:48:12.052291  5064 solver.cpp:228]     Train net output #0: softmax = 3.05391 (* 1 = 3.05391 loss)
I0623 12:48:12.052296  5064 solver.cpp:473] Iteration 18110, lr = 0.0001
I0623 12:48:13.073652  5064 solver.cpp:213] Iteration 18120, loss = 2.95595
I0623 12:48:13.073671  5064 solver.cpp:228]     Train net output #0: softmax = 2.95595 (* 1 = 2.95595 loss)
I0623 12:48:13.073678  5064 solver.cpp:473] Iteration 18120, lr = 0.0001
I0623 12:48:14.094228  5064 solver.cpp:213] Iteration 18130, loss = 2.63096
I0623 12:48:14.094247  5064 solver.cpp:228]     Train net output #0: softmax = 2.63096 (* 1 = 2.63096 loss)
I0623 12:48:14.094252  5064 solver.cpp:473] Iteration 18130, lr = 0.0001
I0623 12:48:15.114261  5064 solver.cpp:213] Iteration 18140, loss = 2.81468
I0623 12:48:15.114281  5064 solver.cpp:228]     Train net output #0: softmax = 2.81468 (* 1 = 2.81468 loss)
I0623 12:48:15.114286  5064 solver.cpp:473] Iteration 18140, lr = 0.0001
I0623 12:48:16.134752  5064 solver.cpp:213] Iteration 18150, loss = 3.04658
I0623 12:48:16.134775  5064 solver.cpp:228]     Train net output #0: softmax = 3.04658 (* 1 = 3.04658 loss)
I0623 12:48:16.134922  5064 solver.cpp:473] Iteration 18150, lr = 0.0001
I0623 12:48:17.155724  5064 solver.cpp:213] Iteration 18160, loss = 3.07101
I0623 12:48:17.155743  5064 solver.cpp:228]     Train net output #0: softmax = 3.07101 (* 1 = 3.07101 loss)
I0623 12:48:17.155748  5064 solver.cpp:473] Iteration 18160, lr = 0.0001
I0623 12:48:18.176565  5064 solver.cpp:213] Iteration 18170, loss = 3.07477
I0623 12:48:18.176584  5064 solver.cpp:228]     Train net output #0: softmax = 3.07477 (* 1 = 3.07477 loss)
I0623 12:48:18.176589  5064 solver.cpp:473] Iteration 18170, lr = 0.0001
I0623 12:48:19.197604  5064 solver.cpp:213] Iteration 18180, loss = 3.02071
I0623 12:48:19.197628  5064 solver.cpp:228]     Train net output #0: softmax = 3.02071 (* 1 = 3.02071 loss)
I0623 12:48:19.197633  5064 solver.cpp:473] Iteration 18180, lr = 0.0001
I0623 12:48:20.218322  5064 solver.cpp:213] Iteration 18190, loss = 2.85318
I0623 12:48:20.218340  5064 solver.cpp:228]     Train net output #0: softmax = 2.85318 (* 1 = 2.85318 loss)
I0623 12:48:20.218346  5064 solver.cpp:473] Iteration 18190, lr = 0.0001
I0623 12:48:21.239266  5064 solver.cpp:213] Iteration 18200, loss = 2.74456
I0623 12:48:21.239292  5064 solver.cpp:228]     Train net output #0: softmax = 2.74456 (* 1 = 2.74456 loss)
I0623 12:48:21.239449  5064 solver.cpp:473] Iteration 18200, lr = 0.0001
I0623 12:48:22.252219  5064 solver.cpp:213] Iteration 18210, loss = 3.06518
I0623 12:48:22.252239  5064 solver.cpp:228]     Train net output #0: softmax = 3.06518 (* 1 = 3.06518 loss)
I0623 12:48:22.252246  5064 solver.cpp:473] Iteration 18210, lr = 0.0001
I0623 12:48:23.273118  5064 solver.cpp:213] Iteration 18220, loss = 2.78468
I0623 12:48:23.273142  5064 solver.cpp:228]     Train net output #0: softmax = 2.78468 (* 1 = 2.78468 loss)
I0623 12:48:23.273147  5064 solver.cpp:473] Iteration 18220, lr = 0.0001
I0623 12:48:24.294420  5064 solver.cpp:213] Iteration 18230, loss = 3.03257
I0623 12:48:24.294440  5064 solver.cpp:228]     Train net output #0: softmax = 3.03257 (* 1 = 3.03257 loss)
I0623 12:48:24.294445  5064 solver.cpp:473] Iteration 18230, lr = 0.0001
I0623 12:48:25.314780  5064 solver.cpp:213] Iteration 18240, loss = 2.96198
I0623 12:48:25.314801  5064 solver.cpp:228]     Train net output #0: softmax = 2.96198 (* 1 = 2.96198 loss)
I0623 12:48:25.314806  5064 solver.cpp:473] Iteration 18240, lr = 0.0001
I0623 12:48:26.335299  5064 solver.cpp:213] Iteration 18250, loss = 2.65865
I0623 12:48:26.335324  5064 solver.cpp:228]     Train net output #0: softmax = 2.65865 (* 1 = 2.65865 loss)
I0623 12:48:26.335445  5064 solver.cpp:473] Iteration 18250, lr = 0.0001
I0623 12:48:27.354147  5064 solver.cpp:213] Iteration 18260, loss = 2.6587
I0623 12:48:27.354166  5064 solver.cpp:228]     Train net output #0: softmax = 2.6587 (* 1 = 2.6587 loss)
I0623 12:48:27.354171  5064 solver.cpp:473] Iteration 18260, lr = 0.0001
I0623 12:48:28.374119  5064 solver.cpp:213] Iteration 18270, loss = 3.06336
I0623 12:48:28.374140  5064 solver.cpp:228]     Train net output #0: softmax = 3.06336 (* 1 = 3.06336 loss)
I0623 12:48:28.374146  5064 solver.cpp:473] Iteration 18270, lr = 0.0001
I0623 12:48:29.394115  5064 solver.cpp:213] Iteration 18280, loss = 2.79932
I0623 12:48:29.394135  5064 solver.cpp:228]     Train net output #0: softmax = 2.79932 (* 1 = 2.79932 loss)
I0623 12:48:29.394140  5064 solver.cpp:473] Iteration 18280, lr = 0.0001
I0623 12:48:30.414574  5064 solver.cpp:213] Iteration 18290, loss = 2.8265
I0623 12:48:30.414594  5064 solver.cpp:228]     Train net output #0: softmax = 2.8265 (* 1 = 2.8265 loss)
I0623 12:48:30.414599  5064 solver.cpp:473] Iteration 18290, lr = 0.0001
I0623 12:48:31.433145  5064 solver.cpp:213] Iteration 18300, loss = 2.99361
I0623 12:48:31.433318  5064 solver.cpp:228]     Train net output #0: softmax = 2.99361 (* 1 = 2.99361 loss)
I0623 12:48:31.433326  5064 solver.cpp:473] Iteration 18300, lr = 0.0001
I0623 12:48:32.453675  5064 solver.cpp:213] Iteration 18310, loss = 2.78589
I0623 12:48:32.453694  5064 solver.cpp:228]     Train net output #0: softmax = 2.78589 (* 1 = 2.78589 loss)
I0623 12:48:32.453699  5064 solver.cpp:473] Iteration 18310, lr = 0.0001
I0623 12:48:33.474447  5064 solver.cpp:213] Iteration 18320, loss = 2.79376
I0623 12:48:33.474465  5064 solver.cpp:228]     Train net output #0: softmax = 2.79376 (* 1 = 2.79376 loss)
I0623 12:48:33.474470  5064 solver.cpp:473] Iteration 18320, lr = 0.0001
I0623 12:48:34.495625  5064 solver.cpp:213] Iteration 18330, loss = 2.91921
I0623 12:48:34.495646  5064 solver.cpp:228]     Train net output #0: softmax = 2.91921 (* 1 = 2.91921 loss)
I0623 12:48:34.495651  5064 solver.cpp:473] Iteration 18330, lr = 0.0001
I0623 12:48:35.516752  5064 solver.cpp:213] Iteration 18340, loss = 2.93221
I0623 12:48:35.516769  5064 solver.cpp:228]     Train net output #0: softmax = 2.93221 (* 1 = 2.93221 loss)
I0623 12:48:35.516774  5064 solver.cpp:473] Iteration 18340, lr = 0.0001
I0623 12:48:36.537392  5064 solver.cpp:213] Iteration 18350, loss = 2.91638
I0623 12:48:36.537418  5064 solver.cpp:228]     Train net output #0: softmax = 2.91638 (* 1 = 2.91638 loss)
I0623 12:48:36.537600  5064 solver.cpp:473] Iteration 18350, lr = 0.0001
I0623 12:48:37.556881  5064 solver.cpp:213] Iteration 18360, loss = 3.15267
I0623 12:48:37.556900  5064 solver.cpp:228]     Train net output #0: softmax = 3.15267 (* 1 = 3.15267 loss)
I0623 12:48:37.556905  5064 solver.cpp:473] Iteration 18360, lr = 0.0001
I0623 12:48:38.577011  5064 solver.cpp:213] Iteration 18370, loss = 2.66998
I0623 12:48:38.577029  5064 solver.cpp:228]     Train net output #0: softmax = 2.66998 (* 1 = 2.66998 loss)
I0623 12:48:38.577034  5064 solver.cpp:473] Iteration 18370, lr = 0.0001
I0623 12:48:39.596915  5064 solver.cpp:213] Iteration 18380, loss = 2.76613
I0623 12:48:39.596936  5064 solver.cpp:228]     Train net output #0: softmax = 2.76613 (* 1 = 2.76613 loss)
I0623 12:48:39.596941  5064 solver.cpp:473] Iteration 18380, lr = 0.0001
I0623 12:48:40.617857  5064 solver.cpp:213] Iteration 18390, loss = 3.16057
I0623 12:48:40.617877  5064 solver.cpp:228]     Train net output #0: softmax = 3.16057 (* 1 = 3.16057 loss)
I0623 12:48:40.617882  5064 solver.cpp:473] Iteration 18390, lr = 0.0001
I0623 12:48:41.639072  5064 solver.cpp:213] Iteration 18400, loss = 2.64871
I0623 12:48:41.639098  5064 solver.cpp:228]     Train net output #0: softmax = 2.64871 (* 1 = 2.64871 loss)
I0623 12:48:41.639250  5064 solver.cpp:473] Iteration 18400, lr = 0.0001
I0623 12:48:42.660053  5064 solver.cpp:213] Iteration 18410, loss = 2.89763
I0623 12:48:42.660075  5064 solver.cpp:228]     Train net output #0: softmax = 2.89763 (* 1 = 2.89763 loss)
I0623 12:48:42.660080  5064 solver.cpp:473] Iteration 18410, lr = 0.0001
I0623 12:48:43.679435  5064 solver.cpp:213] Iteration 18420, loss = 2.99337
I0623 12:48:43.679452  5064 solver.cpp:228]     Train net output #0: softmax = 2.99337 (* 1 = 2.99337 loss)
I0623 12:48:43.679457  5064 solver.cpp:473] Iteration 18420, lr = 0.0001
I0623 12:48:44.700399  5064 solver.cpp:213] Iteration 18430, loss = 2.86681
I0623 12:48:44.700419  5064 solver.cpp:228]     Train net output #0: softmax = 2.86681 (* 1 = 2.86681 loss)
I0623 12:48:44.700424  5064 solver.cpp:473] Iteration 18430, lr = 0.0001
I0623 12:48:45.721094  5064 solver.cpp:213] Iteration 18440, loss = 2.85826
I0623 12:48:45.721117  5064 solver.cpp:228]     Train net output #0: softmax = 2.85826 (* 1 = 2.85826 loss)
I0623 12:48:45.721122  5064 solver.cpp:473] Iteration 18440, lr = 0.0001
I0623 12:48:46.741068  5064 solver.cpp:213] Iteration 18450, loss = 2.8726
I0623 12:48:46.741094  5064 solver.cpp:228]     Train net output #0: softmax = 2.8726 (* 1 = 2.8726 loss)
I0623 12:48:46.741219  5064 solver.cpp:473] Iteration 18450, lr = 0.0001
I0623 12:48:47.759143  5064 solver.cpp:213] Iteration 18460, loss = 2.88464
I0623 12:48:47.759178  5064 solver.cpp:228]     Train net output #0: softmax = 2.88464 (* 1 = 2.88464 loss)
I0623 12:48:47.759184  5064 solver.cpp:473] Iteration 18460, lr = 0.0001
I0623 12:48:48.779057  5064 solver.cpp:213] Iteration 18470, loss = 2.7986
I0623 12:48:48.779075  5064 solver.cpp:228]     Train net output #0: softmax = 2.7986 (* 1 = 2.7986 loss)
I0623 12:48:48.779080  5064 solver.cpp:473] Iteration 18470, lr = 0.0001
I0623 12:48:49.799811  5064 solver.cpp:213] Iteration 18480, loss = 2.93658
I0623 12:48:49.799832  5064 solver.cpp:228]     Train net output #0: softmax = 2.93658 (* 1 = 2.93658 loss)
I0623 12:48:49.799837  5064 solver.cpp:473] Iteration 18480, lr = 0.0001
I0623 12:48:50.820464  5064 solver.cpp:213] Iteration 18490, loss = 2.62656
I0623 12:48:50.820483  5064 solver.cpp:228]     Train net output #0: softmax = 2.62656 (* 1 = 2.62656 loss)
I0623 12:48:50.820488  5064 solver.cpp:473] Iteration 18490, lr = 0.0001
I0623 12:48:51.840714  5064 solver.cpp:213] Iteration 18500, loss = 2.87628
I0623 12:48:51.840737  5064 solver.cpp:228]     Train net output #0: softmax = 2.87628 (* 1 = 2.87628 loss)
I0623 12:48:51.840869  5064 solver.cpp:473] Iteration 18500, lr = 0.0001
I0623 12:48:52.861747  5064 solver.cpp:213] Iteration 18510, loss = 3.00337
I0623 12:48:52.861768  5064 solver.cpp:228]     Train net output #0: softmax = 3.00337 (* 1 = 3.00337 loss)
I0623 12:48:52.861774  5064 solver.cpp:473] Iteration 18510, lr = 0.0001
I0623 12:48:53.882799  5064 solver.cpp:213] Iteration 18520, loss = 2.75503
I0623 12:48:53.882818  5064 solver.cpp:228]     Train net output #0: softmax = 2.75503 (* 1 = 2.75503 loss)
I0623 12:48:53.882824  5064 solver.cpp:473] Iteration 18520, lr = 0.0001
I0623 12:48:54.903699  5064 solver.cpp:213] Iteration 18530, loss = 2.92654
I0623 12:48:54.903717  5064 solver.cpp:228]     Train net output #0: softmax = 2.92654 (* 1 = 2.92654 loss)
I0623 12:48:54.903723  5064 solver.cpp:473] Iteration 18530, lr = 0.0001
I0623 12:48:55.924705  5064 solver.cpp:213] Iteration 18540, loss = 3.00322
I0623 12:48:55.924726  5064 solver.cpp:228]     Train net output #0: softmax = 3.00322 (* 1 = 3.00322 loss)
I0623 12:48:55.924732  5064 solver.cpp:473] Iteration 18540, lr = 0.0001
I0623 12:48:56.945106  5064 solver.cpp:213] Iteration 18550, loss = 3.06405
I0623 12:48:56.945133  5064 solver.cpp:228]     Train net output #0: softmax = 3.06405 (* 1 = 3.06405 loss)
I0623 12:48:56.945257  5064 solver.cpp:473] Iteration 18550, lr = 0.0001
I0623 12:48:57.965982  5064 solver.cpp:213] Iteration 18560, loss = 2.77352
I0623 12:48:57.966002  5064 solver.cpp:228]     Train net output #0: softmax = 2.77352 (* 1 = 2.77352 loss)
I0623 12:48:57.966007  5064 solver.cpp:473] Iteration 18560, lr = 0.0001
I0623 12:48:58.986899  5064 solver.cpp:213] Iteration 18570, loss = 2.82453
I0623 12:48:58.986930  5064 solver.cpp:228]     Train net output #0: softmax = 2.82453 (* 1 = 2.82453 loss)
I0623 12:48:58.986935  5064 solver.cpp:473] Iteration 18570, lr = 0.0001
I0623 12:49:00.007257  5064 solver.cpp:213] Iteration 18580, loss = 2.82863
I0623 12:49:00.007275  5064 solver.cpp:228]     Train net output #0: softmax = 2.82863 (* 1 = 2.82863 loss)
I0623 12:49:00.007280  5064 solver.cpp:473] Iteration 18580, lr = 0.0001
I0623 12:49:01.027931  5064 solver.cpp:213] Iteration 18590, loss = 2.73523
I0623 12:49:01.027952  5064 solver.cpp:228]     Train net output #0: softmax = 2.73523 (* 1 = 2.73523 loss)
I0623 12:49:01.027957  5064 solver.cpp:473] Iteration 18590, lr = 0.0001
I0623 12:49:02.048602  5064 solver.cpp:213] Iteration 18600, loss = 2.86792
I0623 12:49:02.048797  5064 solver.cpp:228]     Train net output #0: softmax = 2.86792 (* 1 = 2.86792 loss)
I0623 12:49:02.048805  5064 solver.cpp:473] Iteration 18600, lr = 0.0001
I0623 12:49:03.069403  5064 solver.cpp:213] Iteration 18610, loss = 2.87941
I0623 12:49:03.069424  5064 solver.cpp:228]     Train net output #0: softmax = 2.87941 (* 1 = 2.87941 loss)
I0623 12:49:03.069430  5064 solver.cpp:473] Iteration 18610, lr = 0.0001
I0623 12:49:04.090271  5064 solver.cpp:213] Iteration 18620, loss = 2.94662
I0623 12:49:04.090291  5064 solver.cpp:228]     Train net output #0: softmax = 2.94662 (* 1 = 2.94662 loss)
I0623 12:49:04.090296  5064 solver.cpp:473] Iteration 18620, lr = 0.0001
I0623 12:49:05.111142  5064 solver.cpp:213] Iteration 18630, loss = 3.01346
I0623 12:49:05.111163  5064 solver.cpp:228]     Train net output #0: softmax = 3.01346 (* 1 = 3.01346 loss)
I0623 12:49:05.111169  5064 solver.cpp:473] Iteration 18630, lr = 0.0001
I0623 12:49:06.131628  5064 solver.cpp:213] Iteration 18640, loss = 2.8137
I0623 12:49:06.131649  5064 solver.cpp:228]     Train net output #0: softmax = 2.8137 (* 1 = 2.8137 loss)
I0623 12:49:06.131654  5064 solver.cpp:473] Iteration 18640, lr = 0.0001
I0623 12:49:07.152228  5064 solver.cpp:213] Iteration 18650, loss = 2.92962
I0623 12:49:07.152257  5064 solver.cpp:228]     Train net output #0: softmax = 2.92962 (* 1 = 2.92962 loss)
I0623 12:49:07.152415  5064 solver.cpp:473] Iteration 18650, lr = 0.0001
I0623 12:49:08.172027  5064 solver.cpp:213] Iteration 18660, loss = 2.92415
I0623 12:49:08.172044  5064 solver.cpp:228]     Train net output #0: softmax = 2.92415 (* 1 = 2.92415 loss)
I0623 12:49:08.172050  5064 solver.cpp:473] Iteration 18660, lr = 0.0001
I0623 12:49:09.185536  5064 solver.cpp:213] Iteration 18670, loss = 2.84404
I0623 12:49:09.185559  5064 solver.cpp:228]     Train net output #0: softmax = 2.84404 (* 1 = 2.84404 loss)
I0623 12:49:09.185564  5064 solver.cpp:473] Iteration 18670, lr = 0.0001
I0623 12:49:10.197509  5064 solver.cpp:213] Iteration 18680, loss = 2.77195
I0623 12:49:10.197531  5064 solver.cpp:228]     Train net output #0: softmax = 2.77195 (* 1 = 2.77195 loss)
I0623 12:49:10.197537  5064 solver.cpp:473] Iteration 18680, lr = 0.0001
I0623 12:49:11.215102  5064 solver.cpp:213] Iteration 18690, loss = 3.06395
I0623 12:49:11.215123  5064 solver.cpp:228]     Train net output #0: softmax = 3.06395 (* 1 = 3.06395 loss)
I0623 12:49:11.215128  5064 solver.cpp:473] Iteration 18690, lr = 0.0001
I0623 12:49:12.235405  5064 solver.cpp:213] Iteration 18700, loss = 2.86171
I0623 12:49:12.235430  5064 solver.cpp:228]     Train net output #0: softmax = 2.86171 (* 1 = 2.86171 loss)
I0623 12:49:12.235553  5064 solver.cpp:473] Iteration 18700, lr = 0.0001
I0623 12:49:13.256218  5064 solver.cpp:213] Iteration 18710, loss = 2.67535
I0623 12:49:13.256237  5064 solver.cpp:228]     Train net output #0: softmax = 2.67535 (* 1 = 2.67535 loss)
I0623 12:49:13.256242  5064 solver.cpp:473] Iteration 18710, lr = 0.0001
I0623 12:49:14.276918  5064 solver.cpp:213] Iteration 18720, loss = 2.70963
I0623 12:49:14.276939  5064 solver.cpp:228]     Train net output #0: softmax = 2.70963 (* 1 = 2.70963 loss)
I0623 12:49:14.276944  5064 solver.cpp:473] Iteration 18720, lr = 0.0001
I0623 12:49:15.297423  5064 solver.cpp:213] Iteration 18730, loss = 2.86798
I0623 12:49:15.297442  5064 solver.cpp:228]     Train net output #0: softmax = 2.86798 (* 1 = 2.86798 loss)
I0623 12:49:15.297447  5064 solver.cpp:473] Iteration 18730, lr = 0.0001
I0623 12:49:16.318029  5064 solver.cpp:213] Iteration 18740, loss = 2.85893
I0623 12:49:16.318050  5064 solver.cpp:228]     Train net output #0: softmax = 2.85893 (* 1 = 2.85893 loss)
I0623 12:49:16.318056  5064 solver.cpp:473] Iteration 18740, lr = 0.0001
I0623 12:49:17.338820  5064 solver.cpp:213] Iteration 18750, loss = 2.8479
I0623 12:49:17.338848  5064 solver.cpp:228]     Train net output #0: softmax = 2.8479 (* 1 = 2.8479 loss)
I0623 12:49:17.338996  5064 solver.cpp:473] Iteration 18750, lr = 0.0001
I0623 12:49:18.359771  5064 solver.cpp:213] Iteration 18760, loss = 2.98093
I0623 12:49:18.359807  5064 solver.cpp:228]     Train net output #0: softmax = 2.98093 (* 1 = 2.98093 loss)
I0623 12:49:18.359812  5064 solver.cpp:473] Iteration 18760, lr = 0.0001
I0623 12:49:19.379945  5064 solver.cpp:213] Iteration 18770, loss = 2.92842
I0623 12:49:19.379966  5064 solver.cpp:228]     Train net output #0: softmax = 2.92842 (* 1 = 2.92842 loss)
I0623 12:49:19.379971  5064 solver.cpp:473] Iteration 18770, lr = 0.0001
I0623 12:49:20.401329  5064 solver.cpp:213] Iteration 18780, loss = 3.00679
I0623 12:49:20.401350  5064 solver.cpp:228]     Train net output #0: softmax = 3.00679 (* 1 = 3.00679 loss)
I0623 12:49:20.401355  5064 solver.cpp:473] Iteration 18780, lr = 0.0001
I0623 12:49:21.422063  5064 solver.cpp:213] Iteration 18790, loss = 2.80763
I0623 12:49:21.422085  5064 solver.cpp:228]     Train net output #0: softmax = 2.80763 (* 1 = 2.80763 loss)
I0623 12:49:21.422091  5064 solver.cpp:473] Iteration 18790, lr = 0.0001
I0623 12:49:22.442840  5064 solver.cpp:213] Iteration 18800, loss = 2.82653
I0623 12:49:22.442868  5064 solver.cpp:228]     Train net output #0: softmax = 2.82653 (* 1 = 2.82653 loss)
I0623 12:49:22.443002  5064 solver.cpp:473] Iteration 18800, lr = 0.0001
I0623 12:49:23.463456  5064 solver.cpp:213] Iteration 18810, loss = 2.96531
I0623 12:49:23.463475  5064 solver.cpp:228]     Train net output #0: softmax = 2.96531 (* 1 = 2.96531 loss)
I0623 12:49:23.463480  5064 solver.cpp:473] Iteration 18810, lr = 0.0001
I0623 12:49:24.484180  5064 solver.cpp:213] Iteration 18820, loss = 3.06515
I0623 12:49:24.484200  5064 solver.cpp:228]     Train net output #0: softmax = 3.06515 (* 1 = 3.06515 loss)
I0623 12:49:24.484205  5064 solver.cpp:473] Iteration 18820, lr = 0.0001
I0623 12:49:25.504595  5064 solver.cpp:213] Iteration 18830, loss = 2.8386
I0623 12:49:25.504613  5064 solver.cpp:228]     Train net output #0: softmax = 2.8386 (* 1 = 2.8386 loss)
I0623 12:49:25.504618  5064 solver.cpp:473] Iteration 18830, lr = 0.0001
I0623 12:49:26.525086  5064 solver.cpp:213] Iteration 18840, loss = 2.85513
I0623 12:49:26.525110  5064 solver.cpp:228]     Train net output #0: softmax = 2.85513 (* 1 = 2.85513 loss)
I0623 12:49:26.525115  5064 solver.cpp:473] Iteration 18840, lr = 0.0001
I0623 12:49:27.545568  5064 solver.cpp:213] Iteration 18850, loss = 2.9485
I0623 12:49:27.545593  5064 solver.cpp:228]     Train net output #0: softmax = 2.9485 (* 1 = 2.9485 loss)
I0623 12:49:27.545719  5064 solver.cpp:473] Iteration 18850, lr = 0.0001
I0623 12:49:28.564308  5064 solver.cpp:213] Iteration 18860, loss = 2.89002
I0623 12:49:28.564332  5064 solver.cpp:228]     Train net output #0: softmax = 2.89002 (* 1 = 2.89002 loss)
I0623 12:49:28.564338  5064 solver.cpp:473] Iteration 18860, lr = 0.0001
I0623 12:49:29.576442  5064 solver.cpp:213] Iteration 18870, loss = 2.98319
I0623 12:49:29.576462  5064 solver.cpp:228]     Train net output #0: softmax = 2.98319 (* 1 = 2.98319 loss)
I0623 12:49:29.576469  5064 solver.cpp:473] Iteration 18870, lr = 0.0001
I0623 12:49:30.587971  5064 solver.cpp:213] Iteration 18880, loss = 3.06826
I0623 12:49:30.587988  5064 solver.cpp:228]     Train net output #0: softmax = 3.06826 (* 1 = 3.06826 loss)
I0623 12:49:30.587993  5064 solver.cpp:473] Iteration 18880, lr = 0.0001
I0623 12:49:31.608642  5064 solver.cpp:213] Iteration 18890, loss = 3.34138
I0623 12:49:31.608665  5064 solver.cpp:228]     Train net output #0: softmax = 3.34138 (* 1 = 3.34138 loss)
I0623 12:49:31.608670  5064 solver.cpp:473] Iteration 18890, lr = 0.0001
I0623 12:49:32.629732  5064 solver.cpp:213] Iteration 18900, loss = 3.04648
I0623 12:49:32.629912  5064 solver.cpp:228]     Train net output #0: softmax = 3.04648 (* 1 = 3.04648 loss)
I0623 12:49:32.629920  5064 solver.cpp:473] Iteration 18900, lr = 0.0001
I0623 12:49:33.650300  5064 solver.cpp:213] Iteration 18910, loss = 3.05358
I0623 12:49:33.650322  5064 solver.cpp:228]     Train net output #0: softmax = 3.05358 (* 1 = 3.05358 loss)
I0623 12:49:33.650327  5064 solver.cpp:473] Iteration 18910, lr = 0.0001
I0623 12:49:34.671138  5064 solver.cpp:213] Iteration 18920, loss = 3.04802
I0623 12:49:34.671157  5064 solver.cpp:228]     Train net output #0: softmax = 3.04802 (* 1 = 3.04802 loss)
I0623 12:49:34.671162  5064 solver.cpp:473] Iteration 18920, lr = 0.0001
I0623 12:49:35.691689  5064 solver.cpp:213] Iteration 18930, loss = 3.19163
I0623 12:49:35.691709  5064 solver.cpp:228]     Train net output #0: softmax = 3.19163 (* 1 = 3.19163 loss)
I0623 12:49:35.691714  5064 solver.cpp:473] Iteration 18930, lr = 0.0001
I0623 12:49:36.711969  5064 solver.cpp:213] Iteration 18940, loss = 3.02565
I0623 12:49:36.711990  5064 solver.cpp:228]     Train net output #0: softmax = 3.02565 (* 1 = 3.02565 loss)
I0623 12:49:36.711995  5064 solver.cpp:473] Iteration 18940, lr = 0.0001
I0623 12:49:37.732645  5064 solver.cpp:213] Iteration 18950, loss = 2.78335
I0623 12:49:37.732676  5064 solver.cpp:228]     Train net output #0: softmax = 2.78335 (* 1 = 2.78335 loss)
I0623 12:49:37.732813  5064 solver.cpp:473] Iteration 18950, lr = 0.0001
I0623 12:49:38.753564  5064 solver.cpp:213] Iteration 18960, loss = 2.43545
I0623 12:49:38.753585  5064 solver.cpp:228]     Train net output #0: softmax = 2.43545 (* 1 = 2.43545 loss)
I0623 12:49:38.753592  5064 solver.cpp:473] Iteration 18960, lr = 0.0001
I0623 12:49:39.772442  5064 solver.cpp:213] Iteration 18970, loss = 2.66618
I0623 12:49:39.772464  5064 solver.cpp:228]     Train net output #0: softmax = 2.66618 (* 1 = 2.66618 loss)
I0623 12:49:39.772469  5064 solver.cpp:473] Iteration 18970, lr = 0.0001
I0623 12:49:40.792969  5064 solver.cpp:213] Iteration 18980, loss = 2.74439
I0623 12:49:40.792989  5064 solver.cpp:228]     Train net output #0: softmax = 2.74439 (* 1 = 2.74439 loss)
I0623 12:49:40.792994  5064 solver.cpp:473] Iteration 18980, lr = 0.0001
I0623 12:49:41.813532  5064 solver.cpp:213] Iteration 18990, loss = 2.83121
I0623 12:49:41.813554  5064 solver.cpp:228]     Train net output #0: softmax = 2.83121 (* 1 = 2.83121 loss)
I0623 12:49:41.813558  5064 solver.cpp:473] Iteration 18990, lr = 0.0001
I0623 12:49:42.763829  5064 solver.cpp:362] Snapshotting to snapshots/16-06-21_16h05m31s_0_11_pretrainClassification_iter_19000.caffemodel
I0623 12:49:42.764891  5064 solver.cpp:370] Snapshotting solver state to snapshots/16-06-21_16h05m31s_0_11_pretrainClassification_iter_19000.solverstate
I0623 12:49:42.765498  5064 solver.cpp:291] Iteration 19000, Testing net (#0)
I0623 12:49:42.926482  5064 solver.cpp:342]     Test net output #0: accuracy_top_1 = 0.309375
I0623 12:49:42.926501  5064 solver.cpp:342]     Test net output #1: accuracy_top_5 = 0.5875
I0623 12:49:42.926508  5064 solver.cpp:342]     Test net output #2: softmax = 2.83554 (* 1 = 2.83554 loss)
I0623 12:49:42.997247  5064 solver.cpp:213] Iteration 19000, loss = 2.70263
I0623 12:49:42.997268  5064 solver.cpp:228]     Train net output #0: softmax = 2.70263 (* 1 = 2.70263 loss)
I0623 12:49:42.997274  5064 solver.cpp:473] Iteration 19000, lr = 0.0001
I0623 12:49:44.017400  5064 solver.cpp:213] Iteration 19010, loss = 2.76641
I0623 12:49:44.017419  5064 solver.cpp:228]     Train net output #0: softmax = 2.76641 (* 1 = 2.76641 loss)
I0623 12:49:44.017424  5064 solver.cpp:473] Iteration 19010, lr = 0.0001
I0623 12:49:45.037722  5064 solver.cpp:213] Iteration 19020, loss = 2.77093
I0623 12:49:45.037744  5064 solver.cpp:228]     Train net output #0: softmax = 2.77093 (* 1 = 2.77093 loss)
I0623 12:49:45.037750  5064 solver.cpp:473] Iteration 19020, lr = 0.0001
I0623 12:49:46.058794  5064 solver.cpp:213] Iteration 19030, loss = 2.82468
I0623 12:49:46.058815  5064 solver.cpp:228]     Train net output #0: softmax = 2.82468 (* 1 = 2.82468 loss)
I0623 12:49:46.058840  5064 solver.cpp:473] Iteration 19030, lr = 0.0001
I0623 12:49:47.079937  5064 solver.cpp:213] Iteration 19040, loss = 2.99577
I0623 12:49:47.079963  5064 solver.cpp:228]     Train net output #0: softmax = 2.99577 (* 1 = 2.99577 loss)
I0623 12:49:47.079970  5064 solver.cpp:473] Iteration 19040, lr = 0.0001
I0623 12:49:48.098317  5064 solver.cpp:213] Iteration 19050, loss = 2.81856
I0623 12:49:48.098340  5064 solver.cpp:228]     Train net output #0: softmax = 2.81856 (* 1 = 2.81856 loss)
I0623 12:49:48.098351  5064 solver.cpp:473] Iteration 19050, lr = 0.0001
I0623 12:49:49.118508  5064 solver.cpp:213] Iteration 19060, loss = 2.69607
I0623 12:49:49.118532  5064 solver.cpp:228]     Train net output #0: softmax = 2.69607 (* 1 = 2.69607 loss)
I0623 12:49:49.118538  5064 solver.cpp:473] Iteration 19060, lr = 0.0001
I0623 12:49:50.139971  5064 solver.cpp:213] Iteration 19070, loss = 2.79219
I0623 12:49:50.139992  5064 solver.cpp:228]     Train net output #0: softmax = 2.79219 (* 1 = 2.79219 loss)
I0623 12:49:50.139997  5064 solver.cpp:473] Iteration 19070, lr = 0.0001
I0623 12:49:51.160025  5064 solver.cpp:213] Iteration 19080, loss = 2.83782
I0623 12:49:51.160050  5064 solver.cpp:228]     Train net output #0: softmax = 2.83782 (* 1 = 2.83782 loss)
I0623 12:49:51.160058  5064 solver.cpp:473] Iteration 19080, lr = 0.0001
I0623 12:49:52.179445  5064 solver.cpp:213] Iteration 19090, loss = 3.23055
I0623 12:49:52.179466  5064 solver.cpp:228]     Train net output #0: softmax = 3.23055 (* 1 = 3.23055 loss)
I0623 12:49:52.179471  5064 solver.cpp:473] Iteration 19090, lr = 0.0001
I0623 12:49:53.200125  5064 solver.cpp:213] Iteration 19100, loss = 2.77934
I0623 12:49:53.200153  5064 solver.cpp:228]     Train net output #0: softmax = 2.77934 (* 1 = 2.77934 loss)
I0623 12:49:53.200307  5064 solver.cpp:473] Iteration 19100, lr = 0.0001
I0623 12:49:54.221531  5064 solver.cpp:213] Iteration 19110, loss = 2.83741
I0623 12:49:54.221552  5064 solver.cpp:228]     Train net output #0: softmax = 2.83741 (* 1 = 2.83741 loss)
I0623 12:49:54.221559  5064 solver.cpp:473] Iteration 19110, lr = 0.0001
I0623 12:49:55.240175  5064 solver.cpp:213] Iteration 19120, loss = 2.92607
I0623 12:49:55.240205  5064 solver.cpp:228]     Train net output #0: softmax = 2.92607 (* 1 = 2.92607 loss)
I0623 12:49:55.240211  5064 solver.cpp:473] Iteration 19120, lr = 0.0001
I0623 12:49:56.261546  5064 solver.cpp:213] Iteration 19130, loss = 3.07918
I0623 12:49:56.261566  5064 solver.cpp:228]     Train net output #0: softmax = 3.07918 (* 1 = 3.07918 loss)
I0623 12:49:56.261571  5064 solver.cpp:473] Iteration 19130, lr = 0.0001
I0623 12:49:57.282485  5064 solver.cpp:213] Iteration 19140, loss = 2.91342
I0623 12:49:57.282505  5064 solver.cpp:228]     Train net output #0: softmax = 2.91342 (* 1 = 2.91342 loss)
I0623 12:49:57.282510  5064 solver.cpp:473] Iteration 19140, lr = 0.0001
I0623 12:49:58.303442  5064 solver.cpp:213] Iteration 19150, loss = 2.75196
I0623 12:49:58.303467  5064 solver.cpp:228]     Train net output #0: softmax = 2.75196 (* 1 = 2.75196 loss)
I0623 12:49:58.303628  5064 solver.cpp:473] Iteration 19150, lr = 0.0001
I0623 12:49:59.322059  5064 solver.cpp:213] Iteration 19160, loss = 2.85367
I0623 12:49:59.322087  5064 solver.cpp:228]     Train net output #0: softmax = 2.85367 (* 1 = 2.85367 loss)
I0623 12:49:59.322091  5064 solver.cpp:473] Iteration 19160, lr = 0.0001
I0623 12:50:00.342634  5064 solver.cpp:213] Iteration 19170, loss = 2.87506
I0623 12:50:00.342653  5064 solver.cpp:228]     Train net output #0: softmax = 2.87506 (* 1 = 2.87506 loss)
I0623 12:50:00.342658  5064 solver.cpp:473] Iteration 19170, lr = 0.0001
I0623 12:50:01.363387  5064 solver.cpp:213] Iteration 19180, loss = 2.92032
I0623 12:50:01.363407  5064 solver.cpp:228]     Train net output #0: softmax = 2.92032 (* 1 = 2.92032 loss)
I0623 12:50:01.363412  5064 solver.cpp:473] Iteration 19180, lr = 0.0001
I0623 12:50:02.384562  5064 solver.cpp:213] Iteration 19190, loss = 3.03982
I0623 12:50:02.384582  5064 solver.cpp:228]     Train net output #0: softmax = 3.03982 (* 1 = 3.03982 loss)
I0623 12:50:02.384605  5064 solver.cpp:473] Iteration 19190, lr = 0.0001
I0623 12:50:03.405339  5064 solver.cpp:213] Iteration 19200, loss = 3.00628
I0623 12:50:03.405391  5064 solver.cpp:228]     Train net output #0: softmax = 3.00628 (* 1 = 3.00628 loss)
I0623 12:50:03.405397  5064 solver.cpp:473] Iteration 19200, lr = 0.0001
I0623 12:50:04.425740  5064 solver.cpp:213] Iteration 19210, loss = 2.86711
I0623 12:50:04.425758  5064 solver.cpp:228]     Train net output #0: softmax = 2.86711 (* 1 = 2.86711 loss)
I0623 12:50:04.425763  5064 solver.cpp:473] Iteration 19210, lr = 0.0001
I0623 12:50:05.446683  5064 solver.cpp:213] Iteration 19220, loss = 2.79588
I0623 12:50:05.446703  5064 solver.cpp:228]     Train net output #0: softmax = 2.79588 (* 1 = 2.79588 loss)
I0623 12:50:05.446708  5064 solver.cpp:473] Iteration 19220, lr = 0.0001
I0623 12:50:06.467317  5064 solver.cpp:213] Iteration 19230, loss = 2.70369
I0623 12:50:06.467339  5064 solver.cpp:228]     Train net output #0: softmax = 2.70369 (* 1 = 2.70369 loss)
I0623 12:50:06.467344  5064 solver.cpp:473] Iteration 19230, lr = 0.0001
I0623 12:50:07.488337  5064 solver.cpp:213] Iteration 19240, loss = 2.61525
I0623 12:50:07.488358  5064 solver.cpp:228]     Train net output #0: softmax = 2.61525 (* 1 = 2.61525 loss)
I0623 12:50:07.488363  5064 solver.cpp:473] Iteration 19240, lr = 0.0001
I0623 12:50:08.508402  5064 solver.cpp:213] Iteration 19250, loss = 2.88182
I0623 12:50:08.508430  5064 solver.cpp:228]     Train net output #0: softmax = 2.88182 (* 1 = 2.88182 loss)
I0623 12:50:08.508558  5064 solver.cpp:473] Iteration 19250, lr = 0.0001
I0623 12:50:09.529520  5064 solver.cpp:213] Iteration 19260, loss = 2.89457
I0623 12:50:09.529541  5064 solver.cpp:228]     Train net output #0: softmax = 2.89457 (* 1 = 2.89457 loss)
I0623 12:50:09.529546  5064 solver.cpp:473] Iteration 19260, lr = 0.0001
I0623 12:50:10.550781  5064 solver.cpp:213] Iteration 19270, loss = 2.87809
I0623 12:50:10.550801  5064 solver.cpp:228]     Train net output #0: softmax = 2.87809 (* 1 = 2.87809 loss)
I0623 12:50:10.550806  5064 solver.cpp:473] Iteration 19270, lr = 0.0001
I0623 12:50:11.570601  5064 solver.cpp:213] Iteration 19280, loss = 3.03296
I0623 12:50:11.570621  5064 solver.cpp:228]     Train net output #0: softmax = 3.03296 (* 1 = 3.03296 loss)
I0623 12:50:11.570626  5064 solver.cpp:473] Iteration 19280, lr = 0.0001
I0623 12:50:12.590948  5064 solver.cpp:213] Iteration 19290, loss = 2.76493
I0623 12:50:12.590968  5064 solver.cpp:228]     Train net output #0: softmax = 2.76493 (* 1 = 2.76493 loss)
I0623 12:50:12.590973  5064 solver.cpp:473] Iteration 19290, lr = 0.0001
I0623 12:50:13.612051  5064 solver.cpp:213] Iteration 19300, loss = 2.88871
I0623 12:50:13.612071  5064 solver.cpp:228]     Train net output #0: softmax = 2.88871 (* 1 = 2.88871 loss)
I0623 12:50:13.612207  5064 solver.cpp:473] Iteration 19300, lr = 0.0001
I0623 12:50:14.631041  5064 solver.cpp:213] Iteration 19310, loss = 2.83592
I0623 12:50:14.631059  5064 solver.cpp:228]     Train net output #0: softmax = 2.83592 (* 1 = 2.83592 loss)
I0623 12:50:14.631064  5064 solver.cpp:473] Iteration 19310, lr = 0.0001
I0623 12:50:15.649860  5064 solver.cpp:213] Iteration 19320, loss = 2.76337
I0623 12:50:15.649883  5064 solver.cpp:228]     Train net output #0: softmax = 2.76337 (* 1 = 2.76337 loss)
I0623 12:50:15.649888  5064 solver.cpp:473] Iteration 19320, lr = 0.0001
I0623 12:50:16.670276  5064 solver.cpp:213] Iteration 19330, loss = 3.12775
I0623 12:50:16.670303  5064 solver.cpp:228]     Train net output #0: softmax = 3.12775 (* 1 = 3.12775 loss)
I0623 12:50:16.670310  5064 solver.cpp:473] Iteration 19330, lr = 0.0001
I0623 12:50:17.690992  5064 solver.cpp:213] Iteration 19340, loss = 3.01084
I0623 12:50:17.691010  5064 solver.cpp:228]     Train net output #0: softmax = 3.01084 (* 1 = 3.01084 loss)
I0623 12:50:17.691015  5064 solver.cpp:473] Iteration 19340, lr = 0.0001
I0623 12:50:18.712321  5064 solver.cpp:213] Iteration 19350, loss = 2.78352
I0623 12:50:18.712342  5064 solver.cpp:228]     Train net output #0: softmax = 2.78352 (* 1 = 2.78352 loss)
I0623 12:50:18.712465  5064 solver.cpp:473] Iteration 19350, lr = 0.0001
I0623 12:50:19.732923  5064 solver.cpp:213] Iteration 19360, loss = 2.72697
I0623 12:50:19.732957  5064 solver.cpp:228]     Train net output #0: softmax = 2.72697 (* 1 = 2.72697 loss)
I0623 12:50:19.732964  5064 solver.cpp:473] Iteration 19360, lr = 0.0001
I0623 12:50:20.753119  5064 solver.cpp:213] Iteration 19370, loss = 2.88127
I0623 12:50:20.753142  5064 solver.cpp:228]     Train net output #0: softmax = 2.88127 (* 1 = 2.88127 loss)
I0623 12:50:20.753149  5064 solver.cpp:473] Iteration 19370, lr = 0.0001
I0623 12:50:21.774515  5064 solver.cpp:213] Iteration 19380, loss = 3.05724
I0623 12:50:21.774536  5064 solver.cpp:228]     Train net output #0: softmax = 3.05724 (* 1 = 3.05724 loss)
I0623 12:50:21.774543  5064 solver.cpp:473] Iteration 19380, lr = 0.0001
I0623 12:50:22.795132  5064 solver.cpp:213] Iteration 19390, loss = 2.72591
I0623 12:50:22.795155  5064 solver.cpp:228]     Train net output #0: softmax = 2.72591 (* 1 = 2.72591 loss)
I0623 12:50:22.795159  5064 solver.cpp:473] Iteration 19390, lr = 0.0001
I0623 12:50:23.815207  5064 solver.cpp:213] Iteration 19400, loss = 2.56849
I0623 12:50:23.815232  5064 solver.cpp:228]     Train net output #0: softmax = 2.56849 (* 1 = 2.56849 loss)
I0623 12:50:23.815378  5064 solver.cpp:473] Iteration 19400, lr = 0.0001
I0623 12:50:24.835739  5064 solver.cpp:213] Iteration 19410, loss = 2.62578
I0623 12:50:24.835762  5064 solver.cpp:228]     Train net output #0: softmax = 2.62578 (* 1 = 2.62578 loss)
I0623 12:50:24.835767  5064 solver.cpp:473] Iteration 19410, lr = 0.0001
I0623 12:50:25.856695  5064 solver.cpp:213] Iteration 19420, loss = 2.77942
I0623 12:50:25.856715  5064 solver.cpp:228]     Train net output #0: softmax = 2.77942 (* 1 = 2.77942 loss)
I0623 12:50:25.856721  5064 solver.cpp:473] Iteration 19420, lr = 0.0001
I0623 12:50:26.877435  5064 solver.cpp:213] Iteration 19430, loss = 2.79998
I0623 12:50:26.877456  5064 solver.cpp:228]     Train net output #0: softmax = 2.79998 (* 1 = 2.79998 loss)
I0623 12:50:26.877461  5064 solver.cpp:473] Iteration 19430, lr = 0.0001
I0623 12:50:27.897403  5064 solver.cpp:213] Iteration 19440, loss = 2.93675
I0623 12:50:27.897423  5064 solver.cpp:228]     Train net output #0: softmax = 2.93675 (* 1 = 2.93675 loss)
I0623 12:50:27.897428  5064 solver.cpp:473] Iteration 19440, lr = 0.0001
I0623 12:50:28.918324  5064 solver.cpp:213] Iteration 19450, loss = 2.74223
I0623 12:50:28.918349  5064 solver.cpp:228]     Train net output #0: softmax = 2.74223 (* 1 = 2.74223 loss)
I0623 12:50:28.918475  5064 solver.cpp:473] Iteration 19450, lr = 0.0001
I0623 12:50:29.939247  5064 solver.cpp:213] Iteration 19460, loss = 2.78501
I0623 12:50:29.939273  5064 solver.cpp:228]     Train net output #0: softmax = 2.78501 (* 1 = 2.78501 loss)
I0623 12:50:29.939278  5064 solver.cpp:473] Iteration 19460, lr = 0.0001
I0623 12:50:30.959760  5064 solver.cpp:213] Iteration 19470, loss = 2.79193
I0623 12:50:30.959780  5064 solver.cpp:228]     Train net output #0: softmax = 2.79193 (* 1 = 2.79193 loss)
I0623 12:50:30.959785  5064 solver.cpp:473] Iteration 19470, lr = 0.0001
I0623 12:50:31.980664  5064 solver.cpp:213] Iteration 19480, loss = 2.8715
I0623 12:50:31.980685  5064 solver.cpp:228]     Train net output #0: softmax = 2.8715 (* 1 = 2.8715 loss)
I0623 12:50:31.980690  5064 solver.cpp:473] Iteration 19480, lr = 0.0001
I0623 12:50:33.001742  5064 solver.cpp:213] Iteration 19490, loss = 2.69647
I0623 12:50:33.001763  5064 solver.cpp:228]     Train net output #0: softmax = 2.69647 (* 1 = 2.69647 loss)
I0623 12:50:33.001770  5064 solver.cpp:473] Iteration 19490, lr = 0.0001
I0623 12:50:34.022722  5064 solver.cpp:213] Iteration 19500, loss = 2.54299
I0623 12:50:34.022907  5064 solver.cpp:228]     Train net output #0: softmax = 2.54299 (* 1 = 2.54299 loss)
I0623 12:50:34.022914  5064 solver.cpp:473] Iteration 19500, lr = 0.0001
I0623 12:50:35.043503  5064 solver.cpp:213] Iteration 19510, loss = 2.89645
I0623 12:50:35.043521  5064 solver.cpp:228]     Train net output #0: softmax = 2.89645 (* 1 = 2.89645 loss)
I0623 12:50:35.043526  5064 solver.cpp:473] Iteration 19510, lr = 0.0001
I0623 12:50:36.064043  5064 solver.cpp:213] Iteration 19520, loss = 2.80984
I0623 12:50:36.064066  5064 solver.cpp:228]     Train net output #0: softmax = 2.80984 (* 1 = 2.80984 loss)
I0623 12:50:36.064071  5064 solver.cpp:473] Iteration 19520, lr = 0.0001
I0623 12:50:37.085139  5064 solver.cpp:213] Iteration 19530, loss = 2.80141
I0623 12:50:37.085160  5064 solver.cpp:228]     Train net output #0: softmax = 2.80141 (* 1 = 2.80141 loss)
I0623 12:50:37.085165  5064 solver.cpp:473] Iteration 19530, lr = 0.0001
I0623 12:50:38.105895  5064 solver.cpp:213] Iteration 19540, loss = 2.54585
I0623 12:50:38.105916  5064 solver.cpp:228]     Train net output #0: softmax = 2.54585 (* 1 = 2.54585 loss)
I0623 12:50:38.105921  5064 solver.cpp:473] Iteration 19540, lr = 0.0001
I0623 12:50:39.126700  5064 solver.cpp:213] Iteration 19550, loss = 2.85313
I0623 12:50:39.126724  5064 solver.cpp:228]     Train net output #0: softmax = 2.85313 (* 1 = 2.85313 loss)
I0623 12:50:39.126858  5064 solver.cpp:473] Iteration 19550, lr = 0.0001
I0623 12:50:40.147518  5064 solver.cpp:213] Iteration 19560, loss = 2.79931
I0623 12:50:40.147541  5064 solver.cpp:228]     Train net output #0: softmax = 2.79931 (* 1 = 2.79931 loss)
I0623 12:50:40.147545  5064 solver.cpp:473] Iteration 19560, lr = 0.0001
I0623 12:50:41.168593  5064 solver.cpp:213] Iteration 19570, loss = 2.67471
I0623 12:50:41.168615  5064 solver.cpp:228]     Train net output #0: softmax = 2.67471 (* 1 = 2.67471 loss)
I0623 12:50:41.168622  5064 solver.cpp:473] Iteration 19570, lr = 0.0001
I0623 12:50:42.188733  5064 solver.cpp:213] Iteration 19580, loss = 2.72037
I0623 12:50:42.188755  5064 solver.cpp:228]     Train net output #0: softmax = 2.72037 (* 1 = 2.72037 loss)
I0623 12:50:42.188760  5064 solver.cpp:473] Iteration 19580, lr = 0.0001
I0623 12:50:43.209491  5064 solver.cpp:213] Iteration 19590, loss = 2.9827
I0623 12:50:43.209511  5064 solver.cpp:228]     Train net output #0: softmax = 2.9827 (* 1 = 2.9827 loss)
I0623 12:50:43.209517  5064 solver.cpp:473] Iteration 19590, lr = 0.0001
I0623 12:50:44.229935  5064 solver.cpp:213] Iteration 19600, loss = 2.80278
I0623 12:50:44.229959  5064 solver.cpp:228]     Train net output #0: softmax = 2.80278 (* 1 = 2.80278 loss)
I0623 12:50:44.230079  5064 solver.cpp:473] Iteration 19600, lr = 0.0001
I0623 12:50:45.247261  5064 solver.cpp:213] Iteration 19610, loss = 2.77519
I0623 12:50:45.247282  5064 solver.cpp:228]     Train net output #0: softmax = 2.77519 (* 1 = 2.77519 loss)
I0623 12:50:45.247287  5064 solver.cpp:473] Iteration 19610, lr = 0.0001
I0623 12:50:46.268216  5064 solver.cpp:213] Iteration 19620, loss = 2.76148
I0623 12:50:46.268239  5064 solver.cpp:228]     Train net output #0: softmax = 2.76148 (* 1 = 2.76148 loss)
I0623 12:50:46.268244  5064 solver.cpp:473] Iteration 19620, lr = 0.0001
I0623 12:50:47.288650  5064 solver.cpp:213] Iteration 19630, loss = 2.87979
I0623 12:50:47.288671  5064 solver.cpp:228]     Train net output #0: softmax = 2.87979 (* 1 = 2.87979 loss)
I0623 12:50:47.288676  5064 solver.cpp:473] Iteration 19630, lr = 0.0001
I0623 12:50:48.309010  5064 solver.cpp:213] Iteration 19640, loss = 2.76472
I0623 12:50:48.309031  5064 solver.cpp:228]     Train net output #0: softmax = 2.76472 (* 1 = 2.76472 loss)
I0623 12:50:48.309036  5064 solver.cpp:473] Iteration 19640, lr = 0.0001
I0623 12:50:49.329432  5064 solver.cpp:213] Iteration 19650, loss = 2.98925
I0623 12:50:49.329455  5064 solver.cpp:228]     Train net output #0: softmax = 2.98925 (* 1 = 2.98925 loss)
I0623 12:50:49.329578  5064 solver.cpp:473] Iteration 19650, lr = 0.0001
I0623 12:50:50.349864  5064 solver.cpp:213] Iteration 19660, loss = 2.74992
I0623 12:50:50.349900  5064 solver.cpp:228]     Train net output #0: softmax = 2.74992 (* 1 = 2.74992 loss)
I0623 12:50:50.349906  5064 solver.cpp:473] Iteration 19660, lr = 0.0001
I0623 12:50:51.370123  5064 solver.cpp:213] Iteration 19670, loss = 2.79435
I0623 12:50:51.370143  5064 solver.cpp:228]     Train net output #0: softmax = 2.79435 (* 1 = 2.79435 loss)
I0623 12:50:51.370148  5064 solver.cpp:473] Iteration 19670, lr = 0.0001
I0623 12:50:52.390344  5064 solver.cpp:213] Iteration 19680, loss = 2.83969
I0623 12:50:52.390363  5064 solver.cpp:228]     Train net output #0: softmax = 2.83969 (* 1 = 2.83969 loss)
I0623 12:50:52.390368  5064 solver.cpp:473] Iteration 19680, lr = 0.0001
I0623 12:50:53.411027  5064 solver.cpp:213] Iteration 19690, loss = 3.05207
I0623 12:50:53.411047  5064 solver.cpp:228]     Train net output #0: softmax = 3.05207 (* 1 = 3.05207 loss)
I0623 12:50:53.411052  5064 solver.cpp:473] Iteration 19690, lr = 0.0001
I0623 12:50:54.431978  5064 solver.cpp:213] Iteration 19700, loss = 2.74159
I0623 12:50:54.431999  5064 solver.cpp:228]     Train net output #0: softmax = 2.74159 (* 1 = 2.74159 loss)
I0623 12:50:54.432004  5064 solver.cpp:473] Iteration 19700, lr = 0.0001
I0623 12:50:55.452384  5064 solver.cpp:213] Iteration 19710, loss = 2.67434
I0623 12:50:55.452406  5064 solver.cpp:228]     Train net output #0: softmax = 2.67434 (* 1 = 2.67434 loss)
I0623 12:50:55.452411  5064 solver.cpp:473] Iteration 19710, lr = 0.0001
I0623 12:50:56.473266  5064 solver.cpp:213] Iteration 19720, loss = 3.1294
I0623 12:50:56.473289  5064 solver.cpp:228]     Train net output #0: softmax = 3.1294 (* 1 = 3.1294 loss)
I0623 12:50:56.473294  5064 solver.cpp:473] Iteration 19720, lr = 0.0001
I0623 12:50:57.494112  5064 solver.cpp:213] Iteration 19730, loss = 2.67899
I0623 12:50:57.494133  5064 solver.cpp:228]     Train net output #0: softmax = 2.67899 (* 1 = 2.67899 loss)
I0623 12:50:57.494138  5064 solver.cpp:473] Iteration 19730, lr = 0.0001
I0623 12:50:58.514878  5064 solver.cpp:213] Iteration 19740, loss = 2.9531
I0623 12:50:58.514899  5064 solver.cpp:228]     Train net output #0: softmax = 2.9531 (* 1 = 2.9531 loss)
I0623 12:50:58.514904  5064 solver.cpp:473] Iteration 19740, lr = 0.0001
I0623 12:50:59.535318  5064 solver.cpp:213] Iteration 19750, loss = 2.68817
I0623 12:50:59.535339  5064 solver.cpp:228]     Train net output #0: softmax = 2.68817 (* 1 = 2.68817 loss)
I0623 12:50:59.535480  5064 solver.cpp:473] Iteration 19750, lr = 0.0001
I0623 12:51:00.556026  5064 solver.cpp:213] Iteration 19760, loss = 2.88242
I0623 12:51:00.556046  5064 solver.cpp:228]     Train net output #0: softmax = 2.88242 (* 1 = 2.88242 loss)
I0623 12:51:00.556051  5064 solver.cpp:473] Iteration 19760, lr = 0.0001
I0623 12:51:01.576614  5064 solver.cpp:213] Iteration 19770, loss = 2.96278
I0623 12:51:01.576634  5064 solver.cpp:228]     Train net output #0: softmax = 2.96278 (* 1 = 2.96278 loss)
I0623 12:51:01.576639  5064 solver.cpp:473] Iteration 19770, lr = 0.0001
I0623 12:51:02.597913  5064 solver.cpp:213] Iteration 19780, loss = 2.59178
I0623 12:51:02.597935  5064 solver.cpp:228]     Train net output #0: softmax = 2.59178 (* 1 = 2.59178 loss)
I0623 12:51:02.597940  5064 solver.cpp:473] Iteration 19780, lr = 0.0001
I0623 12:51:03.618472  5064 solver.cpp:213] Iteration 19790, loss = 2.68511
I0623 12:51:03.618494  5064 solver.cpp:228]     Train net output #0: softmax = 2.68511 (* 1 = 2.68511 loss)
I0623 12:51:03.618499  5064 solver.cpp:473] Iteration 19790, lr = 0.0001
I0623 12:51:04.639019  5064 solver.cpp:213] Iteration 19800, loss = 2.55354
I0623 12:51:04.639191  5064 solver.cpp:228]     Train net output #0: softmax = 2.55354 (* 1 = 2.55354 loss)
I0623 12:51:04.639199  5064 solver.cpp:473] Iteration 19800, lr = 0.0001
I0623 12:51:05.659840  5064 solver.cpp:213] Iteration 19810, loss = 2.67149
I0623 12:51:05.659859  5064 solver.cpp:228]     Train net output #0: softmax = 2.67149 (* 1 = 2.67149 loss)
I0623 12:51:05.659864  5064 solver.cpp:473] Iteration 19810, lr = 0.0001
I0623 12:51:06.679836  5064 solver.cpp:213] Iteration 19820, loss = 2.80983
I0623 12:51:06.679854  5064 solver.cpp:228]     Train net output #0: softmax = 2.80983 (* 1 = 2.80983 loss)
I0623 12:51:06.679860  5064 solver.cpp:473] Iteration 19820, lr = 0.0001
I0623 12:51:07.700834  5064 solver.cpp:213] Iteration 19830, loss = 2.79734
I0623 12:51:07.700855  5064 solver.cpp:228]     Train net output #0: softmax = 2.79734 (* 1 = 2.79734 loss)
I0623 12:51:07.700860  5064 solver.cpp:473] Iteration 19830, lr = 0.0001
I0623 12:51:08.721523  5064 solver.cpp:213] Iteration 19840, loss = 2.86433
I0623 12:51:08.721544  5064 solver.cpp:228]     Train net output #0: softmax = 2.86433 (* 1 = 2.86433 loss)
I0623 12:51:08.721549  5064 solver.cpp:473] Iteration 19840, lr = 0.0001
I0623 12:51:09.742419  5064 solver.cpp:213] Iteration 19850, loss = 2.93786
I0623 12:51:09.742441  5064 solver.cpp:228]     Train net output #0: softmax = 2.93786 (* 1 = 2.93786 loss)
I0623 12:51:09.742605  5064 solver.cpp:473] Iteration 19850, lr = 0.0001
I0623 12:51:10.762936  5064 solver.cpp:213] Iteration 19860, loss = 2.69993
I0623 12:51:10.762955  5064 solver.cpp:228]     Train net output #0: softmax = 2.69993 (* 1 = 2.69993 loss)
I0623 12:51:10.762961  5064 solver.cpp:473] Iteration 19860, lr = 0.0001
I0623 12:51:11.783165  5064 solver.cpp:213] Iteration 19870, loss = 2.96657
I0623 12:51:11.783185  5064 solver.cpp:228]     Train net output #0: softmax = 2.96657 (* 1 = 2.96657 loss)
I0623 12:51:11.783190  5064 solver.cpp:473] Iteration 19870, lr = 0.0001
I0623 12:51:12.803360  5064 solver.cpp:213] Iteration 19880, loss = 2.73053
I0623 12:51:12.803380  5064 solver.cpp:228]     Train net output #0: softmax = 2.73053 (* 1 = 2.73053 loss)
I0623 12:51:12.803385  5064 solver.cpp:473] Iteration 19880, lr = 0.0001
I0623 12:51:13.824182  5064 solver.cpp:213] Iteration 19890, loss = 2.67621
I0623 12:51:13.824204  5064 solver.cpp:228]     Train net output #0: softmax = 2.67621 (* 1 = 2.67621 loss)
I0623 12:51:13.824209  5064 solver.cpp:473] Iteration 19890, lr = 0.0001
I0623 12:51:14.844619  5064 solver.cpp:213] Iteration 19900, loss = 2.77245
I0623 12:51:14.844643  5064 solver.cpp:228]     Train net output #0: softmax = 2.77245 (* 1 = 2.77245 loss)
I0623 12:51:14.844772  5064 solver.cpp:473] Iteration 19900, lr = 0.0001
I0623 12:51:15.865007  5064 solver.cpp:213] Iteration 19910, loss = 3.0039
I0623 12:51:15.865025  5064 solver.cpp:228]     Train net output #0: softmax = 3.0039 (* 1 = 3.0039 loss)
I0623 12:51:15.865031  5064 solver.cpp:473] Iteration 19910, lr = 0.0001
I0623 12:51:16.885018  5064 solver.cpp:213] Iteration 19920, loss = 2.87057
I0623 12:51:16.885037  5064 solver.cpp:228]     Train net output #0: softmax = 2.87057 (* 1 = 2.87057 loss)
I0623 12:51:16.885042  5064 solver.cpp:473] Iteration 19920, lr = 0.0001
I0623 12:51:17.905551  5064 solver.cpp:213] Iteration 19930, loss = 2.67744
I0623 12:51:17.905571  5064 solver.cpp:228]     Train net output #0: softmax = 2.67744 (* 1 = 2.67744 loss)
I0623 12:51:17.905577  5064 solver.cpp:473] Iteration 19930, lr = 0.0001
I0623 12:51:18.926163  5064 solver.cpp:213] Iteration 19940, loss = 2.99785
I0623 12:51:18.926185  5064 solver.cpp:228]     Train net output #0: softmax = 2.99785 (* 1 = 2.99785 loss)
I0623 12:51:18.926192  5064 solver.cpp:473] Iteration 19940, lr = 0.0001
I0623 12:51:19.946184  5064 solver.cpp:213] Iteration 19950, loss = 2.98147
I0623 12:51:19.946207  5064 solver.cpp:228]     Train net output #0: softmax = 2.98147 (* 1 = 2.98147 loss)
I0623 12:51:19.946331  5064 solver.cpp:473] Iteration 19950, lr = 0.0001
I0623 12:51:20.966476  5064 solver.cpp:213] Iteration 19960, loss = 2.75302
I0623 12:51:20.966508  5064 solver.cpp:228]     Train net output #0: softmax = 2.75302 (* 1 = 2.75302 loss)
I0623 12:51:20.966513  5064 solver.cpp:473] Iteration 19960, lr = 0.0001
I0623 12:51:21.987150  5064 solver.cpp:213] Iteration 19970, loss = 2.74183
I0623 12:51:21.987169  5064 solver.cpp:228]     Train net output #0: softmax = 2.74183 (* 1 = 2.74183 loss)
I0623 12:51:21.987174  5064 solver.cpp:473] Iteration 19970, lr = 0.0001
I0623 12:51:23.007199  5064 solver.cpp:213] Iteration 19980, loss = 2.79669
I0623 12:51:23.007218  5064 solver.cpp:228]     Train net output #0: softmax = 2.79669 (* 1 = 2.79669 loss)
I0623 12:51:23.007223  5064 solver.cpp:473] Iteration 19980, lr = 0.0001
I0623 12:51:24.027588  5064 solver.cpp:213] Iteration 19990, loss = 2.86402
I0623 12:51:24.027606  5064 solver.cpp:228]     Train net output #0: softmax = 2.86402 (* 1 = 2.86402 loss)
I0623 12:51:24.027611  5064 solver.cpp:473] Iteration 19990, lr = 0.0001
I0623 12:51:24.978056  5064 solver.cpp:362] Snapshotting to snapshots/16-06-21_16h05m31s_0_11_pretrainClassification_iter_20000.caffemodel
I0623 12:51:24.979121  5064 solver.cpp:370] Snapshotting solver state to snapshots/16-06-21_16h05m31s_0_11_pretrainClassification_iter_20000.solverstate
I0623 12:51:25.011574  5064 solver.cpp:273] Iteration 20000, loss = 2.88835
I0623 12:51:25.011589  5064 solver.cpp:291] Iteration 20000, Testing net (#0)
I0623 12:51:25.172796  5064 solver.cpp:342]     Test net output #0: accuracy_top_1 = 0.303125
I0623 12:51:25.172811  5064 solver.cpp:342]     Test net output #1: accuracy_top_5 = 0.571875
I0623 12:51:25.172818  5064 solver.cpp:342]     Test net output #2: softmax = 2.93193 (* 1 = 2.93193 loss)
I0623 12:51:25.172822  5064 solver.cpp:278] Optimization Done.
I0623 12:51:25.172826  5064 caffe.cpp:121] Optimization Done.
