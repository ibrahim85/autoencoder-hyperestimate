libdc1394 error: Failed to initialize libdc1394
I0615 15:55:08.524662  5063 caffe.cpp:99] Use GPU with device ID 0
I0615 15:55:08.643210  5063 caffe.cpp:107] Starting Optimization
I0615 15:55:08.643271  5063 solver.cpp:32] Initializing solver from parameters: 
test_iter: 5
test_interval: 1000
base_lr: 0.0001
display: 10
max_iter: 20000
lr_policy: "fixed"
momentum: 0.9
weight_decay: 0.004
snapshot: 1000
snapshot_prefix: "snapshots/16-06-15_15h49m18s_0_11_pretrainClassification"
solver_mode: GPU
net: "prototxt/16-06-15_15h49m18s_0_11_pretrainClassification_net.sh"
I0615 15:55:08.643290  5063 solver.cpp:70] Creating training net from net file: prototxt/16-06-15_15h49m18s_0_11_pretrainClassification_net.sh
I0615 15:55:08.643697  5063 net.cpp:277] The NetState phase (0) differed from the phase (1) specified by a rule in layer data
I0615 15:55:08.643715  5063 net.cpp:277] The NetState phase (0) differed from the phase (1) specified by a rule in layer accuracy_top_1
I0615 15:55:08.643719  5063 net.cpp:277] The NetState phase (0) differed from the phase (1) specified by a rule in layer accuracy_top_5
I0615 15:55:08.643810  5063 net.cpp:39] Initializing net from parameters: 
layers {
  top: "data"
  top: "label"
  name: "data"
  type: DATA
  data_param {
    source: "/dataset/cifar100_lmdb_lab/cifar100_train_lmdb"
    batch_size: 128
    backend: LMDB
  }
  include {
    phase: TRAIN
  }
  transform_param {
    mean_file: "/dataset/cifar100_lmdb_lab/mean.binaryproto"
  }
}
layers {
  bottom: "data"
  top: "0_0_conv"
  name: "0_0_conv"
  type: CONVOLUTION
  blobs_lr: 1
  blobs_lr: 2
  weight_decay: 0
  weight_decay: 0
  convolution_param {
    num_output: 8
    pad: 1
    kernel_size: 3
    stride: 1
    weight_filler {
      type: "xavier"
    }
    bias_filler {
      type: "constant"
      value: 0
    }
  }
}
layers {
  bottom: "0_0_conv"
  top: "0_0_conv"
  name: "0_0_conv_ReLU"
  type: RELU
}
layers {
  bottom: "0_0_conv"
  top: "0_1_conv"
  name: "0_1_conv"
  type: CONVOLUTION
  blobs_lr: 1
  blobs_lr: 2
  weight_decay: 0
  weight_decay: 0
  convolution_param {
    num_output: 8
    pad: 1
    kernel_size: 3
    stride: 1
    weight_filler {
      type: "xavier"
    }
    bias_filler {
      type: "constant"
      value: 0
    }
  }
}
layers {
  bottom: "0_1_conv"
  top: "0_1_conv"
  name: "0_1_conv_ReLU"
  type: RELU
}
layers {
  bottom: "0_1_conv"
  top: "0_pool"
  name: "0_pool"
  type: POOLING
  pooling_param {
    pool: MAX
    kernel_size: 2
    stride: 2
  }
}
layers {
  bottom: "0_pool"
  top: "1_0_conv"
  name: "1_0_conv"
  type: CONVOLUTION
  blobs_lr: 1
  blobs_lr: 2
  weight_decay: 0
  weight_decay: 0
  convolution_param {
    num_output: 8
    pad: 1
    kernel_size: 3
    stride: 1
    weight_filler {
      type: "xavier"
    }
    bias_filler {
      type: "constant"
      value: 0
    }
  }
}
layers {
  bottom: "1_0_conv"
  top: "1_0_conv"
  name: "1_0_conv_ReLU"
  type: RELU
}
layers {
  bottom: "1_0_conv"
  top: "1_1_conv"
  name: "1_1_conv"
  type: CONVOLUTION
  blobs_lr: 1
  blobs_lr: 2
  weight_decay: 0
  weight_decay: 0
  convolution_param {
    num_output: 8
    pad: 1
    kernel_size: 3
    stride: 1
    weight_filler {
      type: "xavier"
    }
    bias_filler {
      type: "constant"
      value: 0
    }
  }
}
layers {
  bottom: "1_1_conv"
  top: "1_1_conv"
  name: "1_1_conv_ReLU"
  type: RELU
}
layers {
  bottom: "1_1_conv"
  top: "1_pool"
  name: "1_pool"
  type: POOLING
  pooling_param {
    pool: MAX
    kernel_size: 2
    stride: 2
  }
}
layers {
  bottom: "1_pool"
  top: "2_0_conv"
  name: "2_0_conv"
  type: CONVOLUTION
  blobs_lr: 1
  blobs_lr: 2
  weight_decay: 0
  weight_decay: 0
  convolution_param {
    num_output: 8
    pad: 1
    kernel_size: 3
    stride: 1
    weight_filler {
      type: "xavier"
    }
    bias_filler {
      type: "constant"
      value: 0
    }
  }
}
layers {
  bottom: "2_0_conv"
  top: "2_0_conv"
  name: "2_0_conv_ReLU"
  type: RELU
}
layers {
  bottom: "2_0_conv"
  top: "2_1_conv"
  name: "2_1_conv"
  type: CONVOLUTION
  blobs_lr: 1
  blobs_lr: 2
  weight_decay: 0
  weight_decay: 0
  convolution_param {
    num_output: 8
    pad: 1
    kernel_size: 3
    stride: 1
    weight_filler {
      type: "xavier"
    }
    bias_filler {
      type: "constant"
      value: 0
    }
  }
}
layers {
  bottom: "2_1_conv"
  top: "2_1_conv"
  name: "2_1_conv_ReLU"
  type: RELU
}
layers {
  bottom: "2_1_conv"
  top: "2_pool"
  name: "2_pool"
  type: POOLING
  pooling_param {
    pool: MAX
    kernel_size: 2
    stride: 2
  }
}
layers {
  bottom: "2_pool"
  top: "middle_conv"
  name: "middle_conv"
  type: CONVOLUTION
  blobs_lr: 1
  blobs_lr: 2
  weight_decay: 0
  weight_decay: 0
  convolution_param {
    num_output: 50
    pad: 0
    kernel_size: 4
    stride: 1
    weight_filler {
      type: "xavier"
    }
    bias_filler {
      type: "constant"
      value: 0
    }
  }
}
layers {
  bottom: "middle_conv"
  top: "middle_conv"
  name: "middle_conv_ReLU"
  type: RELU
}
layers {
  bottom: "middle_conv"
  top: "fc1"
  name: "fc1"
  type: INNER_PRODUCT
  blobs_lr: 1
  blobs_lr: 2
  weight_decay: 0
  weight_decay: 0
  inner_product_param {
    num_output: 512
    weight_filler {
      type: "xavier"
    }
    bias_filler {
      type: "constant"
      value: 0
    }
  }
}
layers {
  bottom: "fc1"
  top: "fc1"
  name: "fc1_Dropout"
  type: DROPOUT
  dropout_param {
    dropout_ratio: 0.5
  }
  include {
    phase: TRAIN
  }
}
layers {
  bottom: "fc1"
  top: "fc1"
  name: "fc1_Dropout_ReLU"
  type: RELU
}
layers {
  bottom: "fc1"
  top: "fc2"
  name: "fc2"
  type: INNER_PRODUCT
  blobs_lr: 1
  blobs_lr: 2
  weight_decay: 0
  weight_decay: 0
  inner_product_param {
    num_output: 100
    weight_filler {
      type: "xavier"
    }
    bias_filler {
      type: "constant"
      value: 0
    }
  }
}
layers {
  bottom: "fc2"
  bottom: "label"
  top: "softmax"
  name: "softmax"
  type: SOFTMAX_LOSS
}
state {
  phase: TRAIN
}
I0615 15:55:08.643877  5063 layer_factory.hpp:78] Creating layer data
I0615 15:55:08.643889  5063 data_transformer.cpp:25] Loading mean file from/dataset/cifar100_lmdb_lab/mean.binaryproto
I0615 15:55:08.643935  5063 net.cpp:69] Creating Layer data
I0615 15:55:08.643941  5063 net.cpp:358] data -> data
I0615 15:55:08.643949  5063 net.cpp:358] data -> label
I0615 15:55:08.643954  5063 net.cpp:98] Setting up data
I0615 15:55:08.643959  5063 data_layer.cpp:32] Opening dataset /dataset/cifar100_lmdb_lab/cifar100_train_lmdb
I0615 15:55:08.644028  5063 data_layer.cpp:71] output data size: 128,3,32,32
I0615 15:55:08.644363  5063 net.cpp:105] Top shape: 128 3 32 32 (393216)
I0615 15:55:08.644371  5063 net.cpp:105] Top shape: 128 1 1 1 (128)
I0615 15:55:08.644383  5063 layer_factory.hpp:78] Creating layer 0_0_conv
I0615 15:55:08.644388  5063 net.cpp:69] Creating Layer 0_0_conv
I0615 15:55:08.644392  5063 net.cpp:396] 0_0_conv <- data
I0615 15:55:08.644398  5063 net.cpp:358] 0_0_conv -> 0_0_conv
I0615 15:55:08.644404  5063 net.cpp:98] Setting up 0_0_conv
I0615 15:55:08.644668  5063 net.cpp:105] Top shape: 128 8 32 32 (1048576)
I0615 15:55:08.644681  5063 layer_factory.hpp:78] Creating layer 0_0_conv_ReLU
I0615 15:55:08.644686  5063 net.cpp:69] Creating Layer 0_0_conv_ReLU
I0615 15:55:08.644688  5063 net.cpp:396] 0_0_conv_ReLU <- 0_0_conv
I0615 15:55:08.644696  5063 net.cpp:347] 0_0_conv_ReLU -> 0_0_conv (in-place)
I0615 15:55:08.644701  5063 net.cpp:98] Setting up 0_0_conv_ReLU
I0615 15:55:08.644704  5063 net.cpp:105] Top shape: 128 8 32 32 (1048576)
I0615 15:55:08.644707  5063 layer_factory.hpp:78] Creating layer 0_1_conv
I0615 15:55:08.644711  5063 net.cpp:69] Creating Layer 0_1_conv
I0615 15:55:08.644713  5063 net.cpp:396] 0_1_conv <- 0_0_conv
I0615 15:55:08.644719  5063 net.cpp:358] 0_1_conv -> 0_1_conv
I0615 15:55:08.644724  5063 net.cpp:98] Setting up 0_1_conv
I0615 15:55:08.644736  5063 net.cpp:105] Top shape: 128 8 32 32 (1048576)
I0615 15:55:08.644740  5063 layer_factory.hpp:78] Creating layer 0_1_conv_ReLU
I0615 15:55:08.644744  5063 net.cpp:69] Creating Layer 0_1_conv_ReLU
I0615 15:55:08.644747  5063 net.cpp:396] 0_1_conv_ReLU <- 0_1_conv
I0615 15:55:08.644752  5063 net.cpp:347] 0_1_conv_ReLU -> 0_1_conv (in-place)
I0615 15:55:08.644757  5063 net.cpp:98] Setting up 0_1_conv_ReLU
I0615 15:55:08.644760  5063 net.cpp:105] Top shape: 128 8 32 32 (1048576)
I0615 15:55:08.644768  5063 layer_factory.hpp:78] Creating layer 0_pool
I0615 15:55:08.644773  5063 net.cpp:69] Creating Layer 0_pool
I0615 15:55:08.644775  5063 net.cpp:396] 0_pool <- 0_1_conv
I0615 15:55:08.644781  5063 net.cpp:358] 0_pool -> 0_pool
I0615 15:55:08.644785  5063 net.cpp:98] Setting up 0_pool
I0615 15:55:08.644791  5063 net.cpp:105] Top shape: 128 8 16 16 (262144)
I0615 15:55:08.644794  5063 layer_factory.hpp:78] Creating layer 1_0_conv
I0615 15:55:08.644798  5063 net.cpp:69] Creating Layer 1_0_conv
I0615 15:55:08.644801  5063 net.cpp:396] 1_0_conv <- 0_pool
I0615 15:55:08.644807  5063 net.cpp:358] 1_0_conv -> 1_0_conv
I0615 15:55:08.644812  5063 net.cpp:98] Setting up 1_0_conv
I0615 15:55:08.644821  5063 net.cpp:105] Top shape: 128 8 16 16 (262144)
I0615 15:55:08.644826  5063 layer_factory.hpp:78] Creating layer 1_0_conv_ReLU
I0615 15:55:08.644830  5063 net.cpp:69] Creating Layer 1_0_conv_ReLU
I0615 15:55:08.644832  5063 net.cpp:396] 1_0_conv_ReLU <- 1_0_conv
I0615 15:55:08.644836  5063 net.cpp:347] 1_0_conv_ReLU -> 1_0_conv (in-place)
I0615 15:55:08.644840  5063 net.cpp:98] Setting up 1_0_conv_ReLU
I0615 15:55:08.644842  5063 net.cpp:105] Top shape: 128 8 16 16 (262144)
I0615 15:55:08.644845  5063 layer_factory.hpp:78] Creating layer 1_1_conv
I0615 15:55:08.644848  5063 net.cpp:69] Creating Layer 1_1_conv
I0615 15:55:08.644851  5063 net.cpp:396] 1_1_conv <- 1_0_conv
I0615 15:55:08.644855  5063 net.cpp:358] 1_1_conv -> 1_1_conv
I0615 15:55:08.644860  5063 net.cpp:98] Setting up 1_1_conv
I0615 15:55:08.644870  5063 net.cpp:105] Top shape: 128 8 16 16 (262144)
I0615 15:55:08.644873  5063 layer_factory.hpp:78] Creating layer 1_1_conv_ReLU
I0615 15:55:08.644877  5063 net.cpp:69] Creating Layer 1_1_conv_ReLU
I0615 15:55:08.644881  5063 net.cpp:396] 1_1_conv_ReLU <- 1_1_conv
I0615 15:55:08.644883  5063 net.cpp:347] 1_1_conv_ReLU -> 1_1_conv (in-place)
I0615 15:55:08.644886  5063 net.cpp:98] Setting up 1_1_conv_ReLU
I0615 15:55:08.644889  5063 net.cpp:105] Top shape: 128 8 16 16 (262144)
I0615 15:55:08.644891  5063 layer_factory.hpp:78] Creating layer 1_pool
I0615 15:55:08.644894  5063 net.cpp:69] Creating Layer 1_pool
I0615 15:55:08.644897  5063 net.cpp:396] 1_pool <- 1_1_conv
I0615 15:55:08.644901  5063 net.cpp:358] 1_pool -> 1_pool
I0615 15:55:08.644904  5063 net.cpp:98] Setting up 1_pool
I0615 15:55:08.644907  5063 net.cpp:105] Top shape: 128 8 8 8 (65536)
I0615 15:55:08.644911  5063 layer_factory.hpp:78] Creating layer 2_0_conv
I0615 15:55:08.644914  5063 net.cpp:69] Creating Layer 2_0_conv
I0615 15:55:08.644917  5063 net.cpp:396] 2_0_conv <- 1_pool
I0615 15:55:08.644922  5063 net.cpp:358] 2_0_conv -> 2_0_conv
I0615 15:55:08.644925  5063 net.cpp:98] Setting up 2_0_conv
I0615 15:55:08.644934  5063 net.cpp:105] Top shape: 128 8 8 8 (65536)
I0615 15:55:08.644939  5063 layer_factory.hpp:78] Creating layer 2_0_conv_ReLU
I0615 15:55:08.644942  5063 net.cpp:69] Creating Layer 2_0_conv_ReLU
I0615 15:55:08.644945  5063 net.cpp:396] 2_0_conv_ReLU <- 2_0_conv
I0615 15:55:08.644948  5063 net.cpp:347] 2_0_conv_ReLU -> 2_0_conv (in-place)
I0615 15:55:08.644953  5063 net.cpp:98] Setting up 2_0_conv_ReLU
I0615 15:55:08.644954  5063 net.cpp:105] Top shape: 128 8 8 8 (65536)
I0615 15:55:08.644958  5063 layer_factory.hpp:78] Creating layer 2_1_conv
I0615 15:55:08.644960  5063 net.cpp:69] Creating Layer 2_1_conv
I0615 15:55:08.644963  5063 net.cpp:396] 2_1_conv <- 2_0_conv
I0615 15:55:08.644968  5063 net.cpp:358] 2_1_conv -> 2_1_conv
I0615 15:55:08.644971  5063 net.cpp:98] Setting up 2_1_conv
I0615 15:55:08.644979  5063 net.cpp:105] Top shape: 128 8 8 8 (65536)
I0615 15:55:08.644984  5063 layer_factory.hpp:78] Creating layer 2_1_conv_ReLU
I0615 15:55:08.644986  5063 net.cpp:69] Creating Layer 2_1_conv_ReLU
I0615 15:55:08.644989  5063 net.cpp:396] 2_1_conv_ReLU <- 2_1_conv
I0615 15:55:08.644994  5063 net.cpp:347] 2_1_conv_ReLU -> 2_1_conv (in-place)
I0615 15:55:08.644996  5063 net.cpp:98] Setting up 2_1_conv_ReLU
I0615 15:55:08.645000  5063 net.cpp:105] Top shape: 128 8 8 8 (65536)
I0615 15:55:08.645007  5063 layer_factory.hpp:78] Creating layer 2_pool
I0615 15:55:08.645010  5063 net.cpp:69] Creating Layer 2_pool
I0615 15:55:08.645014  5063 net.cpp:396] 2_pool <- 2_1_conv
I0615 15:55:08.645016  5063 net.cpp:358] 2_pool -> 2_pool
I0615 15:55:08.645020  5063 net.cpp:98] Setting up 2_pool
I0615 15:55:08.645023  5063 net.cpp:105] Top shape: 128 8 4 4 (16384)
I0615 15:55:08.645026  5063 layer_factory.hpp:78] Creating layer middle_conv
I0615 15:55:08.645030  5063 net.cpp:69] Creating Layer middle_conv
I0615 15:55:08.645033  5063 net.cpp:396] middle_conv <- 2_pool
I0615 15:55:08.645037  5063 net.cpp:358] middle_conv -> middle_conv
I0615 15:55:08.645041  5063 net.cpp:98] Setting up middle_conv
I0615 15:55:08.645076  5063 net.cpp:105] Top shape: 128 50 1 1 (6400)
I0615 15:55:08.645081  5063 layer_factory.hpp:78] Creating layer middle_conv_ReLU
I0615 15:55:08.645084  5063 net.cpp:69] Creating Layer middle_conv_ReLU
I0615 15:55:08.645087  5063 net.cpp:396] middle_conv_ReLU <- middle_conv
I0615 15:55:08.645090  5063 net.cpp:347] middle_conv_ReLU -> middle_conv (in-place)
I0615 15:55:08.645093  5063 net.cpp:98] Setting up middle_conv_ReLU
I0615 15:55:08.645097  5063 net.cpp:105] Top shape: 128 50 1 1 (6400)
I0615 15:55:08.645098  5063 layer_factory.hpp:78] Creating layer fc1
I0615 15:55:08.645103  5063 net.cpp:69] Creating Layer fc1
I0615 15:55:08.645107  5063 net.cpp:396] fc1 <- middle_conv
I0615 15:55:08.645110  5063 net.cpp:358] fc1 -> fc1
I0615 15:55:08.645114  5063 net.cpp:98] Setting up fc1
I0615 15:55:08.645261  5063 net.cpp:105] Top shape: 128 512 1 1 (65536)
I0615 15:55:08.645267  5063 layer_factory.hpp:78] Creating layer fc1_Dropout
I0615 15:55:08.645270  5063 net.cpp:69] Creating Layer fc1_Dropout
I0615 15:55:08.645273  5063 net.cpp:396] fc1_Dropout <- fc1
I0615 15:55:08.645279  5063 net.cpp:347] fc1_Dropout -> fc1 (in-place)
I0615 15:55:08.645283  5063 net.cpp:98] Setting up fc1_Dropout
I0615 15:55:08.645287  5063 net.cpp:105] Top shape: 128 512 1 1 (65536)
I0615 15:55:08.645289  5063 layer_factory.hpp:78] Creating layer fc1_Dropout_ReLU
I0615 15:55:08.645292  5063 net.cpp:69] Creating Layer fc1_Dropout_ReLU
I0615 15:55:08.645295  5063 net.cpp:396] fc1_Dropout_ReLU <- fc1
I0615 15:55:08.645298  5063 net.cpp:347] fc1_Dropout_ReLU -> fc1 (in-place)
I0615 15:55:08.645301  5063 net.cpp:98] Setting up fc1_Dropout_ReLU
I0615 15:55:08.645304  5063 net.cpp:105] Top shape: 128 512 1 1 (65536)
I0615 15:55:08.645308  5063 layer_factory.hpp:78] Creating layer fc2
I0615 15:55:08.645310  5063 net.cpp:69] Creating Layer fc2
I0615 15:55:08.645313  5063 net.cpp:396] fc2 <- fc1
I0615 15:55:08.645318  5063 net.cpp:358] fc2 -> fc2
I0615 15:55:08.645323  5063 net.cpp:98] Setting up fc2
I0615 15:55:08.645592  5063 net.cpp:105] Top shape: 128 100 1 1 (12800)
I0615 15:55:08.645599  5063 layer_factory.hpp:78] Creating layer softmax
I0615 15:55:08.645606  5063 net.cpp:69] Creating Layer softmax
I0615 15:55:08.645609  5063 net.cpp:396] softmax <- fc2
I0615 15:55:08.645613  5063 net.cpp:396] softmax <- label
I0615 15:55:08.645617  5063 net.cpp:358] softmax -> softmax
I0615 15:55:08.645620  5063 net.cpp:98] Setting up softmax
I0615 15:55:08.645632  5063 net.cpp:105] Top shape: 1 1 1 1 (1)
I0615 15:55:08.645635  5063 net.cpp:111]     with loss weight 1
I0615 15:55:08.645645  5063 net.cpp:172] softmax needs backward computation.
I0615 15:55:08.645648  5063 net.cpp:172] fc2 needs backward computation.
I0615 15:55:08.645651  5063 net.cpp:172] fc1_Dropout_ReLU needs backward computation.
I0615 15:55:08.645653  5063 net.cpp:172] fc1_Dropout needs backward computation.
I0615 15:55:08.645656  5063 net.cpp:172] fc1 needs backward computation.
I0615 15:55:08.645659  5063 net.cpp:172] middle_conv_ReLU needs backward computation.
I0615 15:55:08.645661  5063 net.cpp:172] middle_conv needs backward computation.
I0615 15:55:08.645664  5063 net.cpp:172] 2_pool needs backward computation.
I0615 15:55:08.645668  5063 net.cpp:172] 2_1_conv_ReLU needs backward computation.
I0615 15:55:08.645671  5063 net.cpp:172] 2_1_conv needs backward computation.
I0615 15:55:08.645678  5063 net.cpp:172] 2_0_conv_ReLU needs backward computation.
I0615 15:55:08.645680  5063 net.cpp:172] 2_0_conv needs backward computation.
I0615 15:55:08.645684  5063 net.cpp:172] 1_pool needs backward computation.
I0615 15:55:08.645686  5063 net.cpp:172] 1_1_conv_ReLU needs backward computation.
I0615 15:55:08.645689  5063 net.cpp:172] 1_1_conv needs backward computation.
I0615 15:55:08.645691  5063 net.cpp:172] 1_0_conv_ReLU needs backward computation.
I0615 15:55:08.645694  5063 net.cpp:172] 1_0_conv needs backward computation.
I0615 15:55:08.645696  5063 net.cpp:172] 0_pool needs backward computation.
I0615 15:55:08.645699  5063 net.cpp:172] 0_1_conv_ReLU needs backward computation.
I0615 15:55:08.645701  5063 net.cpp:172] 0_1_conv needs backward computation.
I0615 15:55:08.645704  5063 net.cpp:172] 0_0_conv_ReLU needs backward computation.
I0615 15:55:08.645706  5063 net.cpp:172] 0_0_conv needs backward computation.
I0615 15:55:08.645709  5063 net.cpp:174] data does not need backward computation.
I0615 15:55:08.645711  5063 net.cpp:210] This network produces output softmax
I0615 15:55:08.645722  5063 net.cpp:469] Collecting Learning Rate and Weight Decay.
I0615 15:55:08.645727  5063 net.cpp:221] Network initialization done.
I0615 15:55:08.645730  5063 net.cpp:222] Memory required for data: 25858564
I0615 15:55:08.646137  5063 solver.cpp:154] Creating test net (#0) specified by net file: prototxt/16-06-15_15h49m18s_0_11_pretrainClassification_net.sh
I0615 15:55:08.646162  5063 net.cpp:277] The NetState phase (1) differed from the phase (0) specified by a rule in layer data
I0615 15:55:08.646172  5063 net.cpp:277] The NetState phase (1) differed from the phase (0) specified by a rule in layer fc1_Dropout
I0615 15:55:08.646260  5063 net.cpp:39] Initializing net from parameters: 
layers {
  top: "data"
  top: "label"
  name: "data"
  type: DATA
  data_param {
    source: "/dataset/cifar100_lmdb_lab/cifar100_test_lmdb"
    batch_size: 128
    backend: LMDB
  }
  include {
    phase: TEST
  }
  transform_param {
    mean_file: "/dataset/cifar100_lmdb_lab/mean.binaryproto"
  }
}
layers {
  bottom: "data"
  top: "0_0_conv"
  name: "0_0_conv"
  type: CONVOLUTION
  blobs_lr: 1
  blobs_lr: 2
  weight_decay: 0
  weight_decay: 0
  convolution_param {
    num_output: 8
    pad: 1
    kernel_size: 3
    stride: 1
    weight_filler {
      type: "xavier"
    }
    bias_filler {
      type: "constant"
      value: 0
    }
  }
}
layers {
  bottom: "0_0_conv"
  top: "0_0_conv"
  name: "0_0_conv_ReLU"
  type: RELU
}
layers {
  bottom: "0_0_conv"
  top: "0_1_conv"
  name: "0_1_conv"
  type: CONVOLUTION
  blobs_lr: 1
  blobs_lr: 2
  weight_decay: 0
  weight_decay: 0
  convolution_param {
    num_output: 8
    pad: 1
    kernel_size: 3
    stride: 1
    weight_filler {
      type: "xavier"
    }
    bias_filler {
      type: "constant"
      value: 0
    }
  }
}
layers {
  bottom: "0_1_conv"
  top: "0_1_conv"
  name: "0_1_conv_ReLU"
  type: RELU
}
layers {
  bottom: "0_1_conv"
  top: "0_pool"
  name: "0_pool"
  type: POOLING
  pooling_param {
    pool: MAX
    kernel_size: 2
    stride: 2
  }
}
layers {
  bottom: "0_pool"
  top: "1_0_conv"
  name: "1_0_conv"
  type: CONVOLUTION
  blobs_lr: 1
  blobs_lr: 2
  weight_decay: 0
  weight_decay: 0
  convolution_param {
    num_output: 8
    pad: 1
    kernel_size: 3
    stride: 1
    weight_filler {
      type: "xavier"
    }
    bias_filler {
      type: "constant"
      value: 0
    }
  }
}
layers {
  bottom: "1_0_conv"
  top: "1_0_conv"
  name: "1_0_conv_ReLU"
  type: RELU
}
layers {
  bottom: "1_0_conv"
  top: "1_1_conv"
  name: "1_1_conv"
  type: CONVOLUTION
  blobs_lr: 1
  blobs_lr: 2
  weight_decay: 0
  weight_decay: 0
  convolution_param {
    num_output: 8
    pad: 1
    kernel_size: 3
    stride: 1
    weight_filler {
      type: "xavier"
    }
    bias_filler {
      type: "constant"
      value: 0
    }
  }
}
layers {
  bottom: "1_1_conv"
  top: "1_1_conv"
  name: "1_1_conv_ReLU"
  type: RELU
}
layers {
  bottom: "1_1_conv"
  top: "1_pool"
  name: "1_pool"
  type: POOLING
  pooling_param {
    pool: MAX
    kernel_size: 2
    stride: 2
  }
}
layers {
  bottom: "1_pool"
  top: "2_0_conv"
  name: "2_0_conv"
  type: CONVOLUTION
  blobs_lr: 1
  blobs_lr: 2
  weight_decay: 0
  weight_decay: 0
  convolution_param {
    num_output: 8
    pad: 1
    kernel_size: 3
    stride: 1
    weight_filler {
      type: "xavier"
    }
    bias_filler {
      type: "constant"
      value: 0
    }
  }
}
layers {
  bottom: "2_0_conv"
  top: "2_0_conv"
  name: "2_0_conv_ReLU"
  type: RELU
}
layers {
  bottom: "2_0_conv"
  top: "2_1_conv"
  name: "2_1_conv"
  type: CONVOLUTION
  blobs_lr: 1
  blobs_lr: 2
  weight_decay: 0
  weight_decay: 0
  convolution_param {
    num_output: 8
    pad: 1
    kernel_size: 3
    stride: 1
    weight_filler {
      type: "xavier"
    }
    bias_filler {
      type: "constant"
      value: 0
    }
  }
}
layers {
  bottom: "2_1_conv"
  top: "2_1_conv"
  name: "2_1_conv_ReLU"
  type: RELU
}
layers {
  bottom: "2_1_conv"
  top: "2_pool"
  name: "2_pool"
  type: POOLING
  pooling_param {
    pool: MAX
    kernel_size: 2
    stride: 2
  }
}
layers {
  bottom: "2_pool"
  top: "middle_conv"
  name: "middle_conv"
  type: CONVOLUTION
  blobs_lr: 1
  blobs_lr: 2
  weight_decay: 0
  weight_decay: 0
  convolution_param {
    num_output: 50
    pad: 0
    kernel_size: 4
    stride: 1
    weight_filler {
      type: "xavier"
    }
    bias_filler {
      type: "constant"
      value: 0
    }
  }
}
layers {
  bottom: "middle_conv"
  top: "middle_conv"
  name: "middle_conv_ReLU"
  type: RELU
}
layers {
  bottom: "middle_conv"
  top: "fc1"
  name: "fc1"
  type: INNER_PRODUCT
  blobs_lr: 1
  blobs_lr: 2
  weight_decay: 0
  weight_decay: 0
  inner_product_param {
    num_output: 512
    weight_filler {
      type: "xavier"
    }
    bias_filler {
      type: "constant"
      value: 0
    }
  }
}
layers {
  bottom: "fc1"
  top: "fc1"
  name: "fc1_Dropout_ReLU"
  type: RELU
}
layers {
  bottom: "fc1"
  top: "fc2"
  name: "fc2"
  type: INNER_PRODUCT
  blobs_lr: 1
  blobs_lr: 2
  weight_decay: 0
  weight_decay: 0
  inner_product_param {
    num_output: 100
    weight_filler {
      type: "xavier"
    }
    bias_filler {
      type: "constant"
      value: 0
    }
  }
}
layers {
  bottom: "fc2"
  bottom: "label"
  top: "softmax"
  name: "softmax"
  type: SOFTMAX_LOSS
}
layers {
  bottom: "fc2"
  bottom: "label"
  top: "accuracy_top_1"
  name: "accuracy_top_1"
  type: ACCURACY
  accuracy_param {
    top_k: 1
  }
  include {
    phase: TEST
  }
}
layers {
  bottom: "fc2"
  bottom: "label"
  top: "accuracy_top_5"
  name: "accuracy_top_5"
  type: ACCURACY
  accuracy_param {
    top_k: 5
  }
  include {
    phase: TEST
  }
}
state {
  phase: TEST
}
I0615 15:55:08.646342  5063 layer_factory.hpp:78] Creating layer data
I0615 15:55:08.646347  5063 data_transformer.cpp:25] Loading mean file from/dataset/cifar100_lmdb_lab/mean.binaryproto
I0615 15:55:08.646381  5063 net.cpp:69] Creating Layer data
I0615 15:55:08.646387  5063 net.cpp:358] data -> data
I0615 15:55:08.646394  5063 net.cpp:358] data -> label
I0615 15:55:08.646399  5063 net.cpp:98] Setting up data
I0615 15:55:08.646402  5063 data_layer.cpp:32] Opening dataset /dataset/cifar100_lmdb_lab/cifar100_test_lmdb
I0615 15:55:08.646450  5063 data_layer.cpp:71] output data size: 128,3,32,32
I0615 15:55:08.646844  5063 net.cpp:105] Top shape: 128 3 32 32 (393216)
I0615 15:55:08.646852  5063 net.cpp:105] Top shape: 128 1 1 1 (128)
I0615 15:55:08.646854  5063 layer_factory.hpp:78] Creating layer label_data_1_split
I0615 15:55:08.646872  5063 net.cpp:69] Creating Layer label_data_1_split
I0615 15:55:08.646875  5063 net.cpp:396] label_data_1_split <- label
I0615 15:55:08.646880  5063 net.cpp:358] label_data_1_split -> label_data_1_split_0
I0615 15:55:08.646894  5063 net.cpp:358] label_data_1_split -> label_data_1_split_1
I0615 15:55:08.646899  5063 net.cpp:358] label_data_1_split -> label_data_1_split_2
I0615 15:55:08.646903  5063 net.cpp:98] Setting up label_data_1_split
I0615 15:55:08.646910  5063 net.cpp:105] Top shape: 128 1 1 1 (128)
I0615 15:55:08.646914  5063 net.cpp:105] Top shape: 128 1 1 1 (128)
I0615 15:55:08.646920  5063 net.cpp:105] Top shape: 128 1 1 1 (128)
I0615 15:55:08.646924  5063 layer_factory.hpp:78] Creating layer 0_0_conv
I0615 15:55:08.646929  5063 net.cpp:69] Creating Layer 0_0_conv
I0615 15:55:08.646931  5063 net.cpp:396] 0_0_conv <- data
I0615 15:55:08.646935  5063 net.cpp:358] 0_0_conv -> 0_0_conv
I0615 15:55:08.646940  5063 net.cpp:98] Setting up 0_0_conv
I0615 15:55:08.646950  5063 net.cpp:105] Top shape: 128 8 32 32 (1048576)
I0615 15:55:08.646956  5063 layer_factory.hpp:78] Creating layer 0_0_conv_ReLU
I0615 15:55:08.646960  5063 net.cpp:69] Creating Layer 0_0_conv_ReLU
I0615 15:55:08.646963  5063 net.cpp:396] 0_0_conv_ReLU <- 0_0_conv
I0615 15:55:08.646966  5063 net.cpp:347] 0_0_conv_ReLU -> 0_0_conv (in-place)
I0615 15:55:08.646970  5063 net.cpp:98] Setting up 0_0_conv_ReLU
I0615 15:55:08.646973  5063 net.cpp:105] Top shape: 128 8 32 32 (1048576)
I0615 15:55:08.646976  5063 layer_factory.hpp:78] Creating layer 0_1_conv
I0615 15:55:08.646980  5063 net.cpp:69] Creating Layer 0_1_conv
I0615 15:55:08.646982  5063 net.cpp:396] 0_1_conv <- 0_0_conv
I0615 15:55:08.646987  5063 net.cpp:358] 0_1_conv -> 0_1_conv
I0615 15:55:08.646991  5063 net.cpp:98] Setting up 0_1_conv
I0615 15:55:08.647003  5063 net.cpp:105] Top shape: 128 8 32 32 (1048576)
I0615 15:55:08.647008  5063 layer_factory.hpp:78] Creating layer 0_1_conv_ReLU
I0615 15:55:08.647012  5063 net.cpp:69] Creating Layer 0_1_conv_ReLU
I0615 15:55:08.647014  5063 net.cpp:396] 0_1_conv_ReLU <- 0_1_conv
I0615 15:55:08.647018  5063 net.cpp:347] 0_1_conv_ReLU -> 0_1_conv (in-place)
I0615 15:55:08.647022  5063 net.cpp:98] Setting up 0_1_conv_ReLU
I0615 15:55:08.647024  5063 net.cpp:105] Top shape: 128 8 32 32 (1048576)
I0615 15:55:08.647027  5063 layer_factory.hpp:78] Creating layer 0_pool
I0615 15:55:08.647032  5063 net.cpp:69] Creating Layer 0_pool
I0615 15:55:08.647034  5063 net.cpp:396] 0_pool <- 0_1_conv
I0615 15:55:08.647037  5063 net.cpp:358] 0_pool -> 0_pool
I0615 15:55:08.647042  5063 net.cpp:98] Setting up 0_pool
I0615 15:55:08.647045  5063 net.cpp:105] Top shape: 128 8 16 16 (262144)
I0615 15:55:08.647048  5063 layer_factory.hpp:78] Creating layer 1_0_conv
I0615 15:55:08.647052  5063 net.cpp:69] Creating Layer 1_0_conv
I0615 15:55:08.647054  5063 net.cpp:396] 1_0_conv <- 0_pool
I0615 15:55:08.647059  5063 net.cpp:358] 1_0_conv -> 1_0_conv
I0615 15:55:08.647063  5063 net.cpp:98] Setting up 1_0_conv
I0615 15:55:08.647073  5063 net.cpp:105] Top shape: 128 8 16 16 (262144)
I0615 15:55:08.647078  5063 layer_factory.hpp:78] Creating layer 1_0_conv_ReLU
I0615 15:55:08.647081  5063 net.cpp:69] Creating Layer 1_0_conv_ReLU
I0615 15:55:08.647084  5063 net.cpp:396] 1_0_conv_ReLU <- 1_0_conv
I0615 15:55:08.647089  5063 net.cpp:347] 1_0_conv_ReLU -> 1_0_conv (in-place)
I0615 15:55:08.647091  5063 net.cpp:98] Setting up 1_0_conv_ReLU
I0615 15:55:08.647094  5063 net.cpp:105] Top shape: 128 8 16 16 (262144)
I0615 15:55:08.647096  5063 layer_factory.hpp:78] Creating layer 1_1_conv
I0615 15:55:08.647100  5063 net.cpp:69] Creating Layer 1_1_conv
I0615 15:55:08.647102  5063 net.cpp:396] 1_1_conv <- 1_0_conv
I0615 15:55:08.647107  5063 net.cpp:358] 1_1_conv -> 1_1_conv
I0615 15:55:08.647111  5063 net.cpp:98] Setting up 1_1_conv
I0615 15:55:08.647121  5063 net.cpp:105] Top shape: 128 8 16 16 (262144)
I0615 15:55:08.647125  5063 layer_factory.hpp:78] Creating layer 1_1_conv_ReLU
I0615 15:55:08.647128  5063 net.cpp:69] Creating Layer 1_1_conv_ReLU
I0615 15:55:08.647131  5063 net.cpp:396] 1_1_conv_ReLU <- 1_1_conv
I0615 15:55:08.647135  5063 net.cpp:347] 1_1_conv_ReLU -> 1_1_conv (in-place)
I0615 15:55:08.647137  5063 net.cpp:98] Setting up 1_1_conv_ReLU
I0615 15:55:08.647140  5063 net.cpp:105] Top shape: 128 8 16 16 (262144)
I0615 15:55:08.647143  5063 layer_factory.hpp:78] Creating layer 1_pool
I0615 15:55:08.647146  5063 net.cpp:69] Creating Layer 1_pool
I0615 15:55:08.647148  5063 net.cpp:396] 1_pool <- 1_1_conv
I0615 15:55:08.647155  5063 net.cpp:358] 1_pool -> 1_pool
I0615 15:55:08.647158  5063 net.cpp:98] Setting up 1_pool
I0615 15:55:08.647166  5063 net.cpp:105] Top shape: 128 8 8 8 (65536)
I0615 15:55:08.647168  5063 layer_factory.hpp:78] Creating layer 2_0_conv
I0615 15:55:08.647173  5063 net.cpp:69] Creating Layer 2_0_conv
I0615 15:55:08.647176  5063 net.cpp:396] 2_0_conv <- 1_pool
I0615 15:55:08.647181  5063 net.cpp:358] 2_0_conv -> 2_0_conv
I0615 15:55:08.647184  5063 net.cpp:98] Setting up 2_0_conv
I0615 15:55:08.647192  5063 net.cpp:105] Top shape: 128 8 8 8 (65536)
I0615 15:55:08.647198  5063 layer_factory.hpp:78] Creating layer 2_0_conv_ReLU
I0615 15:55:08.647202  5063 net.cpp:69] Creating Layer 2_0_conv_ReLU
I0615 15:55:08.647204  5063 net.cpp:396] 2_0_conv_ReLU <- 2_0_conv
I0615 15:55:08.647207  5063 net.cpp:347] 2_0_conv_ReLU -> 2_0_conv (in-place)
I0615 15:55:08.647212  5063 net.cpp:98] Setting up 2_0_conv_ReLU
I0615 15:55:08.647213  5063 net.cpp:105] Top shape: 128 8 8 8 (65536)
I0615 15:55:08.647217  5063 layer_factory.hpp:78] Creating layer 2_1_conv
I0615 15:55:08.647219  5063 net.cpp:69] Creating Layer 2_1_conv
I0615 15:55:08.647222  5063 net.cpp:396] 2_1_conv <- 2_0_conv
I0615 15:55:08.647227  5063 net.cpp:358] 2_1_conv -> 2_1_conv
I0615 15:55:08.647230  5063 net.cpp:98] Setting up 2_1_conv
I0615 15:55:08.647238  5063 net.cpp:105] Top shape: 128 8 8 8 (65536)
I0615 15:55:08.647243  5063 layer_factory.hpp:78] Creating layer 2_1_conv_ReLU
I0615 15:55:08.647246  5063 net.cpp:69] Creating Layer 2_1_conv_ReLU
I0615 15:55:08.647249  5063 net.cpp:396] 2_1_conv_ReLU <- 2_1_conv
I0615 15:55:08.647253  5063 net.cpp:347] 2_1_conv_ReLU -> 2_1_conv (in-place)
I0615 15:55:08.647255  5063 net.cpp:98] Setting up 2_1_conv_ReLU
I0615 15:55:08.647258  5063 net.cpp:105] Top shape: 128 8 8 8 (65536)
I0615 15:55:08.647260  5063 layer_factory.hpp:78] Creating layer 2_pool
I0615 15:55:08.647264  5063 net.cpp:69] Creating Layer 2_pool
I0615 15:55:08.647267  5063 net.cpp:396] 2_pool <- 2_1_conv
I0615 15:55:08.647270  5063 net.cpp:358] 2_pool -> 2_pool
I0615 15:55:08.647274  5063 net.cpp:98] Setting up 2_pool
I0615 15:55:08.647279  5063 net.cpp:105] Top shape: 128 8 4 4 (16384)
I0615 15:55:08.647281  5063 layer_factory.hpp:78] Creating layer middle_conv
I0615 15:55:08.647284  5063 net.cpp:69] Creating Layer middle_conv
I0615 15:55:08.647287  5063 net.cpp:396] middle_conv <- 2_pool
I0615 15:55:08.647294  5063 net.cpp:358] middle_conv -> middle_conv
I0615 15:55:08.647299  5063 net.cpp:98] Setting up middle_conv
I0615 15:55:08.647332  5063 net.cpp:105] Top shape: 128 50 1 1 (6400)
I0615 15:55:08.647337  5063 layer_factory.hpp:78] Creating layer middle_conv_ReLU
I0615 15:55:08.647341  5063 net.cpp:69] Creating Layer middle_conv_ReLU
I0615 15:55:08.647342  5063 net.cpp:396] middle_conv_ReLU <- middle_conv
I0615 15:55:08.647347  5063 net.cpp:347] middle_conv_ReLU -> middle_conv (in-place)
I0615 15:55:08.647351  5063 net.cpp:98] Setting up middle_conv_ReLU
I0615 15:55:08.647353  5063 net.cpp:105] Top shape: 128 50 1 1 (6400)
I0615 15:55:08.647356  5063 layer_factory.hpp:78] Creating layer fc1
I0615 15:55:08.647361  5063 net.cpp:69] Creating Layer fc1
I0615 15:55:08.647363  5063 net.cpp:396] fc1 <- middle_conv
I0615 15:55:08.647367  5063 net.cpp:358] fc1 -> fc1
I0615 15:55:08.647372  5063 net.cpp:98] Setting up fc1
I0615 15:55:08.647503  5063 net.cpp:105] Top shape: 128 512 1 1 (65536)
I0615 15:55:08.647508  5063 layer_factory.hpp:78] Creating layer fc1_Dropout_ReLU
I0615 15:55:08.647511  5063 net.cpp:69] Creating Layer fc1_Dropout_ReLU
I0615 15:55:08.647516  5063 net.cpp:396] fc1_Dropout_ReLU <- fc1
I0615 15:55:08.647519  5063 net.cpp:347] fc1_Dropout_ReLU -> fc1 (in-place)
I0615 15:55:08.647522  5063 net.cpp:98] Setting up fc1_Dropout_ReLU
I0615 15:55:08.647526  5063 net.cpp:105] Top shape: 128 512 1 1 (65536)
I0615 15:55:08.647528  5063 layer_factory.hpp:78] Creating layer fc2
I0615 15:55:08.647531  5063 net.cpp:69] Creating Layer fc2
I0615 15:55:08.647534  5063 net.cpp:396] fc2 <- fc1
I0615 15:55:08.647539  5063 net.cpp:358] fc2 -> fc2
I0615 15:55:08.647545  5063 net.cpp:98] Setting up fc2
I0615 15:55:08.647830  5063 net.cpp:105] Top shape: 128 100 1 1 (12800)
I0615 15:55:08.647841  5063 layer_factory.hpp:78] Creating layer fc2_fc2_0_split
I0615 15:55:08.647856  5063 net.cpp:69] Creating Layer fc2_fc2_0_split
I0615 15:55:08.647858  5063 net.cpp:396] fc2_fc2_0_split <- fc2
I0615 15:55:08.647864  5063 net.cpp:358] fc2_fc2_0_split -> fc2_fc2_0_split_0
I0615 15:55:08.647881  5063 net.cpp:358] fc2_fc2_0_split -> fc2_fc2_0_split_1
I0615 15:55:08.647886  5063 net.cpp:358] fc2_fc2_0_split -> fc2_fc2_0_split_2
I0615 15:55:08.647891  5063 net.cpp:98] Setting up fc2_fc2_0_split
I0615 15:55:08.647894  5063 net.cpp:105] Top shape: 128 100 1 1 (12800)
I0615 15:55:08.647897  5063 net.cpp:105] Top shape: 128 100 1 1 (12800)
I0615 15:55:08.647900  5063 net.cpp:105] Top shape: 128 100 1 1 (12800)
I0615 15:55:08.647903  5063 layer_factory.hpp:78] Creating layer softmax
I0615 15:55:08.647908  5063 net.cpp:69] Creating Layer softmax
I0615 15:55:08.647912  5063 net.cpp:396] softmax <- fc2_fc2_0_split_0
I0615 15:55:08.647915  5063 net.cpp:396] softmax <- label_data_1_split_0
I0615 15:55:08.647918  5063 net.cpp:358] softmax -> softmax
I0615 15:55:08.647923  5063 net.cpp:98] Setting up softmax
I0615 15:55:08.647928  5063 net.cpp:105] Top shape: 1 1 1 1 (1)
I0615 15:55:08.647933  5063 net.cpp:111]     with loss weight 1
I0615 15:55:08.647944  5063 layer_factory.hpp:78] Creating layer accuracy_top_1
I0615 15:55:08.647950  5063 net.cpp:69] Creating Layer accuracy_top_1
I0615 15:55:08.647953  5063 net.cpp:396] accuracy_top_1 <- fc2_fc2_0_split_1
I0615 15:55:08.647956  5063 net.cpp:396] accuracy_top_1 <- label_data_1_split_1
I0615 15:55:08.647964  5063 net.cpp:358] accuracy_top_1 -> accuracy_top_1
I0615 15:55:08.647969  5063 net.cpp:98] Setting up accuracy_top_1
I0615 15:55:08.647971  5063 net.cpp:105] Top shape: 1 1 1 1 (1)
I0615 15:55:08.647974  5063 layer_factory.hpp:78] Creating layer accuracy_top_5
I0615 15:55:08.647979  5063 net.cpp:69] Creating Layer accuracy_top_5
I0615 15:55:08.647981  5063 net.cpp:396] accuracy_top_5 <- fc2_fc2_0_split_2
I0615 15:55:08.647984  5063 net.cpp:396] accuracy_top_5 <- label_data_1_split_2
I0615 15:55:08.647989  5063 net.cpp:358] accuracy_top_5 -> accuracy_top_5
I0615 15:55:08.647992  5063 net.cpp:98] Setting up accuracy_top_5
I0615 15:55:08.647996  5063 net.cpp:105] Top shape: 1 1 1 1 (1)
I0615 15:55:08.648000  5063 net.cpp:174] accuracy_top_5 does not need backward computation.
I0615 15:55:08.648003  5063 net.cpp:174] accuracy_top_1 does not need backward computation.
I0615 15:55:08.648006  5063 net.cpp:172] softmax needs backward computation.
I0615 15:55:08.648010  5063 net.cpp:172] fc2_fc2_0_split needs backward computation.
I0615 15:55:08.648012  5063 net.cpp:172] fc2 needs backward computation.
I0615 15:55:08.648016  5063 net.cpp:172] fc1_Dropout_ReLU needs backward computation.
I0615 15:55:08.648018  5063 net.cpp:172] fc1 needs backward computation.
I0615 15:55:08.648021  5063 net.cpp:172] middle_conv_ReLU needs backward computation.
I0615 15:55:08.648025  5063 net.cpp:172] middle_conv needs backward computation.
I0615 15:55:08.648027  5063 net.cpp:172] 2_pool needs backward computation.
I0615 15:55:08.648030  5063 net.cpp:172] 2_1_conv_ReLU needs backward computation.
I0615 15:55:08.648033  5063 net.cpp:172] 2_1_conv needs backward computation.
I0615 15:55:08.648036  5063 net.cpp:172] 2_0_conv_ReLU needs backward computation.
I0615 15:55:08.648039  5063 net.cpp:172] 2_0_conv needs backward computation.
I0615 15:55:08.648042  5063 net.cpp:172] 1_pool needs backward computation.
I0615 15:55:08.648046  5063 net.cpp:172] 1_1_conv_ReLU needs backward computation.
I0615 15:55:08.648048  5063 net.cpp:172] 1_1_conv needs backward computation.
I0615 15:55:08.648051  5063 net.cpp:172] 1_0_conv_ReLU needs backward computation.
I0615 15:55:08.648056  5063 net.cpp:172] 1_0_conv needs backward computation.
I0615 15:55:08.648059  5063 net.cpp:172] 0_pool needs backward computation.
I0615 15:55:08.648062  5063 net.cpp:172] 0_1_conv_ReLU needs backward computation.
I0615 15:55:08.648067  5063 net.cpp:172] 0_1_conv needs backward computation.
I0615 15:55:08.648074  5063 net.cpp:172] 0_0_conv_ReLU needs backward computation.
I0615 15:55:08.648077  5063 net.cpp:172] 0_0_conv needs backward computation.
I0615 15:55:08.648080  5063 net.cpp:174] label_data_1_split does not need backward computation.
I0615 15:55:08.648083  5063 net.cpp:174] data does not need backward computation.
I0615 15:55:08.648087  5063 net.cpp:210] This network produces output accuracy_top_1
I0615 15:55:08.648089  5063 net.cpp:210] This network produces output accuracy_top_5
I0615 15:55:08.648093  5063 net.cpp:210] This network produces output softmax
I0615 15:55:08.648107  5063 net.cpp:469] Collecting Learning Rate and Weight Decay.
I0615 15:55:08.648111  5063 net.cpp:221] Network initialization done.
I0615 15:55:08.648114  5063 net.cpp:222] Memory required for data: 25751564
I0615 15:55:08.648188  5063 solver.cpp:42] Solver scaffolding done.
I0615 15:55:08.648211  5063 caffe.cpp:115] Finetuning from snapshots/16-06-15_15h49m18s_0_10_pretrainClassificationFrozen_iter_3000.caffemodel
I0615 15:55:08.648736  5063 solver.cpp:247] Solving 
I0615 15:55:08.648741  5063 solver.cpp:248] Learning Rate Policy: fixed
I0615 15:55:08.649080  5063 solver.cpp:291] Iteration 0, Testing net (#0)
I0615 15:55:08.740025  5063 solver.cpp:342]     Test net output #0: accuracy_top_1 = 0.059375
I0615 15:55:08.740043  5063 solver.cpp:342]     Test net output #1: accuracy_top_5 = 0.185937
I0615 15:55:08.740049  5063 solver.cpp:342]     Test net output #2: softmax = 4.29729 (* 1 = 4.29729 loss)
I0615 15:55:08.797658  5063 solver.cpp:213] Iteration 0, loss = 4.39081
I0615 15:55:08.797675  5063 solver.cpp:228]     Train net output #0: softmax = 4.39081 (* 1 = 4.39081 loss)
I0615 15:55:08.797680  5063 solver.cpp:473] Iteration 0, lr = 0.0001
I0615 15:55:09.556229  5063 solver.cpp:213] Iteration 10, loss = 4.19785
I0615 15:55:09.556257  5063 solver.cpp:228]     Train net output #0: softmax = 4.19785 (* 1 = 4.19785 loss)
I0615 15:55:09.556262  5063 solver.cpp:473] Iteration 10, lr = 0.0001
I0615 15:55:10.314141  5063 solver.cpp:213] Iteration 20, loss = 4.26478
I0615 15:55:10.314160  5063 solver.cpp:228]     Train net output #0: softmax = 4.26478 (* 1 = 4.26478 loss)
I0615 15:55:10.314165  5063 solver.cpp:473] Iteration 20, lr = 0.0001
I0615 15:55:11.071640  5063 solver.cpp:213] Iteration 30, loss = 4.1504
I0615 15:55:11.071660  5063 solver.cpp:228]     Train net output #0: softmax = 4.1504 (* 1 = 4.1504 loss)
I0615 15:55:11.071665  5063 solver.cpp:473] Iteration 30, lr = 0.0001
I0615 15:55:11.829324  5063 solver.cpp:213] Iteration 40, loss = 4.17716
I0615 15:55:11.829344  5063 solver.cpp:228]     Train net output #0: softmax = 4.17716 (* 1 = 4.17716 loss)
I0615 15:55:11.829349  5063 solver.cpp:473] Iteration 40, lr = 0.0001
I0615 15:55:12.587059  5063 solver.cpp:213] Iteration 50, loss = 4.39086
I0615 15:55:12.587079  5063 solver.cpp:228]     Train net output #0: softmax = 4.39086 (* 1 = 4.39086 loss)
I0615 15:55:12.587083  5063 solver.cpp:473] Iteration 50, lr = 0.0001
I0615 15:55:13.344764  5063 solver.cpp:213] Iteration 60, loss = 4.33515
I0615 15:55:13.344784  5063 solver.cpp:228]     Train net output #0: softmax = 4.33515 (* 1 = 4.33515 loss)
I0615 15:55:13.344789  5063 solver.cpp:473] Iteration 60, lr = 0.0001
I0615 15:55:14.102468  5063 solver.cpp:213] Iteration 70, loss = 4.27268
I0615 15:55:14.102488  5063 solver.cpp:228]     Train net output #0: softmax = 4.27268 (* 1 = 4.27268 loss)
I0615 15:55:14.102493  5063 solver.cpp:473] Iteration 70, lr = 0.0001
I0615 15:55:14.860302  5063 solver.cpp:213] Iteration 80, loss = 4.17635
I0615 15:55:14.860328  5063 solver.cpp:228]     Train net output #0: softmax = 4.17635 (* 1 = 4.17635 loss)
I0615 15:55:14.860333  5063 solver.cpp:473] Iteration 80, lr = 0.0001
I0615 15:55:15.617667  5063 solver.cpp:213] Iteration 90, loss = 4.11225
I0615 15:55:15.617689  5063 solver.cpp:228]     Train net output #0: softmax = 4.11225 (* 1 = 4.11225 loss)
I0615 15:55:15.617694  5063 solver.cpp:473] Iteration 90, lr = 0.0001
I0615 15:55:16.375422  5063 solver.cpp:213] Iteration 100, loss = 4.19227
I0615 15:55:16.375459  5063 solver.cpp:228]     Train net output #0: softmax = 4.19227 (* 1 = 4.19227 loss)
I0615 15:55:16.375464  5063 solver.cpp:473] Iteration 100, lr = 0.0001
I0615 15:55:17.133340  5063 solver.cpp:213] Iteration 110, loss = 4.18928
I0615 15:55:17.133361  5063 solver.cpp:228]     Train net output #0: softmax = 4.18928 (* 1 = 4.18928 loss)
I0615 15:55:17.133365  5063 solver.cpp:473] Iteration 110, lr = 0.0001
I0615 15:55:17.891193  5063 solver.cpp:213] Iteration 120, loss = 4.15452
I0615 15:55:17.891212  5063 solver.cpp:228]     Train net output #0: softmax = 4.15452 (* 1 = 4.15452 loss)
I0615 15:55:17.891216  5063 solver.cpp:473] Iteration 120, lr = 0.0001
I0615 15:55:18.648829  5063 solver.cpp:213] Iteration 130, loss = 4.07404
I0615 15:55:18.648852  5063 solver.cpp:228]     Train net output #0: softmax = 4.07404 (* 1 = 4.07404 loss)
I0615 15:55:18.648857  5063 solver.cpp:473] Iteration 130, lr = 0.0001
I0615 15:55:19.406322  5063 solver.cpp:213] Iteration 140, loss = 4.31643
I0615 15:55:19.406345  5063 solver.cpp:228]     Train net output #0: softmax = 4.31643 (* 1 = 4.31643 loss)
I0615 15:55:19.406349  5063 solver.cpp:473] Iteration 140, lr = 0.0001
I0615 15:55:20.163735  5063 solver.cpp:213] Iteration 150, loss = 4.14463
I0615 15:55:20.163756  5063 solver.cpp:228]     Train net output #0: softmax = 4.14463 (* 1 = 4.14463 loss)
I0615 15:55:20.163760  5063 solver.cpp:473] Iteration 150, lr = 0.0001
I0615 15:55:20.921587  5063 solver.cpp:213] Iteration 160, loss = 4.10481
I0615 15:55:20.921608  5063 solver.cpp:228]     Train net output #0: softmax = 4.10481 (* 1 = 4.10481 loss)
I0615 15:55:20.921613  5063 solver.cpp:473] Iteration 160, lr = 0.0001
I0615 15:55:21.679582  5063 solver.cpp:213] Iteration 170, loss = 4.31471
I0615 15:55:21.679601  5063 solver.cpp:228]     Train net output #0: softmax = 4.31471 (* 1 = 4.31471 loss)
I0615 15:55:21.679606  5063 solver.cpp:473] Iteration 170, lr = 0.0001
I0615 15:55:22.438195  5063 solver.cpp:213] Iteration 180, loss = 4.30887
I0615 15:55:22.438215  5063 solver.cpp:228]     Train net output #0: softmax = 4.30887 (* 1 = 4.30887 loss)
I0615 15:55:22.438220  5063 solver.cpp:473] Iteration 180, lr = 0.0001
I0615 15:55:23.196318  5063 solver.cpp:213] Iteration 190, loss = 4.21112
I0615 15:55:23.196339  5063 solver.cpp:228]     Train net output #0: softmax = 4.21112 (* 1 = 4.21112 loss)
I0615 15:55:23.196343  5063 solver.cpp:473] Iteration 190, lr = 0.0001
I0615 15:55:23.955744  5063 solver.cpp:213] Iteration 200, loss = 4.09792
I0615 15:55:23.955775  5063 solver.cpp:228]     Train net output #0: softmax = 4.09792 (* 1 = 4.09792 loss)
I0615 15:55:23.955785  5063 solver.cpp:473] Iteration 200, lr = 0.0001
I0615 15:55:24.714210  5063 solver.cpp:213] Iteration 210, loss = 3.98228
I0615 15:55:24.714233  5063 solver.cpp:228]     Train net output #0: softmax = 3.98228 (* 1 = 3.98228 loss)
I0615 15:55:24.714238  5063 solver.cpp:473] Iteration 210, lr = 0.0001
I0615 15:55:25.472425  5063 solver.cpp:213] Iteration 220, loss = 4.01825
I0615 15:55:25.472455  5063 solver.cpp:228]     Train net output #0: softmax = 4.01825 (* 1 = 4.01825 loss)
I0615 15:55:25.472460  5063 solver.cpp:473] Iteration 220, lr = 0.0001
I0615 15:55:26.231118  5063 solver.cpp:213] Iteration 230, loss = 4.1208
I0615 15:55:26.231139  5063 solver.cpp:228]     Train net output #0: softmax = 4.1208 (* 1 = 4.1208 loss)
I0615 15:55:26.231143  5063 solver.cpp:473] Iteration 230, lr = 0.0001
I0615 15:55:26.989542  5063 solver.cpp:213] Iteration 240, loss = 4.05466
I0615 15:55:26.989573  5063 solver.cpp:228]     Train net output #0: softmax = 4.05466 (* 1 = 4.05466 loss)
I0615 15:55:26.989578  5063 solver.cpp:473] Iteration 240, lr = 0.0001
I0615 15:55:27.748492  5063 solver.cpp:213] Iteration 250, loss = 4.20192
I0615 15:55:27.748512  5063 solver.cpp:228]     Train net output #0: softmax = 4.20192 (* 1 = 4.20192 loss)
I0615 15:55:27.748517  5063 solver.cpp:473] Iteration 250, lr = 0.0001
I0615 15:55:28.507495  5063 solver.cpp:213] Iteration 260, loss = 4.1694
I0615 15:55:28.507532  5063 solver.cpp:228]     Train net output #0: softmax = 4.1694 (* 1 = 4.1694 loss)
I0615 15:55:28.507549  5063 solver.cpp:473] Iteration 260, lr = 0.0001
I0615 15:55:29.265422  5063 solver.cpp:213] Iteration 270, loss = 4.07618
I0615 15:55:29.265444  5063 solver.cpp:228]     Train net output #0: softmax = 4.07618 (* 1 = 4.07618 loss)
I0615 15:55:29.265449  5063 solver.cpp:473] Iteration 270, lr = 0.0001
I0615 15:55:30.024318  5063 solver.cpp:213] Iteration 280, loss = 4.17886
I0615 15:55:30.024343  5063 solver.cpp:228]     Train net output #0: softmax = 4.17886 (* 1 = 4.17886 loss)
I0615 15:55:30.024348  5063 solver.cpp:473] Iteration 280, lr = 0.0001
I0615 15:55:30.782966  5063 solver.cpp:213] Iteration 290, loss = 4.17262
I0615 15:55:30.782986  5063 solver.cpp:228]     Train net output #0: softmax = 4.17262 (* 1 = 4.17262 loss)
I0615 15:55:30.782990  5063 solver.cpp:473] Iteration 290, lr = 0.0001
I0615 15:55:31.541668  5063 solver.cpp:213] Iteration 300, loss = 4.05757
I0615 15:55:31.541688  5063 solver.cpp:228]     Train net output #0: softmax = 4.05757 (* 1 = 4.05757 loss)
I0615 15:55:31.541693  5063 solver.cpp:473] Iteration 300, lr = 0.0001
I0615 15:55:32.300669  5063 solver.cpp:213] Iteration 310, loss = 4.14316
I0615 15:55:32.300690  5063 solver.cpp:228]     Train net output #0: softmax = 4.14316 (* 1 = 4.14316 loss)
I0615 15:55:32.300695  5063 solver.cpp:473] Iteration 310, lr = 0.0001
I0615 15:55:33.058769  5063 solver.cpp:213] Iteration 320, loss = 4.01896
I0615 15:55:33.058789  5063 solver.cpp:228]     Train net output #0: softmax = 4.01896 (* 1 = 4.01896 loss)
I0615 15:55:33.058794  5063 solver.cpp:473] Iteration 320, lr = 0.0001
I0615 15:55:33.817467  5063 solver.cpp:213] Iteration 330, loss = 4.08692
I0615 15:55:33.817488  5063 solver.cpp:228]     Train net output #0: softmax = 4.08692 (* 1 = 4.08692 loss)
I0615 15:55:33.817493  5063 solver.cpp:473] Iteration 330, lr = 0.0001
I0615 15:55:34.575554  5063 solver.cpp:213] Iteration 340, loss = 4.22563
I0615 15:55:34.575579  5063 solver.cpp:228]     Train net output #0: softmax = 4.22563 (* 1 = 4.22563 loss)
I0615 15:55:34.575584  5063 solver.cpp:473] Iteration 340, lr = 0.0001
I0615 15:55:35.334311  5063 solver.cpp:213] Iteration 350, loss = 3.99218
I0615 15:55:35.334331  5063 solver.cpp:228]     Train net output #0: softmax = 3.99218 (* 1 = 3.99218 loss)
I0615 15:55:35.334336  5063 solver.cpp:473] Iteration 350, lr = 0.0001
I0615 15:55:36.092895  5063 solver.cpp:213] Iteration 360, loss = 4.20699
I0615 15:55:36.092916  5063 solver.cpp:228]     Train net output #0: softmax = 4.20699 (* 1 = 4.20699 loss)
I0615 15:55:36.092921  5063 solver.cpp:473] Iteration 360, lr = 0.0001
I0615 15:55:36.851796  5063 solver.cpp:213] Iteration 370, loss = 4.16565
I0615 15:55:36.851837  5063 solver.cpp:228]     Train net output #0: softmax = 4.16565 (* 1 = 4.16565 loss)
I0615 15:55:36.851842  5063 solver.cpp:473] Iteration 370, lr = 0.0001
I0615 15:55:37.610224  5063 solver.cpp:213] Iteration 380, loss = 4.14233
I0615 15:55:37.610245  5063 solver.cpp:228]     Train net output #0: softmax = 4.14233 (* 1 = 4.14233 loss)
I0615 15:55:37.610249  5063 solver.cpp:473] Iteration 380, lr = 0.0001
I0615 15:55:38.368839  5063 solver.cpp:213] Iteration 390, loss = 4.08671
I0615 15:55:38.368860  5063 solver.cpp:228]     Train net output #0: softmax = 4.08671 (* 1 = 4.08671 loss)
I0615 15:55:38.368863  5063 solver.cpp:473] Iteration 390, lr = 0.0001
I0615 15:55:39.127351  5063 solver.cpp:213] Iteration 400, loss = 4.07013
I0615 15:55:39.127395  5063 solver.cpp:228]     Train net output #0: softmax = 4.07013 (* 1 = 4.07013 loss)
I0615 15:55:39.127400  5063 solver.cpp:473] Iteration 400, lr = 0.0001
I0615 15:55:39.886034  5063 solver.cpp:213] Iteration 410, loss = 4.18819
I0615 15:55:39.886070  5063 solver.cpp:228]     Train net output #0: softmax = 4.18819 (* 1 = 4.18819 loss)
I0615 15:55:39.886078  5063 solver.cpp:473] Iteration 410, lr = 0.0001
I0615 15:55:40.644390  5063 solver.cpp:213] Iteration 420, loss = 4.05022
I0615 15:55:40.644418  5063 solver.cpp:228]     Train net output #0: softmax = 4.05022 (* 1 = 4.05022 loss)
I0615 15:55:40.644423  5063 solver.cpp:473] Iteration 420, lr = 0.0001
I0615 15:55:41.402746  5063 solver.cpp:213] Iteration 430, loss = 4.12951
I0615 15:55:41.402765  5063 solver.cpp:228]     Train net output #0: softmax = 4.12951 (* 1 = 4.12951 loss)
I0615 15:55:41.402770  5063 solver.cpp:473] Iteration 430, lr = 0.0001
I0615 15:55:42.161487  5063 solver.cpp:213] Iteration 440, loss = 4.27766
I0615 15:55:42.161506  5063 solver.cpp:228]     Train net output #0: softmax = 4.27766 (* 1 = 4.27766 loss)
I0615 15:55:42.161511  5063 solver.cpp:473] Iteration 440, lr = 0.0001
I0615 15:55:42.919891  5063 solver.cpp:213] Iteration 450, loss = 4.25088
I0615 15:55:42.919911  5063 solver.cpp:228]     Train net output #0: softmax = 4.25088 (* 1 = 4.25088 loss)
I0615 15:55:42.919916  5063 solver.cpp:473] Iteration 450, lr = 0.0001
I0615 15:55:43.678346  5063 solver.cpp:213] Iteration 460, loss = 4.20201
I0615 15:55:43.678369  5063 solver.cpp:228]     Train net output #0: softmax = 4.20201 (* 1 = 4.20201 loss)
I0615 15:55:43.678375  5063 solver.cpp:473] Iteration 460, lr = 0.0001
I0615 15:55:44.437085  5063 solver.cpp:213] Iteration 470, loss = 4.05952
I0615 15:55:44.437105  5063 solver.cpp:228]     Train net output #0: softmax = 4.05952 (* 1 = 4.05952 loss)
I0615 15:55:44.437110  5063 solver.cpp:473] Iteration 470, lr = 0.0001
I0615 15:55:45.194936  5063 solver.cpp:213] Iteration 480, loss = 4.08905
I0615 15:55:45.194960  5063 solver.cpp:228]     Train net output #0: softmax = 4.08905 (* 1 = 4.08905 loss)
I0615 15:55:45.194965  5063 solver.cpp:473] Iteration 480, lr = 0.0001
I0615 15:55:45.953013  5063 solver.cpp:213] Iteration 490, loss = 4.06863
I0615 15:55:45.953034  5063 solver.cpp:228]     Train net output #0: softmax = 4.06863 (* 1 = 4.06863 loss)
I0615 15:55:45.953038  5063 solver.cpp:473] Iteration 490, lr = 0.0001
I0615 15:55:46.711702  5063 solver.cpp:213] Iteration 500, loss = 4.06072
I0615 15:55:46.711722  5063 solver.cpp:228]     Train net output #0: softmax = 4.06072 (* 1 = 4.06072 loss)
I0615 15:55:46.711726  5063 solver.cpp:473] Iteration 500, lr = 0.0001
I0615 15:55:47.469579  5063 solver.cpp:213] Iteration 510, loss = 4.06187
I0615 15:55:47.469605  5063 solver.cpp:228]     Train net output #0: softmax = 4.06187 (* 1 = 4.06187 loss)
I0615 15:55:47.469609  5063 solver.cpp:473] Iteration 510, lr = 0.0001
I0615 15:55:48.227622  5063 solver.cpp:213] Iteration 520, loss = 4.17954
I0615 15:55:48.227643  5063 solver.cpp:228]     Train net output #0: softmax = 4.17954 (* 1 = 4.17954 loss)
I0615 15:55:48.227648  5063 solver.cpp:473] Iteration 520, lr = 0.0001
I0615 15:55:48.986320  5063 solver.cpp:213] Iteration 530, loss = 4.12102
I0615 15:55:48.986340  5063 solver.cpp:228]     Train net output #0: softmax = 4.12102 (* 1 = 4.12102 loss)
I0615 15:55:48.986346  5063 solver.cpp:473] Iteration 530, lr = 0.0001
I0615 15:55:49.745223  5063 solver.cpp:213] Iteration 540, loss = 3.971
I0615 15:55:49.745244  5063 solver.cpp:228]     Train net output #0: softmax = 3.971 (* 1 = 3.971 loss)
I0615 15:55:49.745249  5063 solver.cpp:473] Iteration 540, lr = 0.0001
I0615 15:55:50.504101  5063 solver.cpp:213] Iteration 550, loss = 4.18574
I0615 15:55:50.504125  5063 solver.cpp:228]     Train net output #0: softmax = 4.18574 (* 1 = 4.18574 loss)
I0615 15:55:50.504133  5063 solver.cpp:473] Iteration 550, lr = 0.0001
I0615 15:55:51.262809  5063 solver.cpp:213] Iteration 560, loss = 4.14784
I0615 15:55:51.262828  5063 solver.cpp:228]     Train net output #0: softmax = 4.14784 (* 1 = 4.14784 loss)
I0615 15:55:51.262850  5063 solver.cpp:473] Iteration 560, lr = 0.0001
I0615 15:55:52.021157  5063 solver.cpp:213] Iteration 570, loss = 4.01315
I0615 15:55:52.021175  5063 solver.cpp:228]     Train net output #0: softmax = 4.01315 (* 1 = 4.01315 loss)
I0615 15:55:52.021179  5063 solver.cpp:473] Iteration 570, lr = 0.0001
I0615 15:55:52.779769  5063 solver.cpp:213] Iteration 580, loss = 4.07481
I0615 15:55:52.779789  5063 solver.cpp:228]     Train net output #0: softmax = 4.07481 (* 1 = 4.07481 loss)
I0615 15:55:52.779799  5063 solver.cpp:473] Iteration 580, lr = 0.0001
I0615 15:55:53.538538  5063 solver.cpp:213] Iteration 590, loss = 4.09014
I0615 15:55:53.538557  5063 solver.cpp:228]     Train net output #0: softmax = 4.09014 (* 1 = 4.09014 loss)
I0615 15:55:53.538561  5063 solver.cpp:473] Iteration 590, lr = 0.0001
I0615 15:55:54.297503  5063 solver.cpp:213] Iteration 600, loss = 3.93463
I0615 15:55:54.297523  5063 solver.cpp:228]     Train net output #0: softmax = 3.93463 (* 1 = 3.93463 loss)
I0615 15:55:54.297528  5063 solver.cpp:473] Iteration 600, lr = 0.0001
I0615 15:55:55.056368  5063 solver.cpp:213] Iteration 610, loss = 4.09035
I0615 15:55:55.056387  5063 solver.cpp:228]     Train net output #0: softmax = 4.09035 (* 1 = 4.09035 loss)
I0615 15:55:55.056392  5063 solver.cpp:473] Iteration 610, lr = 0.0001
I0615 15:55:55.814870  5063 solver.cpp:213] Iteration 620, loss = 4.1275
I0615 15:55:55.814901  5063 solver.cpp:228]     Train net output #0: softmax = 4.1275 (* 1 = 4.1275 loss)
I0615 15:55:55.814911  5063 solver.cpp:473] Iteration 620, lr = 0.0001
I0615 15:55:56.573303  5063 solver.cpp:213] Iteration 630, loss = 4.16592
I0615 15:55:56.573323  5063 solver.cpp:228]     Train net output #0: softmax = 4.16592 (* 1 = 4.16592 loss)
I0615 15:55:56.573328  5063 solver.cpp:473] Iteration 630, lr = 0.0001
I0615 15:55:57.331385  5063 solver.cpp:213] Iteration 640, loss = 4.10556
I0615 15:55:57.331415  5063 solver.cpp:228]     Train net output #0: softmax = 4.10556 (* 1 = 4.10556 loss)
I0615 15:55:57.331420  5063 solver.cpp:473] Iteration 640, lr = 0.0001
I0615 15:55:58.089493  5063 solver.cpp:213] Iteration 650, loss = 3.96702
I0615 15:55:58.089517  5063 solver.cpp:228]     Train net output #0: softmax = 3.96702 (* 1 = 3.96702 loss)
I0615 15:55:58.089522  5063 solver.cpp:473] Iteration 650, lr = 0.0001
I0615 15:55:58.848862  5063 solver.cpp:213] Iteration 660, loss = 3.93467
I0615 15:55:58.848886  5063 solver.cpp:228]     Train net output #0: softmax = 3.93467 (* 1 = 3.93467 loss)
I0615 15:55:58.848891  5063 solver.cpp:473] Iteration 660, lr = 0.0001
I0615 15:55:59.607156  5063 solver.cpp:213] Iteration 670, loss = 3.96351
I0615 15:55:59.607175  5063 solver.cpp:228]     Train net output #0: softmax = 3.96351 (* 1 = 3.96351 loss)
I0615 15:55:59.607179  5063 solver.cpp:473] Iteration 670, lr = 0.0001
I0615 15:56:00.366662  5063 solver.cpp:213] Iteration 680, loss = 4.05103
I0615 15:56:00.366682  5063 solver.cpp:228]     Train net output #0: softmax = 4.05103 (* 1 = 4.05103 loss)
I0615 15:56:00.366685  5063 solver.cpp:473] Iteration 680, lr = 0.0001
I0615 15:56:01.124670  5063 solver.cpp:213] Iteration 690, loss = 4.13899
I0615 15:56:01.124692  5063 solver.cpp:228]     Train net output #0: softmax = 4.13899 (* 1 = 4.13899 loss)
I0615 15:56:01.124701  5063 solver.cpp:473] Iteration 690, lr = 0.0001
I0615 15:56:01.883541  5063 solver.cpp:213] Iteration 700, loss = 4.10123
I0615 15:56:01.883563  5063 solver.cpp:228]     Train net output #0: softmax = 4.10123 (* 1 = 4.10123 loss)
I0615 15:56:01.883566  5063 solver.cpp:473] Iteration 700, lr = 0.0001
I0615 15:56:02.642868  5063 solver.cpp:213] Iteration 710, loss = 4.06492
I0615 15:56:02.642889  5063 solver.cpp:228]     Train net output #0: softmax = 4.06492 (* 1 = 4.06492 loss)
I0615 15:56:02.642894  5063 solver.cpp:473] Iteration 710, lr = 0.0001
I0615 15:56:03.402392  5063 solver.cpp:213] Iteration 720, loss = 3.96544
I0615 15:56:03.402413  5063 solver.cpp:228]     Train net output #0: softmax = 3.96544 (* 1 = 3.96544 loss)
I0615 15:56:03.402436  5063 solver.cpp:473] Iteration 720, lr = 0.0001
I0615 15:56:04.161542  5063 solver.cpp:213] Iteration 730, loss = 4.02595
I0615 15:56:04.161562  5063 solver.cpp:228]     Train net output #0: softmax = 4.02595 (* 1 = 4.02595 loss)
I0615 15:56:04.161566  5063 solver.cpp:473] Iteration 730, lr = 0.0001
I0615 15:56:04.920835  5063 solver.cpp:213] Iteration 740, loss = 3.92009
I0615 15:56:04.920855  5063 solver.cpp:228]     Train net output #0: softmax = 3.92009 (* 1 = 3.92009 loss)
I0615 15:56:04.920866  5063 solver.cpp:473] Iteration 740, lr = 0.0001
I0615 15:56:05.679317  5063 solver.cpp:213] Iteration 750, loss = 4.08636
I0615 15:56:05.679337  5063 solver.cpp:228]     Train net output #0: softmax = 4.08636 (* 1 = 4.08636 loss)
I0615 15:56:05.679342  5063 solver.cpp:473] Iteration 750, lr = 0.0001
I0615 15:56:06.437922  5063 solver.cpp:213] Iteration 760, loss = 4.15967
I0615 15:56:06.437943  5063 solver.cpp:228]     Train net output #0: softmax = 4.15967 (* 1 = 4.15967 loss)
I0615 15:56:06.437948  5063 solver.cpp:473] Iteration 760, lr = 0.0001
I0615 15:56:07.196689  5063 solver.cpp:213] Iteration 770, loss = 4.10331
I0615 15:56:07.196720  5063 solver.cpp:228]     Train net output #0: softmax = 4.10331 (* 1 = 4.10331 loss)
I0615 15:56:07.196725  5063 solver.cpp:473] Iteration 770, lr = 0.0001
I0615 15:56:07.955677  5063 solver.cpp:213] Iteration 780, loss = 4.08439
I0615 15:56:07.955701  5063 solver.cpp:228]     Train net output #0: softmax = 4.08439 (* 1 = 4.08439 loss)
I0615 15:56:07.955706  5063 solver.cpp:473] Iteration 780, lr = 0.0001
I0615 15:56:08.714673  5063 solver.cpp:213] Iteration 790, loss = 3.90623
I0615 15:56:08.714695  5063 solver.cpp:228]     Train net output #0: softmax = 3.90623 (* 1 = 3.90623 loss)
I0615 15:56:08.714699  5063 solver.cpp:473] Iteration 790, lr = 0.0001
I0615 15:56:09.472728  5063 solver.cpp:213] Iteration 800, loss = 4.05326
I0615 15:56:09.472772  5063 solver.cpp:228]     Train net output #0: softmax = 4.05326 (* 1 = 4.05326 loss)
I0615 15:56:09.472777  5063 solver.cpp:473] Iteration 800, lr = 0.0001
I0615 15:56:10.231464  5063 solver.cpp:213] Iteration 810, loss = 4.07264
I0615 15:56:10.231483  5063 solver.cpp:228]     Train net output #0: softmax = 4.07264 (* 1 = 4.07264 loss)
I0615 15:56:10.231488  5063 solver.cpp:473] Iteration 810, lr = 0.0001
I0615 15:56:10.991083  5063 solver.cpp:213] Iteration 820, loss = 4.01967
I0615 15:56:10.991102  5063 solver.cpp:228]     Train net output #0: softmax = 4.01967 (* 1 = 4.01967 loss)
I0615 15:56:10.991106  5063 solver.cpp:473] Iteration 820, lr = 0.0001
I0615 15:56:11.750509  5063 solver.cpp:213] Iteration 830, loss = 4.04959
I0615 15:56:11.750541  5063 solver.cpp:228]     Train net output #0: softmax = 4.04959 (* 1 = 4.04959 loss)
I0615 15:56:11.750548  5063 solver.cpp:473] Iteration 830, lr = 0.0001
I0615 15:56:12.509723  5063 solver.cpp:213] Iteration 840, loss = 4.06731
I0615 15:56:12.509743  5063 solver.cpp:228]     Train net output #0: softmax = 4.06731 (* 1 = 4.06731 loss)
I0615 15:56:12.509748  5063 solver.cpp:473] Iteration 840, lr = 0.0001
I0615 15:56:13.268308  5063 solver.cpp:213] Iteration 850, loss = 4.08855
I0615 15:56:13.268333  5063 solver.cpp:228]     Train net output #0: softmax = 4.08855 (* 1 = 4.08855 loss)
I0615 15:56:13.268337  5063 solver.cpp:473] Iteration 850, lr = 0.0001
I0615 15:56:14.027223  5063 solver.cpp:213] Iteration 860, loss = 3.9819
I0615 15:56:14.027243  5063 solver.cpp:228]     Train net output #0: softmax = 3.9819 (* 1 = 3.9819 loss)
I0615 15:56:14.027247  5063 solver.cpp:473] Iteration 860, lr = 0.0001
I0615 15:56:14.785995  5063 solver.cpp:213] Iteration 870, loss = 3.96408
I0615 15:56:14.786015  5063 solver.cpp:228]     Train net output #0: softmax = 3.96408 (* 1 = 3.96408 loss)
I0615 15:56:14.786020  5063 solver.cpp:473] Iteration 870, lr = 0.0001
I0615 15:56:15.545048  5063 solver.cpp:213] Iteration 880, loss = 4.03105
I0615 15:56:15.545068  5063 solver.cpp:228]     Train net output #0: softmax = 4.03105 (* 1 = 4.03105 loss)
I0615 15:56:15.545073  5063 solver.cpp:473] Iteration 880, lr = 0.0001
I0615 15:56:16.303841  5063 solver.cpp:213] Iteration 890, loss = 4.06307
I0615 15:56:16.303860  5063 solver.cpp:228]     Train net output #0: softmax = 4.06307 (* 1 = 4.06307 loss)
I0615 15:56:16.303865  5063 solver.cpp:473] Iteration 890, lr = 0.0001
I0615 15:56:17.062383  5063 solver.cpp:213] Iteration 900, loss = 4.01759
I0615 15:56:17.062417  5063 solver.cpp:228]     Train net output #0: softmax = 4.01759 (* 1 = 4.01759 loss)
I0615 15:56:17.062422  5063 solver.cpp:473] Iteration 900, lr = 0.0001
I0615 15:56:17.820341  5063 solver.cpp:213] Iteration 910, loss = 4.09292
I0615 15:56:17.820366  5063 solver.cpp:228]     Train net output #0: softmax = 4.09292 (* 1 = 4.09292 loss)
I0615 15:56:17.820371  5063 solver.cpp:473] Iteration 910, lr = 0.0001
I0615 15:56:18.578704  5063 solver.cpp:213] Iteration 920, loss = 3.84571
I0615 15:56:18.578727  5063 solver.cpp:228]     Train net output #0: softmax = 3.84571 (* 1 = 3.84571 loss)
I0615 15:56:18.578730  5063 solver.cpp:473] Iteration 920, lr = 0.0001
I0615 15:56:19.337697  5063 solver.cpp:213] Iteration 930, loss = 4.03083
I0615 15:56:19.337716  5063 solver.cpp:228]     Train net output #0: softmax = 4.03083 (* 1 = 4.03083 loss)
I0615 15:56:19.337721  5063 solver.cpp:473] Iteration 930, lr = 0.0001
I0615 15:56:20.096776  5063 solver.cpp:213] Iteration 940, loss = 4.10884
I0615 15:56:20.096796  5063 solver.cpp:228]     Train net output #0: softmax = 4.10884 (* 1 = 4.10884 loss)
I0615 15:56:20.096801  5063 solver.cpp:473] Iteration 940, lr = 0.0001
I0615 15:56:20.855538  5063 solver.cpp:213] Iteration 950, loss = 3.92012
I0615 15:56:20.855558  5063 solver.cpp:228]     Train net output #0: softmax = 3.92012 (* 1 = 3.92012 loss)
I0615 15:56:20.855563  5063 solver.cpp:473] Iteration 950, lr = 0.0001
I0615 15:56:21.614202  5063 solver.cpp:213] Iteration 960, loss = 4.03083
I0615 15:56:21.614223  5063 solver.cpp:228]     Train net output #0: softmax = 4.03083 (* 1 = 4.03083 loss)
I0615 15:56:21.614245  5063 solver.cpp:473] Iteration 960, lr = 0.0001
I0615 15:56:22.373245  5063 solver.cpp:213] Iteration 970, loss = 4.10978
I0615 15:56:22.373276  5063 solver.cpp:228]     Train net output #0: softmax = 4.10978 (* 1 = 4.10978 loss)
I0615 15:56:22.373284  5063 solver.cpp:473] Iteration 970, lr = 0.0001
I0615 15:56:23.132366  5063 solver.cpp:213] Iteration 980, loss = 3.99848
I0615 15:56:23.132392  5063 solver.cpp:228]     Train net output #0: softmax = 3.99848 (* 1 = 3.99848 loss)
I0615 15:56:23.132397  5063 solver.cpp:473] Iteration 980, lr = 0.0001
I0615 15:56:23.890768  5063 solver.cpp:213] Iteration 990, loss = 4.07242
I0615 15:56:23.890789  5063 solver.cpp:228]     Train net output #0: softmax = 4.07242 (* 1 = 4.07242 loss)
I0615 15:56:23.890794  5063 solver.cpp:473] Iteration 990, lr = 0.0001
I0615 15:56:24.595880  5063 solver.cpp:362] Snapshotting to snapshots/16-06-15_15h49m18s_0_11_pretrainClassification_iter_1000.caffemodel
I0615 15:56:24.596762  5063 solver.cpp:370] Snapshotting solver state to snapshots/16-06-15_15h49m18s_0_11_pretrainClassification_iter_1000.solverstate
I0615 15:56:24.597156  5063 solver.cpp:291] Iteration 1000, Testing net (#0)
I0615 15:56:24.691398  5063 solver.cpp:342]     Test net output #0: accuracy_top_1 = 0.1
I0615 15:56:24.691414  5063 solver.cpp:342]     Test net output #1: accuracy_top_5 = 0.309375
I0615 15:56:24.691421  5063 solver.cpp:342]     Test net output #2: softmax = 3.97175 (* 1 = 3.97175 loss)
I0615 15:56:24.745014  5063 solver.cpp:213] Iteration 1000, loss = 4.06362
I0615 15:56:24.745028  5063 solver.cpp:228]     Train net output #0: softmax = 4.06362 (* 1 = 4.06362 loss)
I0615 15:56:24.745033  5063 solver.cpp:473] Iteration 1000, lr = 0.0001
I0615 15:56:25.503384  5063 solver.cpp:213] Iteration 1010, loss = 4.12022
I0615 15:56:25.503407  5063 solver.cpp:228]     Train net output #0: softmax = 4.12022 (* 1 = 4.12022 loss)
I0615 15:56:25.503410  5063 solver.cpp:473] Iteration 1010, lr = 0.0001
I0615 15:56:26.262022  5063 solver.cpp:213] Iteration 1020, loss = 4.14056
I0615 15:56:26.262042  5063 solver.cpp:228]     Train net output #0: softmax = 4.14056 (* 1 = 4.14056 loss)
I0615 15:56:26.262047  5063 solver.cpp:473] Iteration 1020, lr = 0.0001
I0615 15:56:27.020629  5063 solver.cpp:213] Iteration 1030, loss = 3.91735
I0615 15:56:27.020655  5063 solver.cpp:228]     Train net output #0: softmax = 3.91735 (* 1 = 3.91735 loss)
I0615 15:56:27.020660  5063 solver.cpp:473] Iteration 1030, lr = 0.0001
I0615 15:56:27.779014  5063 solver.cpp:213] Iteration 1040, loss = 3.93076
I0615 15:56:27.779041  5063 solver.cpp:228]     Train net output #0: softmax = 3.93076 (* 1 = 3.93076 loss)
I0615 15:56:27.779052  5063 solver.cpp:473] Iteration 1040, lr = 0.0001
I0615 15:56:28.537662  5063 solver.cpp:213] Iteration 1050, loss = 3.82441
I0615 15:56:28.537684  5063 solver.cpp:228]     Train net output #0: softmax = 3.82441 (* 1 = 3.82441 loss)
I0615 15:56:28.537689  5063 solver.cpp:473] Iteration 1050, lr = 0.0001
I0615 15:56:29.296368  5063 solver.cpp:213] Iteration 1060, loss = 3.90459
I0615 15:56:29.296387  5063 solver.cpp:228]     Train net output #0: softmax = 3.90459 (* 1 = 3.90459 loss)
I0615 15:56:29.296391  5063 solver.cpp:473] Iteration 1060, lr = 0.0001
I0615 15:56:30.055065  5063 solver.cpp:213] Iteration 1070, loss = 4.00157
I0615 15:56:30.055085  5063 solver.cpp:228]     Train net output #0: softmax = 4.00157 (* 1 = 4.00157 loss)
I0615 15:56:30.055090  5063 solver.cpp:473] Iteration 1070, lr = 0.0001
I0615 15:56:30.813892  5063 solver.cpp:213] Iteration 1080, loss = 4.02063
I0615 15:56:30.813911  5063 solver.cpp:228]     Train net output #0: softmax = 4.02063 (* 1 = 4.02063 loss)
I0615 15:56:30.813916  5063 solver.cpp:473] Iteration 1080, lr = 0.0001
I0615 15:56:31.572827  5063 solver.cpp:213] Iteration 1090, loss = 4.15862
I0615 15:56:31.572846  5063 solver.cpp:228]     Train net output #0: softmax = 4.15862 (* 1 = 4.15862 loss)
I0615 15:56:31.572851  5063 solver.cpp:473] Iteration 1090, lr = 0.0001
I0615 15:56:32.332134  5063 solver.cpp:213] Iteration 1100, loss = 4.00687
I0615 15:56:32.332173  5063 solver.cpp:228]     Train net output #0: softmax = 4.00687 (* 1 = 4.00687 loss)
I0615 15:56:32.332178  5063 solver.cpp:473] Iteration 1100, lr = 0.0001
I0615 15:56:33.091284  5063 solver.cpp:213] Iteration 1110, loss = 3.97963
I0615 15:56:33.091318  5063 solver.cpp:228]     Train net output #0: softmax = 3.97963 (* 1 = 3.97963 loss)
I0615 15:56:33.091326  5063 solver.cpp:473] Iteration 1110, lr = 0.0001
I0615 15:56:33.849716  5063 solver.cpp:213] Iteration 1120, loss = 4.18228
I0615 15:56:33.849738  5063 solver.cpp:228]     Train net output #0: softmax = 4.18228 (* 1 = 4.18228 loss)
I0615 15:56:33.849742  5063 solver.cpp:473] Iteration 1120, lr = 0.0001
I0615 15:56:34.608144  5063 solver.cpp:213] Iteration 1130, loss = 3.9961
I0615 15:56:34.608163  5063 solver.cpp:228]     Train net output #0: softmax = 3.9961 (* 1 = 3.9961 loss)
I0615 15:56:34.608167  5063 solver.cpp:473] Iteration 1130, lr = 0.0001
I0615 15:56:35.367064  5063 solver.cpp:213] Iteration 1140, loss = 3.96258
I0615 15:56:35.367084  5063 solver.cpp:228]     Train net output #0: softmax = 3.96258 (* 1 = 3.96258 loss)
I0615 15:56:35.367089  5063 solver.cpp:473] Iteration 1140, lr = 0.0001
I0615 15:56:36.125916  5063 solver.cpp:213] Iteration 1150, loss = 4.04648
I0615 15:56:36.125936  5063 solver.cpp:228]     Train net output #0: softmax = 4.04648 (* 1 = 4.04648 loss)
I0615 15:56:36.125941  5063 solver.cpp:473] Iteration 1150, lr = 0.0001
I0615 15:56:36.884564  5063 solver.cpp:213] Iteration 1160, loss = 3.94116
I0615 15:56:36.884585  5063 solver.cpp:228]     Train net output #0: softmax = 3.94116 (* 1 = 3.94116 loss)
I0615 15:56:36.884590  5063 solver.cpp:473] Iteration 1160, lr = 0.0001
I0615 15:56:37.642691  5063 solver.cpp:213] Iteration 1170, loss = 4.1953
I0615 15:56:37.642714  5063 solver.cpp:228]     Train net output #0: softmax = 4.1953 (* 1 = 4.1953 loss)
I0615 15:56:37.642717  5063 solver.cpp:473] Iteration 1170, lr = 0.0001
I0615 15:56:38.401289  5063 solver.cpp:213] Iteration 1180, loss = 3.81566
I0615 15:56:38.401311  5063 solver.cpp:228]     Train net output #0: softmax = 3.81566 (* 1 = 3.81566 loss)
I0615 15:56:38.401316  5063 solver.cpp:473] Iteration 1180, lr = 0.0001
I0615 15:56:39.159584  5063 solver.cpp:213] Iteration 1190, loss = 4.10279
I0615 15:56:39.159610  5063 solver.cpp:228]     Train net output #0: softmax = 4.10279 (* 1 = 4.10279 loss)
I0615 15:56:39.159615  5063 solver.cpp:473] Iteration 1190, lr = 0.0001
I0615 15:56:39.918957  5063 solver.cpp:213] Iteration 1200, loss = 4.10462
I0615 15:56:39.919003  5063 solver.cpp:228]     Train net output #0: softmax = 4.10462 (* 1 = 4.10462 loss)
I0615 15:56:39.919011  5063 solver.cpp:473] Iteration 1200, lr = 0.0001
I0615 15:56:40.677311  5063 solver.cpp:213] Iteration 1210, loss = 3.96169
I0615 15:56:40.677331  5063 solver.cpp:228]     Train net output #0: softmax = 3.96169 (* 1 = 3.96169 loss)
I0615 15:56:40.677335  5063 solver.cpp:473] Iteration 1210, lr = 0.0001
I0615 15:56:41.434732  5063 solver.cpp:213] Iteration 1220, loss = 4.00378
I0615 15:56:41.434752  5063 solver.cpp:228]     Train net output #0: softmax = 4.00378 (* 1 = 4.00378 loss)
I0615 15:56:41.434756  5063 solver.cpp:473] Iteration 1220, lr = 0.0001
I0615 15:56:42.193169  5063 solver.cpp:213] Iteration 1230, loss = 3.98489
I0615 15:56:42.193189  5063 solver.cpp:228]     Train net output #0: softmax = 3.98489 (* 1 = 3.98489 loss)
I0615 15:56:42.193194  5063 solver.cpp:473] Iteration 1230, lr = 0.0001
I0615 15:56:42.951660  5063 solver.cpp:213] Iteration 1240, loss = 4.05797
I0615 15:56:42.951681  5063 solver.cpp:228]     Train net output #0: softmax = 4.05797 (* 1 = 4.05797 loss)
I0615 15:56:42.951685  5063 solver.cpp:473] Iteration 1240, lr = 0.0001
I0615 15:56:43.710520  5063 solver.cpp:213] Iteration 1250, loss = 4.01405
I0615 15:56:43.710546  5063 solver.cpp:228]     Train net output #0: softmax = 4.01405 (* 1 = 4.01405 loss)
I0615 15:56:43.710551  5063 solver.cpp:473] Iteration 1250, lr = 0.0001
I0615 15:56:44.469175  5063 solver.cpp:213] Iteration 1260, loss = 3.81094
I0615 15:56:44.469197  5063 solver.cpp:228]     Train net output #0: softmax = 3.81094 (* 1 = 3.81094 loss)
I0615 15:56:44.469202  5063 solver.cpp:473] Iteration 1260, lr = 0.0001
I0615 15:56:45.228307  5063 solver.cpp:213] Iteration 1270, loss = 3.96912
I0615 15:56:45.228327  5063 solver.cpp:228]     Train net output #0: softmax = 3.96912 (* 1 = 3.96912 loss)
I0615 15:56:45.228330  5063 solver.cpp:473] Iteration 1270, lr = 0.0001
I0615 15:56:45.986999  5063 solver.cpp:213] Iteration 1280, loss = 3.87931
I0615 15:56:45.987018  5063 solver.cpp:228]     Train net output #0: softmax = 3.87931 (* 1 = 3.87931 loss)
I0615 15:56:45.987023  5063 solver.cpp:473] Iteration 1280, lr = 0.0001
I0615 15:56:46.745838  5063 solver.cpp:213] Iteration 1290, loss = 4.00788
I0615 15:56:46.745858  5063 solver.cpp:228]     Train net output #0: softmax = 4.00788 (* 1 = 4.00788 loss)
I0615 15:56:46.745862  5063 solver.cpp:473] Iteration 1290, lr = 0.0001
I0615 15:56:47.504518  5063 solver.cpp:213] Iteration 1300, loss = 4.15575
I0615 15:56:47.504536  5063 solver.cpp:228]     Train net output #0: softmax = 4.15575 (* 1 = 4.15575 loss)
I0615 15:56:47.504541  5063 solver.cpp:473] Iteration 1300, lr = 0.0001
I0615 15:56:48.263656  5063 solver.cpp:213] Iteration 1310, loss = 3.84881
I0615 15:56:48.263679  5063 solver.cpp:228]     Train net output #0: softmax = 3.84881 (* 1 = 3.84881 loss)
I0615 15:56:48.263682  5063 solver.cpp:473] Iteration 1310, lr = 0.0001
I0615 15:56:49.021994  5063 solver.cpp:213] Iteration 1320, loss = 4.00866
I0615 15:56:49.022020  5063 solver.cpp:228]     Train net output #0: softmax = 4.00866 (* 1 = 4.00866 loss)
I0615 15:56:49.022025  5063 solver.cpp:473] Iteration 1320, lr = 0.0001
I0615 15:56:49.780776  5063 solver.cpp:213] Iteration 1330, loss = 4.04935
I0615 15:56:49.780797  5063 solver.cpp:228]     Train net output #0: softmax = 4.04935 (* 1 = 4.04935 loss)
I0615 15:56:49.780802  5063 solver.cpp:473] Iteration 1330, lr = 0.0001
I0615 15:56:50.539751  5063 solver.cpp:213] Iteration 1340, loss = 4.16462
I0615 15:56:50.539768  5063 solver.cpp:228]     Train net output #0: softmax = 4.16462 (* 1 = 4.16462 loss)
I0615 15:56:50.539773  5063 solver.cpp:473] Iteration 1340, lr = 0.0001
I0615 15:56:51.298434  5063 solver.cpp:213] Iteration 1350, loss = 3.92709
I0615 15:56:51.298454  5063 solver.cpp:228]     Train net output #0: softmax = 3.92709 (* 1 = 3.92709 loss)
I0615 15:56:51.298458  5063 solver.cpp:473] Iteration 1350, lr = 0.0001
I0615 15:56:52.057241  5063 solver.cpp:213] Iteration 1360, loss = 3.94721
I0615 15:56:52.057263  5063 solver.cpp:228]     Train net output #0: softmax = 3.94721 (* 1 = 3.94721 loss)
I0615 15:56:52.057286  5063 solver.cpp:473] Iteration 1360, lr = 0.0001
I0615 15:56:52.816233  5063 solver.cpp:213] Iteration 1370, loss = 3.9417
I0615 15:56:52.816253  5063 solver.cpp:228]     Train net output #0: softmax = 3.9417 (* 1 = 3.9417 loss)
I0615 15:56:52.816257  5063 solver.cpp:473] Iteration 1370, lr = 0.0001
I0615 15:56:53.574944  5063 solver.cpp:213] Iteration 1380, loss = 4.0249
I0615 15:56:53.574965  5063 solver.cpp:228]     Train net output #0: softmax = 4.0249 (* 1 = 4.0249 loss)
I0615 15:56:53.574970  5063 solver.cpp:473] Iteration 1380, lr = 0.0001
I0615 15:56:54.332495  5063 solver.cpp:213] Iteration 1390, loss = 4.05178
I0615 15:56:54.332522  5063 solver.cpp:228]     Train net output #0: softmax = 4.05178 (* 1 = 4.05178 loss)
I0615 15:56:54.332527  5063 solver.cpp:473] Iteration 1390, lr = 0.0001
I0615 15:56:55.091174  5063 solver.cpp:213] Iteration 1400, loss = 3.93859
I0615 15:56:55.091197  5063 solver.cpp:228]     Train net output #0: softmax = 3.93859 (* 1 = 3.93859 loss)
I0615 15:56:55.091200  5063 solver.cpp:473] Iteration 1400, lr = 0.0001
I0615 15:56:55.850034  5063 solver.cpp:213] Iteration 1410, loss = 4.04683
I0615 15:56:55.850054  5063 solver.cpp:228]     Train net output #0: softmax = 4.04683 (* 1 = 4.04683 loss)
I0615 15:56:55.850059  5063 solver.cpp:473] Iteration 1410, lr = 0.0001
I0615 15:56:56.609078  5063 solver.cpp:213] Iteration 1420, loss = 3.81617
I0615 15:56:56.609098  5063 solver.cpp:228]     Train net output #0: softmax = 3.81617 (* 1 = 3.81617 loss)
I0615 15:56:56.609103  5063 solver.cpp:473] Iteration 1420, lr = 0.0001
I0615 15:56:57.367152  5063 solver.cpp:213] Iteration 1430, loss = 3.91894
I0615 15:56:57.367172  5063 solver.cpp:228]     Train net output #0: softmax = 3.91894 (* 1 = 3.91894 loss)
I0615 15:56:57.367177  5063 solver.cpp:473] Iteration 1430, lr = 0.0001
I0615 15:56:58.126132  5063 solver.cpp:213] Iteration 1440, loss = 4.02791
I0615 15:56:58.126153  5063 solver.cpp:228]     Train net output #0: softmax = 4.02791 (* 1 = 4.02791 loss)
I0615 15:56:58.126158  5063 solver.cpp:473] Iteration 1440, lr = 0.0001
I0615 15:56:58.885103  5063 solver.cpp:213] Iteration 1450, loss = 3.97194
I0615 15:56:58.885128  5063 solver.cpp:228]     Train net output #0: softmax = 3.97194 (* 1 = 3.97194 loss)
I0615 15:56:58.885133  5063 solver.cpp:473] Iteration 1450, lr = 0.0001
I0615 15:56:59.643431  5063 solver.cpp:213] Iteration 1460, loss = 3.95818
I0615 15:56:59.643453  5063 solver.cpp:228]     Train net output #0: softmax = 3.95818 (* 1 = 3.95818 loss)
I0615 15:56:59.643458  5063 solver.cpp:473] Iteration 1460, lr = 0.0001
I0615 15:57:00.401736  5063 solver.cpp:213] Iteration 1470, loss = 3.84278
I0615 15:57:00.401757  5063 solver.cpp:228]     Train net output #0: softmax = 3.84278 (* 1 = 3.84278 loss)
I0615 15:57:00.401762  5063 solver.cpp:473] Iteration 1470, lr = 0.0001
I0615 15:57:01.159973  5063 solver.cpp:213] Iteration 1480, loss = 4.12888
I0615 15:57:01.159993  5063 solver.cpp:228]     Train net output #0: softmax = 4.12888 (* 1 = 4.12888 loss)
I0615 15:57:01.159998  5063 solver.cpp:473] Iteration 1480, lr = 0.0001
I0615 15:57:01.918617  5063 solver.cpp:213] Iteration 1490, loss = 4.03613
I0615 15:57:01.918637  5063 solver.cpp:228]     Train net output #0: softmax = 4.03613 (* 1 = 4.03613 loss)
I0615 15:57:01.918642  5063 solver.cpp:473] Iteration 1490, lr = 0.0001
I0615 15:57:02.677332  5063 solver.cpp:213] Iteration 1500, loss = 4.02603
I0615 15:57:02.677351  5063 solver.cpp:228]     Train net output #0: softmax = 4.02603 (* 1 = 4.02603 loss)
I0615 15:57:02.677356  5063 solver.cpp:473] Iteration 1500, lr = 0.0001
I0615 15:57:03.435613  5063 solver.cpp:213] Iteration 1510, loss = 3.97397
I0615 15:57:03.435633  5063 solver.cpp:228]     Train net output #0: softmax = 3.97397 (* 1 = 3.97397 loss)
I0615 15:57:03.435638  5063 solver.cpp:473] Iteration 1510, lr = 0.0001
I0615 15:57:04.194120  5063 solver.cpp:213] Iteration 1520, loss = 3.93106
I0615 15:57:04.194147  5063 solver.cpp:228]     Train net output #0: softmax = 3.93106 (* 1 = 3.93106 loss)
I0615 15:57:04.194170  5063 solver.cpp:473] Iteration 1520, lr = 0.0001
I0615 15:57:04.952513  5063 solver.cpp:213] Iteration 1530, loss = 3.94679
I0615 15:57:04.952534  5063 solver.cpp:228]     Train net output #0: softmax = 3.94679 (* 1 = 3.94679 loss)
I0615 15:57:04.952539  5063 solver.cpp:473] Iteration 1530, lr = 0.0001
I0615 15:57:05.710475  5063 solver.cpp:213] Iteration 1540, loss = 4.11114
I0615 15:57:05.710496  5063 solver.cpp:228]     Train net output #0: softmax = 4.11114 (* 1 = 4.11114 loss)
I0615 15:57:05.710501  5063 solver.cpp:473] Iteration 1540, lr = 0.0001
I0615 15:57:06.469089  5063 solver.cpp:213] Iteration 1550, loss = 3.98957
I0615 15:57:06.469110  5063 solver.cpp:228]     Train net output #0: softmax = 3.98957 (* 1 = 3.98957 loss)
I0615 15:57:06.469115  5063 solver.cpp:473] Iteration 1550, lr = 0.0001
I0615 15:57:07.228160  5063 solver.cpp:213] Iteration 1560, loss = 4.04089
I0615 15:57:07.228180  5063 solver.cpp:228]     Train net output #0: softmax = 4.04089 (* 1 = 4.04089 loss)
I0615 15:57:07.228185  5063 solver.cpp:473] Iteration 1560, lr = 0.0001
I0615 15:57:07.987030  5063 solver.cpp:213] Iteration 1570, loss = 3.9514
I0615 15:57:07.987051  5063 solver.cpp:228]     Train net output #0: softmax = 3.9514 (* 1 = 3.9514 loss)
I0615 15:57:07.987054  5063 solver.cpp:473] Iteration 1570, lr = 0.0001
I0615 15:57:08.746004  5063 solver.cpp:213] Iteration 1580, loss = 3.90466
I0615 15:57:08.746029  5063 solver.cpp:228]     Train net output #0: softmax = 3.90466 (* 1 = 3.90466 loss)
I0615 15:57:08.746034  5063 solver.cpp:473] Iteration 1580, lr = 0.0001
I0615 15:57:09.504348  5063 solver.cpp:213] Iteration 1590, loss = 4.14375
I0615 15:57:09.504371  5063 solver.cpp:228]     Train net output #0: softmax = 4.14375 (* 1 = 4.14375 loss)
I0615 15:57:09.504376  5063 solver.cpp:473] Iteration 1590, lr = 0.0001
I0615 15:57:10.261907  5063 solver.cpp:213] Iteration 1600, loss = 4.06885
I0615 15:57:10.261945  5063 solver.cpp:228]     Train net output #0: softmax = 4.06885 (* 1 = 4.06885 loss)
I0615 15:57:10.262086  5063 solver.cpp:473] Iteration 1600, lr = 0.0001
I0615 15:57:11.020185  5063 solver.cpp:213] Iteration 1610, loss = 3.86995
I0615 15:57:11.020210  5063 solver.cpp:228]     Train net output #0: softmax = 3.86995 (* 1 = 3.86995 loss)
I0615 15:57:11.020215  5063 solver.cpp:473] Iteration 1610, lr = 0.0001
I0615 15:57:11.778692  5063 solver.cpp:213] Iteration 1620, loss = 3.96377
I0615 15:57:11.778719  5063 solver.cpp:228]     Train net output #0: softmax = 3.96377 (* 1 = 3.96377 loss)
I0615 15:57:11.778723  5063 solver.cpp:473] Iteration 1620, lr = 0.0001
I0615 15:57:12.536499  5063 solver.cpp:213] Iteration 1630, loss = 3.91685
I0615 15:57:12.536520  5063 solver.cpp:228]     Train net output #0: softmax = 3.91685 (* 1 = 3.91685 loss)
I0615 15:57:12.536525  5063 solver.cpp:473] Iteration 1630, lr = 0.0001
I0615 15:57:13.293967  5063 solver.cpp:213] Iteration 1640, loss = 4.10271
I0615 15:57:13.293989  5063 solver.cpp:228]     Train net output #0: softmax = 4.10271 (* 1 = 4.10271 loss)
I0615 15:57:13.293994  5063 solver.cpp:473] Iteration 1640, lr = 0.0001
I0615 15:57:14.051301  5063 solver.cpp:213] Iteration 1650, loss = 3.79818
I0615 15:57:14.051326  5063 solver.cpp:228]     Train net output #0: softmax = 3.79818 (* 1 = 3.79818 loss)
I0615 15:57:14.051331  5063 solver.cpp:473] Iteration 1650, lr = 0.0001
I0615 15:57:14.808195  5063 solver.cpp:213] Iteration 1660, loss = 3.88173
I0615 15:57:14.808217  5063 solver.cpp:228]     Train net output #0: softmax = 3.88173 (* 1 = 3.88173 loss)
I0615 15:57:14.808221  5063 solver.cpp:473] Iteration 1660, lr = 0.0001
I0615 15:57:15.565003  5063 solver.cpp:213] Iteration 1670, loss = 3.79384
I0615 15:57:15.565026  5063 solver.cpp:228]     Train net output #0: softmax = 3.79384 (* 1 = 3.79384 loss)
I0615 15:57:15.565031  5063 solver.cpp:473] Iteration 1670, lr = 0.0001
I0615 15:57:16.323796  5063 solver.cpp:213] Iteration 1680, loss = 4.09895
I0615 15:57:16.323817  5063 solver.cpp:228]     Train net output #0: softmax = 4.09895 (* 1 = 4.09895 loss)
I0615 15:57:16.323828  5063 solver.cpp:473] Iteration 1680, lr = 0.0001
I0615 15:57:17.081979  5063 solver.cpp:213] Iteration 1690, loss = 4.04083
I0615 15:57:17.082000  5063 solver.cpp:228]     Train net output #0: softmax = 4.04083 (* 1 = 4.04083 loss)
I0615 15:57:17.082005  5063 solver.cpp:473] Iteration 1690, lr = 0.0001
I0615 15:57:17.840417  5063 solver.cpp:213] Iteration 1700, loss = 3.91386
I0615 15:57:17.840438  5063 solver.cpp:228]     Train net output #0: softmax = 3.91386 (* 1 = 3.91386 loss)
I0615 15:57:17.840443  5063 solver.cpp:473] Iteration 1700, lr = 0.0001
I0615 15:57:18.598835  5063 solver.cpp:213] Iteration 1710, loss = 3.98198
I0615 15:57:18.598857  5063 solver.cpp:228]     Train net output #0: softmax = 3.98198 (* 1 = 3.98198 loss)
I0615 15:57:18.598861  5063 solver.cpp:473] Iteration 1710, lr = 0.0001
I0615 15:57:19.358268  5063 solver.cpp:213] Iteration 1720, loss = 3.79932
I0615 15:57:19.358294  5063 solver.cpp:228]     Train net output #0: softmax = 3.79932 (* 1 = 3.79932 loss)
I0615 15:57:19.358297  5063 solver.cpp:473] Iteration 1720, lr = 0.0001
I0615 15:57:20.117784  5063 solver.cpp:213] Iteration 1730, loss = 4.08002
I0615 15:57:20.117805  5063 solver.cpp:228]     Train net output #0: softmax = 4.08002 (* 1 = 4.08002 loss)
I0615 15:57:20.117810  5063 solver.cpp:473] Iteration 1730, lr = 0.0001
I0615 15:57:20.876710  5063 solver.cpp:213] Iteration 1740, loss = 3.87588
I0615 15:57:20.876734  5063 solver.cpp:228]     Train net output #0: softmax = 3.87588 (* 1 = 3.87588 loss)
I0615 15:57:20.876860  5063 solver.cpp:473] Iteration 1740, lr = 0.0001
I0615 15:57:21.636009  5063 solver.cpp:213] Iteration 1750, loss = 3.90017
I0615 15:57:21.636030  5063 solver.cpp:228]     Train net output #0: softmax = 3.90017 (* 1 = 3.90017 loss)
I0615 15:57:21.636034  5063 solver.cpp:473] Iteration 1750, lr = 0.0001
I0615 15:57:22.394135  5063 solver.cpp:213] Iteration 1760, loss = 3.93956
I0615 15:57:22.394156  5063 solver.cpp:228]     Train net output #0: softmax = 3.93956 (* 1 = 3.93956 loss)
I0615 15:57:22.394179  5063 solver.cpp:473] Iteration 1760, lr = 0.0001
I0615 15:57:23.152287  5063 solver.cpp:213] Iteration 1770, loss = 3.76834
I0615 15:57:23.152307  5063 solver.cpp:228]     Train net output #0: softmax = 3.76834 (* 1 = 3.76834 loss)
I0615 15:57:23.152312  5063 solver.cpp:473] Iteration 1770, lr = 0.0001
I0615 15:57:23.910598  5063 solver.cpp:213] Iteration 1780, loss = 3.87013
I0615 15:57:23.910619  5063 solver.cpp:228]     Train net output #0: softmax = 3.87013 (* 1 = 3.87013 loss)
I0615 15:57:23.910624  5063 solver.cpp:473] Iteration 1780, lr = 0.0001
I0615 15:57:24.669178  5063 solver.cpp:213] Iteration 1790, loss = 4.07406
I0615 15:57:24.669200  5063 solver.cpp:228]     Train net output #0: softmax = 4.07406 (* 1 = 4.07406 loss)
I0615 15:57:24.669205  5063 solver.cpp:473] Iteration 1790, lr = 0.0001
I0615 15:57:25.427217  5063 solver.cpp:213] Iteration 1800, loss = 3.89441
I0615 15:57:25.427243  5063 solver.cpp:228]     Train net output #0: softmax = 3.89441 (* 1 = 3.89441 loss)
I0615 15:57:25.427248  5063 solver.cpp:473] Iteration 1800, lr = 0.0001
I0615 15:57:26.185493  5063 solver.cpp:213] Iteration 1810, loss = 3.82103
I0615 15:57:26.185518  5063 solver.cpp:228]     Train net output #0: softmax = 3.82103 (* 1 = 3.82103 loss)
I0615 15:57:26.185650  5063 solver.cpp:473] Iteration 1810, lr = 0.0001
I0615 15:57:26.944211  5063 solver.cpp:213] Iteration 1820, loss = 4.06347
I0615 15:57:26.944242  5063 solver.cpp:228]     Train net output #0: softmax = 4.06347 (* 1 = 4.06347 loss)
I0615 15:57:26.944247  5063 solver.cpp:473] Iteration 1820, lr = 0.0001
I0615 15:57:27.702178  5063 solver.cpp:213] Iteration 1830, loss = 3.96468
I0615 15:57:27.702198  5063 solver.cpp:228]     Train net output #0: softmax = 3.96468 (* 1 = 3.96468 loss)
I0615 15:57:27.702203  5063 solver.cpp:473] Iteration 1830, lr = 0.0001
I0615 15:57:28.460772  5063 solver.cpp:213] Iteration 1840, loss = 3.92538
I0615 15:57:28.460795  5063 solver.cpp:228]     Train net output #0: softmax = 3.92538 (* 1 = 3.92538 loss)
I0615 15:57:28.460800  5063 solver.cpp:473] Iteration 1840, lr = 0.0001
I0615 15:57:29.218955  5063 solver.cpp:213] Iteration 1850, loss = 3.89887
I0615 15:57:29.218977  5063 solver.cpp:228]     Train net output #0: softmax = 3.89887 (* 1 = 3.89887 loss)
I0615 15:57:29.218981  5063 solver.cpp:473] Iteration 1850, lr = 0.0001
I0615 15:57:29.977360  5063 solver.cpp:213] Iteration 1860, loss = 3.85567
I0615 15:57:29.977380  5063 solver.cpp:228]     Train net output #0: softmax = 3.85567 (* 1 = 3.85567 loss)
I0615 15:57:29.977385  5063 solver.cpp:473] Iteration 1860, lr = 0.0001
I0615 15:57:30.736531  5063 solver.cpp:213] Iteration 1870, loss = 3.95753
I0615 15:57:30.736555  5063 solver.cpp:228]     Train net output #0: softmax = 3.95753 (* 1 = 3.95753 loss)
I0615 15:57:30.736560  5063 solver.cpp:473] Iteration 1870, lr = 0.0001
I0615 15:57:31.494797  5063 solver.cpp:213] Iteration 1880, loss = 3.91501
I0615 15:57:31.494818  5063 solver.cpp:228]     Train net output #0: softmax = 3.91501 (* 1 = 3.91501 loss)
I0615 15:57:31.494823  5063 solver.cpp:473] Iteration 1880, lr = 0.0001
I0615 15:57:32.253068  5063 solver.cpp:213] Iteration 1890, loss = 3.91028
I0615 15:57:32.253089  5063 solver.cpp:228]     Train net output #0: softmax = 3.91028 (* 1 = 3.91028 loss)
I0615 15:57:32.253093  5063 solver.cpp:473] Iteration 1890, lr = 0.0001
I0615 15:57:33.011515  5063 solver.cpp:213] Iteration 1900, loss = 3.93959
I0615 15:57:33.011538  5063 solver.cpp:228]     Train net output #0: softmax = 3.93959 (* 1 = 3.93959 loss)
I0615 15:57:33.011543  5063 solver.cpp:473] Iteration 1900, lr = 0.0001
I0615 15:57:33.770135  5063 solver.cpp:213] Iteration 1910, loss = 3.98204
I0615 15:57:33.770156  5063 solver.cpp:228]     Train net output #0: softmax = 3.98204 (* 1 = 3.98204 loss)
I0615 15:57:33.770161  5063 solver.cpp:473] Iteration 1910, lr = 0.0001
I0615 15:57:34.528823  5063 solver.cpp:213] Iteration 1920, loss = 3.90197
I0615 15:57:34.528843  5063 solver.cpp:228]     Train net output #0: softmax = 3.90197 (* 1 = 3.90197 loss)
I0615 15:57:34.528866  5063 solver.cpp:473] Iteration 1920, lr = 0.0001
I0615 15:57:35.287621  5063 solver.cpp:213] Iteration 1930, loss = 3.89864
I0615 15:57:35.287642  5063 solver.cpp:228]     Train net output #0: softmax = 3.89864 (* 1 = 3.89864 loss)
I0615 15:57:35.287647  5063 solver.cpp:473] Iteration 1930, lr = 0.0001
I0615 15:57:36.046277  5063 solver.cpp:213] Iteration 1940, loss = 4.13187
I0615 15:57:36.046299  5063 solver.cpp:228]     Train net output #0: softmax = 4.13187 (* 1 = 4.13187 loss)
I0615 15:57:36.046303  5063 solver.cpp:473] Iteration 1940, lr = 0.0001
I0615 15:57:36.804988  5063 solver.cpp:213] Iteration 1950, loss = 4.04052
I0615 15:57:36.805013  5063 solver.cpp:228]     Train net output #0: softmax = 4.04052 (* 1 = 4.04052 loss)
I0615 15:57:36.805142  5063 solver.cpp:473] Iteration 1950, lr = 0.0001
I0615 15:57:37.563725  5063 solver.cpp:213] Iteration 1960, loss = 3.87669
I0615 15:57:37.563751  5063 solver.cpp:228]     Train net output #0: softmax = 3.87669 (* 1 = 3.87669 loss)
I0615 15:57:37.563755  5063 solver.cpp:473] Iteration 1960, lr = 0.0001
I0615 15:57:38.321698  5063 solver.cpp:213] Iteration 1970, loss = 3.85861
I0615 15:57:38.321722  5063 solver.cpp:228]     Train net output #0: softmax = 3.85861 (* 1 = 3.85861 loss)
I0615 15:57:38.321727  5063 solver.cpp:473] Iteration 1970, lr = 0.0001
I0615 15:57:39.079010  5063 solver.cpp:213] Iteration 1980, loss = 4.05839
I0615 15:57:39.079032  5063 solver.cpp:228]     Train net output #0: softmax = 4.05839 (* 1 = 4.05839 loss)
I0615 15:57:39.079036  5063 solver.cpp:473] Iteration 1980, lr = 0.0001
I0615 15:57:39.837548  5063 solver.cpp:213] Iteration 1990, loss = 4.03021
I0615 15:57:39.837569  5063 solver.cpp:228]     Train net output #0: softmax = 4.03021 (* 1 = 4.03021 loss)
I0615 15:57:39.837574  5063 solver.cpp:473] Iteration 1990, lr = 0.0001
I0615 15:57:40.541702  5063 solver.cpp:362] Snapshotting to snapshots/16-06-15_15h49m18s_0_11_pretrainClassification_iter_2000.caffemodel
I0615 15:57:40.542428  5063 solver.cpp:370] Snapshotting solver state to snapshots/16-06-15_15h49m18s_0_11_pretrainClassification_iter_2000.solverstate
I0615 15:57:40.542806  5063 solver.cpp:291] Iteration 2000, Testing net (#0)
I0615 15:57:40.637276  5063 solver.cpp:342]     Test net output #0: accuracy_top_1 = 0.10625
I0615 15:57:40.637292  5063 solver.cpp:342]     Test net output #1: accuracy_top_5 = 0.309375
I0615 15:57:40.637300  5063 solver.cpp:342]     Test net output #2: softmax = 3.98543 (* 1 = 3.98543 loss)
I0615 15:57:40.690881  5063 solver.cpp:213] Iteration 2000, loss = 3.84227
I0615 15:57:40.690896  5063 solver.cpp:228]     Train net output #0: softmax = 3.84227 (* 1 = 3.84227 loss)
I0615 15:57:40.690901  5063 solver.cpp:473] Iteration 2000, lr = 0.0001
I0615 15:57:41.449842  5063 solver.cpp:213] Iteration 2010, loss = 4.07048
I0615 15:57:41.449863  5063 solver.cpp:228]     Train net output #0: softmax = 4.07048 (* 1 = 4.07048 loss)
I0615 15:57:41.449868  5063 solver.cpp:473] Iteration 2010, lr = 0.0001
I0615 15:57:42.208243  5063 solver.cpp:213] Iteration 2020, loss = 3.95027
I0615 15:57:42.208269  5063 solver.cpp:228]     Train net output #0: softmax = 3.95027 (* 1 = 3.95027 loss)
I0615 15:57:42.208472  5063 solver.cpp:473] Iteration 2020, lr = 0.0001
I0615 15:57:42.967347  5063 solver.cpp:213] Iteration 2030, loss = 3.76678
I0615 15:57:42.967367  5063 solver.cpp:228]     Train net output #0: softmax = 3.76678 (* 1 = 3.76678 loss)
I0615 15:57:42.967371  5063 solver.cpp:473] Iteration 2030, lr = 0.0001
I0615 15:57:43.726094  5063 solver.cpp:213] Iteration 2040, loss = 3.90586
I0615 15:57:43.726119  5063 solver.cpp:228]     Train net output #0: softmax = 3.90586 (* 1 = 3.90586 loss)
I0615 15:57:43.726124  5063 solver.cpp:473] Iteration 2040, lr = 0.0001
I0615 15:57:44.484498  5063 solver.cpp:213] Iteration 2050, loss = 4.07946
I0615 15:57:44.484518  5063 solver.cpp:228]     Train net output #0: softmax = 4.07946 (* 1 = 4.07946 loss)
I0615 15:57:44.484522  5063 solver.cpp:473] Iteration 2050, lr = 0.0001
I0615 15:57:45.243448  5063 solver.cpp:213] Iteration 2060, loss = 3.94658
I0615 15:57:45.243468  5063 solver.cpp:228]     Train net output #0: softmax = 3.94658 (* 1 = 3.94658 loss)
I0615 15:57:45.243474  5063 solver.cpp:473] Iteration 2060, lr = 0.0001
I0615 15:57:46.001947  5063 solver.cpp:213] Iteration 2070, loss = 4.00882
I0615 15:57:46.001967  5063 solver.cpp:228]     Train net output #0: softmax = 4.00882 (* 1 = 4.00882 loss)
I0615 15:57:46.001973  5063 solver.cpp:473] Iteration 2070, lr = 0.0001
I0615 15:57:46.760571  5063 solver.cpp:213] Iteration 2080, loss = 4.03534
I0615 15:57:46.760591  5063 solver.cpp:228]     Train net output #0: softmax = 4.03534 (* 1 = 4.03534 loss)
I0615 15:57:46.760596  5063 solver.cpp:473] Iteration 2080, lr = 0.0001
I0615 15:57:47.519242  5063 solver.cpp:213] Iteration 2090, loss = 3.76228
I0615 15:57:47.519268  5063 solver.cpp:228]     Train net output #0: softmax = 3.76228 (* 1 = 3.76228 loss)
I0615 15:57:47.519273  5063 solver.cpp:473] Iteration 2090, lr = 0.0001
I0615 15:57:48.277382  5063 solver.cpp:213] Iteration 2100, loss = 4.00253
I0615 15:57:48.277403  5063 solver.cpp:228]     Train net output #0: softmax = 4.00253 (* 1 = 4.00253 loss)
I0615 15:57:48.277408  5063 solver.cpp:473] Iteration 2100, lr = 0.0001
I0615 15:57:49.035923  5063 solver.cpp:213] Iteration 2110, loss = 3.69484
I0615 15:57:49.035943  5063 solver.cpp:228]     Train net output #0: softmax = 3.69484 (* 1 = 3.69484 loss)
I0615 15:57:49.035948  5063 solver.cpp:473] Iteration 2110, lr = 0.0001
I0615 15:57:49.794260  5063 solver.cpp:213] Iteration 2120, loss = 4.00701
I0615 15:57:49.794281  5063 solver.cpp:228]     Train net output #0: softmax = 4.00701 (* 1 = 4.00701 loss)
I0615 15:57:49.794286  5063 solver.cpp:473] Iteration 2120, lr = 0.0001
I0615 15:57:50.552253  5063 solver.cpp:213] Iteration 2130, loss = 4.0899
I0615 15:57:50.552273  5063 solver.cpp:228]     Train net output #0: softmax = 4.0899 (* 1 = 4.0899 loss)
I0615 15:57:50.552278  5063 solver.cpp:473] Iteration 2130, lr = 0.0001
I0615 15:57:51.310230  5063 solver.cpp:213] Iteration 2140, loss = 3.93942
I0615 15:57:51.310271  5063 solver.cpp:228]     Train net output #0: softmax = 3.93942 (* 1 = 3.93942 loss)
I0615 15:57:51.310278  5063 solver.cpp:473] Iteration 2140, lr = 0.0001
I0615 15:57:52.068517  5063 solver.cpp:213] Iteration 2150, loss = 3.69476
I0615 15:57:52.068538  5063 solver.cpp:228]     Train net output #0: softmax = 3.69476 (* 1 = 3.69476 loss)
I0615 15:57:52.068543  5063 solver.cpp:473] Iteration 2150, lr = 0.0001
I0615 15:57:52.826009  5063 solver.cpp:213] Iteration 2160, loss = 3.57634
I0615 15:57:52.826035  5063 solver.cpp:228]     Train net output #0: softmax = 3.57634 (* 1 = 3.57634 loss)
I0615 15:57:52.826162  5063 solver.cpp:473] Iteration 2160, lr = 0.0001
I0615 15:57:53.582715  5063 solver.cpp:213] Iteration 2170, loss = 3.77327
I0615 15:57:53.582741  5063 solver.cpp:228]     Train net output #0: softmax = 3.77327 (* 1 = 3.77327 loss)
I0615 15:57:53.582746  5063 solver.cpp:473] Iteration 2170, lr = 0.0001
I0615 15:57:54.340625  5063 solver.cpp:213] Iteration 2180, loss = 4.02761
I0615 15:57:54.340646  5063 solver.cpp:228]     Train net output #0: softmax = 4.02761 (* 1 = 4.02761 loss)
I0615 15:57:54.340651  5063 solver.cpp:473] Iteration 2180, lr = 0.0001
I0615 15:57:55.098987  5063 solver.cpp:213] Iteration 2190, loss = 3.98104
I0615 15:57:55.099009  5063 solver.cpp:228]     Train net output #0: softmax = 3.98104 (* 1 = 3.98104 loss)
I0615 15:57:55.099014  5063 solver.cpp:473] Iteration 2190, lr = 0.0001
I0615 15:57:55.856997  5063 solver.cpp:213] Iteration 2200, loss = 3.75124
I0615 15:57:55.857019  5063 solver.cpp:228]     Train net output #0: softmax = 3.75124 (* 1 = 3.75124 loss)
I0615 15:57:55.857024  5063 solver.cpp:473] Iteration 2200, lr = 0.0001
I0615 15:57:56.615571  5063 solver.cpp:213] Iteration 2210, loss = 4.04087
I0615 15:57:56.615592  5063 solver.cpp:228]     Train net output #0: softmax = 4.04087 (* 1 = 4.04087 loss)
I0615 15:57:56.615597  5063 solver.cpp:473] Iteration 2210, lr = 0.0001
I0615 15:57:57.374517  5063 solver.cpp:213] Iteration 2220, loss = 3.89916
I0615 15:57:57.374539  5063 solver.cpp:228]     Train net output #0: softmax = 3.89916 (* 1 = 3.89916 loss)
I0615 15:57:57.374544  5063 solver.cpp:473] Iteration 2220, lr = 0.0001
I0615 15:57:58.133244  5063 solver.cpp:213] Iteration 2230, loss = 3.74283
I0615 15:57:58.133267  5063 solver.cpp:228]     Train net output #0: softmax = 3.74283 (* 1 = 3.74283 loss)
I0615 15:57:58.133384  5063 solver.cpp:473] Iteration 2230, lr = 0.0001
I0615 15:57:58.892365  5063 solver.cpp:213] Iteration 2240, loss = 3.78306
I0615 15:57:58.892390  5063 solver.cpp:228]     Train net output #0: softmax = 3.78306 (* 1 = 3.78306 loss)
I0615 15:57:58.892395  5063 solver.cpp:473] Iteration 2240, lr = 0.0001
I0615 15:57:59.651193  5063 solver.cpp:213] Iteration 2250, loss = 3.9228
I0615 15:57:59.651216  5063 solver.cpp:228]     Train net output #0: softmax = 3.9228 (* 1 = 3.9228 loss)
I0615 15:57:59.651221  5063 solver.cpp:473] Iteration 2250, lr = 0.0001
I0615 15:58:00.409680  5063 solver.cpp:213] Iteration 2260, loss = 4.25558
I0615 15:58:00.409701  5063 solver.cpp:228]     Train net output #0: softmax = 4.25558 (* 1 = 4.25558 loss)
I0615 15:58:00.409708  5063 solver.cpp:473] Iteration 2260, lr = 0.0001
I0615 15:58:01.167973  5063 solver.cpp:213] Iteration 2270, loss = 4.06059
I0615 15:58:01.167992  5063 solver.cpp:228]     Train net output #0: softmax = 4.06059 (* 1 = 4.06059 loss)
I0615 15:58:01.167997  5063 solver.cpp:473] Iteration 2270, lr = 0.0001
I0615 15:58:01.925858  5063 solver.cpp:213] Iteration 2280, loss = 4.06515
I0615 15:58:01.925880  5063 solver.cpp:228]     Train net output #0: softmax = 4.06515 (* 1 = 4.06515 loss)
I0615 15:58:01.925885  5063 solver.cpp:473] Iteration 2280, lr = 0.0001
I0615 15:58:02.683670  5063 solver.cpp:213] Iteration 2290, loss = 3.74802
I0615 15:58:02.683691  5063 solver.cpp:228]     Train net output #0: softmax = 3.74802 (* 1 = 3.74802 loss)
I0615 15:58:02.683696  5063 solver.cpp:473] Iteration 2290, lr = 0.0001
I0615 15:58:03.441334  5063 solver.cpp:213] Iteration 2300, loss = 4.05231
I0615 15:58:03.441372  5063 solver.cpp:228]     Train net output #0: softmax = 4.05231 (* 1 = 4.05231 loss)
I0615 15:58:03.441382  5063 solver.cpp:473] Iteration 2300, lr = 0.0001
I0615 15:58:04.199282  5063 solver.cpp:213] Iteration 2310, loss = 4.0335
I0615 15:58:04.199302  5063 solver.cpp:228]     Train net output #0: softmax = 4.0335 (* 1 = 4.0335 loss)
I0615 15:58:04.199306  5063 solver.cpp:473] Iteration 2310, lr = 0.0001
I0615 15:58:04.958093  5063 solver.cpp:213] Iteration 2320, loss = 3.90005
I0615 15:58:04.958113  5063 solver.cpp:228]     Train net output #0: softmax = 3.90005 (* 1 = 3.90005 loss)
I0615 15:58:04.958117  5063 solver.cpp:473] Iteration 2320, lr = 0.0001
I0615 15:58:05.716631  5063 solver.cpp:213] Iteration 2330, loss = 4.03184
I0615 15:58:05.716652  5063 solver.cpp:228]     Train net output #0: softmax = 4.03184 (* 1 = 4.03184 loss)
I0615 15:58:05.716657  5063 solver.cpp:473] Iteration 2330, lr = 0.0001
I0615 15:58:06.475563  5063 solver.cpp:213] Iteration 2340, loss = 3.94063
I0615 15:58:06.475587  5063 solver.cpp:228]     Train net output #0: softmax = 3.94063 (* 1 = 3.94063 loss)
I0615 15:58:06.475592  5063 solver.cpp:473] Iteration 2340, lr = 0.0001
I0615 15:58:07.234443  5063 solver.cpp:213] Iteration 2350, loss = 3.80389
I0615 15:58:07.234464  5063 solver.cpp:228]     Train net output #0: softmax = 3.80389 (* 1 = 3.80389 loss)
I0615 15:58:07.234469  5063 solver.cpp:473] Iteration 2350, lr = 0.0001
I0615 15:58:07.993204  5063 solver.cpp:213] Iteration 2360, loss = 3.78203
I0615 15:58:07.993224  5063 solver.cpp:228]     Train net output #0: softmax = 3.78203 (* 1 = 3.78203 loss)
I0615 15:58:07.993229  5063 solver.cpp:473] Iteration 2360, lr = 0.0001
I0615 15:58:08.750125  5063 solver.cpp:213] Iteration 2370, loss = 3.86877
I0615 15:58:08.750151  5063 solver.cpp:228]     Train net output #0: softmax = 3.86877 (* 1 = 3.86877 loss)
I0615 15:58:08.750284  5063 solver.cpp:473] Iteration 2370, lr = 0.0001
I0615 15:58:09.508725  5063 solver.cpp:213] Iteration 2380, loss = 3.9731
I0615 15:58:09.508744  5063 solver.cpp:228]     Train net output #0: softmax = 3.9731 (* 1 = 3.9731 loss)
I0615 15:58:09.508749  5063 solver.cpp:473] Iteration 2380, lr = 0.0001
I0615 15:58:10.267861  5063 solver.cpp:213] Iteration 2390, loss = 3.8352
I0615 15:58:10.267882  5063 solver.cpp:228]     Train net output #0: softmax = 3.8352 (* 1 = 3.8352 loss)
I0615 15:58:10.267887  5063 solver.cpp:473] Iteration 2390, lr = 0.0001
I0615 15:58:11.026087  5063 solver.cpp:213] Iteration 2400, loss = 3.91452
I0615 15:58:11.026125  5063 solver.cpp:228]     Train net output #0: softmax = 3.91452 (* 1 = 3.91452 loss)
I0615 15:58:11.026130  5063 solver.cpp:473] Iteration 2400, lr = 0.0001
I0615 15:58:11.784265  5063 solver.cpp:213] Iteration 2410, loss = 3.85281
I0615 15:58:11.784286  5063 solver.cpp:228]     Train net output #0: softmax = 3.85281 (* 1 = 3.85281 loss)
I0615 15:58:11.784291  5063 solver.cpp:473] Iteration 2410, lr = 0.0001
I0615 15:58:12.542820  5063 solver.cpp:213] Iteration 2420, loss = 3.80024
I0615 15:58:12.542840  5063 solver.cpp:228]     Train net output #0: softmax = 3.80024 (* 1 = 3.80024 loss)
I0615 15:58:12.542845  5063 solver.cpp:473] Iteration 2420, lr = 0.0001
I0615 15:58:13.301666  5063 solver.cpp:213] Iteration 2430, loss = 3.67858
I0615 15:58:13.301687  5063 solver.cpp:228]     Train net output #0: softmax = 3.67858 (* 1 = 3.67858 loss)
I0615 15:58:13.301692  5063 solver.cpp:473] Iteration 2430, lr = 0.0001
I0615 15:58:14.060281  5063 solver.cpp:213] Iteration 2440, loss = 3.92771
I0615 15:58:14.060304  5063 solver.cpp:228]     Train net output #0: softmax = 3.92771 (* 1 = 3.92771 loss)
I0615 15:58:14.060484  5063 solver.cpp:473] Iteration 2440, lr = 0.0001
I0615 15:58:14.819129  5063 solver.cpp:213] Iteration 2450, loss = 3.87074
I0615 15:58:14.819150  5063 solver.cpp:228]     Train net output #0: softmax = 3.87074 (* 1 = 3.87074 loss)
I0615 15:58:14.819154  5063 solver.cpp:473] Iteration 2450, lr = 0.0001
I0615 15:58:15.577455  5063 solver.cpp:213] Iteration 2460, loss = 3.96317
I0615 15:58:15.577476  5063 solver.cpp:228]     Train net output #0: softmax = 3.96317 (* 1 = 3.96317 loss)
I0615 15:58:15.577487  5063 solver.cpp:473] Iteration 2460, lr = 0.0001
I0615 15:58:16.335979  5063 solver.cpp:213] Iteration 2470, loss = 3.93838
I0615 15:58:16.335999  5063 solver.cpp:228]     Train net output #0: softmax = 3.93838 (* 1 = 3.93838 loss)
I0615 15:58:16.336004  5063 solver.cpp:473] Iteration 2470, lr = 0.0001
I0615 15:58:17.093695  5063 solver.cpp:213] Iteration 2480, loss = 3.79364
I0615 15:58:17.093716  5063 solver.cpp:228]     Train net output #0: softmax = 3.79364 (* 1 = 3.79364 loss)
I0615 15:58:17.093721  5063 solver.cpp:473] Iteration 2480, lr = 0.0001
I0615 15:58:17.852255  5063 solver.cpp:213] Iteration 2490, loss = 3.94563
I0615 15:58:17.852282  5063 solver.cpp:228]     Train net output #0: softmax = 3.94563 (* 1 = 3.94563 loss)
I0615 15:58:17.852285  5063 solver.cpp:473] Iteration 2490, lr = 0.0001
I0615 15:58:18.610126  5063 solver.cpp:213] Iteration 2500, loss = 3.9577
I0615 15:58:18.610148  5063 solver.cpp:228]     Train net output #0: softmax = 3.9577 (* 1 = 3.9577 loss)
I0615 15:58:18.610153  5063 solver.cpp:473] Iteration 2500, lr = 0.0001
I0615 15:58:19.368531  5063 solver.cpp:213] Iteration 2510, loss = 3.96444
I0615 15:58:19.368554  5063 solver.cpp:228]     Train net output #0: softmax = 3.96444 (* 1 = 3.96444 loss)
I0615 15:58:19.368680  5063 solver.cpp:473] Iteration 2510, lr = 0.0001
I0615 15:58:20.127277  5063 solver.cpp:213] Iteration 2520, loss = 3.90477
I0615 15:58:20.127297  5063 solver.cpp:228]     Train net output #0: softmax = 3.90477 (* 1 = 3.90477 loss)
I0615 15:58:20.127301  5063 solver.cpp:473] Iteration 2520, lr = 0.0001
I0615 15:58:20.884970  5063 solver.cpp:213] Iteration 2530, loss = 3.8532
I0615 15:58:20.884992  5063 solver.cpp:228]     Train net output #0: softmax = 3.8532 (* 1 = 3.8532 loss)
I0615 15:58:20.884996  5063 solver.cpp:473] Iteration 2530, lr = 0.0001
I0615 15:58:21.643393  5063 solver.cpp:213] Iteration 2540, loss = 3.77637
I0615 15:58:21.643414  5063 solver.cpp:228]     Train net output #0: softmax = 3.77637 (* 1 = 3.77637 loss)
I0615 15:58:21.643419  5063 solver.cpp:473] Iteration 2540, lr = 0.0001
I0615 15:58:22.401563  5063 solver.cpp:213] Iteration 2550, loss = 3.83774
I0615 15:58:22.401588  5063 solver.cpp:228]     Train net output #0: softmax = 3.83774 (* 1 = 3.83774 loss)
I0615 15:58:22.401593  5063 solver.cpp:473] Iteration 2550, lr = 0.0001
I0615 15:58:23.160290  5063 solver.cpp:213] Iteration 2560, loss = 3.77959
I0615 15:58:23.160312  5063 solver.cpp:228]     Train net output #0: softmax = 3.77959 (* 1 = 3.77959 loss)
I0615 15:58:23.160336  5063 solver.cpp:473] Iteration 2560, lr = 0.0001
I0615 15:58:23.919003  5063 solver.cpp:213] Iteration 2570, loss = 3.97434
I0615 15:58:23.919023  5063 solver.cpp:228]     Train net output #0: softmax = 3.97434 (* 1 = 3.97434 loss)
I0615 15:58:23.919028  5063 solver.cpp:473] Iteration 2570, lr = 0.0001
I0615 15:58:24.676553  5063 solver.cpp:213] Iteration 2580, loss = 3.8663
I0615 15:58:24.676576  5063 solver.cpp:228]     Train net output #0: softmax = 3.8663 (* 1 = 3.8663 loss)
I0615 15:58:24.676707  5063 solver.cpp:473] Iteration 2580, lr = 0.0001
I0615 15:58:25.434862  5063 solver.cpp:213] Iteration 2590, loss = 3.74733
I0615 15:58:25.434882  5063 solver.cpp:228]     Train net output #0: softmax = 3.74733 (* 1 = 3.74733 loss)
I0615 15:58:25.434887  5063 solver.cpp:473] Iteration 2590, lr = 0.0001
I0615 15:58:26.191246  5063 solver.cpp:213] Iteration 2600, loss = 3.93066
I0615 15:58:26.191269  5063 solver.cpp:228]     Train net output #0: softmax = 3.93066 (* 1 = 3.93066 loss)
I0615 15:58:26.191275  5063 solver.cpp:473] Iteration 2600, lr = 0.0001
I0615 15:58:26.948606  5063 solver.cpp:213] Iteration 2610, loss = 3.83575
I0615 15:58:26.948627  5063 solver.cpp:228]     Train net output #0: softmax = 3.83575 (* 1 = 3.83575 loss)
I0615 15:58:26.948632  5063 solver.cpp:473] Iteration 2610, lr = 0.0001
I0615 15:58:27.707103  5063 solver.cpp:213] Iteration 2620, loss = 3.72238
I0615 15:58:27.707123  5063 solver.cpp:228]     Train net output #0: softmax = 3.72238 (* 1 = 3.72238 loss)
I0615 15:58:27.707134  5063 solver.cpp:473] Iteration 2620, lr = 0.0001
I0615 15:58:28.465203  5063 solver.cpp:213] Iteration 2630, loss = 3.87131
I0615 15:58:28.465224  5063 solver.cpp:228]     Train net output #0: softmax = 3.87131 (* 1 = 3.87131 loss)
I0615 15:58:28.465229  5063 solver.cpp:473] Iteration 2630, lr = 0.0001
I0615 15:58:29.223278  5063 solver.cpp:213] Iteration 2640, loss = 3.98644
I0615 15:58:29.223299  5063 solver.cpp:228]     Train net output #0: softmax = 3.98644 (* 1 = 3.98644 loss)
I0615 15:58:29.223304  5063 solver.cpp:473] Iteration 2640, lr = 0.0001
I0615 15:58:29.981930  5063 solver.cpp:213] Iteration 2650, loss = 4.10914
I0615 15:58:29.981958  5063 solver.cpp:228]     Train net output #0: softmax = 4.10914 (* 1 = 4.10914 loss)
I0615 15:58:29.982079  5063 solver.cpp:473] Iteration 2650, lr = 0.0001
I0615 15:58:30.738343  5063 solver.cpp:213] Iteration 2660, loss = 4.19729
I0615 15:58:30.738363  5063 solver.cpp:228]     Train net output #0: softmax = 4.19729 (* 1 = 4.19729 loss)
I0615 15:58:30.738368  5063 solver.cpp:473] Iteration 2660, lr = 0.0001
I0615 15:58:31.497089  5063 solver.cpp:213] Iteration 2670, loss = 3.88791
I0615 15:58:31.497109  5063 solver.cpp:228]     Train net output #0: softmax = 3.88791 (* 1 = 3.88791 loss)
I0615 15:58:31.497113  5063 solver.cpp:473] Iteration 2670, lr = 0.0001
I0615 15:58:32.255692  5063 solver.cpp:213] Iteration 2680, loss = 3.86353
I0615 15:58:32.255713  5063 solver.cpp:228]     Train net output #0: softmax = 3.86353 (* 1 = 3.86353 loss)
I0615 15:58:32.255718  5063 solver.cpp:473] Iteration 2680, lr = 0.0001
I0615 15:58:33.014421  5063 solver.cpp:213] Iteration 2690, loss = 3.95919
I0615 15:58:33.014442  5063 solver.cpp:228]     Train net output #0: softmax = 3.95919 (* 1 = 3.95919 loss)
I0615 15:58:33.014447  5063 solver.cpp:473] Iteration 2690, lr = 0.0001
I0615 15:58:33.773334  5063 solver.cpp:213] Iteration 2700, loss = 3.93621
I0615 15:58:33.773353  5063 solver.cpp:228]     Train net output #0: softmax = 3.93621 (* 1 = 3.93621 loss)
I0615 15:58:33.773358  5063 solver.cpp:473] Iteration 2700, lr = 0.0001
I0615 15:58:34.531702  5063 solver.cpp:213] Iteration 2710, loss = 3.99314
I0615 15:58:34.531723  5063 solver.cpp:228]     Train net output #0: softmax = 3.99314 (* 1 = 3.99314 loss)
I0615 15:58:34.531726  5063 solver.cpp:473] Iteration 2710, lr = 0.0001
I0615 15:58:35.290369  5063 solver.cpp:213] Iteration 2720, loss = 3.96767
I0615 15:58:35.290395  5063 solver.cpp:228]     Train net output #0: softmax = 3.96767 (* 1 = 3.96767 loss)
I0615 15:58:35.290540  5063 solver.cpp:473] Iteration 2720, lr = 0.0001
I0615 15:58:36.048779  5063 solver.cpp:213] Iteration 2730, loss = 3.83462
I0615 15:58:36.048799  5063 solver.cpp:228]     Train net output #0: softmax = 3.83462 (* 1 = 3.83462 loss)
I0615 15:58:36.048804  5063 solver.cpp:473] Iteration 2730, lr = 0.0001
I0615 15:58:36.807129  5063 solver.cpp:213] Iteration 2740, loss = 3.85831
I0615 15:58:36.807152  5063 solver.cpp:228]     Train net output #0: softmax = 3.85831 (* 1 = 3.85831 loss)
I0615 15:58:36.807157  5063 solver.cpp:473] Iteration 2740, lr = 0.0001
I0615 15:58:37.564775  5063 solver.cpp:213] Iteration 2750, loss = 3.70904
I0615 15:58:37.564800  5063 solver.cpp:228]     Train net output #0: softmax = 3.70904 (* 1 = 3.70904 loss)
I0615 15:58:37.564805  5063 solver.cpp:473] Iteration 2750, lr = 0.0001
I0615 15:58:38.322306  5063 solver.cpp:213] Iteration 2760, loss = 3.76008
I0615 15:58:38.322329  5063 solver.cpp:228]     Train net output #0: softmax = 3.76008 (* 1 = 3.76008 loss)
I0615 15:58:38.322335  5063 solver.cpp:473] Iteration 2760, lr = 0.0001
I0615 15:58:39.079823  5063 solver.cpp:213] Iteration 2770, loss = 3.8647
I0615 15:58:39.079843  5063 solver.cpp:228]     Train net output #0: softmax = 3.8647 (* 1 = 3.8647 loss)
I0615 15:58:39.079848  5063 solver.cpp:473] Iteration 2770, lr = 0.0001
I0615 15:58:39.837537  5063 solver.cpp:213] Iteration 2780, loss = 3.71984
I0615 15:58:39.837558  5063 solver.cpp:228]     Train net output #0: softmax = 3.71984 (* 1 = 3.71984 loss)
I0615 15:58:39.837570  5063 solver.cpp:473] Iteration 2780, lr = 0.0001
I0615 15:58:40.596385  5063 solver.cpp:213] Iteration 2790, loss = 3.77369
I0615 15:58:40.596406  5063 solver.cpp:228]     Train net output #0: softmax = 3.77369 (* 1 = 3.77369 loss)
I0615 15:58:40.596411  5063 solver.cpp:473] Iteration 2790, lr = 0.0001
I0615 15:58:41.354516  5063 solver.cpp:213] Iteration 2800, loss = 3.7844
I0615 15:58:41.354564  5063 solver.cpp:228]     Train net output #0: softmax = 3.7844 (* 1 = 3.7844 loss)
I0615 15:58:41.354570  5063 solver.cpp:473] Iteration 2800, lr = 0.0001
I0615 15:58:42.112759  5063 solver.cpp:213] Iteration 2810, loss = 3.83116
I0615 15:58:42.112782  5063 solver.cpp:228]     Train net output #0: softmax = 3.83116 (* 1 = 3.83116 loss)
I0615 15:58:42.112787  5063 solver.cpp:473] Iteration 2810, lr = 0.0001
I0615 15:58:42.870872  5063 solver.cpp:213] Iteration 2820, loss = 4.01706
I0615 15:58:42.870893  5063 solver.cpp:228]     Train net output #0: softmax = 4.01706 (* 1 = 4.01706 loss)
I0615 15:58:42.870898  5063 solver.cpp:473] Iteration 2820, lr = 0.0001
I0615 15:58:43.629344  5063 solver.cpp:213] Iteration 2830, loss = 3.96261
I0615 15:58:43.629364  5063 solver.cpp:228]     Train net output #0: softmax = 3.96261 (* 1 = 3.96261 loss)
I0615 15:58:43.629369  5063 solver.cpp:473] Iteration 2830, lr = 0.0001
I0615 15:58:44.387511  5063 solver.cpp:213] Iteration 2840, loss = 3.63719
I0615 15:58:44.387532  5063 solver.cpp:228]     Train net output #0: softmax = 3.63719 (* 1 = 3.63719 loss)
I0615 15:58:44.387537  5063 solver.cpp:473] Iteration 2840, lr = 0.0001
I0615 15:58:45.145256  5063 solver.cpp:213] Iteration 2850, loss = 3.74474
I0615 15:58:45.145277  5063 solver.cpp:228]     Train net output #0: softmax = 3.74474 (* 1 = 3.74474 loss)
I0615 15:58:45.145282  5063 solver.cpp:473] Iteration 2850, lr = 0.0001
I0615 15:58:45.899438  5063 solver.cpp:213] Iteration 2860, loss = 3.90886
I0615 15:58:45.899463  5063 solver.cpp:228]     Train net output #0: softmax = 3.90886 (* 1 = 3.90886 loss)
I0615 15:58:45.899595  5063 solver.cpp:473] Iteration 2860, lr = 0.0001
I0615 15:58:46.654799  5063 solver.cpp:213] Iteration 2870, loss = 3.84304
I0615 15:58:46.654822  5063 solver.cpp:228]     Train net output #0: softmax = 3.84304 (* 1 = 3.84304 loss)
I0615 15:58:46.654826  5063 solver.cpp:473] Iteration 2870, lr = 0.0001
I0615 15:58:47.412753  5063 solver.cpp:213] Iteration 2880, loss = 3.85139
I0615 15:58:47.412776  5063 solver.cpp:228]     Train net output #0: softmax = 3.85139 (* 1 = 3.85139 loss)
I0615 15:58:47.412781  5063 solver.cpp:473] Iteration 2880, lr = 0.0001
I0615 15:58:48.171126  5063 solver.cpp:213] Iteration 2890, loss = 4.00151
I0615 15:58:48.171149  5063 solver.cpp:228]     Train net output #0: softmax = 4.00151 (* 1 = 4.00151 loss)
I0615 15:58:48.171154  5063 solver.cpp:473] Iteration 2890, lr = 0.0001
I0615 15:58:48.929693  5063 solver.cpp:213] Iteration 2900, loss = 3.7968
I0615 15:58:48.929713  5063 solver.cpp:228]     Train net output #0: softmax = 3.7968 (* 1 = 3.7968 loss)
I0615 15:58:48.929718  5063 solver.cpp:473] Iteration 2900, lr = 0.0001
I0615 15:58:49.688333  5063 solver.cpp:213] Iteration 2910, loss = 3.89125
I0615 15:58:49.688354  5063 solver.cpp:228]     Train net output #0: softmax = 3.89125 (* 1 = 3.89125 loss)
I0615 15:58:49.688357  5063 solver.cpp:473] Iteration 2910, lr = 0.0001
I0615 15:58:50.446929  5063 solver.cpp:213] Iteration 2920, loss = 3.75941
I0615 15:58:50.446950  5063 solver.cpp:228]     Train net output #0: softmax = 3.75941 (* 1 = 3.75941 loss)
I0615 15:58:50.446955  5063 solver.cpp:473] Iteration 2920, lr = 0.0001
I0615 15:58:51.205526  5063 solver.cpp:213] Iteration 2930, loss = 3.89187
I0615 15:58:51.205550  5063 solver.cpp:228]     Train net output #0: softmax = 3.89187 (* 1 = 3.89187 loss)
I0615 15:58:51.205684  5063 solver.cpp:473] Iteration 2930, lr = 0.0001
I0615 15:58:51.964339  5063 solver.cpp:213] Iteration 2940, loss = 3.94401
I0615 15:58:51.964359  5063 solver.cpp:228]     Train net output #0: softmax = 3.94401 (* 1 = 3.94401 loss)
I0615 15:58:51.964365  5063 solver.cpp:473] Iteration 2940, lr = 0.0001
I0615 15:58:52.722838  5063 solver.cpp:213] Iteration 2950, loss = 3.82181
I0615 15:58:52.722863  5063 solver.cpp:228]     Train net output #0: softmax = 3.82181 (* 1 = 3.82181 loss)
I0615 15:58:52.722868  5063 solver.cpp:473] Iteration 2950, lr = 0.0001
I0615 15:58:53.481263  5063 solver.cpp:213] Iteration 2960, loss = 3.98293
I0615 15:58:53.481286  5063 solver.cpp:228]     Train net output #0: softmax = 3.98293 (* 1 = 3.98293 loss)
I0615 15:58:53.481309  5063 solver.cpp:473] Iteration 2960, lr = 0.0001
I0615 15:58:54.238710  5063 solver.cpp:213] Iteration 2970, loss = 3.81542
I0615 15:58:54.238731  5063 solver.cpp:228]     Train net output #0: softmax = 3.81542 (* 1 = 3.81542 loss)
I0615 15:58:54.238736  5063 solver.cpp:473] Iteration 2970, lr = 0.0001
I0615 15:58:54.997405  5063 solver.cpp:213] Iteration 2980, loss = 3.82637
I0615 15:58:54.997426  5063 solver.cpp:228]     Train net output #0: softmax = 3.82637 (* 1 = 3.82637 loss)
I0615 15:58:54.997431  5063 solver.cpp:473] Iteration 2980, lr = 0.0001
I0615 15:58:55.756197  5063 solver.cpp:213] Iteration 2990, loss = 3.78849
I0615 15:58:55.756217  5063 solver.cpp:228]     Train net output #0: softmax = 3.78849 (* 1 = 3.78849 loss)
I0615 15:58:55.756222  5063 solver.cpp:473] Iteration 2990, lr = 0.0001
I0615 15:58:56.461325  5063 solver.cpp:362] Snapshotting to snapshots/16-06-15_15h49m18s_0_11_pretrainClassification_iter_3000.caffemodel
I0615 15:58:56.462020  5063 solver.cpp:370] Snapshotting solver state to snapshots/16-06-15_15h49m18s_0_11_pretrainClassification_iter_3000.solverstate
I0615 15:58:56.462402  5063 solver.cpp:291] Iteration 3000, Testing net (#0)
I0615 15:58:56.556874  5063 solver.cpp:342]     Test net output #0: accuracy_top_1 = 0.1375
I0615 15:58:56.556890  5063 solver.cpp:342]     Test net output #1: accuracy_top_5 = 0.3625
I0615 15:58:56.556896  5063 solver.cpp:342]     Test net output #2: softmax = 3.6817 (* 1 = 3.6817 loss)
I0615 15:58:56.610527  5063 solver.cpp:213] Iteration 3000, loss = 3.86203
I0615 15:58:56.610539  5063 solver.cpp:228]     Train net output #0: softmax = 3.86203 (* 1 = 3.86203 loss)
I0615 15:58:56.610544  5063 solver.cpp:473] Iteration 3000, lr = 0.0001
I0615 15:58:57.368801  5063 solver.cpp:213] Iteration 3010, loss = 3.68036
I0615 15:58:57.368823  5063 solver.cpp:228]     Train net output #0: softmax = 3.68036 (* 1 = 3.68036 loss)
I0615 15:58:57.368826  5063 solver.cpp:473] Iteration 3010, lr = 0.0001
I0615 15:58:58.127296  5063 solver.cpp:213] Iteration 3020, loss = 3.80733
I0615 15:58:58.127321  5063 solver.cpp:228]     Train net output #0: softmax = 3.80733 (* 1 = 3.80733 loss)
I0615 15:58:58.127326  5063 solver.cpp:473] Iteration 3020, lr = 0.0001
I0615 15:58:58.884675  5063 solver.cpp:213] Iteration 3030, loss = 3.84006
I0615 15:58:58.884697  5063 solver.cpp:228]     Train net output #0: softmax = 3.84006 (* 1 = 3.84006 loss)
I0615 15:58:58.884702  5063 solver.cpp:473] Iteration 3030, lr = 0.0001
I0615 15:58:59.643368  5063 solver.cpp:213] Iteration 3040, loss = 3.84742
I0615 15:58:59.643388  5063 solver.cpp:228]     Train net output #0: softmax = 3.84742 (* 1 = 3.84742 loss)
I0615 15:58:59.643393  5063 solver.cpp:473] Iteration 3040, lr = 0.0001
I0615 15:59:00.401594  5063 solver.cpp:213] Iteration 3050, loss = 3.96309
I0615 15:59:00.401613  5063 solver.cpp:228]     Train net output #0: softmax = 3.96309 (* 1 = 3.96309 loss)
I0615 15:59:00.401618  5063 solver.cpp:473] Iteration 3050, lr = 0.0001
I0615 15:59:01.160627  5063 solver.cpp:213] Iteration 3060, loss = 3.79492
I0615 15:59:01.160647  5063 solver.cpp:228]     Train net output #0: softmax = 3.79492 (* 1 = 3.79492 loss)
I0615 15:59:01.160652  5063 solver.cpp:473] Iteration 3060, lr = 0.0001
I0615 15:59:01.919296  5063 solver.cpp:213] Iteration 3070, loss = 3.73969
I0615 15:59:01.919319  5063 solver.cpp:228]     Train net output #0: softmax = 3.73969 (* 1 = 3.73969 loss)
I0615 15:59:01.919528  5063 solver.cpp:473] Iteration 3070, lr = 0.0001
I0615 15:59:02.678270  5063 solver.cpp:213] Iteration 3080, loss = 3.77784
I0615 15:59:02.678298  5063 solver.cpp:228]     Train net output #0: softmax = 3.77784 (* 1 = 3.77784 loss)
I0615 15:59:02.678303  5063 solver.cpp:473] Iteration 3080, lr = 0.0001
I0615 15:59:03.437024  5063 solver.cpp:213] Iteration 3090, loss = 3.85953
I0615 15:59:03.437046  5063 solver.cpp:228]     Train net output #0: softmax = 3.85953 (* 1 = 3.85953 loss)
I0615 15:59:03.437050  5063 solver.cpp:473] Iteration 3090, lr = 0.0001
I0615 15:59:04.195226  5063 solver.cpp:213] Iteration 3100, loss = 3.87206
I0615 15:59:04.195246  5063 solver.cpp:228]     Train net output #0: softmax = 3.87206 (* 1 = 3.87206 loss)
I0615 15:59:04.195251  5063 solver.cpp:473] Iteration 3100, lr = 0.0001
I0615 15:59:04.953656  5063 solver.cpp:213] Iteration 3110, loss = 3.84271
I0615 15:59:04.953676  5063 solver.cpp:228]     Train net output #0: softmax = 3.84271 (* 1 = 3.84271 loss)
I0615 15:59:04.953681  5063 solver.cpp:473] Iteration 3110, lr = 0.0001
I0615 15:59:05.711959  5063 solver.cpp:213] Iteration 3120, loss = 3.89222
I0615 15:59:05.711978  5063 solver.cpp:228]     Train net output #0: softmax = 3.89222 (* 1 = 3.89222 loss)
I0615 15:59:05.711983  5063 solver.cpp:473] Iteration 3120, lr = 0.0001
I0615 15:59:06.471215  5063 solver.cpp:213] Iteration 3130, loss = 3.99911
I0615 15:59:06.471235  5063 solver.cpp:228]     Train net output #0: softmax = 3.99911 (* 1 = 3.99911 loss)
I0615 15:59:06.471240  5063 solver.cpp:473] Iteration 3130, lr = 0.0001
I0615 15:59:07.229753  5063 solver.cpp:213] Iteration 3140, loss = 3.89889
I0615 15:59:07.229778  5063 solver.cpp:228]     Train net output #0: softmax = 3.89889 (* 1 = 3.89889 loss)
I0615 15:59:07.229972  5063 solver.cpp:473] Iteration 3140, lr = 0.0001
I0615 15:59:07.986011  5063 solver.cpp:213] Iteration 3150, loss = 3.77305
I0615 15:59:07.986030  5063 solver.cpp:228]     Train net output #0: softmax = 3.77305 (* 1 = 3.77305 loss)
I0615 15:59:07.986035  5063 solver.cpp:473] Iteration 3150, lr = 0.0001
I0615 15:59:08.743417  5063 solver.cpp:213] Iteration 3160, loss = 4.00061
I0615 15:59:08.743437  5063 solver.cpp:228]     Train net output #0: softmax = 4.00061 (* 1 = 4.00061 loss)
I0615 15:59:08.743441  5063 solver.cpp:473] Iteration 3160, lr = 0.0001
I0615 15:59:09.502346  5063 solver.cpp:213] Iteration 3170, loss = 3.86058
I0615 15:59:09.502365  5063 solver.cpp:228]     Train net output #0: softmax = 3.86058 (* 1 = 3.86058 loss)
I0615 15:59:09.502370  5063 solver.cpp:473] Iteration 3170, lr = 0.0001
I0615 15:59:10.260949  5063 solver.cpp:213] Iteration 3180, loss = 3.76005
I0615 15:59:10.260970  5063 solver.cpp:228]     Train net output #0: softmax = 3.76005 (* 1 = 3.76005 loss)
I0615 15:59:10.260975  5063 solver.cpp:473] Iteration 3180, lr = 0.0001
I0615 15:59:11.019773  5063 solver.cpp:213] Iteration 3190, loss = 3.81983
I0615 15:59:11.019794  5063 solver.cpp:228]     Train net output #0: softmax = 3.81983 (* 1 = 3.81983 loss)
I0615 15:59:11.019799  5063 solver.cpp:473] Iteration 3190, lr = 0.0001
I0615 15:59:11.778584  5063 solver.cpp:213] Iteration 3200, loss = 3.86488
I0615 15:59:11.778626  5063 solver.cpp:228]     Train net output #0: softmax = 3.86488 (* 1 = 3.86488 loss)
I0615 15:59:11.778632  5063 solver.cpp:473] Iteration 3200, lr = 0.0001
I0615 15:59:12.537336  5063 solver.cpp:213] Iteration 3210, loss = 3.83266
I0615 15:59:12.537358  5063 solver.cpp:228]     Train net output #0: softmax = 3.83266 (* 1 = 3.83266 loss)
I0615 15:59:12.537364  5063 solver.cpp:473] Iteration 3210, lr = 0.0001
I0615 15:59:13.296525  5063 solver.cpp:213] Iteration 3220, loss = 3.89995
I0615 15:59:13.296545  5063 solver.cpp:228]     Train net output #0: softmax = 3.89995 (* 1 = 3.89995 loss)
I0615 15:59:13.296550  5063 solver.cpp:473] Iteration 3220, lr = 0.0001
I0615 15:59:14.054177  5063 solver.cpp:213] Iteration 3230, loss = 3.8433
I0615 15:59:14.054198  5063 solver.cpp:228]     Train net output #0: softmax = 3.8433 (* 1 = 3.8433 loss)
I0615 15:59:14.054203  5063 solver.cpp:473] Iteration 3230, lr = 0.0001
I0615 15:59:14.812448  5063 solver.cpp:213] Iteration 3240, loss = 3.78869
I0615 15:59:14.812469  5063 solver.cpp:228]     Train net output #0: softmax = 3.78869 (* 1 = 3.78869 loss)
I0615 15:59:14.812481  5063 solver.cpp:473] Iteration 3240, lr = 0.0001
I0615 15:59:15.571444  5063 solver.cpp:213] Iteration 3250, loss = 3.9432
I0615 15:59:15.571465  5063 solver.cpp:228]     Train net output #0: softmax = 3.9432 (* 1 = 3.9432 loss)
I0615 15:59:15.571470  5063 solver.cpp:473] Iteration 3250, lr = 0.0001
I0615 15:59:16.329671  5063 solver.cpp:213] Iteration 3260, loss = 3.89527
I0615 15:59:16.329694  5063 solver.cpp:228]     Train net output #0: softmax = 3.89527 (* 1 = 3.89527 loss)
I0615 15:59:16.329697  5063 solver.cpp:473] Iteration 3260, lr = 0.0001
I0615 15:59:17.088408  5063 solver.cpp:213] Iteration 3270, loss = 3.83304
I0615 15:59:17.088435  5063 solver.cpp:228]     Train net output #0: softmax = 3.83304 (* 1 = 3.83304 loss)
I0615 15:59:17.088440  5063 solver.cpp:473] Iteration 3270, lr = 0.0001
I0615 15:59:17.846993  5063 solver.cpp:213] Iteration 3280, loss = 3.7706
I0615 15:59:17.847018  5063 solver.cpp:228]     Train net output #0: softmax = 3.7706 (* 1 = 3.7706 loss)
I0615 15:59:17.847172  5063 solver.cpp:473] Iteration 3280, lr = 0.0001
I0615 15:59:18.603652  5063 solver.cpp:213] Iteration 3290, loss = 3.66185
I0615 15:59:18.603672  5063 solver.cpp:228]     Train net output #0: softmax = 3.66185 (* 1 = 3.66185 loss)
I0615 15:59:18.603677  5063 solver.cpp:473] Iteration 3290, lr = 0.0001
I0615 15:59:19.362356  5063 solver.cpp:213] Iteration 3300, loss = 3.84412
I0615 15:59:19.362377  5063 solver.cpp:228]     Train net output #0: softmax = 3.84412 (* 1 = 3.84412 loss)
I0615 15:59:19.362382  5063 solver.cpp:473] Iteration 3300, lr = 0.0001
I0615 15:59:20.120764  5063 solver.cpp:213] Iteration 3310, loss = 3.76542
I0615 15:59:20.120784  5063 solver.cpp:228]     Train net output #0: softmax = 3.76542 (* 1 = 3.76542 loss)
I0615 15:59:20.120790  5063 solver.cpp:473] Iteration 3310, lr = 0.0001
I0615 15:59:20.879514  5063 solver.cpp:213] Iteration 3320, loss = 3.82979
I0615 15:59:20.879535  5063 solver.cpp:228]     Train net output #0: softmax = 3.82979 (* 1 = 3.82979 loss)
I0615 15:59:20.879540  5063 solver.cpp:473] Iteration 3320, lr = 0.0001
I0615 15:59:21.638345  5063 solver.cpp:213] Iteration 3330, loss = 3.76405
I0615 15:59:21.638368  5063 solver.cpp:228]     Train net output #0: softmax = 3.76405 (* 1 = 3.76405 loss)
I0615 15:59:21.638373  5063 solver.cpp:473] Iteration 3330, lr = 0.0001
I0615 15:59:22.395843  5063 solver.cpp:213] Iteration 3340, loss = 3.7529
I0615 15:59:22.395865  5063 solver.cpp:228]     Train net output #0: softmax = 3.7529 (* 1 = 3.7529 loss)
I0615 15:59:22.395870  5063 solver.cpp:473] Iteration 3340, lr = 0.0001
I0615 15:59:23.154425  5063 solver.cpp:213] Iteration 3350, loss = 3.67578
I0615 15:59:23.154449  5063 solver.cpp:228]     Train net output #0: softmax = 3.67578 (* 1 = 3.67578 loss)
I0615 15:59:23.154595  5063 solver.cpp:473] Iteration 3350, lr = 0.0001
I0615 15:59:23.913265  5063 solver.cpp:213] Iteration 3360, loss = 3.74155
I0615 15:59:23.913285  5063 solver.cpp:228]     Train net output #0: softmax = 3.74155 (* 1 = 3.74155 loss)
I0615 15:59:23.913306  5063 solver.cpp:473] Iteration 3360, lr = 0.0001
I0615 15:59:24.671869  5063 solver.cpp:213] Iteration 3370, loss = 3.74055
I0615 15:59:24.671888  5063 solver.cpp:228]     Train net output #0: softmax = 3.74055 (* 1 = 3.74055 loss)
I0615 15:59:24.671893  5063 solver.cpp:473] Iteration 3370, lr = 0.0001
I0615 15:59:25.430312  5063 solver.cpp:213] Iteration 3380, loss = 3.74254
I0615 15:59:25.430335  5063 solver.cpp:228]     Train net output #0: softmax = 3.74254 (* 1 = 3.74254 loss)
I0615 15:59:25.430340  5063 solver.cpp:473] Iteration 3380, lr = 0.0001
I0615 15:59:26.188675  5063 solver.cpp:213] Iteration 3390, loss = 3.62666
I0615 15:59:26.188697  5063 solver.cpp:228]     Train net output #0: softmax = 3.62666 (* 1 = 3.62666 loss)
I0615 15:59:26.188702  5063 solver.cpp:473] Iteration 3390, lr = 0.0001
I0615 15:59:26.947322  5063 solver.cpp:213] Iteration 3400, loss = 3.91938
I0615 15:59:26.947343  5063 solver.cpp:228]     Train net output #0: softmax = 3.91938 (* 1 = 3.91938 loss)
I0615 15:59:26.947355  5063 solver.cpp:473] Iteration 3400, lr = 0.0001
I0615 15:59:27.705883  5063 solver.cpp:213] Iteration 3410, loss = 3.76965
I0615 15:59:27.705914  5063 solver.cpp:228]     Train net output #0: softmax = 3.76965 (* 1 = 3.76965 loss)
I0615 15:59:27.705919  5063 solver.cpp:473] Iteration 3410, lr = 0.0001
I0615 15:59:28.464586  5063 solver.cpp:213] Iteration 3420, loss = 3.76083
I0615 15:59:28.464609  5063 solver.cpp:228]     Train net output #0: softmax = 3.76083 (* 1 = 3.76083 loss)
I0615 15:59:28.464614  5063 solver.cpp:473] Iteration 3420, lr = 0.0001
I0615 15:59:29.223863  5063 solver.cpp:213] Iteration 3430, loss = 3.68575
I0615 15:59:29.223884  5063 solver.cpp:228]     Train net output #0: softmax = 3.68575 (* 1 = 3.68575 loss)
I0615 15:59:29.223889  5063 solver.cpp:473] Iteration 3430, lr = 0.0001
I0615 15:59:29.982846  5063 solver.cpp:213] Iteration 3440, loss = 3.80096
I0615 15:59:29.982867  5063 solver.cpp:228]     Train net output #0: softmax = 3.80096 (* 1 = 3.80096 loss)
I0615 15:59:29.982872  5063 solver.cpp:473] Iteration 3440, lr = 0.0001
I0615 15:59:30.741497  5063 solver.cpp:213] Iteration 3450, loss = 3.78248
I0615 15:59:30.741516  5063 solver.cpp:228]     Train net output #0: softmax = 3.78248 (* 1 = 3.78248 loss)
I0615 15:59:30.741521  5063 solver.cpp:473] Iteration 3450, lr = 0.0001
I0615 15:59:31.500422  5063 solver.cpp:213] Iteration 3460, loss = 3.8517
I0615 15:59:31.500443  5063 solver.cpp:228]     Train net output #0: softmax = 3.8517 (* 1 = 3.8517 loss)
I0615 15:59:31.500447  5063 solver.cpp:473] Iteration 3460, lr = 0.0001
I0615 15:59:32.258872  5063 solver.cpp:213] Iteration 3470, loss = 3.78943
I0615 15:59:32.258893  5063 solver.cpp:228]     Train net output #0: softmax = 3.78943 (* 1 = 3.78943 loss)
I0615 15:59:32.258898  5063 solver.cpp:473] Iteration 3470, lr = 0.0001
I0615 15:59:33.017551  5063 solver.cpp:213] Iteration 3480, loss = 3.73151
I0615 15:59:33.017577  5063 solver.cpp:228]     Train net output #0: softmax = 3.73151 (* 1 = 3.73151 loss)
I0615 15:59:33.017582  5063 solver.cpp:473] Iteration 3480, lr = 0.0001
I0615 15:59:33.776198  5063 solver.cpp:213] Iteration 3490, loss = 3.76525
I0615 15:59:33.776221  5063 solver.cpp:228]     Train net output #0: softmax = 3.76525 (* 1 = 3.76525 loss)
I0615 15:59:33.776370  5063 solver.cpp:473] Iteration 3490, lr = 0.0001
I0615 15:59:34.533726  5063 solver.cpp:213] Iteration 3500, loss = 3.78467
I0615 15:59:34.533747  5063 solver.cpp:228]     Train net output #0: softmax = 3.78467 (* 1 = 3.78467 loss)
I0615 15:59:34.533751  5063 solver.cpp:473] Iteration 3500, lr = 0.0001
I0615 15:59:35.292764  5063 solver.cpp:213] Iteration 3510, loss = 3.65036
I0615 15:59:35.292785  5063 solver.cpp:228]     Train net output #0: softmax = 3.65036 (* 1 = 3.65036 loss)
I0615 15:59:35.292790  5063 solver.cpp:473] Iteration 3510, lr = 0.0001
I0615 15:59:36.050647  5063 solver.cpp:213] Iteration 3520, loss = 3.88472
I0615 15:59:36.050668  5063 solver.cpp:228]     Train net output #0: softmax = 3.88472 (* 1 = 3.88472 loss)
I0615 15:59:36.050689  5063 solver.cpp:473] Iteration 3520, lr = 0.0001
I0615 15:59:36.809208  5063 solver.cpp:213] Iteration 3530, loss = 3.91607
I0615 15:59:36.809229  5063 solver.cpp:228]     Train net output #0: softmax = 3.91607 (* 1 = 3.91607 loss)
I0615 15:59:36.809234  5063 solver.cpp:473] Iteration 3530, lr = 0.0001
I0615 15:59:37.567605  5063 solver.cpp:213] Iteration 3540, loss = 3.76646
I0615 15:59:37.567626  5063 solver.cpp:228]     Train net output #0: softmax = 3.76646 (* 1 = 3.76646 loss)
I0615 15:59:37.567631  5063 solver.cpp:473] Iteration 3540, lr = 0.0001
I0615 15:59:38.325320  5063 solver.cpp:213] Iteration 3550, loss = 3.80323
I0615 15:59:38.325341  5063 solver.cpp:228]     Train net output #0: softmax = 3.80323 (* 1 = 3.80323 loss)
I0615 15:59:38.325346  5063 solver.cpp:473] Iteration 3550, lr = 0.0001
I0615 15:59:39.084255  5063 solver.cpp:213] Iteration 3560, loss = 3.88283
I0615 15:59:39.084280  5063 solver.cpp:228]     Train net output #0: softmax = 3.88283 (* 1 = 3.88283 loss)
I0615 15:59:39.084419  5063 solver.cpp:473] Iteration 3560, lr = 0.0001
I0615 15:59:39.843303  5063 solver.cpp:213] Iteration 3570, loss = 3.72019
I0615 15:59:39.843323  5063 solver.cpp:228]     Train net output #0: softmax = 3.72019 (* 1 = 3.72019 loss)
I0615 15:59:39.843328  5063 solver.cpp:473] Iteration 3570, lr = 0.0001
I0615 15:59:40.601891  5063 solver.cpp:213] Iteration 3580, loss = 3.75012
I0615 15:59:40.601912  5063 solver.cpp:228]     Train net output #0: softmax = 3.75012 (* 1 = 3.75012 loss)
I0615 15:59:40.601917  5063 solver.cpp:473] Iteration 3580, lr = 0.0001
I0615 15:59:41.360245  5063 solver.cpp:213] Iteration 3590, loss = 3.71697
I0615 15:59:41.360265  5063 solver.cpp:228]     Train net output #0: softmax = 3.71697 (* 1 = 3.71697 loss)
I0615 15:59:41.360270  5063 solver.cpp:473] Iteration 3590, lr = 0.0001
I0615 15:59:42.118511  5063 solver.cpp:213] Iteration 3600, loss = 3.89964
I0615 15:59:42.118556  5063 solver.cpp:228]     Train net output #0: softmax = 3.89964 (* 1 = 3.89964 loss)
I0615 15:59:42.118562  5063 solver.cpp:473] Iteration 3600, lr = 0.0001
I0615 15:59:42.877125  5063 solver.cpp:213] Iteration 3610, loss = 3.84494
I0615 15:59:42.877145  5063 solver.cpp:228]     Train net output #0: softmax = 3.84494 (* 1 = 3.84494 loss)
I0615 15:59:42.877149  5063 solver.cpp:473] Iteration 3610, lr = 0.0001
I0615 15:59:43.636035  5063 solver.cpp:213] Iteration 3620, loss = 4.03523
I0615 15:59:43.636059  5063 solver.cpp:228]     Train net output #0: softmax = 4.03523 (* 1 = 4.03523 loss)
I0615 15:59:43.636065  5063 solver.cpp:473] Iteration 3620, lr = 0.0001
I0615 15:59:44.394497  5063 solver.cpp:213] Iteration 3630, loss = 3.93571
I0615 15:59:44.394520  5063 solver.cpp:228]     Train net output #0: softmax = 3.93571 (* 1 = 3.93571 loss)
I0615 15:59:44.394526  5063 solver.cpp:473] Iteration 3630, lr = 0.0001
I0615 15:59:45.152951  5063 solver.cpp:213] Iteration 3640, loss = 3.85636
I0615 15:59:45.152971  5063 solver.cpp:228]     Train net output #0: softmax = 3.85636 (* 1 = 3.85636 loss)
I0615 15:59:45.152976  5063 solver.cpp:473] Iteration 3640, lr = 0.0001
I0615 15:59:45.911540  5063 solver.cpp:213] Iteration 3650, loss = 4.04243
I0615 15:59:45.911561  5063 solver.cpp:228]     Train net output #0: softmax = 4.04243 (* 1 = 4.04243 loss)
I0615 15:59:45.911566  5063 solver.cpp:473] Iteration 3650, lr = 0.0001
I0615 15:59:46.670370  5063 solver.cpp:213] Iteration 3660, loss = 3.85255
I0615 15:59:46.670392  5063 solver.cpp:228]     Train net output #0: softmax = 3.85255 (* 1 = 3.85255 loss)
I0615 15:59:46.670397  5063 solver.cpp:473] Iteration 3660, lr = 0.0001
I0615 15:59:47.429208  5063 solver.cpp:213] Iteration 3670, loss = 3.69442
I0615 15:59:47.429229  5063 solver.cpp:228]     Train net output #0: softmax = 3.69442 (* 1 = 3.69442 loss)
I0615 15:59:47.429234  5063 solver.cpp:473] Iteration 3670, lr = 0.0001
I0615 15:59:48.187139  5063 solver.cpp:213] Iteration 3680, loss = 3.64398
I0615 15:59:48.187160  5063 solver.cpp:228]     Train net output #0: softmax = 3.64398 (* 1 = 3.64398 loss)
I0615 15:59:48.187165  5063 solver.cpp:473] Iteration 3680, lr = 0.0001
I0615 15:59:48.945322  5063 solver.cpp:213] Iteration 3690, loss = 3.90246
I0615 15:59:48.945344  5063 solver.cpp:228]     Train net output #0: softmax = 3.90246 (* 1 = 3.90246 loss)
I0615 15:59:48.945348  5063 solver.cpp:473] Iteration 3690, lr = 0.0001
I0615 15:59:49.704318  5063 solver.cpp:213] Iteration 3700, loss = 3.80089
I0615 15:59:49.704342  5063 solver.cpp:228]     Train net output #0: softmax = 3.80089 (* 1 = 3.80089 loss)
I0615 15:59:49.704466  5063 solver.cpp:473] Iteration 3700, lr = 0.0001
I0615 15:59:50.462837  5063 solver.cpp:213] Iteration 3710, loss = 3.72748
I0615 15:59:50.462857  5063 solver.cpp:228]     Train net output #0: softmax = 3.72748 (* 1 = 3.72748 loss)
I0615 15:59:50.462862  5063 solver.cpp:473] Iteration 3710, lr = 0.0001
I0615 15:59:51.221534  5063 solver.cpp:213] Iteration 3720, loss = 3.68472
I0615 15:59:51.221554  5063 solver.cpp:228]     Train net output #0: softmax = 3.68472 (* 1 = 3.68472 loss)
I0615 15:59:51.221566  5063 solver.cpp:473] Iteration 3720, lr = 0.0001
I0615 15:59:51.980028  5063 solver.cpp:213] Iteration 3730, loss = 3.60843
I0615 15:59:51.980051  5063 solver.cpp:228]     Train net output #0: softmax = 3.60843 (* 1 = 3.60843 loss)
I0615 15:59:51.980054  5063 solver.cpp:473] Iteration 3730, lr = 0.0001
I0615 15:59:52.738471  5063 solver.cpp:213] Iteration 3740, loss = 3.8289
I0615 15:59:52.738493  5063 solver.cpp:228]     Train net output #0: softmax = 3.8289 (* 1 = 3.8289 loss)
I0615 15:59:52.738498  5063 solver.cpp:473] Iteration 3740, lr = 0.0001
I0615 15:59:53.497051  5063 solver.cpp:213] Iteration 3750, loss = 3.82986
I0615 15:59:53.497076  5063 solver.cpp:228]     Train net output #0: softmax = 3.82986 (* 1 = 3.82986 loss)
I0615 15:59:53.497081  5063 solver.cpp:473] Iteration 3750, lr = 0.0001
I0615 15:59:54.254914  5063 solver.cpp:213] Iteration 3760, loss = 3.758
I0615 15:59:54.254935  5063 solver.cpp:228]     Train net output #0: softmax = 3.758 (* 1 = 3.758 loss)
I0615 15:59:54.254959  5063 solver.cpp:473] Iteration 3760, lr = 0.0001
I0615 15:59:55.013520  5063 solver.cpp:213] Iteration 3770, loss = 3.78782
I0615 15:59:55.013543  5063 solver.cpp:228]     Train net output #0: softmax = 3.78782 (* 1 = 3.78782 loss)
I0615 15:59:55.013677  5063 solver.cpp:473] Iteration 3770, lr = 0.0001
I0615 15:59:55.772475  5063 solver.cpp:213] Iteration 3780, loss = 3.84838
I0615 15:59:55.772495  5063 solver.cpp:228]     Train net output #0: softmax = 3.84838 (* 1 = 3.84838 loss)
I0615 15:59:55.772500  5063 solver.cpp:473] Iteration 3780, lr = 0.0001
I0615 15:59:56.531523  5063 solver.cpp:213] Iteration 3790, loss = 3.78475
I0615 15:59:56.531544  5063 solver.cpp:228]     Train net output #0: softmax = 3.78475 (* 1 = 3.78475 loss)
I0615 15:59:56.531548  5063 solver.cpp:473] Iteration 3790, lr = 0.0001
I0615 15:59:57.289964  5063 solver.cpp:213] Iteration 3800, loss = 3.87065
I0615 15:59:57.289986  5063 solver.cpp:228]     Train net output #0: softmax = 3.87065 (* 1 = 3.87065 loss)
I0615 15:59:57.289991  5063 solver.cpp:473] Iteration 3800, lr = 0.0001
I0615 15:59:58.048552  5063 solver.cpp:213] Iteration 3810, loss = 3.88054
I0615 15:59:58.048573  5063 solver.cpp:228]     Train net output #0: softmax = 3.88054 (* 1 = 3.88054 loss)
I0615 15:59:58.048578  5063 solver.cpp:473] Iteration 3810, lr = 0.0001
I0615 15:59:58.806874  5063 solver.cpp:213] Iteration 3820, loss = 3.63316
I0615 15:59:58.806895  5063 solver.cpp:228]     Train net output #0: softmax = 3.63316 (* 1 = 3.63316 loss)
I0615 15:59:58.806900  5063 solver.cpp:473] Iteration 3820, lr = 0.0001
I0615 15:59:59.565762  5063 solver.cpp:213] Iteration 3830, loss = 3.92327
I0615 15:59:59.565781  5063 solver.cpp:228]     Train net output #0: softmax = 3.92327 (* 1 = 3.92327 loss)
I0615 15:59:59.565786  5063 solver.cpp:473] Iteration 3830, lr = 0.0001
I0615 16:00:00.324190  5063 solver.cpp:213] Iteration 3840, loss = 3.80804
I0615 16:00:00.324214  5063 solver.cpp:228]     Train net output #0: softmax = 3.80804 (* 1 = 3.80804 loss)
I0615 16:00:00.324371  5063 solver.cpp:473] Iteration 3840, lr = 0.0001
I0615 16:00:01.083318  5063 solver.cpp:213] Iteration 3850, loss = 3.94248
I0615 16:00:01.083340  5063 solver.cpp:228]     Train net output #0: softmax = 3.94248 (* 1 = 3.94248 loss)
I0615 16:00:01.083345  5063 solver.cpp:473] Iteration 3850, lr = 0.0001
I0615 16:00:01.841553  5063 solver.cpp:213] Iteration 3860, loss = 3.76126
I0615 16:00:01.841575  5063 solver.cpp:228]     Train net output #0: softmax = 3.76126 (* 1 = 3.76126 loss)
I0615 16:00:01.841580  5063 solver.cpp:473] Iteration 3860, lr = 0.0001
I0615 16:00:02.600492  5063 solver.cpp:213] Iteration 3870, loss = 3.6697
I0615 16:00:02.600514  5063 solver.cpp:228]     Train net output #0: softmax = 3.6697 (* 1 = 3.6697 loss)
I0615 16:00:02.600519  5063 solver.cpp:473] Iteration 3870, lr = 0.0001
I0615 16:00:03.359298  5063 solver.cpp:213] Iteration 3880, loss = 3.68114
I0615 16:00:03.359318  5063 solver.cpp:228]     Train net output #0: softmax = 3.68114 (* 1 = 3.68114 loss)
I0615 16:00:03.359330  5063 solver.cpp:473] Iteration 3880, lr = 0.0001
I0615 16:00:04.118146  5063 solver.cpp:213] Iteration 3890, loss = 3.78635
I0615 16:00:04.118168  5063 solver.cpp:228]     Train net output #0: softmax = 3.78635 (* 1 = 3.78635 loss)
I0615 16:00:04.118173  5063 solver.cpp:473] Iteration 3890, lr = 0.0001
I0615 16:00:04.876181  5063 solver.cpp:213] Iteration 3900, loss = 3.66025
I0615 16:00:04.876202  5063 solver.cpp:228]     Train net output #0: softmax = 3.66025 (* 1 = 3.66025 loss)
I0615 16:00:04.876207  5063 solver.cpp:473] Iteration 3900, lr = 0.0001
I0615 16:00:05.634528  5063 solver.cpp:213] Iteration 3910, loss = 3.93485
I0615 16:00:05.634552  5063 solver.cpp:228]     Train net output #0: softmax = 3.93485 (* 1 = 3.93485 loss)
I0615 16:00:05.634557  5063 solver.cpp:473] Iteration 3910, lr = 0.0001
I0615 16:00:06.392514  5063 solver.cpp:213] Iteration 3920, loss = 3.6777
I0615 16:00:06.392534  5063 solver.cpp:228]     Train net output #0: softmax = 3.6777 (* 1 = 3.6777 loss)
I0615 16:00:06.392554  5063 solver.cpp:473] Iteration 3920, lr = 0.0001
I0615 16:00:07.151216  5063 solver.cpp:213] Iteration 3930, loss = 3.88982
I0615 16:00:07.151237  5063 solver.cpp:228]     Train net output #0: softmax = 3.88982 (* 1 = 3.88982 loss)
I0615 16:00:07.151242  5063 solver.cpp:473] Iteration 3930, lr = 0.0001
I0615 16:00:07.909247  5063 solver.cpp:213] Iteration 3940, loss = 3.86907
I0615 16:00:07.909267  5063 solver.cpp:228]     Train net output #0: softmax = 3.86907 (* 1 = 3.86907 loss)
I0615 16:00:07.909272  5063 solver.cpp:473] Iteration 3940, lr = 0.0001
I0615 16:00:08.667392  5063 solver.cpp:213] Iteration 3950, loss = 3.78867
I0615 16:00:08.667415  5063 solver.cpp:228]     Train net output #0: softmax = 3.78867 (* 1 = 3.78867 loss)
I0615 16:00:08.667420  5063 solver.cpp:473] Iteration 3950, lr = 0.0001
I0615 16:00:09.425827  5063 solver.cpp:213] Iteration 3960, loss = 3.85301
I0615 16:00:09.425848  5063 solver.cpp:228]     Train net output #0: softmax = 3.85301 (* 1 = 3.85301 loss)
I0615 16:00:09.425853  5063 solver.cpp:473] Iteration 3960, lr = 0.0001
I0615 16:00:10.183838  5063 solver.cpp:213] Iteration 3970, loss = 3.74868
I0615 16:00:10.183859  5063 solver.cpp:228]     Train net output #0: softmax = 3.74868 (* 1 = 3.74868 loss)
I0615 16:00:10.183864  5063 solver.cpp:473] Iteration 3970, lr = 0.0001
I0615 16:00:10.942646  5063 solver.cpp:213] Iteration 3980, loss = 3.92469
I0615 16:00:10.942672  5063 solver.cpp:228]     Train net output #0: softmax = 3.92469 (* 1 = 3.92469 loss)
I0615 16:00:10.942821  5063 solver.cpp:473] Iteration 3980, lr = 0.0001
I0615 16:00:11.701423  5063 solver.cpp:213] Iteration 3990, loss = 3.9167
I0615 16:00:11.701445  5063 solver.cpp:228]     Train net output #0: softmax = 3.9167 (* 1 = 3.9167 loss)
I0615 16:00:11.701449  5063 solver.cpp:473] Iteration 3990, lr = 0.0001
I0615 16:00:12.406723  5063 solver.cpp:362] Snapshotting to snapshots/16-06-15_15h49m18s_0_11_pretrainClassification_iter_4000.caffemodel
I0615 16:00:12.407423  5063 solver.cpp:370] Snapshotting solver state to snapshots/16-06-15_15h49m18s_0_11_pretrainClassification_iter_4000.solverstate
I0615 16:00:12.407800  5063 solver.cpp:291] Iteration 4000, Testing net (#0)
I0615 16:00:12.502184  5063 solver.cpp:342]     Test net output #0: accuracy_top_1 = 0.139062
I0615 16:00:12.502200  5063 solver.cpp:342]     Test net output #1: accuracy_top_5 = 0.340625
I0615 16:00:12.502207  5063 solver.cpp:342]     Test net output #2: softmax = 3.76241 (* 1 = 3.76241 loss)
I0615 16:00:12.555826  5063 solver.cpp:213] Iteration 4000, loss = 3.8243
I0615 16:00:12.555840  5063 solver.cpp:228]     Train net output #0: softmax = 3.8243 (* 1 = 3.8243 loss)
I0615 16:00:12.555846  5063 solver.cpp:473] Iteration 4000, lr = 0.0001
I0615 16:00:13.314692  5063 solver.cpp:213] Iteration 4010, loss = 3.99352
I0615 16:00:13.314713  5063 solver.cpp:228]     Train net output #0: softmax = 3.99352 (* 1 = 3.99352 loss)
I0615 16:00:13.314718  5063 solver.cpp:473] Iteration 4010, lr = 0.0001
I0615 16:00:14.073334  5063 solver.cpp:213] Iteration 4020, loss = 3.90713
I0615 16:00:14.073364  5063 solver.cpp:228]     Train net output #0: softmax = 3.90713 (* 1 = 3.90713 loss)
I0615 16:00:14.073369  5063 solver.cpp:473] Iteration 4020, lr = 0.0001
I0615 16:00:14.831817  5063 solver.cpp:213] Iteration 4030, loss = 3.75084
I0615 16:00:14.831838  5063 solver.cpp:228]     Train net output #0: softmax = 3.75084 (* 1 = 3.75084 loss)
I0615 16:00:14.831843  5063 solver.cpp:473] Iteration 4030, lr = 0.0001
I0615 16:00:15.590070  5063 solver.cpp:213] Iteration 4040, loss = 3.88337
I0615 16:00:15.590091  5063 solver.cpp:228]     Train net output #0: softmax = 3.88337 (* 1 = 3.88337 loss)
I0615 16:00:15.590096  5063 solver.cpp:473] Iteration 4040, lr = 0.0001
I0615 16:00:16.349156  5063 solver.cpp:213] Iteration 4050, loss = 3.70484
I0615 16:00:16.349180  5063 solver.cpp:228]     Train net output #0: softmax = 3.70484 (* 1 = 3.70484 loss)
I0615 16:00:16.349388  5063 solver.cpp:473] Iteration 4050, lr = 0.0001
I0615 16:00:17.107650  5063 solver.cpp:213] Iteration 4060, loss = 3.90834
I0615 16:00:17.107672  5063 solver.cpp:228]     Train net output #0: softmax = 3.90834 (* 1 = 3.90834 loss)
I0615 16:00:17.107677  5063 solver.cpp:473] Iteration 4060, lr = 0.0001
I0615 16:00:17.866462  5063 solver.cpp:213] Iteration 4070, loss = 3.61425
I0615 16:00:17.866482  5063 solver.cpp:228]     Train net output #0: softmax = 3.61425 (* 1 = 3.61425 loss)
I0615 16:00:17.866487  5063 solver.cpp:473] Iteration 4070, lr = 0.0001
I0615 16:00:18.624477  5063 solver.cpp:213] Iteration 4080, loss = 3.62447
I0615 16:00:18.624498  5063 solver.cpp:228]     Train net output #0: softmax = 3.62447 (* 1 = 3.62447 loss)
I0615 16:00:18.624503  5063 solver.cpp:473] Iteration 4080, lr = 0.0001
I0615 16:00:19.383227  5063 solver.cpp:213] Iteration 4090, loss = 3.9359
I0615 16:00:19.383247  5063 solver.cpp:228]     Train net output #0: softmax = 3.9359 (* 1 = 3.9359 loss)
I0615 16:00:19.383252  5063 solver.cpp:473] Iteration 4090, lr = 0.0001
I0615 16:00:20.141657  5063 solver.cpp:213] Iteration 4100, loss = 3.823
I0615 16:00:20.141679  5063 solver.cpp:228]     Train net output #0: softmax = 3.823 (* 1 = 3.823 loss)
I0615 16:00:20.141683  5063 solver.cpp:473] Iteration 4100, lr = 0.0001
I0615 16:00:20.899497  5063 solver.cpp:213] Iteration 4110, loss = 3.72916
I0615 16:00:20.899518  5063 solver.cpp:228]     Train net output #0: softmax = 3.72916 (* 1 = 3.72916 loss)
I0615 16:00:20.899523  5063 solver.cpp:473] Iteration 4110, lr = 0.0001
I0615 16:00:21.658193  5063 solver.cpp:213] Iteration 4120, loss = 3.78912
I0615 16:00:21.658215  5063 solver.cpp:228]     Train net output #0: softmax = 3.78912 (* 1 = 3.78912 loss)
I0615 16:00:21.658221  5063 solver.cpp:473] Iteration 4120, lr = 0.0001
I0615 16:00:22.415493  5063 solver.cpp:213] Iteration 4130, loss = 3.83471
I0615 16:00:22.415513  5063 solver.cpp:228]     Train net output #0: softmax = 3.83471 (* 1 = 3.83471 loss)
I0615 16:00:22.415518  5063 solver.cpp:473] Iteration 4130, lr = 0.0001
I0615 16:00:23.174315  5063 solver.cpp:213] Iteration 4140, loss = 3.99429
I0615 16:00:23.174352  5063 solver.cpp:228]     Train net output #0: softmax = 3.99429 (* 1 = 3.99429 loss)
I0615 16:00:23.174357  5063 solver.cpp:473] Iteration 4140, lr = 0.0001
I0615 16:00:23.932960  5063 solver.cpp:213] Iteration 4150, loss = 3.81765
I0615 16:00:23.932979  5063 solver.cpp:228]     Train net output #0: softmax = 3.81765 (* 1 = 3.81765 loss)
I0615 16:00:23.932984  5063 solver.cpp:473] Iteration 4150, lr = 0.0001
I0615 16:00:24.691684  5063 solver.cpp:213] Iteration 4160, loss = 3.91153
I0615 16:00:24.691704  5063 solver.cpp:228]     Train net output #0: softmax = 3.91153 (* 1 = 3.91153 loss)
I0615 16:00:24.691709  5063 solver.cpp:473] Iteration 4160, lr = 0.0001
I0615 16:00:25.450655  5063 solver.cpp:213] Iteration 4170, loss = 3.96861
I0615 16:00:25.450676  5063 solver.cpp:228]     Train net output #0: softmax = 3.96861 (* 1 = 3.96861 loss)
I0615 16:00:25.450681  5063 solver.cpp:473] Iteration 4170, lr = 0.0001
I0615 16:00:26.208060  5063 solver.cpp:213] Iteration 4180, loss = 3.94216
I0615 16:00:26.208089  5063 solver.cpp:228]     Train net output #0: softmax = 3.94216 (* 1 = 3.94216 loss)
I0615 16:00:26.208094  5063 solver.cpp:473] Iteration 4180, lr = 0.0001
I0615 16:00:26.966578  5063 solver.cpp:213] Iteration 4190, loss = 3.78996
I0615 16:00:26.966603  5063 solver.cpp:228]     Train net output #0: softmax = 3.78996 (* 1 = 3.78996 loss)
I0615 16:00:26.966732  5063 solver.cpp:473] Iteration 4190, lr = 0.0001
I0615 16:00:27.725150  5063 solver.cpp:213] Iteration 4200, loss = 3.88468
I0615 16:00:27.725169  5063 solver.cpp:228]     Train net output #0: softmax = 3.88468 (* 1 = 3.88468 loss)
I0615 16:00:27.725174  5063 solver.cpp:473] Iteration 4200, lr = 0.0001
I0615 16:00:28.483722  5063 solver.cpp:213] Iteration 4210, loss = 3.6506
I0615 16:00:28.483743  5063 solver.cpp:228]     Train net output #0: softmax = 3.6506 (* 1 = 3.6506 loss)
I0615 16:00:28.483748  5063 solver.cpp:473] Iteration 4210, lr = 0.0001
I0615 16:00:29.241991  5063 solver.cpp:213] Iteration 4220, loss = 3.87902
I0615 16:00:29.242012  5063 solver.cpp:228]     Train net output #0: softmax = 3.87902 (* 1 = 3.87902 loss)
I0615 16:00:29.242017  5063 solver.cpp:473] Iteration 4220, lr = 0.0001
I0615 16:00:30.001240  5063 solver.cpp:213] Iteration 4230, loss = 3.71099
I0615 16:00:30.001260  5063 solver.cpp:228]     Train net output #0: softmax = 3.71099 (* 1 = 3.71099 loss)
I0615 16:00:30.001263  5063 solver.cpp:473] Iteration 4230, lr = 0.0001
I0615 16:00:30.759930  5063 solver.cpp:213] Iteration 4240, loss = 3.76918
I0615 16:00:30.759951  5063 solver.cpp:228]     Train net output #0: softmax = 3.76918 (* 1 = 3.76918 loss)
I0615 16:00:30.759956  5063 solver.cpp:473] Iteration 4240, lr = 0.0001
I0615 16:00:31.518319  5063 solver.cpp:213] Iteration 4250, loss = 3.65648
I0615 16:00:31.518339  5063 solver.cpp:228]     Train net output #0: softmax = 3.65648 (* 1 = 3.65648 loss)
I0615 16:00:31.518344  5063 solver.cpp:473] Iteration 4250, lr = 0.0001
I0615 16:00:32.277242  5063 solver.cpp:213] Iteration 4260, loss = 3.76057
I0615 16:00:32.277266  5063 solver.cpp:228]     Train net output #0: softmax = 3.76057 (* 1 = 3.76057 loss)
I0615 16:00:32.277390  5063 solver.cpp:473] Iteration 4260, lr = 0.0001
I0615 16:00:33.035775  5063 solver.cpp:213] Iteration 4270, loss = 3.67841
I0615 16:00:33.035796  5063 solver.cpp:228]     Train net output #0: softmax = 3.67841 (* 1 = 3.67841 loss)
I0615 16:00:33.035801  5063 solver.cpp:473] Iteration 4270, lr = 0.0001
I0615 16:00:33.794404  5063 solver.cpp:213] Iteration 4280, loss = 3.81749
I0615 16:00:33.794425  5063 solver.cpp:228]     Train net output #0: softmax = 3.81749 (* 1 = 3.81749 loss)
I0615 16:00:33.794430  5063 solver.cpp:473] Iteration 4280, lr = 0.0001
I0615 16:00:34.552284  5063 solver.cpp:213] Iteration 4290, loss = 3.76575
I0615 16:00:34.552305  5063 solver.cpp:228]     Train net output #0: softmax = 3.76575 (* 1 = 3.76575 loss)
I0615 16:00:34.552310  5063 solver.cpp:473] Iteration 4290, lr = 0.0001
I0615 16:00:35.310995  5063 solver.cpp:213] Iteration 4300, loss = 3.74873
I0615 16:00:35.311033  5063 solver.cpp:228]     Train net output #0: softmax = 3.74873 (* 1 = 3.74873 loss)
I0615 16:00:35.311038  5063 solver.cpp:473] Iteration 4300, lr = 0.0001
I0615 16:00:36.069239  5063 solver.cpp:213] Iteration 4310, loss = 3.67498
I0615 16:00:36.069260  5063 solver.cpp:228]     Train net output #0: softmax = 3.67498 (* 1 = 3.67498 loss)
I0615 16:00:36.069265  5063 solver.cpp:473] Iteration 4310, lr = 0.0001
I0615 16:00:36.827785  5063 solver.cpp:213] Iteration 4320, loss = 3.93356
I0615 16:00:36.827805  5063 solver.cpp:228]     Train net output #0: softmax = 3.93356 (* 1 = 3.93356 loss)
I0615 16:00:36.827811  5063 solver.cpp:473] Iteration 4320, lr = 0.0001
I0615 16:00:37.586441  5063 solver.cpp:213] Iteration 4330, loss = 3.66702
I0615 16:00:37.586462  5063 solver.cpp:228]     Train net output #0: softmax = 3.66702 (* 1 = 3.66702 loss)
I0615 16:00:37.586467  5063 solver.cpp:473] Iteration 4330, lr = 0.0001
I0615 16:00:38.343075  5063 solver.cpp:213] Iteration 4340, loss = 3.70589
I0615 16:00:38.343096  5063 solver.cpp:228]     Train net output #0: softmax = 3.70589 (* 1 = 3.70589 loss)
I0615 16:00:38.343106  5063 solver.cpp:473] Iteration 4340, lr = 0.0001
I0615 16:00:39.101974  5063 solver.cpp:213] Iteration 4350, loss = 3.8169
I0615 16:00:39.101994  5063 solver.cpp:228]     Train net output #0: softmax = 3.8169 (* 1 = 3.8169 loss)
I0615 16:00:39.101999  5063 solver.cpp:473] Iteration 4350, lr = 0.0001
I0615 16:00:39.860604  5063 solver.cpp:213] Iteration 4360, loss = 3.75214
I0615 16:00:39.860625  5063 solver.cpp:228]     Train net output #0: softmax = 3.75214 (* 1 = 3.75214 loss)
I0615 16:00:39.860630  5063 solver.cpp:473] Iteration 4360, lr = 0.0001
I0615 16:00:40.619534  5063 solver.cpp:213] Iteration 4370, loss = 3.66233
I0615 16:00:40.619560  5063 solver.cpp:228]     Train net output #0: softmax = 3.66233 (* 1 = 3.66233 loss)
I0615 16:00:40.619565  5063 solver.cpp:473] Iteration 4370, lr = 0.0001
I0615 16:00:41.377948  5063 solver.cpp:213] Iteration 4380, loss = 3.86853
I0615 16:00:41.377969  5063 solver.cpp:228]     Train net output #0: softmax = 3.86853 (* 1 = 3.86853 loss)
I0615 16:00:41.377974  5063 solver.cpp:473] Iteration 4380, lr = 0.0001
I0615 16:00:42.136572  5063 solver.cpp:213] Iteration 4390, loss = 3.77479
I0615 16:00:42.136593  5063 solver.cpp:228]     Train net output #0: softmax = 3.77479 (* 1 = 3.77479 loss)
I0615 16:00:42.136598  5063 solver.cpp:473] Iteration 4390, lr = 0.0001
I0615 16:00:42.895265  5063 solver.cpp:213] Iteration 4400, loss = 3.86951
I0615 16:00:42.895447  5063 solver.cpp:228]     Train net output #0: softmax = 3.86951 (* 1 = 3.86951 loss)
I0615 16:00:42.895453  5063 solver.cpp:473] Iteration 4400, lr = 0.0001
I0615 16:00:43.654323  5063 solver.cpp:213] Iteration 4410, loss = 3.75578
I0615 16:00:43.654343  5063 solver.cpp:228]     Train net output #0: softmax = 3.75578 (* 1 = 3.75578 loss)
I0615 16:00:43.654348  5063 solver.cpp:473] Iteration 4410, lr = 0.0001
I0615 16:00:44.413416  5063 solver.cpp:213] Iteration 4420, loss = 3.73967
I0615 16:00:44.413436  5063 solver.cpp:228]     Train net output #0: softmax = 3.73967 (* 1 = 3.73967 loss)
I0615 16:00:44.413440  5063 solver.cpp:473] Iteration 4420, lr = 0.0001
I0615 16:00:45.172248  5063 solver.cpp:213] Iteration 4430, loss = 3.91388
I0615 16:00:45.172269  5063 solver.cpp:228]     Train net output #0: softmax = 3.91388 (* 1 = 3.91388 loss)
I0615 16:00:45.172273  5063 solver.cpp:473] Iteration 4430, lr = 0.0001
I0615 16:00:45.931210  5063 solver.cpp:213] Iteration 4440, loss = 3.82499
I0615 16:00:45.931232  5063 solver.cpp:228]     Train net output #0: softmax = 3.82499 (* 1 = 3.82499 loss)
I0615 16:00:45.931236  5063 solver.cpp:473] Iteration 4440, lr = 0.0001
I0615 16:00:46.689443  5063 solver.cpp:213] Iteration 4450, loss = 3.82329
I0615 16:00:46.689463  5063 solver.cpp:228]     Train net output #0: softmax = 3.82329 (* 1 = 3.82329 loss)
I0615 16:00:46.689468  5063 solver.cpp:473] Iteration 4450, lr = 0.0001
I0615 16:00:47.447774  5063 solver.cpp:213] Iteration 4460, loss = 3.59912
I0615 16:00:47.447793  5063 solver.cpp:228]     Train net output #0: softmax = 3.59912 (* 1 = 3.59912 loss)
I0615 16:00:47.447798  5063 solver.cpp:473] Iteration 4460, lr = 0.0001
I0615 16:00:48.206421  5063 solver.cpp:213] Iteration 4470, loss = 3.74546
I0615 16:00:48.206447  5063 solver.cpp:228]     Train net output #0: softmax = 3.74546 (* 1 = 3.74546 loss)
I0615 16:00:48.206591  5063 solver.cpp:473] Iteration 4470, lr = 0.0001
I0615 16:00:48.964545  5063 solver.cpp:213] Iteration 4480, loss = 3.73851
I0615 16:00:48.964567  5063 solver.cpp:228]     Train net output #0: softmax = 3.73851 (* 1 = 3.73851 loss)
I0615 16:00:48.964572  5063 solver.cpp:473] Iteration 4480, lr = 0.0001
I0615 16:00:49.723114  5063 solver.cpp:213] Iteration 4490, loss = 3.76415
I0615 16:00:49.723137  5063 solver.cpp:228]     Train net output #0: softmax = 3.76415 (* 1 = 3.76415 loss)
I0615 16:00:49.723142  5063 solver.cpp:473] Iteration 4490, lr = 0.0001
I0615 16:00:50.481215  5063 solver.cpp:213] Iteration 4500, loss = 3.69827
I0615 16:00:50.481236  5063 solver.cpp:228]     Train net output #0: softmax = 3.69827 (* 1 = 3.69827 loss)
I0615 16:00:50.481248  5063 solver.cpp:473] Iteration 4500, lr = 0.0001
I0615 16:00:51.239313  5063 solver.cpp:213] Iteration 4510, loss = 3.80597
I0615 16:00:51.239334  5063 solver.cpp:228]     Train net output #0: softmax = 3.80597 (* 1 = 3.80597 loss)
I0615 16:00:51.239338  5063 solver.cpp:473] Iteration 4510, lr = 0.0001
I0615 16:00:51.997370  5063 solver.cpp:213] Iteration 4520, loss = 3.67771
I0615 16:00:51.997390  5063 solver.cpp:228]     Train net output #0: softmax = 3.67771 (* 1 = 3.67771 loss)
I0615 16:00:51.997395  5063 solver.cpp:473] Iteration 4520, lr = 0.0001
I0615 16:00:52.755070  5063 solver.cpp:213] Iteration 4530, loss = 3.67798
I0615 16:00:52.755090  5063 solver.cpp:228]     Train net output #0: softmax = 3.67798 (* 1 = 3.67798 loss)
I0615 16:00:52.755095  5063 solver.cpp:473] Iteration 4530, lr = 0.0001
I0615 16:00:53.513746  5063 solver.cpp:213] Iteration 4540, loss = 3.95673
I0615 16:00:53.513769  5063 solver.cpp:228]     Train net output #0: softmax = 3.95673 (* 1 = 3.95673 loss)
I0615 16:00:53.513773  5063 solver.cpp:473] Iteration 4540, lr = 0.0001
I0615 16:00:54.272025  5063 solver.cpp:213] Iteration 4550, loss = 3.74944
I0615 16:00:54.272045  5063 solver.cpp:228]     Train net output #0: softmax = 3.74944 (* 1 = 3.74944 loss)
I0615 16:00:54.272050  5063 solver.cpp:473] Iteration 4550, lr = 0.0001
I0615 16:00:55.030486  5063 solver.cpp:213] Iteration 4560, loss = 3.8564
I0615 16:00:55.030513  5063 solver.cpp:228]     Train net output #0: softmax = 3.8564 (* 1 = 3.8564 loss)
I0615 16:00:55.030534  5063 solver.cpp:473] Iteration 4560, lr = 0.0001
I0615 16:00:55.788133  5063 solver.cpp:213] Iteration 4570, loss = 4.02755
I0615 16:00:55.788156  5063 solver.cpp:228]     Train net output #0: softmax = 4.02755 (* 1 = 4.02755 loss)
I0615 16:00:55.788161  5063 solver.cpp:473] Iteration 4570, lr = 0.0001
I0615 16:00:56.546663  5063 solver.cpp:213] Iteration 4580, loss = 3.70333
I0615 16:00:56.546682  5063 solver.cpp:228]     Train net output #0: softmax = 3.70333 (* 1 = 3.70333 loss)
I0615 16:00:56.546687  5063 solver.cpp:473] Iteration 4580, lr = 0.0001
I0615 16:00:57.304859  5063 solver.cpp:213] Iteration 4590, loss = 3.7598
I0615 16:00:57.304879  5063 solver.cpp:228]     Train net output #0: softmax = 3.7598 (* 1 = 3.7598 loss)
I0615 16:00:57.304883  5063 solver.cpp:473] Iteration 4590, lr = 0.0001
I0615 16:00:58.063040  5063 solver.cpp:213] Iteration 4600, loss = 3.89293
I0615 16:00:58.063062  5063 solver.cpp:228]     Train net output #0: softmax = 3.89293 (* 1 = 3.89293 loss)
I0615 16:00:58.063067  5063 solver.cpp:473] Iteration 4600, lr = 0.0001
I0615 16:00:58.819875  5063 solver.cpp:213] Iteration 4610, loss = 3.63556
I0615 16:00:58.819900  5063 solver.cpp:228]     Train net output #0: softmax = 3.63556 (* 1 = 3.63556 loss)
I0615 16:00:58.820045  5063 solver.cpp:473] Iteration 4610, lr = 0.0001
I0615 16:00:59.578481  5063 solver.cpp:213] Iteration 4620, loss = 3.64888
I0615 16:00:59.578500  5063 solver.cpp:228]     Train net output #0: softmax = 3.64888 (* 1 = 3.64888 loss)
I0615 16:00:59.578505  5063 solver.cpp:473] Iteration 4620, lr = 0.0001
I0615 16:01:00.336977  5063 solver.cpp:213] Iteration 4630, loss = 3.52497
I0615 16:01:00.336998  5063 solver.cpp:228]     Train net output #0: softmax = 3.52497 (* 1 = 3.52497 loss)
I0615 16:01:00.337003  5063 solver.cpp:473] Iteration 4630, lr = 0.0001
I0615 16:01:01.094208  5063 solver.cpp:213] Iteration 4640, loss = 3.80612
I0615 16:01:01.094233  5063 solver.cpp:228]     Train net output #0: softmax = 3.80612 (* 1 = 3.80612 loss)
I0615 16:01:01.094238  5063 solver.cpp:473] Iteration 4640, lr = 0.0001
I0615 16:01:01.853008  5063 solver.cpp:213] Iteration 4650, loss = 3.84455
I0615 16:01:01.853029  5063 solver.cpp:228]     Train net output #0: softmax = 3.84455 (* 1 = 3.84455 loss)
I0615 16:01:01.853034  5063 solver.cpp:473] Iteration 4650, lr = 0.0001
I0615 16:01:02.611223  5063 solver.cpp:213] Iteration 4660, loss = 3.77831
I0615 16:01:02.611243  5063 solver.cpp:228]     Train net output #0: softmax = 3.77831 (* 1 = 3.77831 loss)
I0615 16:01:02.611255  5063 solver.cpp:473] Iteration 4660, lr = 0.0001
I0615 16:01:03.369607  5063 solver.cpp:213] Iteration 4670, loss = 3.71719
I0615 16:01:03.369628  5063 solver.cpp:228]     Train net output #0: softmax = 3.71719 (* 1 = 3.71719 loss)
I0615 16:01:03.369633  5063 solver.cpp:473] Iteration 4670, lr = 0.0001
I0615 16:01:04.127882  5063 solver.cpp:213] Iteration 4680, loss = 3.86584
I0615 16:01:04.127907  5063 solver.cpp:228]     Train net output #0: softmax = 3.86584 (* 1 = 3.86584 loss)
I0615 16:01:04.128038  5063 solver.cpp:473] Iteration 4680, lr = 0.0001
I0615 16:01:04.886711  5063 solver.cpp:213] Iteration 4690, loss = 3.60378
I0615 16:01:04.886732  5063 solver.cpp:228]     Train net output #0: softmax = 3.60378 (* 1 = 3.60378 loss)
I0615 16:01:04.886737  5063 solver.cpp:473] Iteration 4690, lr = 0.0001
I0615 16:01:05.645086  5063 solver.cpp:213] Iteration 4700, loss = 3.54337
I0615 16:01:05.645104  5063 solver.cpp:228]     Train net output #0: softmax = 3.54337 (* 1 = 3.54337 loss)
I0615 16:01:05.645109  5063 solver.cpp:473] Iteration 4700, lr = 0.0001
I0615 16:01:06.402565  5063 solver.cpp:213] Iteration 4710, loss = 3.74665
I0615 16:01:06.402585  5063 solver.cpp:228]     Train net output #0: softmax = 3.74665 (* 1 = 3.74665 loss)
I0615 16:01:06.402590  5063 solver.cpp:473] Iteration 4710, lr = 0.0001
I0615 16:01:07.160816  5063 solver.cpp:213] Iteration 4720, loss = 3.67074
I0615 16:01:07.160836  5063 solver.cpp:228]     Train net output #0: softmax = 3.67074 (* 1 = 3.67074 loss)
I0615 16:01:07.160858  5063 solver.cpp:473] Iteration 4720, lr = 0.0001
I0615 16:01:07.918898  5063 solver.cpp:213] Iteration 4730, loss = 3.68265
I0615 16:01:07.918918  5063 solver.cpp:228]     Train net output #0: softmax = 3.68265 (* 1 = 3.68265 loss)
I0615 16:01:07.918923  5063 solver.cpp:473] Iteration 4730, lr = 0.0001
I0615 16:01:08.676127  5063 solver.cpp:213] Iteration 4740, loss = 3.77184
I0615 16:01:08.676151  5063 solver.cpp:228]     Train net output #0: softmax = 3.77184 (* 1 = 3.77184 loss)
I0615 16:01:08.676156  5063 solver.cpp:473] Iteration 4740, lr = 0.0001
I0615 16:01:09.434665  5063 solver.cpp:213] Iteration 4750, loss = 3.78174
I0615 16:01:09.434686  5063 solver.cpp:228]     Train net output #0: softmax = 3.78174 (* 1 = 3.78174 loss)
I0615 16:01:09.434691  5063 solver.cpp:473] Iteration 4750, lr = 0.0001
I0615 16:01:10.193346  5063 solver.cpp:213] Iteration 4760, loss = 3.62837
I0615 16:01:10.193364  5063 solver.cpp:228]     Train net output #0: softmax = 3.62837 (* 1 = 3.62837 loss)
I0615 16:01:10.193369  5063 solver.cpp:473] Iteration 4760, lr = 0.0001
I0615 16:01:10.951802  5063 solver.cpp:213] Iteration 4770, loss = 3.81781
I0615 16:01:10.951822  5063 solver.cpp:228]     Train net output #0: softmax = 3.81781 (* 1 = 3.81781 loss)
I0615 16:01:10.951828  5063 solver.cpp:473] Iteration 4770, lr = 0.0001
I0615 16:01:11.710355  5063 solver.cpp:213] Iteration 4780, loss = 3.69353
I0615 16:01:11.710377  5063 solver.cpp:228]     Train net output #0: softmax = 3.69353 (* 1 = 3.69353 loss)
I0615 16:01:11.710382  5063 solver.cpp:473] Iteration 4780, lr = 0.0001
I0615 16:01:12.468760  5063 solver.cpp:213] Iteration 4790, loss = 3.78652
I0615 16:01:12.468780  5063 solver.cpp:228]     Train net output #0: softmax = 3.78652 (* 1 = 3.78652 loss)
I0615 16:01:12.468786  5063 solver.cpp:473] Iteration 4790, lr = 0.0001
I0615 16:01:13.227071  5063 solver.cpp:213] Iteration 4800, loss = 3.80857
I0615 16:01:13.227113  5063 solver.cpp:228]     Train net output #0: softmax = 3.80857 (* 1 = 3.80857 loss)
I0615 16:01:13.227118  5063 solver.cpp:473] Iteration 4800, lr = 0.0001
I0615 16:01:13.984079  5063 solver.cpp:213] Iteration 4810, loss = 4.06563
I0615 16:01:13.984109  5063 solver.cpp:228]     Train net output #0: softmax = 4.06563 (* 1 = 4.06563 loss)
I0615 16:01:13.984114  5063 solver.cpp:473] Iteration 4810, lr = 0.0001
I0615 16:01:14.740591  5063 solver.cpp:213] Iteration 4820, loss = 3.83264
I0615 16:01:14.740613  5063 solver.cpp:228]     Train net output #0: softmax = 3.83264 (* 1 = 3.83264 loss)
I0615 16:01:14.740757  5063 solver.cpp:473] Iteration 4820, lr = 0.0001
I0615 16:01:15.499661  5063 solver.cpp:213] Iteration 4830, loss = 3.78422
I0615 16:01:15.499682  5063 solver.cpp:228]     Train net output #0: softmax = 3.78422 (* 1 = 3.78422 loss)
I0615 16:01:15.499687  5063 solver.cpp:473] Iteration 4830, lr = 0.0001
I0615 16:01:16.258430  5063 solver.cpp:213] Iteration 4840, loss = 3.90309
I0615 16:01:16.258451  5063 solver.cpp:228]     Train net output #0: softmax = 3.90309 (* 1 = 3.90309 loss)
I0615 16:01:16.258456  5063 solver.cpp:473] Iteration 4840, lr = 0.0001
I0615 16:01:17.017418  5063 solver.cpp:213] Iteration 4850, loss = 3.64263
I0615 16:01:17.017439  5063 solver.cpp:228]     Train net output #0: softmax = 3.64263 (* 1 = 3.64263 loss)
I0615 16:01:17.017444  5063 solver.cpp:473] Iteration 4850, lr = 0.0001
I0615 16:01:17.775951  5063 solver.cpp:213] Iteration 4860, loss = 3.68695
I0615 16:01:17.775974  5063 solver.cpp:228]     Train net output #0: softmax = 3.68695 (* 1 = 3.68695 loss)
I0615 16:01:17.775977  5063 solver.cpp:473] Iteration 4860, lr = 0.0001
I0615 16:01:18.532099  5063 solver.cpp:213] Iteration 4870, loss = 3.62463
I0615 16:01:18.532126  5063 solver.cpp:228]     Train net output #0: softmax = 3.62463 (* 1 = 3.62463 loss)
I0615 16:01:18.532136  5063 solver.cpp:473] Iteration 4870, lr = 0.0001
I0615 16:01:19.290467  5063 solver.cpp:213] Iteration 4880, loss = 3.75112
I0615 16:01:19.290488  5063 solver.cpp:228]     Train net output #0: softmax = 3.75112 (* 1 = 3.75112 loss)
I0615 16:01:19.290493  5063 solver.cpp:473] Iteration 4880, lr = 0.0001
I0615 16:01:20.048933  5063 solver.cpp:213] Iteration 4890, loss = 3.7079
I0615 16:01:20.048956  5063 solver.cpp:228]     Train net output #0: softmax = 3.7079 (* 1 = 3.7079 loss)
I0615 16:01:20.049091  5063 solver.cpp:473] Iteration 4890, lr = 0.0001
I0615 16:01:20.806114  5063 solver.cpp:213] Iteration 4900, loss = 3.62443
I0615 16:01:20.806135  5063 solver.cpp:228]     Train net output #0: softmax = 3.62443 (* 1 = 3.62443 loss)
I0615 16:01:20.806140  5063 solver.cpp:473] Iteration 4900, lr = 0.0001
I0615 16:01:21.564582  5063 solver.cpp:213] Iteration 4910, loss = 3.74872
I0615 16:01:21.564610  5063 solver.cpp:228]     Train net output #0: softmax = 3.74872 (* 1 = 3.74872 loss)
I0615 16:01:21.564615  5063 solver.cpp:473] Iteration 4910, lr = 0.0001
I0615 16:01:22.322635  5063 solver.cpp:213] Iteration 4920, loss = 3.5883
I0615 16:01:22.322655  5063 solver.cpp:228]     Train net output #0: softmax = 3.5883 (* 1 = 3.5883 loss)
I0615 16:01:22.322660  5063 solver.cpp:473] Iteration 4920, lr = 0.0001
I0615 16:01:23.080302  5063 solver.cpp:213] Iteration 4930, loss = 3.72965
I0615 16:01:23.080322  5063 solver.cpp:228]     Train net output #0: softmax = 3.72965 (* 1 = 3.72965 loss)
I0615 16:01:23.080327  5063 solver.cpp:473] Iteration 4930, lr = 0.0001
I0615 16:01:23.836709  5063 solver.cpp:213] Iteration 4940, loss = 3.86091
I0615 16:01:23.836730  5063 solver.cpp:228]     Train net output #0: softmax = 3.86091 (* 1 = 3.86091 loss)
I0615 16:01:23.836735  5063 solver.cpp:473] Iteration 4940, lr = 0.0001
I0615 16:01:24.594404  5063 solver.cpp:213] Iteration 4950, loss = 3.83091
I0615 16:01:24.594425  5063 solver.cpp:228]     Train net output #0: softmax = 3.83091 (* 1 = 3.83091 loss)
I0615 16:01:24.594430  5063 solver.cpp:473] Iteration 4950, lr = 0.0001
I0615 16:01:25.353369  5063 solver.cpp:213] Iteration 4960, loss = 3.93785
I0615 16:01:25.353394  5063 solver.cpp:228]     Train net output #0: softmax = 3.93785 (* 1 = 3.93785 loss)
I0615 16:01:25.353538  5063 solver.cpp:473] Iteration 4960, lr = 0.0001
I0615 16:01:26.112017  5063 solver.cpp:213] Iteration 4970, loss = 3.74938
I0615 16:01:26.112038  5063 solver.cpp:228]     Train net output #0: softmax = 3.74938 (* 1 = 3.74938 loss)
I0615 16:01:26.112043  5063 solver.cpp:473] Iteration 4970, lr = 0.0001
I0615 16:01:26.870585  5063 solver.cpp:213] Iteration 4980, loss = 3.83149
I0615 16:01:26.870610  5063 solver.cpp:228]     Train net output #0: softmax = 3.83149 (* 1 = 3.83149 loss)
I0615 16:01:26.870621  5063 solver.cpp:473] Iteration 4980, lr = 0.0001
I0615 16:01:27.629395  5063 solver.cpp:213] Iteration 4990, loss = 3.85511
I0615 16:01:27.629418  5063 solver.cpp:228]     Train net output #0: softmax = 3.85511 (* 1 = 3.85511 loss)
I0615 16:01:27.629422  5063 solver.cpp:473] Iteration 4990, lr = 0.0001
I0615 16:01:28.334795  5063 solver.cpp:362] Snapshotting to snapshots/16-06-15_15h49m18s_0_11_pretrainClassification_iter_5000.caffemodel
I0615 16:01:28.335500  5063 solver.cpp:370] Snapshotting solver state to snapshots/16-06-15_15h49m18s_0_11_pretrainClassification_iter_5000.solverstate
I0615 16:01:28.335877  5063 solver.cpp:291] Iteration 5000, Testing net (#0)
I0615 16:01:28.430713  5063 solver.cpp:342]     Test net output #0: accuracy_top_1 = 0.14375
I0615 16:01:28.430739  5063 solver.cpp:342]     Test net output #1: accuracy_top_5 = 0.375
I0615 16:01:28.430747  5063 solver.cpp:342]     Test net output #2: softmax = 3.71918 (* 1 = 3.71918 loss)
I0615 16:01:28.484148  5063 solver.cpp:213] Iteration 5000, loss = 3.74716
I0615 16:01:28.484163  5063 solver.cpp:228]     Train net output #0: softmax = 3.74716 (* 1 = 3.74716 loss)
I0615 16:01:28.484168  5063 solver.cpp:473] Iteration 5000, lr = 0.0001
I0615 16:01:29.240494  5063 solver.cpp:213] Iteration 5010, loss = 3.64998
I0615 16:01:29.240514  5063 solver.cpp:228]     Train net output #0: softmax = 3.64998 (* 1 = 3.64998 loss)
I0615 16:01:29.240519  5063 solver.cpp:473] Iteration 5010, lr = 0.0001
I0615 16:01:29.998184  5063 solver.cpp:213] Iteration 5020, loss = 3.68315
I0615 16:01:29.998205  5063 solver.cpp:228]     Train net output #0: softmax = 3.68315 (* 1 = 3.68315 loss)
I0615 16:01:29.998210  5063 solver.cpp:473] Iteration 5020, lr = 0.0001
I0615 16:01:30.756750  5063 solver.cpp:213] Iteration 5030, loss = 3.91641
I0615 16:01:30.756772  5063 solver.cpp:228]     Train net output #0: softmax = 3.91641 (* 1 = 3.91641 loss)
I0615 16:01:30.756777  5063 solver.cpp:473] Iteration 5030, lr = 0.0001
I0615 16:01:31.515856  5063 solver.cpp:213] Iteration 5040, loss = 3.81911
I0615 16:01:31.515877  5063 solver.cpp:228]     Train net output #0: softmax = 3.81911 (* 1 = 3.81911 loss)
I0615 16:01:31.515882  5063 solver.cpp:473] Iteration 5040, lr = 0.0001
I0615 16:01:32.275219  5063 solver.cpp:213] Iteration 5050, loss = 3.93163
I0615 16:01:32.275240  5063 solver.cpp:228]     Train net output #0: softmax = 3.93163 (* 1 = 3.93163 loss)
I0615 16:01:32.275245  5063 solver.cpp:473] Iteration 5050, lr = 0.0001
I0615 16:01:33.033197  5063 solver.cpp:213] Iteration 5060, loss = 3.87457
I0615 16:01:33.033218  5063 solver.cpp:228]     Train net output #0: softmax = 3.87457 (* 1 = 3.87457 loss)
I0615 16:01:33.033223  5063 solver.cpp:473] Iteration 5060, lr = 0.0001
I0615 16:01:33.792012  5063 solver.cpp:213] Iteration 5070, loss = 3.63722
I0615 16:01:33.792037  5063 solver.cpp:228]     Train net output #0: softmax = 3.63722 (* 1 = 3.63722 loss)
I0615 16:01:33.792042  5063 solver.cpp:473] Iteration 5070, lr = 0.0001
I0615 16:01:34.551496  5063 solver.cpp:213] Iteration 5080, loss = 3.69546
I0615 16:01:34.551517  5063 solver.cpp:228]     Train net output #0: softmax = 3.69546 (* 1 = 3.69546 loss)
I0615 16:01:34.551522  5063 solver.cpp:473] Iteration 5080, lr = 0.0001
I0615 16:01:35.310108  5063 solver.cpp:213] Iteration 5090, loss = 3.63422
I0615 16:01:35.310129  5063 solver.cpp:228]     Train net output #0: softmax = 3.63422 (* 1 = 3.63422 loss)
I0615 16:01:35.310134  5063 solver.cpp:473] Iteration 5090, lr = 0.0001
I0615 16:01:36.068841  5063 solver.cpp:213] Iteration 5100, loss = 3.60427
I0615 16:01:36.068866  5063 solver.cpp:228]     Train net output #0: softmax = 3.60427 (* 1 = 3.60427 loss)
I0615 16:01:36.068995  5063 solver.cpp:473] Iteration 5100, lr = 0.0001
I0615 16:01:36.826797  5063 solver.cpp:213] Iteration 5110, loss = 3.73313
I0615 16:01:36.826817  5063 solver.cpp:228]     Train net output #0: softmax = 3.73313 (* 1 = 3.73313 loss)
I0615 16:01:36.826822  5063 solver.cpp:473] Iteration 5110, lr = 0.0001
I0615 16:01:37.584949  5063 solver.cpp:213] Iteration 5120, loss = 3.73766
I0615 16:01:37.584975  5063 solver.cpp:228]     Train net output #0: softmax = 3.73766 (* 1 = 3.73766 loss)
I0615 16:01:37.584981  5063 solver.cpp:473] Iteration 5120, lr = 0.0001
I0615 16:01:38.342212  5063 solver.cpp:213] Iteration 5130, loss = 3.83415
I0615 16:01:38.342233  5063 solver.cpp:228]     Train net output #0: softmax = 3.83415 (* 1 = 3.83415 loss)
I0615 16:01:38.342238  5063 solver.cpp:473] Iteration 5130, lr = 0.0001
I0615 16:01:39.099900  5063 solver.cpp:213] Iteration 5140, loss = 3.62665
I0615 16:01:39.099920  5063 solver.cpp:228]     Train net output #0: softmax = 3.62665 (* 1 = 3.62665 loss)
I0615 16:01:39.099925  5063 solver.cpp:473] Iteration 5140, lr = 0.0001
I0615 16:01:39.858397  5063 solver.cpp:213] Iteration 5150, loss = 3.76578
I0615 16:01:39.858418  5063 solver.cpp:228]     Train net output #0: softmax = 3.76578 (* 1 = 3.76578 loss)
I0615 16:01:39.858423  5063 solver.cpp:473] Iteration 5150, lr = 0.0001
I0615 16:01:40.616258  5063 solver.cpp:213] Iteration 5160, loss = 3.64751
I0615 16:01:40.616283  5063 solver.cpp:228]     Train net output #0: softmax = 3.64751 (* 1 = 3.64751 loss)
I0615 16:01:40.616289  5063 solver.cpp:473] Iteration 5160, lr = 0.0001
I0615 16:01:41.374599  5063 solver.cpp:213] Iteration 5170, loss = 3.54224
I0615 16:01:41.374622  5063 solver.cpp:228]     Train net output #0: softmax = 3.54224 (* 1 = 3.54224 loss)
I0615 16:01:41.374627  5063 solver.cpp:473] Iteration 5170, lr = 0.0001
I0615 16:01:42.133167  5063 solver.cpp:213] Iteration 5180, loss = 3.5885
I0615 16:01:42.133190  5063 solver.cpp:228]     Train net output #0: softmax = 3.5885 (* 1 = 3.5885 loss)
I0615 16:01:42.133195  5063 solver.cpp:473] Iteration 5180, lr = 0.0001
I0615 16:01:42.892300  5063 solver.cpp:213] Iteration 5190, loss = 3.92799
I0615 16:01:42.892328  5063 solver.cpp:228]     Train net output #0: softmax = 3.92799 (* 1 = 3.92799 loss)
I0615 16:01:42.892333  5063 solver.cpp:473] Iteration 5190, lr = 0.0001
I0615 16:01:43.651216  5063 solver.cpp:213] Iteration 5200, loss = 3.81808
I0615 16:01:43.651267  5063 solver.cpp:228]     Train net output #0: softmax = 3.81808 (* 1 = 3.81808 loss)
I0615 16:01:43.651271  5063 solver.cpp:473] Iteration 5200, lr = 0.0001
I0615 16:01:44.409893  5063 solver.cpp:213] Iteration 5210, loss = 3.64479
I0615 16:01:44.409912  5063 solver.cpp:228]     Train net output #0: softmax = 3.64479 (* 1 = 3.64479 loss)
I0615 16:01:44.409917  5063 solver.cpp:473] Iteration 5210, lr = 0.0001
I0615 16:01:45.168611  5063 solver.cpp:213] Iteration 5220, loss = 3.62629
I0615 16:01:45.168630  5063 solver.cpp:228]     Train net output #0: softmax = 3.62629 (* 1 = 3.62629 loss)
I0615 16:01:45.168635  5063 solver.cpp:473] Iteration 5220, lr = 0.0001
I0615 16:01:45.927366  5063 solver.cpp:213] Iteration 5230, loss = 3.79019
I0615 16:01:45.927386  5063 solver.cpp:228]     Train net output #0: softmax = 3.79019 (* 1 = 3.79019 loss)
I0615 16:01:45.927389  5063 solver.cpp:473] Iteration 5230, lr = 0.0001
I0615 16:01:46.686159  5063 solver.cpp:213] Iteration 5240, loss = 3.49246
I0615 16:01:46.686183  5063 solver.cpp:228]     Train net output #0: softmax = 3.49246 (* 1 = 3.49246 loss)
I0615 16:01:46.686192  5063 solver.cpp:473] Iteration 5240, lr = 0.0001
I0615 16:01:47.444891  5063 solver.cpp:213] Iteration 5250, loss = 3.62153
I0615 16:01:47.444911  5063 solver.cpp:228]     Train net output #0: softmax = 3.62153 (* 1 = 3.62153 loss)
I0615 16:01:47.444916  5063 solver.cpp:473] Iteration 5250, lr = 0.0001
I0615 16:01:48.203692  5063 solver.cpp:213] Iteration 5260, loss = 3.82973
I0615 16:01:48.203713  5063 solver.cpp:228]     Train net output #0: softmax = 3.82973 (* 1 = 3.82973 loss)
I0615 16:01:48.203718  5063 solver.cpp:473] Iteration 5260, lr = 0.0001
I0615 16:01:48.962494  5063 solver.cpp:213] Iteration 5270, loss = 3.75414
I0615 16:01:48.962515  5063 solver.cpp:228]     Train net output #0: softmax = 3.75414 (* 1 = 3.75414 loss)
I0615 16:01:48.962519  5063 solver.cpp:473] Iteration 5270, lr = 0.0001
I0615 16:01:49.721091  5063 solver.cpp:213] Iteration 5280, loss = 3.64929
I0615 16:01:49.721117  5063 solver.cpp:228]     Train net output #0: softmax = 3.64929 (* 1 = 3.64929 loss)
I0615 16:01:49.721123  5063 solver.cpp:473] Iteration 5280, lr = 0.0001
I0615 16:01:50.478461  5063 solver.cpp:213] Iteration 5290, loss = 3.69014
I0615 16:01:50.478483  5063 solver.cpp:228]     Train net output #0: softmax = 3.69014 (* 1 = 3.69014 loss)
I0615 16:01:50.478487  5063 solver.cpp:473] Iteration 5290, lr = 0.0001
I0615 16:01:51.236920  5063 solver.cpp:213] Iteration 5300, loss = 3.79167
I0615 16:01:51.236942  5063 solver.cpp:228]     Train net output #0: softmax = 3.79167 (* 1 = 3.79167 loss)
I0615 16:01:51.236945  5063 solver.cpp:473] Iteration 5300, lr = 0.0001
I0615 16:01:51.995760  5063 solver.cpp:213] Iteration 5310, loss = 3.52142
I0615 16:01:51.995784  5063 solver.cpp:228]     Train net output #0: softmax = 3.52142 (* 1 = 3.52142 loss)
I0615 16:01:51.995904  5063 solver.cpp:473] Iteration 5310, lr = 0.0001
I0615 16:01:52.753661  5063 solver.cpp:213] Iteration 5320, loss = 3.71424
I0615 16:01:52.753681  5063 solver.cpp:228]     Train net output #0: softmax = 3.71424 (* 1 = 3.71424 loss)
I0615 16:01:52.753686  5063 solver.cpp:473] Iteration 5320, lr = 0.0001
I0615 16:01:53.512795  5063 solver.cpp:213] Iteration 5330, loss = 3.73286
I0615 16:01:53.512815  5063 solver.cpp:228]     Train net output #0: softmax = 3.73286 (* 1 = 3.73286 loss)
I0615 16:01:53.512820  5063 solver.cpp:473] Iteration 5330, lr = 0.0001
I0615 16:01:54.268549  5063 solver.cpp:213] Iteration 5340, loss = 3.86712
I0615 16:01:54.268573  5063 solver.cpp:228]     Train net output #0: softmax = 3.86712 (* 1 = 3.86712 loss)
I0615 16:01:54.268580  5063 solver.cpp:473] Iteration 5340, lr = 0.0001
I0615 16:01:55.026487  5063 solver.cpp:213] Iteration 5350, loss = 3.88369
I0615 16:01:55.026509  5063 solver.cpp:228]     Train net output #0: softmax = 3.88369 (* 1 = 3.88369 loss)
I0615 16:01:55.026513  5063 solver.cpp:473] Iteration 5350, lr = 0.0001
I0615 16:01:55.785326  5063 solver.cpp:213] Iteration 5360, loss = 3.71284
I0615 16:01:55.785349  5063 solver.cpp:228]     Train net output #0: softmax = 3.71284 (* 1 = 3.71284 loss)
I0615 16:01:55.785370  5063 solver.cpp:473] Iteration 5360, lr = 0.0001
I0615 16:01:56.544565  5063 solver.cpp:213] Iteration 5370, loss = 3.54497
I0615 16:01:56.544584  5063 solver.cpp:228]     Train net output #0: softmax = 3.54497 (* 1 = 3.54497 loss)
I0615 16:01:56.544589  5063 solver.cpp:473] Iteration 5370, lr = 0.0001
I0615 16:01:57.302664  5063 solver.cpp:213] Iteration 5380, loss = 3.7791
I0615 16:01:57.302685  5063 solver.cpp:228]     Train net output #0: softmax = 3.7791 (* 1 = 3.7791 loss)
I0615 16:01:57.302690  5063 solver.cpp:473] Iteration 5380, lr = 0.0001
I0615 16:01:58.061430  5063 solver.cpp:213] Iteration 5390, loss = 3.67103
I0615 16:01:58.061451  5063 solver.cpp:228]     Train net output #0: softmax = 3.67103 (* 1 = 3.67103 loss)
I0615 16:01:58.061456  5063 solver.cpp:473] Iteration 5390, lr = 0.0001
I0615 16:01:58.820348  5063 solver.cpp:213] Iteration 5400, loss = 3.73412
I0615 16:01:58.820369  5063 solver.cpp:228]     Train net output #0: softmax = 3.73412 (* 1 = 3.73412 loss)
I0615 16:01:58.820372  5063 solver.cpp:473] Iteration 5400, lr = 0.0001
I0615 16:01:59.579424  5063 solver.cpp:213] Iteration 5410, loss = 3.72569
I0615 16:01:59.579447  5063 solver.cpp:228]     Train net output #0: softmax = 3.72569 (* 1 = 3.72569 loss)
I0615 16:01:59.579452  5063 solver.cpp:473] Iteration 5410, lr = 0.0001
I0615 16:02:00.338137  5063 solver.cpp:213] Iteration 5420, loss = 3.84542
I0615 16:02:00.338160  5063 solver.cpp:228]     Train net output #0: softmax = 3.84542 (* 1 = 3.84542 loss)
I0615 16:02:00.338163  5063 solver.cpp:473] Iteration 5420, lr = 0.0001
I0615 16:02:01.097466  5063 solver.cpp:213] Iteration 5430, loss = 3.84028
I0615 16:02:01.097488  5063 solver.cpp:228]     Train net output #0: softmax = 3.84028 (* 1 = 3.84028 loss)
I0615 16:02:01.097493  5063 solver.cpp:473] Iteration 5430, lr = 0.0001
I0615 16:02:01.856101  5063 solver.cpp:213] Iteration 5440, loss = 3.72588
I0615 16:02:01.856130  5063 solver.cpp:228]     Train net output #0: softmax = 3.72588 (* 1 = 3.72588 loss)
I0615 16:02:01.856137  5063 solver.cpp:473] Iteration 5440, lr = 0.0001
I0615 16:02:02.614853  5063 solver.cpp:213] Iteration 5450, loss = 3.68771
I0615 16:02:02.614876  5063 solver.cpp:228]     Train net output #0: softmax = 3.68771 (* 1 = 3.68771 loss)
I0615 16:02:02.614881  5063 solver.cpp:473] Iteration 5450, lr = 0.0001
I0615 16:02:03.373347  5063 solver.cpp:213] Iteration 5460, loss = 3.70208
I0615 16:02:03.373368  5063 solver.cpp:228]     Train net output #0: softmax = 3.70208 (* 1 = 3.70208 loss)
I0615 16:02:03.373373  5063 solver.cpp:473] Iteration 5460, lr = 0.0001
I0615 16:02:04.131474  5063 solver.cpp:213] Iteration 5470, loss = 3.83365
I0615 16:02:04.131503  5063 solver.cpp:228]     Train net output #0: softmax = 3.83365 (* 1 = 3.83365 loss)
I0615 16:02:04.131507  5063 solver.cpp:473] Iteration 5470, lr = 0.0001
I0615 16:02:04.890481  5063 solver.cpp:213] Iteration 5480, loss = 3.70525
I0615 16:02:04.890503  5063 solver.cpp:228]     Train net output #0: softmax = 3.70525 (* 1 = 3.70525 loss)
I0615 16:02:04.890507  5063 solver.cpp:473] Iteration 5480, lr = 0.0001
I0615 16:02:05.649153  5063 solver.cpp:213] Iteration 5490, loss = 3.52211
I0615 16:02:05.649175  5063 solver.cpp:228]     Train net output #0: softmax = 3.52211 (* 1 = 3.52211 loss)
I0615 16:02:05.649180  5063 solver.cpp:473] Iteration 5490, lr = 0.0001
I0615 16:02:06.407594  5063 solver.cpp:213] Iteration 5500, loss = 3.74687
I0615 16:02:06.407614  5063 solver.cpp:228]     Train net output #0: softmax = 3.74687 (* 1 = 3.74687 loss)
I0615 16:02:06.407619  5063 solver.cpp:473] Iteration 5500, lr = 0.0001
I0615 16:02:07.166491  5063 solver.cpp:213] Iteration 5510, loss = 3.67093
I0615 16:02:07.166510  5063 solver.cpp:228]     Train net output #0: softmax = 3.67093 (* 1 = 3.67093 loss)
I0615 16:02:07.166515  5063 solver.cpp:473] Iteration 5510, lr = 0.0001
I0615 16:02:07.924734  5063 solver.cpp:213] Iteration 5520, loss = 3.73438
I0615 16:02:07.924756  5063 solver.cpp:228]     Train net output #0: softmax = 3.73438 (* 1 = 3.73438 loss)
I0615 16:02:07.924777  5063 solver.cpp:473] Iteration 5520, lr = 0.0001
I0615 16:02:08.683486  5063 solver.cpp:213] Iteration 5530, loss = 3.70846
I0615 16:02:08.683507  5063 solver.cpp:228]     Train net output #0: softmax = 3.70846 (* 1 = 3.70846 loss)
I0615 16:02:08.683512  5063 solver.cpp:473] Iteration 5530, lr = 0.0001
I0615 16:02:09.442523  5063 solver.cpp:213] Iteration 5540, loss = 3.69323
I0615 16:02:09.442543  5063 solver.cpp:228]     Train net output #0: softmax = 3.69323 (* 1 = 3.69323 loss)
I0615 16:02:09.442546  5063 solver.cpp:473] Iteration 5540, lr = 0.0001
I0615 16:02:10.199726  5063 solver.cpp:213] Iteration 5550, loss = 3.53777
I0615 16:02:10.199750  5063 solver.cpp:228]     Train net output #0: softmax = 3.53777 (* 1 = 3.53777 loss)
I0615 16:02:10.199756  5063 solver.cpp:473] Iteration 5550, lr = 0.0001
I0615 16:02:10.957648  5063 solver.cpp:213] Iteration 5560, loss = 3.6531
I0615 16:02:10.957669  5063 solver.cpp:228]     Train net output #0: softmax = 3.6531 (* 1 = 3.6531 loss)
I0615 16:02:10.957674  5063 solver.cpp:473] Iteration 5560, lr = 0.0001
I0615 16:02:11.716428  5063 solver.cpp:213] Iteration 5570, loss = 3.59515
I0615 16:02:11.716449  5063 solver.cpp:228]     Train net output #0: softmax = 3.59515 (* 1 = 3.59515 loss)
I0615 16:02:11.716454  5063 solver.cpp:473] Iteration 5570, lr = 0.0001
I0615 16:02:12.475016  5063 solver.cpp:213] Iteration 5580, loss = 3.7512
I0615 16:02:12.475039  5063 solver.cpp:228]     Train net output #0: softmax = 3.7512 (* 1 = 3.7512 loss)
I0615 16:02:12.475044  5063 solver.cpp:473] Iteration 5580, lr = 0.0001
I0615 16:02:13.234257  5063 solver.cpp:213] Iteration 5590, loss = 3.49414
I0615 16:02:13.234280  5063 solver.cpp:228]     Train net output #0: softmax = 3.49414 (* 1 = 3.49414 loss)
I0615 16:02:13.234416  5063 solver.cpp:473] Iteration 5590, lr = 0.0001
I0615 16:02:13.992924  5063 solver.cpp:213] Iteration 5600, loss = 3.55759
I0615 16:02:13.992967  5063 solver.cpp:228]     Train net output #0: softmax = 3.55759 (* 1 = 3.55759 loss)
I0615 16:02:13.992974  5063 solver.cpp:473] Iteration 5600, lr = 0.0001
I0615 16:02:14.751641  5063 solver.cpp:213] Iteration 5610, loss = 3.75193
I0615 16:02:14.751662  5063 solver.cpp:228]     Train net output #0: softmax = 3.75193 (* 1 = 3.75193 loss)
I0615 16:02:14.751667  5063 solver.cpp:473] Iteration 5610, lr = 0.0001
I0615 16:02:15.510807  5063 solver.cpp:213] Iteration 5620, loss = 3.64651
I0615 16:02:15.510828  5063 solver.cpp:228]     Train net output #0: softmax = 3.64651 (* 1 = 3.64651 loss)
I0615 16:02:15.510833  5063 solver.cpp:473] Iteration 5620, lr = 0.0001
I0615 16:02:16.269243  5063 solver.cpp:213] Iteration 5630, loss = 3.43833
I0615 16:02:16.269263  5063 solver.cpp:228]     Train net output #0: softmax = 3.43833 (* 1 = 3.43833 loss)
I0615 16:02:16.269266  5063 solver.cpp:473] Iteration 5630, lr = 0.0001
I0615 16:02:17.028044  5063 solver.cpp:213] Iteration 5640, loss = 3.57474
I0615 16:02:17.028064  5063 solver.cpp:228]     Train net output #0: softmax = 3.57474 (* 1 = 3.57474 loss)
I0615 16:02:17.028069  5063 solver.cpp:473] Iteration 5640, lr = 0.0001
I0615 16:02:17.786834  5063 solver.cpp:213] Iteration 5650, loss = 3.75857
I0615 16:02:17.786854  5063 solver.cpp:228]     Train net output #0: softmax = 3.75857 (* 1 = 3.75857 loss)
I0615 16:02:17.786859  5063 solver.cpp:473] Iteration 5650, lr = 0.0001
I0615 16:02:18.542464  5063 solver.cpp:213] Iteration 5660, loss = 3.80224
I0615 16:02:18.542486  5063 solver.cpp:228]     Train net output #0: softmax = 3.80224 (* 1 = 3.80224 loss)
I0615 16:02:18.542491  5063 solver.cpp:473] Iteration 5660, lr = 0.0001
I0615 16:02:19.301419  5063 solver.cpp:213] Iteration 5670, loss = 3.6867
I0615 16:02:19.301440  5063 solver.cpp:228]     Train net output #0: softmax = 3.6867 (* 1 = 3.6867 loss)
I0615 16:02:19.301443  5063 solver.cpp:473] Iteration 5670, lr = 0.0001
I0615 16:02:20.059998  5063 solver.cpp:213] Iteration 5680, loss = 3.72091
I0615 16:02:20.060019  5063 solver.cpp:228]     Train net output #0: softmax = 3.72091 (* 1 = 3.72091 loss)
I0615 16:02:20.060024  5063 solver.cpp:473] Iteration 5680, lr = 0.0001
I0615 16:02:20.818819  5063 solver.cpp:213] Iteration 5690, loss = 3.79487
I0615 16:02:20.818840  5063 solver.cpp:228]     Train net output #0: softmax = 3.79487 (* 1 = 3.79487 loss)
I0615 16:02:20.818845  5063 solver.cpp:473] Iteration 5690, lr = 0.0001
I0615 16:02:21.577831  5063 solver.cpp:213] Iteration 5700, loss = 3.54727
I0615 16:02:21.577852  5063 solver.cpp:228]     Train net output #0: softmax = 3.54727 (* 1 = 3.54727 loss)
I0615 16:02:21.577857  5063 solver.cpp:473] Iteration 5700, lr = 0.0001
I0615 16:02:22.336802  5063 solver.cpp:213] Iteration 5710, loss = 3.7466
I0615 16:02:22.336820  5063 solver.cpp:228]     Train net output #0: softmax = 3.7466 (* 1 = 3.7466 loss)
I0615 16:02:22.336825  5063 solver.cpp:473] Iteration 5710, lr = 0.0001
I0615 16:02:23.095933  5063 solver.cpp:213] Iteration 5720, loss = 3.51219
I0615 16:02:23.095952  5063 solver.cpp:228]     Train net output #0: softmax = 3.51219 (* 1 = 3.51219 loss)
I0615 16:02:23.095957  5063 solver.cpp:473] Iteration 5720, lr = 0.0001
I0615 16:02:23.853435  5063 solver.cpp:213] Iteration 5730, loss = 3.58801
I0615 16:02:23.853458  5063 solver.cpp:228]     Train net output #0: softmax = 3.58801 (* 1 = 3.58801 loss)
I0615 16:02:23.853582  5063 solver.cpp:473] Iteration 5730, lr = 0.0001
I0615 16:02:24.612011  5063 solver.cpp:213] Iteration 5740, loss = 3.64801
I0615 16:02:24.612032  5063 solver.cpp:228]     Train net output #0: softmax = 3.64801 (* 1 = 3.64801 loss)
I0615 16:02:24.612037  5063 solver.cpp:473] Iteration 5740, lr = 0.0001
I0615 16:02:25.369946  5063 solver.cpp:213] Iteration 5750, loss = 3.40762
I0615 16:02:25.369967  5063 solver.cpp:228]     Train net output #0: softmax = 3.40762 (* 1 = 3.40762 loss)
I0615 16:02:25.369972  5063 solver.cpp:473] Iteration 5750, lr = 0.0001
I0615 16:02:26.128599  5063 solver.cpp:213] Iteration 5760, loss = 3.49927
I0615 16:02:26.128620  5063 solver.cpp:228]     Train net output #0: softmax = 3.49927 (* 1 = 3.49927 loss)
I0615 16:02:26.128645  5063 solver.cpp:473] Iteration 5760, lr = 0.0001
I0615 16:02:26.887308  5063 solver.cpp:213] Iteration 5770, loss = 3.92263
I0615 16:02:26.887331  5063 solver.cpp:228]     Train net output #0: softmax = 3.92263 (* 1 = 3.92263 loss)
I0615 16:02:26.887336  5063 solver.cpp:473] Iteration 5770, lr = 0.0001
I0615 16:02:27.646078  5063 solver.cpp:213] Iteration 5780, loss = 3.57284
I0615 16:02:27.646100  5063 solver.cpp:228]     Train net output #0: softmax = 3.57284 (* 1 = 3.57284 loss)
I0615 16:02:27.646105  5063 solver.cpp:473] Iteration 5780, lr = 0.0001
I0615 16:02:28.404683  5063 solver.cpp:213] Iteration 5790, loss = 3.63465
I0615 16:02:28.404705  5063 solver.cpp:228]     Train net output #0: softmax = 3.63465 (* 1 = 3.63465 loss)
I0615 16:02:28.404709  5063 solver.cpp:473] Iteration 5790, lr = 0.0001
I0615 16:02:29.163120  5063 solver.cpp:213] Iteration 5800, loss = 3.896
I0615 16:02:29.163147  5063 solver.cpp:228]     Train net output #0: softmax = 3.896 (* 1 = 3.896 loss)
I0615 16:02:29.163158  5063 solver.cpp:473] Iteration 5800, lr = 0.0001
I0615 16:02:29.920819  5063 solver.cpp:213] Iteration 5810, loss = 3.6939
I0615 16:02:29.920840  5063 solver.cpp:228]     Train net output #0: softmax = 3.6939 (* 1 = 3.6939 loss)
I0615 16:02:29.920843  5063 solver.cpp:473] Iteration 5810, lr = 0.0001
I0615 16:02:30.679273  5063 solver.cpp:213] Iteration 5820, loss = 3.60831
I0615 16:02:30.679294  5063 solver.cpp:228]     Train net output #0: softmax = 3.60831 (* 1 = 3.60831 loss)
I0615 16:02:30.679299  5063 solver.cpp:473] Iteration 5820, lr = 0.0001
I0615 16:02:31.437666  5063 solver.cpp:213] Iteration 5830, loss = 3.74118
I0615 16:02:31.437687  5063 solver.cpp:228]     Train net output #0: softmax = 3.74118 (* 1 = 3.74118 loss)
I0615 16:02:31.437692  5063 solver.cpp:473] Iteration 5830, lr = 0.0001
I0615 16:02:32.196288  5063 solver.cpp:213] Iteration 5840, loss = 3.66437
I0615 16:02:32.196310  5063 solver.cpp:228]     Train net output #0: softmax = 3.66437 (* 1 = 3.66437 loss)
I0615 16:02:32.196316  5063 solver.cpp:473] Iteration 5840, lr = 0.0001
I0615 16:02:32.955345  5063 solver.cpp:213] Iteration 5850, loss = 3.62904
I0615 16:02:32.955366  5063 solver.cpp:228]     Train net output #0: softmax = 3.62904 (* 1 = 3.62904 loss)
I0615 16:02:32.955371  5063 solver.cpp:473] Iteration 5850, lr = 0.0001
I0615 16:02:33.714210  5063 solver.cpp:213] Iteration 5860, loss = 3.83417
I0615 16:02:33.714231  5063 solver.cpp:228]     Train net output #0: softmax = 3.83417 (* 1 = 3.83417 loss)
I0615 16:02:33.714236  5063 solver.cpp:473] Iteration 5860, lr = 0.0001
I0615 16:02:34.472180  5063 solver.cpp:213] Iteration 5870, loss = 3.55135
I0615 16:02:34.472201  5063 solver.cpp:228]     Train net output #0: softmax = 3.55135 (* 1 = 3.55135 loss)
I0615 16:02:34.472206  5063 solver.cpp:473] Iteration 5870, lr = 0.0001
I0615 16:02:35.230234  5063 solver.cpp:213] Iteration 5880, loss = 3.62965
I0615 16:02:35.230255  5063 solver.cpp:228]     Train net output #0: softmax = 3.62965 (* 1 = 3.62965 loss)
I0615 16:02:35.230260  5063 solver.cpp:473] Iteration 5880, lr = 0.0001
I0615 16:02:35.988822  5063 solver.cpp:213] Iteration 5890, loss = 3.89517
I0615 16:02:35.988843  5063 solver.cpp:228]     Train net output #0: softmax = 3.89517 (* 1 = 3.89517 loss)
I0615 16:02:35.988848  5063 solver.cpp:473] Iteration 5890, lr = 0.0001
I0615 16:02:36.747298  5063 solver.cpp:213] Iteration 5900, loss = 3.5696
I0615 16:02:36.747318  5063 solver.cpp:228]     Train net output #0: softmax = 3.5696 (* 1 = 3.5696 loss)
I0615 16:02:36.747323  5063 solver.cpp:473] Iteration 5900, lr = 0.0001
I0615 16:02:37.506140  5063 solver.cpp:213] Iteration 5910, loss = 3.79347
I0615 16:02:37.506161  5063 solver.cpp:228]     Train net output #0: softmax = 3.79347 (* 1 = 3.79347 loss)
I0615 16:02:37.506166  5063 solver.cpp:473] Iteration 5910, lr = 0.0001
I0615 16:02:38.262864  5063 solver.cpp:213] Iteration 5920, loss = 3.69622
I0615 16:02:38.262886  5063 solver.cpp:228]     Train net output #0: softmax = 3.69622 (* 1 = 3.69622 loss)
I0615 16:02:38.262908  5063 solver.cpp:473] Iteration 5920, lr = 0.0001
I0615 16:02:39.021083  5063 solver.cpp:213] Iteration 5930, loss = 3.75135
I0615 16:02:39.021105  5063 solver.cpp:228]     Train net output #0: softmax = 3.75135 (* 1 = 3.75135 loss)
I0615 16:02:39.021109  5063 solver.cpp:473] Iteration 5930, lr = 0.0001
I0615 16:02:39.779150  5063 solver.cpp:213] Iteration 5940, loss = 3.73488
I0615 16:02:39.779173  5063 solver.cpp:228]     Train net output #0: softmax = 3.73488 (* 1 = 3.73488 loss)
I0615 16:02:39.779183  5063 solver.cpp:473] Iteration 5940, lr = 0.0001
I0615 16:02:40.535146  5063 solver.cpp:213] Iteration 5950, loss = 3.62405
I0615 16:02:40.535173  5063 solver.cpp:228]     Train net output #0: softmax = 3.62405 (* 1 = 3.62405 loss)
I0615 16:02:40.535179  5063 solver.cpp:473] Iteration 5950, lr = 0.0001
I0615 16:02:41.292824  5063 solver.cpp:213] Iteration 5960, loss = 3.62215
I0615 16:02:41.292846  5063 solver.cpp:228]     Train net output #0: softmax = 3.62215 (* 1 = 3.62215 loss)
I0615 16:02:41.292851  5063 solver.cpp:473] Iteration 5960, lr = 0.0001
I0615 16:02:42.051565  5063 solver.cpp:213] Iteration 5970, loss = 3.73317
I0615 16:02:42.051586  5063 solver.cpp:228]     Train net output #0: softmax = 3.73317 (* 1 = 3.73317 loss)
I0615 16:02:42.051590  5063 solver.cpp:473] Iteration 5970, lr = 0.0001
I0615 16:02:42.810552  5063 solver.cpp:213] Iteration 5980, loss = 3.50451
I0615 16:02:42.810573  5063 solver.cpp:228]     Train net output #0: softmax = 3.50451 (* 1 = 3.50451 loss)
I0615 16:02:42.810578  5063 solver.cpp:473] Iteration 5980, lr = 0.0001
I0615 16:02:43.569077  5063 solver.cpp:213] Iteration 5990, loss = 3.40604
I0615 16:02:43.569097  5063 solver.cpp:228]     Train net output #0: softmax = 3.40604 (* 1 = 3.40604 loss)
I0615 16:02:43.569102  5063 solver.cpp:473] Iteration 5990, lr = 0.0001
I0615 16:02:44.273861  5063 solver.cpp:362] Snapshotting to snapshots/16-06-15_15h49m18s_0_11_pretrainClassification_iter_6000.caffemodel
I0615 16:02:44.274569  5063 solver.cpp:370] Snapshotting solver state to snapshots/16-06-15_15h49m18s_0_11_pretrainClassification_iter_6000.solverstate
I0615 16:02:44.274962  5063 solver.cpp:291] Iteration 6000, Testing net (#0)
I0615 16:02:44.369420  5063 solver.cpp:342]     Test net output #0: accuracy_top_1 = 0.148438
I0615 16:02:44.369436  5063 solver.cpp:342]     Test net output #1: accuracy_top_5 = 0.404687
I0615 16:02:44.369442  5063 solver.cpp:342]     Test net output #2: softmax = 3.67824 (* 1 = 3.67824 loss)
I0615 16:02:44.423063  5063 solver.cpp:213] Iteration 6000, loss = 3.62241
I0615 16:02:44.423074  5063 solver.cpp:228]     Train net output #0: softmax = 3.62241 (* 1 = 3.62241 loss)
I0615 16:02:44.423079  5063 solver.cpp:473] Iteration 6000, lr = 0.0001
I0615 16:02:45.180966  5063 solver.cpp:213] Iteration 6010, loss = 3.58783
I0615 16:02:45.180996  5063 solver.cpp:228]     Train net output #0: softmax = 3.58783 (* 1 = 3.58783 loss)
I0615 16:02:45.181187  5063 solver.cpp:473] Iteration 6010, lr = 0.0001
I0615 16:02:45.937614  5063 solver.cpp:213] Iteration 6020, loss = 3.59239
I0615 16:02:45.937635  5063 solver.cpp:228]     Train net output #0: softmax = 3.59239 (* 1 = 3.59239 loss)
I0615 16:02:45.937639  5063 solver.cpp:473] Iteration 6020, lr = 0.0001
I0615 16:02:46.696243  5063 solver.cpp:213] Iteration 6030, loss = 3.69394
I0615 16:02:46.696264  5063 solver.cpp:228]     Train net output #0: softmax = 3.69394 (* 1 = 3.69394 loss)
I0615 16:02:46.696267  5063 solver.cpp:473] Iteration 6030, lr = 0.0001
I0615 16:02:47.455150  5063 solver.cpp:213] Iteration 6040, loss = 3.82608
I0615 16:02:47.455170  5063 solver.cpp:228]     Train net output #0: softmax = 3.82608 (* 1 = 3.82608 loss)
I0615 16:02:47.455175  5063 solver.cpp:473] Iteration 6040, lr = 0.0001
I0615 16:02:48.214573  5063 solver.cpp:213] Iteration 6050, loss = 3.845
I0615 16:02:48.214596  5063 solver.cpp:228]     Train net output #0: softmax = 3.845 (* 1 = 3.845 loss)
I0615 16:02:48.214601  5063 solver.cpp:473] Iteration 6050, lr = 0.0001
I0615 16:02:48.973227  5063 solver.cpp:213] Iteration 6060, loss = 3.57152
I0615 16:02:48.973254  5063 solver.cpp:228]     Train net output #0: softmax = 3.57152 (* 1 = 3.57152 loss)
I0615 16:02:48.973260  5063 solver.cpp:473] Iteration 6060, lr = 0.0001
I0615 16:02:49.732138  5063 solver.cpp:213] Iteration 6070, loss = 3.54882
I0615 16:02:49.732158  5063 solver.cpp:228]     Train net output #0: softmax = 3.54882 (* 1 = 3.54882 loss)
I0615 16:02:49.732163  5063 solver.cpp:473] Iteration 6070, lr = 0.0001
I0615 16:02:50.490917  5063 solver.cpp:213] Iteration 6080, loss = 3.58052
I0615 16:02:50.490939  5063 solver.cpp:228]     Train net output #0: softmax = 3.58052 (* 1 = 3.58052 loss)
I0615 16:02:50.490943  5063 solver.cpp:473] Iteration 6080, lr = 0.0001
I0615 16:02:51.250005  5063 solver.cpp:213] Iteration 6090, loss = 3.61661
I0615 16:02:51.250025  5063 solver.cpp:228]     Train net output #0: softmax = 3.61661 (* 1 = 3.61661 loss)
I0615 16:02:51.250030  5063 solver.cpp:473] Iteration 6090, lr = 0.0001
I0615 16:02:52.008708  5063 solver.cpp:213] Iteration 6100, loss = 3.64945
I0615 16:02:52.008728  5063 solver.cpp:228]     Train net output #0: softmax = 3.64945 (* 1 = 3.64945 loss)
I0615 16:02:52.008733  5063 solver.cpp:473] Iteration 6100, lr = 0.0001
I0615 16:02:52.767510  5063 solver.cpp:213] Iteration 6110, loss = 3.66874
I0615 16:02:52.767531  5063 solver.cpp:228]     Train net output #0: softmax = 3.66874 (* 1 = 3.66874 loss)
I0615 16:02:52.767536  5063 solver.cpp:473] Iteration 6110, lr = 0.0001
I0615 16:02:53.526680  5063 solver.cpp:213] Iteration 6120, loss = 3.7153
I0615 16:02:53.526700  5063 solver.cpp:228]     Train net output #0: softmax = 3.7153 (* 1 = 3.7153 loss)
I0615 16:02:53.526705  5063 solver.cpp:473] Iteration 6120, lr = 0.0001
I0615 16:02:54.285723  5063 solver.cpp:213] Iteration 6130, loss = 3.70791
I0615 16:02:54.285744  5063 solver.cpp:228]     Train net output #0: softmax = 3.70791 (* 1 = 3.70791 loss)
I0615 16:02:54.285749  5063 solver.cpp:473] Iteration 6130, lr = 0.0001
I0615 16:02:55.044656  5063 solver.cpp:213] Iteration 6140, loss = 3.52053
I0615 16:02:55.044692  5063 solver.cpp:228]     Train net output #0: softmax = 3.52053 (* 1 = 3.52053 loss)
I0615 16:02:55.044697  5063 solver.cpp:473] Iteration 6140, lr = 0.0001
I0615 16:02:55.803475  5063 solver.cpp:213] Iteration 6150, loss = 3.54471
I0615 16:02:55.803499  5063 solver.cpp:228]     Train net output #0: softmax = 3.54471 (* 1 = 3.54471 loss)
I0615 16:02:55.803632  5063 solver.cpp:473] Iteration 6150, lr = 0.0001
I0615 16:02:56.562784  5063 solver.cpp:213] Iteration 6160, loss = 3.81766
I0615 16:02:56.562804  5063 solver.cpp:228]     Train net output #0: softmax = 3.81766 (* 1 = 3.81766 loss)
I0615 16:02:56.562808  5063 solver.cpp:473] Iteration 6160, lr = 0.0001
I0615 16:02:57.321328  5063 solver.cpp:213] Iteration 6170, loss = 3.54459
I0615 16:02:57.321349  5063 solver.cpp:228]     Train net output #0: softmax = 3.54459 (* 1 = 3.54459 loss)
I0615 16:02:57.321353  5063 solver.cpp:473] Iteration 6170, lr = 0.0001
I0615 16:02:58.080636  5063 solver.cpp:213] Iteration 6180, loss = 3.60991
I0615 16:02:58.080657  5063 solver.cpp:228]     Train net output #0: softmax = 3.60991 (* 1 = 3.60991 loss)
I0615 16:02:58.080662  5063 solver.cpp:473] Iteration 6180, lr = 0.0001
I0615 16:02:58.839598  5063 solver.cpp:213] Iteration 6190, loss = 3.86818
I0615 16:02:58.839618  5063 solver.cpp:228]     Train net output #0: softmax = 3.86818 (* 1 = 3.86818 loss)
I0615 16:02:58.839623  5063 solver.cpp:473] Iteration 6190, lr = 0.0001
I0615 16:02:59.598327  5063 solver.cpp:213] Iteration 6200, loss = 3.84742
I0615 16:02:59.598350  5063 solver.cpp:228]     Train net output #0: softmax = 3.84742 (* 1 = 3.84742 loss)
I0615 16:02:59.598354  5063 solver.cpp:473] Iteration 6200, lr = 0.0001
I0615 16:03:00.356487  5063 solver.cpp:213] Iteration 6210, loss = 3.49461
I0615 16:03:00.356508  5063 solver.cpp:228]     Train net output #0: softmax = 3.49461 (* 1 = 3.49461 loss)
I0615 16:03:00.356513  5063 solver.cpp:473] Iteration 6210, lr = 0.0001
I0615 16:03:01.115269  5063 solver.cpp:213] Iteration 6220, loss = 3.69972
I0615 16:03:01.115447  5063 solver.cpp:228]     Train net output #0: softmax = 3.69972 (* 1 = 3.69972 loss)
I0615 16:03:01.115454  5063 solver.cpp:473] Iteration 6220, lr = 0.0001
I0615 16:03:01.874207  5063 solver.cpp:213] Iteration 6230, loss = 3.73251
I0615 16:03:01.874228  5063 solver.cpp:228]     Train net output #0: softmax = 3.73251 (* 1 = 3.73251 loss)
I0615 16:03:01.874233  5063 solver.cpp:473] Iteration 6230, lr = 0.0001
I0615 16:03:02.632525  5063 solver.cpp:213] Iteration 6240, loss = 3.56366
I0615 16:03:02.632546  5063 solver.cpp:228]     Train net output #0: softmax = 3.56366 (* 1 = 3.56366 loss)
I0615 16:03:02.632551  5063 solver.cpp:473] Iteration 6240, lr = 0.0001
I0615 16:03:03.390744  5063 solver.cpp:213] Iteration 6250, loss = 3.75974
I0615 16:03:03.390765  5063 solver.cpp:228]     Train net output #0: softmax = 3.75974 (* 1 = 3.75974 loss)
I0615 16:03:03.390769  5063 solver.cpp:473] Iteration 6250, lr = 0.0001
I0615 16:03:04.149283  5063 solver.cpp:213] Iteration 6260, loss = 3.6196
I0615 16:03:04.149303  5063 solver.cpp:228]     Train net output #0: softmax = 3.6196 (* 1 = 3.6196 loss)
I0615 16:03:04.149308  5063 solver.cpp:473] Iteration 6260, lr = 0.0001
I0615 16:03:04.908174  5063 solver.cpp:213] Iteration 6270, loss = 3.76589
I0615 16:03:04.908193  5063 solver.cpp:228]     Train net output #0: softmax = 3.76589 (* 1 = 3.76589 loss)
I0615 16:03:04.908198  5063 solver.cpp:473] Iteration 6270, lr = 0.0001
I0615 16:03:05.667158  5063 solver.cpp:213] Iteration 6280, loss = 3.60007
I0615 16:03:05.667179  5063 solver.cpp:228]     Train net output #0: softmax = 3.60007 (* 1 = 3.60007 loss)
I0615 16:03:05.667184  5063 solver.cpp:473] Iteration 6280, lr = 0.0001
I0615 16:03:06.425618  5063 solver.cpp:213] Iteration 6290, loss = 3.60079
I0615 16:03:06.425639  5063 solver.cpp:228]     Train net output #0: softmax = 3.60079 (* 1 = 3.60079 loss)
I0615 16:03:06.425644  5063 solver.cpp:473] Iteration 6290, lr = 0.0001
I0615 16:03:07.184298  5063 solver.cpp:213] Iteration 6300, loss = 3.82967
I0615 16:03:07.184335  5063 solver.cpp:228]     Train net output #0: softmax = 3.82967 (* 1 = 3.82967 loss)
I0615 16:03:07.184340  5063 solver.cpp:473] Iteration 6300, lr = 0.0001
I0615 16:03:07.942919  5063 solver.cpp:213] Iteration 6310, loss = 3.81882
I0615 16:03:07.942939  5063 solver.cpp:228]     Train net output #0: softmax = 3.81882 (* 1 = 3.81882 loss)
I0615 16:03:07.942945  5063 solver.cpp:473] Iteration 6310, lr = 0.0001
I0615 16:03:08.700011  5063 solver.cpp:213] Iteration 6320, loss = 3.84178
I0615 16:03:08.700034  5063 solver.cpp:228]     Train net output #0: softmax = 3.84178 (* 1 = 3.84178 loss)
I0615 16:03:08.700039  5063 solver.cpp:473] Iteration 6320, lr = 0.0001
I0615 16:03:09.458343  5063 solver.cpp:213] Iteration 6330, loss = 3.729
I0615 16:03:09.458362  5063 solver.cpp:228]     Train net output #0: softmax = 3.729 (* 1 = 3.729 loss)
I0615 16:03:09.458367  5063 solver.cpp:473] Iteration 6330, lr = 0.0001
I0615 16:03:10.216547  5063 solver.cpp:213] Iteration 6340, loss = 3.49445
I0615 16:03:10.216567  5063 solver.cpp:228]     Train net output #0: softmax = 3.49445 (* 1 = 3.49445 loss)
I0615 16:03:10.216572  5063 solver.cpp:473] Iteration 6340, lr = 0.0001
I0615 16:03:10.974892  5063 solver.cpp:213] Iteration 6350, loss = 3.54253
I0615 16:03:10.974913  5063 solver.cpp:228]     Train net output #0: softmax = 3.54253 (* 1 = 3.54253 loss)
I0615 16:03:10.974918  5063 solver.cpp:473] Iteration 6350, lr = 0.0001
I0615 16:03:11.733542  5063 solver.cpp:213] Iteration 6360, loss = 3.64166
I0615 16:03:11.733568  5063 solver.cpp:228]     Train net output #0: softmax = 3.64166 (* 1 = 3.64166 loss)
I0615 16:03:11.733700  5063 solver.cpp:473] Iteration 6360, lr = 0.0001
I0615 16:03:12.491992  5063 solver.cpp:213] Iteration 6370, loss = 3.67595
I0615 16:03:12.492013  5063 solver.cpp:228]     Train net output #0: softmax = 3.67595 (* 1 = 3.67595 loss)
I0615 16:03:12.492018  5063 solver.cpp:473] Iteration 6370, lr = 0.0001
I0615 16:03:13.250494  5063 solver.cpp:213] Iteration 6380, loss = 3.70485
I0615 16:03:13.250524  5063 solver.cpp:228]     Train net output #0: softmax = 3.70485 (* 1 = 3.70485 loss)
I0615 16:03:13.250529  5063 solver.cpp:473] Iteration 6380, lr = 0.0001
I0615 16:03:14.006992  5063 solver.cpp:213] Iteration 6390, loss = 3.87885
I0615 16:03:14.007014  5063 solver.cpp:228]     Train net output #0: softmax = 3.87885 (* 1 = 3.87885 loss)
I0615 16:03:14.007019  5063 solver.cpp:473] Iteration 6390, lr = 0.0001
I0615 16:03:14.764581  5063 solver.cpp:213] Iteration 6400, loss = 3.54381
I0615 16:03:14.764626  5063 solver.cpp:228]     Train net output #0: softmax = 3.54381 (* 1 = 3.54381 loss)
I0615 16:03:14.764631  5063 solver.cpp:473] Iteration 6400, lr = 0.0001
I0615 16:03:15.522943  5063 solver.cpp:213] Iteration 6410, loss = 3.79993
I0615 16:03:15.522964  5063 solver.cpp:228]     Train net output #0: softmax = 3.79993 (* 1 = 3.79993 loss)
I0615 16:03:15.522969  5063 solver.cpp:473] Iteration 6410, lr = 0.0001
I0615 16:03:16.281397  5063 solver.cpp:213] Iteration 6420, loss = 3.79761
I0615 16:03:16.281417  5063 solver.cpp:228]     Train net output #0: softmax = 3.79761 (* 1 = 3.79761 loss)
I0615 16:03:16.281422  5063 solver.cpp:473] Iteration 6420, lr = 0.0001
I0615 16:03:17.036069  5063 solver.cpp:213] Iteration 6430, loss = 4.00727
I0615 16:03:17.036095  5063 solver.cpp:228]     Train net output #0: softmax = 4.00727 (* 1 = 4.00727 loss)
I0615 16:03:17.036231  5063 solver.cpp:473] Iteration 6430, lr = 0.0001
I0615 16:03:17.794971  5063 solver.cpp:213] Iteration 6440, loss = 3.82672
I0615 16:03:17.794991  5063 solver.cpp:228]     Train net output #0: softmax = 3.82672 (* 1 = 3.82672 loss)
I0615 16:03:17.794996  5063 solver.cpp:473] Iteration 6440, lr = 0.0001
I0615 16:03:18.553732  5063 solver.cpp:213] Iteration 6450, loss = 3.57168
I0615 16:03:18.553753  5063 solver.cpp:228]     Train net output #0: softmax = 3.57168 (* 1 = 3.57168 loss)
I0615 16:03:18.553757  5063 solver.cpp:473] Iteration 6450, lr = 0.0001
I0615 16:03:19.312297  5063 solver.cpp:213] Iteration 6460, loss = 3.35436
I0615 16:03:19.312317  5063 solver.cpp:228]     Train net output #0: softmax = 3.35436 (* 1 = 3.35436 loss)
I0615 16:03:19.312321  5063 solver.cpp:473] Iteration 6460, lr = 0.0001
I0615 16:03:20.071544  5063 solver.cpp:213] Iteration 6470, loss = 3.54142
I0615 16:03:20.071565  5063 solver.cpp:228]     Train net output #0: softmax = 3.54142 (* 1 = 3.54142 loss)
I0615 16:03:20.071570  5063 solver.cpp:473] Iteration 6470, lr = 0.0001
I0615 16:03:20.830756  5063 solver.cpp:213] Iteration 6480, loss = 3.67103
I0615 16:03:20.830777  5063 solver.cpp:228]     Train net output #0: softmax = 3.67103 (* 1 = 3.67103 loss)
I0615 16:03:20.830781  5063 solver.cpp:473] Iteration 6480, lr = 0.0001
I0615 16:03:21.589653  5063 solver.cpp:213] Iteration 6490, loss = 3.47533
I0615 16:03:21.589673  5063 solver.cpp:228]     Train net output #0: softmax = 3.47533 (* 1 = 3.47533 loss)
I0615 16:03:21.589678  5063 solver.cpp:473] Iteration 6490, lr = 0.0001
I0615 16:03:22.347990  5063 solver.cpp:213] Iteration 6500, loss = 3.56539
I0615 16:03:22.348013  5063 solver.cpp:228]     Train net output #0: softmax = 3.56539 (* 1 = 3.56539 loss)
I0615 16:03:22.348163  5063 solver.cpp:473] Iteration 6500, lr = 0.0001
I0615 16:03:23.106832  5063 solver.cpp:213] Iteration 6510, loss = 3.58304
I0615 16:03:23.106851  5063 solver.cpp:228]     Train net output #0: softmax = 3.58304 (* 1 = 3.58304 loss)
I0615 16:03:23.106856  5063 solver.cpp:473] Iteration 6510, lr = 0.0001
I0615 16:03:23.864709  5063 solver.cpp:213] Iteration 6520, loss = 3.41949
I0615 16:03:23.864729  5063 solver.cpp:228]     Train net output #0: softmax = 3.41949 (* 1 = 3.41949 loss)
I0615 16:03:23.864734  5063 solver.cpp:473] Iteration 6520, lr = 0.0001
I0615 16:03:24.623421  5063 solver.cpp:213] Iteration 6530, loss = 3.62294
I0615 16:03:24.623443  5063 solver.cpp:228]     Train net output #0: softmax = 3.62294 (* 1 = 3.62294 loss)
I0615 16:03:24.623447  5063 solver.cpp:473] Iteration 6530, lr = 0.0001
I0615 16:03:25.382031  5063 solver.cpp:213] Iteration 6540, loss = 3.68812
I0615 16:03:25.382061  5063 solver.cpp:228]     Train net output #0: softmax = 3.68812 (* 1 = 3.68812 loss)
I0615 16:03:25.382066  5063 solver.cpp:473] Iteration 6540, lr = 0.0001
I0615 16:03:26.140964  5063 solver.cpp:213] Iteration 6550, loss = 3.69521
I0615 16:03:26.140985  5063 solver.cpp:228]     Train net output #0: softmax = 3.69521 (* 1 = 3.69521 loss)
I0615 16:03:26.140990  5063 solver.cpp:473] Iteration 6550, lr = 0.0001
I0615 16:03:26.899792  5063 solver.cpp:213] Iteration 6560, loss = 3.52617
I0615 16:03:26.899818  5063 solver.cpp:228]     Train net output #0: softmax = 3.52617 (* 1 = 3.52617 loss)
I0615 16:03:26.899842  5063 solver.cpp:473] Iteration 6560, lr = 0.0001
I0615 16:03:27.658660  5063 solver.cpp:213] Iteration 6570, loss = 3.62532
I0615 16:03:27.658681  5063 solver.cpp:228]     Train net output #0: softmax = 3.62532 (* 1 = 3.62532 loss)
I0615 16:03:27.658686  5063 solver.cpp:473] Iteration 6570, lr = 0.0001
I0615 16:03:28.416826  5063 solver.cpp:213] Iteration 6580, loss = 3.61033
I0615 16:03:28.416846  5063 solver.cpp:228]     Train net output #0: softmax = 3.61033 (* 1 = 3.61033 loss)
I0615 16:03:28.416851  5063 solver.cpp:473] Iteration 6580, lr = 0.0001
I0615 16:03:29.175446  5063 solver.cpp:213] Iteration 6590, loss = 3.89865
I0615 16:03:29.175467  5063 solver.cpp:228]     Train net output #0: softmax = 3.89865 (* 1 = 3.89865 loss)
I0615 16:03:29.175472  5063 solver.cpp:473] Iteration 6590, lr = 0.0001
I0615 16:03:29.933799  5063 solver.cpp:213] Iteration 6600, loss = 3.49473
I0615 16:03:29.933828  5063 solver.cpp:228]     Train net output #0: softmax = 3.49473 (* 1 = 3.49473 loss)
I0615 16:03:29.933832  5063 solver.cpp:473] Iteration 6600, lr = 0.0001
I0615 16:03:30.691964  5063 solver.cpp:213] Iteration 6610, loss = 3.59395
I0615 16:03:30.691984  5063 solver.cpp:228]     Train net output #0: softmax = 3.59395 (* 1 = 3.59395 loss)
I0615 16:03:30.691989  5063 solver.cpp:473] Iteration 6610, lr = 0.0001
I0615 16:03:31.450142  5063 solver.cpp:213] Iteration 6620, loss = 3.71753
I0615 16:03:31.450163  5063 solver.cpp:228]     Train net output #0: softmax = 3.71753 (* 1 = 3.71753 loss)
I0615 16:03:31.450168  5063 solver.cpp:473] Iteration 6620, lr = 0.0001
I0615 16:03:32.208735  5063 solver.cpp:213] Iteration 6630, loss = 3.70932
I0615 16:03:32.208756  5063 solver.cpp:228]     Train net output #0: softmax = 3.70932 (* 1 = 3.70932 loss)
I0615 16:03:32.208761  5063 solver.cpp:473] Iteration 6630, lr = 0.0001
I0615 16:03:32.965968  5063 solver.cpp:213] Iteration 6640, loss = 3.67095
I0615 16:03:32.965993  5063 solver.cpp:228]     Train net output #0: softmax = 3.67095 (* 1 = 3.67095 loss)
I0615 16:03:32.966117  5063 solver.cpp:473] Iteration 6640, lr = 0.0001
I0615 16:03:33.724318  5063 solver.cpp:213] Iteration 6650, loss = 3.5761
I0615 16:03:33.724337  5063 solver.cpp:228]     Train net output #0: softmax = 3.5761 (* 1 = 3.5761 loss)
I0615 16:03:33.724342  5063 solver.cpp:473] Iteration 6650, lr = 0.0001
I0615 16:03:34.482678  5063 solver.cpp:213] Iteration 6660, loss = 3.76907
I0615 16:03:34.482698  5063 solver.cpp:228]     Train net output #0: softmax = 3.76907 (* 1 = 3.76907 loss)
I0615 16:03:34.482703  5063 solver.cpp:473] Iteration 6660, lr = 0.0001
I0615 16:03:35.240948  5063 solver.cpp:213] Iteration 6670, loss = 3.48205
I0615 16:03:35.240969  5063 solver.cpp:228]     Train net output #0: softmax = 3.48205 (* 1 = 3.48205 loss)
I0615 16:03:35.240973  5063 solver.cpp:473] Iteration 6670, lr = 0.0001
I0615 16:03:35.998899  5063 solver.cpp:213] Iteration 6680, loss = 3.59768
I0615 16:03:35.998919  5063 solver.cpp:228]     Train net output #0: softmax = 3.59768 (* 1 = 3.59768 loss)
I0615 16:03:35.998924  5063 solver.cpp:473] Iteration 6680, lr = 0.0001
I0615 16:03:36.756933  5063 solver.cpp:213] Iteration 6690, loss = 3.93348
I0615 16:03:36.756954  5063 solver.cpp:228]     Train net output #0: softmax = 3.93348 (* 1 = 3.93348 loss)
I0615 16:03:36.756959  5063 solver.cpp:473] Iteration 6690, lr = 0.0001
I0615 16:03:37.514767  5063 solver.cpp:213] Iteration 6700, loss = 3.90966
I0615 16:03:37.514787  5063 solver.cpp:228]     Train net output #0: softmax = 3.90966 (* 1 = 3.90966 loss)
I0615 16:03:37.514797  5063 solver.cpp:473] Iteration 6700, lr = 0.0001
I0615 16:03:38.273007  5063 solver.cpp:213] Iteration 6710, loss = 3.66788
I0615 16:03:38.273033  5063 solver.cpp:228]     Train net output #0: softmax = 3.66788 (* 1 = 3.66788 loss)
I0615 16:03:38.273176  5063 solver.cpp:473] Iteration 6710, lr = 0.0001
I0615 16:03:39.031134  5063 solver.cpp:213] Iteration 6720, loss = 3.6675
I0615 16:03:39.031155  5063 solver.cpp:228]     Train net output #0: softmax = 3.6675 (* 1 = 3.6675 loss)
I0615 16:03:39.031177  5063 solver.cpp:473] Iteration 6720, lr = 0.0001
I0615 16:03:39.789564  5063 solver.cpp:213] Iteration 6730, loss = 3.49892
I0615 16:03:39.789585  5063 solver.cpp:228]     Train net output #0: softmax = 3.49892 (* 1 = 3.49892 loss)
I0615 16:03:39.789590  5063 solver.cpp:473] Iteration 6730, lr = 0.0001
I0615 16:03:40.548579  5063 solver.cpp:213] Iteration 6740, loss = 3.43263
I0615 16:03:40.548611  5063 solver.cpp:228]     Train net output #0: softmax = 3.43263 (* 1 = 3.43263 loss)
I0615 16:03:40.548616  5063 solver.cpp:473] Iteration 6740, lr = 0.0001
I0615 16:03:41.307250  5063 solver.cpp:213] Iteration 6750, loss = 3.60755
I0615 16:03:41.307272  5063 solver.cpp:228]     Train net output #0: softmax = 3.60755 (* 1 = 3.60755 loss)
I0615 16:03:41.307276  5063 solver.cpp:473] Iteration 6750, lr = 0.0001
I0615 16:03:42.065613  5063 solver.cpp:213] Iteration 6760, loss = 3.5445
I0615 16:03:42.065634  5063 solver.cpp:228]     Train net output #0: softmax = 3.5445 (* 1 = 3.5445 loss)
I0615 16:03:42.065639  5063 solver.cpp:473] Iteration 6760, lr = 0.0001
I0615 16:03:42.824530  5063 solver.cpp:213] Iteration 6770, loss = 3.67775
I0615 16:03:42.824550  5063 solver.cpp:228]     Train net output #0: softmax = 3.67775 (* 1 = 3.67775 loss)
I0615 16:03:42.824555  5063 solver.cpp:473] Iteration 6770, lr = 0.0001
I0615 16:03:43.582710  5063 solver.cpp:213] Iteration 6780, loss = 3.63547
I0615 16:03:43.582734  5063 solver.cpp:228]     Train net output #0: softmax = 3.63547 (* 1 = 3.63547 loss)
I0615 16:03:43.582739  5063 solver.cpp:473] Iteration 6780, lr = 0.0001
I0615 16:03:44.341069  5063 solver.cpp:213] Iteration 6790, loss = 3.537
I0615 16:03:44.341090  5063 solver.cpp:228]     Train net output #0: softmax = 3.537 (* 1 = 3.537 loss)
I0615 16:03:44.341094  5063 solver.cpp:473] Iteration 6790, lr = 0.0001
I0615 16:03:45.097355  5063 solver.cpp:213] Iteration 6800, loss = 3.68419
I0615 16:03:45.097414  5063 solver.cpp:228]     Train net output #0: softmax = 3.68419 (* 1 = 3.68419 loss)
I0615 16:03:45.097420  5063 solver.cpp:473] Iteration 6800, lr = 0.0001
I0615 16:03:45.855626  5063 solver.cpp:213] Iteration 6810, loss = 3.50594
I0615 16:03:45.855648  5063 solver.cpp:228]     Train net output #0: softmax = 3.50594 (* 1 = 3.50594 loss)
I0615 16:03:45.855653  5063 solver.cpp:473] Iteration 6810, lr = 0.0001
I0615 16:03:46.613920  5063 solver.cpp:213] Iteration 6820, loss = 3.73206
I0615 16:03:46.613941  5063 solver.cpp:228]     Train net output #0: softmax = 3.73206 (* 1 = 3.73206 loss)
I0615 16:03:46.613946  5063 solver.cpp:473] Iteration 6820, lr = 0.0001
I0615 16:03:47.371966  5063 solver.cpp:213] Iteration 6830, loss = 3.77799
I0615 16:03:47.371987  5063 solver.cpp:228]     Train net output #0: softmax = 3.77799 (* 1 = 3.77799 loss)
I0615 16:03:47.371992  5063 solver.cpp:473] Iteration 6830, lr = 0.0001
I0615 16:03:48.130478  5063 solver.cpp:213] Iteration 6840, loss = 3.64401
I0615 16:03:48.130501  5063 solver.cpp:228]     Train net output #0: softmax = 3.64401 (* 1 = 3.64401 loss)
I0615 16:03:48.130506  5063 solver.cpp:473] Iteration 6840, lr = 0.0001
I0615 16:03:48.888826  5063 solver.cpp:213] Iteration 6850, loss = 3.54025
I0615 16:03:48.888852  5063 solver.cpp:228]     Train net output #0: softmax = 3.54025 (* 1 = 3.54025 loss)
I0615 16:03:48.888862  5063 solver.cpp:473] Iteration 6850, lr = 0.0001
I0615 16:03:49.647258  5063 solver.cpp:213] Iteration 6860, loss = 3.53448
I0615 16:03:49.647279  5063 solver.cpp:228]     Train net output #0: softmax = 3.53448 (* 1 = 3.53448 loss)
I0615 16:03:49.647290  5063 solver.cpp:473] Iteration 6860, lr = 0.0001
I0615 16:03:50.406077  5063 solver.cpp:213] Iteration 6870, loss = 3.59578
I0615 16:03:50.406097  5063 solver.cpp:228]     Train net output #0: softmax = 3.59578 (* 1 = 3.59578 loss)
I0615 16:03:50.406101  5063 solver.cpp:473] Iteration 6870, lr = 0.0001
I0615 16:03:51.165071  5063 solver.cpp:213] Iteration 6880, loss = 3.7952
I0615 16:03:51.165092  5063 solver.cpp:228]     Train net output #0: softmax = 3.7952 (* 1 = 3.7952 loss)
I0615 16:03:51.165096  5063 solver.cpp:473] Iteration 6880, lr = 0.0001
I0615 16:03:51.923300  5063 solver.cpp:213] Iteration 6890, loss = 3.65186
I0615 16:03:51.923322  5063 solver.cpp:228]     Train net output #0: softmax = 3.65186 (* 1 = 3.65186 loss)
I0615 16:03:51.923327  5063 solver.cpp:473] Iteration 6890, lr = 0.0001
I0615 16:03:52.681548  5063 solver.cpp:213] Iteration 6900, loss = 3.33265
I0615 16:03:52.681568  5063 solver.cpp:228]     Train net output #0: softmax = 3.33265 (* 1 = 3.33265 loss)
I0615 16:03:52.681573  5063 solver.cpp:473] Iteration 6900, lr = 0.0001
I0615 16:03:53.439357  5063 solver.cpp:213] Iteration 6910, loss = 3.36546
I0615 16:03:53.439383  5063 solver.cpp:228]     Train net output #0: softmax = 3.36546 (* 1 = 3.36546 loss)
I0615 16:03:53.439388  5063 solver.cpp:473] Iteration 6910, lr = 0.0001
I0615 16:03:54.195487  5063 solver.cpp:213] Iteration 6920, loss = 3.53114
I0615 16:03:54.195511  5063 solver.cpp:228]     Train net output #0: softmax = 3.53114 (* 1 = 3.53114 loss)
I0615 16:03:54.195638  5063 solver.cpp:473] Iteration 6920, lr = 0.0001
I0615 16:03:54.953438  5063 solver.cpp:213] Iteration 6930, loss = 3.58396
I0615 16:03:54.953459  5063 solver.cpp:228]     Train net output #0: softmax = 3.58396 (* 1 = 3.58396 loss)
I0615 16:03:54.953464  5063 solver.cpp:473] Iteration 6930, lr = 0.0001
I0615 16:03:55.712656  5063 solver.cpp:213] Iteration 6940, loss = 3.74468
I0615 16:03:55.712679  5063 solver.cpp:228]     Train net output #0: softmax = 3.74468 (* 1 = 3.74468 loss)
I0615 16:03:55.712683  5063 solver.cpp:473] Iteration 6940, lr = 0.0001
I0615 16:03:56.471961  5063 solver.cpp:213] Iteration 6950, loss = 3.68802
I0615 16:03:56.471981  5063 solver.cpp:228]     Train net output #0: softmax = 3.68802 (* 1 = 3.68802 loss)
I0615 16:03:56.471984  5063 solver.cpp:473] Iteration 6950, lr = 0.0001
I0615 16:03:57.230614  5063 solver.cpp:213] Iteration 6960, loss = 3.60503
I0615 16:03:57.230636  5063 solver.cpp:228]     Train net output #0: softmax = 3.60503 (* 1 = 3.60503 loss)
I0615 16:03:57.230660  5063 solver.cpp:473] Iteration 6960, lr = 0.0001
I0615 16:03:57.989696  5063 solver.cpp:213] Iteration 6970, loss = 3.35569
I0615 16:03:57.989714  5063 solver.cpp:228]     Train net output #0: softmax = 3.35569 (* 1 = 3.35569 loss)
I0615 16:03:57.989719  5063 solver.cpp:473] Iteration 6970, lr = 0.0001
I0615 16:03:58.748843  5063 solver.cpp:213] Iteration 6980, loss = 3.59828
I0615 16:03:58.748865  5063 solver.cpp:228]     Train net output #0: softmax = 3.59828 (* 1 = 3.59828 loss)
I0615 16:03:58.748869  5063 solver.cpp:473] Iteration 6980, lr = 0.0001
I0615 16:03:59.507472  5063 solver.cpp:213] Iteration 6990, loss = 3.49427
I0615 16:03:59.507498  5063 solver.cpp:228]     Train net output #0: softmax = 3.49427 (* 1 = 3.49427 loss)
I0615 16:03:59.507503  5063 solver.cpp:473] Iteration 6990, lr = 0.0001
I0615 16:04:00.212635  5063 solver.cpp:362] Snapshotting to snapshots/16-06-15_15h49m18s_0_11_pretrainClassification_iter_7000.caffemodel
I0615 16:04:00.213302  5063 solver.cpp:370] Snapshotting solver state to snapshots/16-06-15_15h49m18s_0_11_pretrainClassification_iter_7000.solverstate
I0615 16:04:00.213686  5063 solver.cpp:291] Iteration 7000, Testing net (#0)
I0615 16:04:00.308090  5063 solver.cpp:342]     Test net output #0: accuracy_top_1 = 0.16875
I0615 16:04:00.308107  5063 solver.cpp:342]     Test net output #1: accuracy_top_5 = 0.404687
I0615 16:04:00.308114  5063 solver.cpp:342]     Test net output #2: softmax = 3.62469 (* 1 = 3.62469 loss)
I0615 16:04:00.361744  5063 solver.cpp:213] Iteration 7000, loss = 3.54694
I0615 16:04:00.361758  5063 solver.cpp:228]     Train net output #0: softmax = 3.54694 (* 1 = 3.54694 loss)
I0615 16:04:00.361763  5063 solver.cpp:473] Iteration 7000, lr = 0.0001
I0615 16:04:01.120100  5063 solver.cpp:213] Iteration 7010, loss = 3.74844
I0615 16:04:01.120120  5063 solver.cpp:228]     Train net output #0: softmax = 3.74844 (* 1 = 3.74844 loss)
I0615 16:04:01.120124  5063 solver.cpp:473] Iteration 7010, lr = 0.0001
I0615 16:04:01.878989  5063 solver.cpp:213] Iteration 7020, loss = 3.70187
I0615 16:04:01.879010  5063 solver.cpp:228]     Train net output #0: softmax = 3.70187 (* 1 = 3.70187 loss)
I0615 16:04:01.879014  5063 solver.cpp:473] Iteration 7020, lr = 0.0001
I0615 16:04:02.637846  5063 solver.cpp:213] Iteration 7030, loss = 3.62808
I0615 16:04:02.637866  5063 solver.cpp:228]     Train net output #0: softmax = 3.62808 (* 1 = 3.62808 loss)
I0615 16:04:02.637871  5063 solver.cpp:473] Iteration 7030, lr = 0.0001
I0615 16:04:03.396456  5063 solver.cpp:213] Iteration 7040, loss = 3.43562
I0615 16:04:03.396476  5063 solver.cpp:228]     Train net output #0: softmax = 3.43562 (* 1 = 3.43562 loss)
I0615 16:04:03.396481  5063 solver.cpp:473] Iteration 7040, lr = 0.0001
I0615 16:04:04.155360  5063 solver.cpp:213] Iteration 7050, loss = 3.62729
I0615 16:04:04.155381  5063 solver.cpp:228]     Train net output #0: softmax = 3.62729 (* 1 = 3.62729 loss)
I0615 16:04:04.155385  5063 solver.cpp:473] Iteration 7050, lr = 0.0001
I0615 16:04:04.914811  5063 solver.cpp:213] Iteration 7060, loss = 3.58939
I0615 16:04:04.914849  5063 solver.cpp:228]     Train net output #0: softmax = 3.58939 (* 1 = 3.58939 loss)
I0615 16:04:04.914855  5063 solver.cpp:473] Iteration 7060, lr = 0.0001
I0615 16:04:05.673305  5063 solver.cpp:213] Iteration 7070, loss = 3.5656
I0615 16:04:05.673326  5063 solver.cpp:228]     Train net output #0: softmax = 3.5656 (* 1 = 3.5656 loss)
I0615 16:04:05.673331  5063 solver.cpp:473] Iteration 7070, lr = 0.0001
I0615 16:04:06.431704  5063 solver.cpp:213] Iteration 7080, loss = 3.69439
I0615 16:04:06.431725  5063 solver.cpp:228]     Train net output #0: softmax = 3.69439 (* 1 = 3.69439 loss)
I0615 16:04:06.431730  5063 solver.cpp:473] Iteration 7080, lr = 0.0001
I0615 16:04:07.189394  5063 solver.cpp:213] Iteration 7090, loss = 3.71455
I0615 16:04:07.189414  5063 solver.cpp:228]     Train net output #0: softmax = 3.71455 (* 1 = 3.71455 loss)
I0615 16:04:07.189419  5063 solver.cpp:473] Iteration 7090, lr = 0.0001
I0615 16:04:07.947798  5063 solver.cpp:213] Iteration 7100, loss = 3.60401
I0615 16:04:07.947819  5063 solver.cpp:228]     Train net output #0: softmax = 3.60401 (* 1 = 3.60401 loss)
I0615 16:04:07.947824  5063 solver.cpp:473] Iteration 7100, lr = 0.0001
I0615 16:04:08.707099  5063 solver.cpp:213] Iteration 7110, loss = 3.61131
I0615 16:04:08.707120  5063 solver.cpp:228]     Train net output #0: softmax = 3.61131 (* 1 = 3.61131 loss)
I0615 16:04:08.707125  5063 solver.cpp:473] Iteration 7110, lr = 0.0001
I0615 16:04:09.466295  5063 solver.cpp:213] Iteration 7120, loss = 3.62865
I0615 16:04:09.466315  5063 solver.cpp:228]     Train net output #0: softmax = 3.62865 (* 1 = 3.62865 loss)
I0615 16:04:09.466320  5063 solver.cpp:473] Iteration 7120, lr = 0.0001
I0615 16:04:10.225388  5063 solver.cpp:213] Iteration 7130, loss = 3.50912
I0615 16:04:10.225419  5063 solver.cpp:228]     Train net output #0: softmax = 3.50912 (* 1 = 3.50912 loss)
I0615 16:04:10.225426  5063 solver.cpp:473] Iteration 7130, lr = 0.0001
I0615 16:04:10.984097  5063 solver.cpp:213] Iteration 7140, loss = 3.53961
I0615 16:04:10.984117  5063 solver.cpp:228]     Train net output #0: softmax = 3.53961 (* 1 = 3.53961 loss)
I0615 16:04:10.984122  5063 solver.cpp:473] Iteration 7140, lr = 0.0001
I0615 16:04:11.742094  5063 solver.cpp:213] Iteration 7150, loss = 3.65317
I0615 16:04:11.742115  5063 solver.cpp:228]     Train net output #0: softmax = 3.65317 (* 1 = 3.65317 loss)
I0615 16:04:11.742120  5063 solver.cpp:473] Iteration 7150, lr = 0.0001
I0615 16:04:12.500694  5063 solver.cpp:213] Iteration 7160, loss = 3.57356
I0615 16:04:12.500721  5063 solver.cpp:228]     Train net output #0: softmax = 3.57356 (* 1 = 3.57356 loss)
I0615 16:04:12.500726  5063 solver.cpp:473] Iteration 7160, lr = 0.0001
I0615 16:04:13.258792  5063 solver.cpp:213] Iteration 7170, loss = 3.46021
I0615 16:04:13.258815  5063 solver.cpp:228]     Train net output #0: softmax = 3.46021 (* 1 = 3.46021 loss)
I0615 16:04:13.258819  5063 solver.cpp:473] Iteration 7170, lr = 0.0001
I0615 16:04:14.016921  5063 solver.cpp:213] Iteration 7180, loss = 3.6732
I0615 16:04:14.016942  5063 solver.cpp:228]     Train net output #0: softmax = 3.6732 (* 1 = 3.6732 loss)
I0615 16:04:14.016947  5063 solver.cpp:473] Iteration 7180, lr = 0.0001
I0615 16:04:14.776221  5063 solver.cpp:213] Iteration 7190, loss = 3.73449
I0615 16:04:14.776240  5063 solver.cpp:228]     Train net output #0: softmax = 3.73449 (* 1 = 3.73449 loss)
I0615 16:04:14.776245  5063 solver.cpp:473] Iteration 7190, lr = 0.0001
I0615 16:04:15.535610  5063 solver.cpp:213] Iteration 7200, loss = 3.38359
I0615 16:04:15.535672  5063 solver.cpp:228]     Train net output #0: softmax = 3.38359 (* 1 = 3.38359 loss)
I0615 16:04:15.535687  5063 solver.cpp:473] Iteration 7200, lr = 0.0001
I0615 16:04:16.294631  5063 solver.cpp:213] Iteration 7210, loss = 3.62759
I0615 16:04:16.294651  5063 solver.cpp:228]     Train net output #0: softmax = 3.62759 (* 1 = 3.62759 loss)
I0615 16:04:16.294656  5063 solver.cpp:473] Iteration 7210, lr = 0.0001
I0615 16:04:17.053208  5063 solver.cpp:213] Iteration 7220, loss = 3.70952
I0615 16:04:17.053231  5063 solver.cpp:228]     Train net output #0: softmax = 3.70952 (* 1 = 3.70952 loss)
I0615 16:04:17.053236  5063 solver.cpp:473] Iteration 7220, lr = 0.0001
I0615 16:04:17.811646  5063 solver.cpp:213] Iteration 7230, loss = 3.57419
I0615 16:04:17.811666  5063 solver.cpp:228]     Train net output #0: softmax = 3.57419 (* 1 = 3.57419 loss)
I0615 16:04:17.811671  5063 solver.cpp:473] Iteration 7230, lr = 0.0001
I0615 16:04:18.570704  5063 solver.cpp:213] Iteration 7240, loss = 3.68019
I0615 16:04:18.570725  5063 solver.cpp:228]     Train net output #0: softmax = 3.68019 (* 1 = 3.68019 loss)
I0615 16:04:18.570729  5063 solver.cpp:473] Iteration 7240, lr = 0.0001
I0615 16:04:19.330524  5063 solver.cpp:213] Iteration 7250, loss = 3.59668
I0615 16:04:19.330543  5063 solver.cpp:228]     Train net output #0: softmax = 3.59668 (* 1 = 3.59668 loss)
I0615 16:04:19.330549  5063 solver.cpp:473] Iteration 7250, lr = 0.0001
I0615 16:04:20.088960  5063 solver.cpp:213] Iteration 7260, loss = 3.63729
I0615 16:04:20.088981  5063 solver.cpp:228]     Train net output #0: softmax = 3.63729 (* 1 = 3.63729 loss)
I0615 16:04:20.088986  5063 solver.cpp:473] Iteration 7260, lr = 0.0001
I0615 16:04:20.847939  5063 solver.cpp:213] Iteration 7270, loss = 3.76758
I0615 16:04:20.847972  5063 solver.cpp:228]     Train net output #0: softmax = 3.76758 (* 1 = 3.76758 loss)
I0615 16:04:20.847980  5063 solver.cpp:473] Iteration 7270, lr = 0.0001
I0615 16:04:21.607592  5063 solver.cpp:213] Iteration 7280, loss = 3.53466
I0615 16:04:21.607614  5063 solver.cpp:228]     Train net output #0: softmax = 3.53466 (* 1 = 3.53466 loss)
I0615 16:04:21.607619  5063 solver.cpp:473] Iteration 7280, lr = 0.0001
I0615 16:04:22.366819  5063 solver.cpp:213] Iteration 7290, loss = 3.55072
I0615 16:04:22.366839  5063 solver.cpp:228]     Train net output #0: softmax = 3.55072 (* 1 = 3.55072 loss)
I0615 16:04:22.366844  5063 solver.cpp:473] Iteration 7290, lr = 0.0001
I0615 16:04:23.125344  5063 solver.cpp:213] Iteration 7300, loss = 3.47629
I0615 16:04:23.125365  5063 solver.cpp:228]     Train net output #0: softmax = 3.47629 (* 1 = 3.47629 loss)
I0615 16:04:23.125370  5063 solver.cpp:473] Iteration 7300, lr = 0.0001
I0615 16:04:23.884599  5063 solver.cpp:213] Iteration 7310, loss = 3.45039
I0615 16:04:23.884619  5063 solver.cpp:228]     Train net output #0: softmax = 3.45039 (* 1 = 3.45039 loss)
I0615 16:04:23.884624  5063 solver.cpp:473] Iteration 7310, lr = 0.0001
I0615 16:04:24.643410  5063 solver.cpp:213] Iteration 7320, loss = 3.64543
I0615 16:04:24.643437  5063 solver.cpp:228]     Train net output #0: softmax = 3.64543 (* 1 = 3.64543 loss)
I0615 16:04:24.643442  5063 solver.cpp:473] Iteration 7320, lr = 0.0001
I0615 16:04:25.401216  5063 solver.cpp:213] Iteration 7330, loss = 3.62468
I0615 16:04:25.401237  5063 solver.cpp:228]     Train net output #0: softmax = 3.62468 (* 1 = 3.62468 loss)
I0615 16:04:25.401242  5063 solver.cpp:473] Iteration 7330, lr = 0.0001
I0615 16:04:26.159235  5063 solver.cpp:213] Iteration 7340, loss = 3.75206
I0615 16:04:26.159266  5063 solver.cpp:228]     Train net output #0: softmax = 3.75206 (* 1 = 3.75206 loss)
I0615 16:04:26.159277  5063 solver.cpp:473] Iteration 7340, lr = 0.0001
I0615 16:04:26.917938  5063 solver.cpp:213] Iteration 7350, loss = 3.63042
I0615 16:04:26.917958  5063 solver.cpp:228]     Train net output #0: softmax = 3.63042 (* 1 = 3.63042 loss)
I0615 16:04:26.917963  5063 solver.cpp:473] Iteration 7350, lr = 0.0001
I0615 16:04:27.676277  5063 solver.cpp:213] Iteration 7360, loss = 3.51358
I0615 16:04:27.676302  5063 solver.cpp:228]     Train net output #0: softmax = 3.51358 (* 1 = 3.51358 loss)
I0615 16:04:27.676323  5063 solver.cpp:473] Iteration 7360, lr = 0.0001
I0615 16:04:28.434376  5063 solver.cpp:213] Iteration 7370, loss = 3.72
I0615 16:04:28.434398  5063 solver.cpp:228]     Train net output #0: softmax = 3.72 (* 1 = 3.72 loss)
I0615 16:04:28.434402  5063 solver.cpp:473] Iteration 7370, lr = 0.0001
I0615 16:04:29.192509  5063 solver.cpp:213] Iteration 7380, loss = 3.47579
I0615 16:04:29.192530  5063 solver.cpp:228]     Train net output #0: softmax = 3.47579 (* 1 = 3.47579 loss)
I0615 16:04:29.192535  5063 solver.cpp:473] Iteration 7380, lr = 0.0001
I0615 16:04:29.951288  5063 solver.cpp:213] Iteration 7390, loss = 3.6415
I0615 16:04:29.951308  5063 solver.cpp:228]     Train net output #0: softmax = 3.6415 (* 1 = 3.6415 loss)
I0615 16:04:29.951313  5063 solver.cpp:473] Iteration 7390, lr = 0.0001
I0615 16:04:30.709728  5063 solver.cpp:213] Iteration 7400, loss = 3.70609
I0615 16:04:30.709748  5063 solver.cpp:228]     Train net output #0: softmax = 3.70609 (* 1 = 3.70609 loss)
I0615 16:04:30.709751  5063 solver.cpp:473] Iteration 7400, lr = 0.0001
I0615 16:04:31.468698  5063 solver.cpp:213] Iteration 7410, loss = 3.50299
I0615 16:04:31.468720  5063 solver.cpp:228]     Train net output #0: softmax = 3.50299 (* 1 = 3.50299 loss)
I0615 16:04:31.468725  5063 solver.cpp:473] Iteration 7410, lr = 0.0001
I0615 16:04:32.227061  5063 solver.cpp:213] Iteration 7420, loss = 3.85329
I0615 16:04:32.227082  5063 solver.cpp:228]     Train net output #0: softmax = 3.85329 (* 1 = 3.85329 loss)
I0615 16:04:32.227087  5063 solver.cpp:473] Iteration 7420, lr = 0.0001
I0615 16:04:32.985704  5063 solver.cpp:213] Iteration 7430, loss = 3.44186
I0615 16:04:32.985725  5063 solver.cpp:228]     Train net output #0: softmax = 3.44186 (* 1 = 3.44186 loss)
I0615 16:04:32.985729  5063 solver.cpp:473] Iteration 7430, lr = 0.0001
I0615 16:04:33.744456  5063 solver.cpp:213] Iteration 7440, loss = 3.71706
I0615 16:04:33.744477  5063 solver.cpp:228]     Train net output #0: softmax = 3.71706 (* 1 = 3.71706 loss)
I0615 16:04:33.744482  5063 solver.cpp:473] Iteration 7440, lr = 0.0001
I0615 16:04:34.503064  5063 solver.cpp:213] Iteration 7450, loss = 3.73252
I0615 16:04:34.503084  5063 solver.cpp:228]     Train net output #0: softmax = 3.73252 (* 1 = 3.73252 loss)
I0615 16:04:34.503088  5063 solver.cpp:473] Iteration 7450, lr = 0.0001
I0615 16:04:35.261277  5063 solver.cpp:213] Iteration 7460, loss = 3.60438
I0615 16:04:35.261297  5063 solver.cpp:228]     Train net output #0: softmax = 3.60438 (* 1 = 3.60438 loss)
I0615 16:04:35.261301  5063 solver.cpp:473] Iteration 7460, lr = 0.0001
I0615 16:04:36.019522  5063 solver.cpp:213] Iteration 7470, loss = 3.58125
I0615 16:04:36.019544  5063 solver.cpp:228]     Train net output #0: softmax = 3.58125 (* 1 = 3.58125 loss)
I0615 16:04:36.019549  5063 solver.cpp:473] Iteration 7470, lr = 0.0001
I0615 16:04:36.778275  5063 solver.cpp:213] Iteration 7480, loss = 3.62466
I0615 16:04:36.778304  5063 solver.cpp:228]     Train net output #0: softmax = 3.62466 (* 1 = 3.62466 loss)
I0615 16:04:36.778311  5063 solver.cpp:473] Iteration 7480, lr = 0.0001
I0615 16:04:37.536479  5063 solver.cpp:213] Iteration 7490, loss = 3.72374
I0615 16:04:37.536499  5063 solver.cpp:228]     Train net output #0: softmax = 3.72374 (* 1 = 3.72374 loss)
I0615 16:04:37.536504  5063 solver.cpp:473] Iteration 7490, lr = 0.0001
I0615 16:04:38.295644  5063 solver.cpp:213] Iteration 7500, loss = 3.58382
I0615 16:04:38.295663  5063 solver.cpp:228]     Train net output #0: softmax = 3.58382 (* 1 = 3.58382 loss)
I0615 16:04:38.295668  5063 solver.cpp:473] Iteration 7500, lr = 0.0001
I0615 16:04:39.054719  5063 solver.cpp:213] Iteration 7510, loss = 3.48684
I0615 16:04:39.054741  5063 solver.cpp:228]     Train net output #0: softmax = 3.48684 (* 1 = 3.48684 loss)
I0615 16:04:39.054745  5063 solver.cpp:473] Iteration 7510, lr = 0.0001
I0615 16:04:39.812903  5063 solver.cpp:213] Iteration 7520, loss = 3.52825
I0615 16:04:39.812924  5063 solver.cpp:228]     Train net output #0: softmax = 3.52825 (* 1 = 3.52825 loss)
I0615 16:04:39.812944  5063 solver.cpp:473] Iteration 7520, lr = 0.0001
I0615 16:04:40.571108  5063 solver.cpp:213] Iteration 7530, loss = 3.48635
I0615 16:04:40.571130  5063 solver.cpp:228]     Train net output #0: softmax = 3.48635 (* 1 = 3.48635 loss)
I0615 16:04:40.571135  5063 solver.cpp:473] Iteration 7530, lr = 0.0001
I0615 16:04:41.329257  5063 solver.cpp:213] Iteration 7540, loss = 3.65711
I0615 16:04:41.329277  5063 solver.cpp:228]     Train net output #0: softmax = 3.65711 (* 1 = 3.65711 loss)
I0615 16:04:41.329282  5063 solver.cpp:473] Iteration 7540, lr = 0.0001
I0615 16:04:42.087512  5063 solver.cpp:213] Iteration 7550, loss = 3.69838
I0615 16:04:42.087544  5063 solver.cpp:228]     Train net output #0: softmax = 3.69838 (* 1 = 3.69838 loss)
I0615 16:04:42.087553  5063 solver.cpp:473] Iteration 7550, lr = 0.0001
I0615 16:04:42.846418  5063 solver.cpp:213] Iteration 7560, loss = 3.59728
I0615 16:04:42.846438  5063 solver.cpp:228]     Train net output #0: softmax = 3.59728 (* 1 = 3.59728 loss)
I0615 16:04:42.846443  5063 solver.cpp:473] Iteration 7560, lr = 0.0001
I0615 16:04:43.605587  5063 solver.cpp:213] Iteration 7570, loss = 3.7672
I0615 16:04:43.605607  5063 solver.cpp:228]     Train net output #0: softmax = 3.7672 (* 1 = 3.7672 loss)
I0615 16:04:43.605612  5063 solver.cpp:473] Iteration 7570, lr = 0.0001
I0615 16:04:44.364408  5063 solver.cpp:213] Iteration 7580, loss = 3.67819
I0615 16:04:44.364429  5063 solver.cpp:228]     Train net output #0: softmax = 3.67819 (* 1 = 3.67819 loss)
I0615 16:04:44.364434  5063 solver.cpp:473] Iteration 7580, lr = 0.0001
I0615 16:04:45.123183  5063 solver.cpp:213] Iteration 7590, loss = 3.60505
I0615 16:04:45.123204  5063 solver.cpp:228]     Train net output #0: softmax = 3.60505 (* 1 = 3.60505 loss)
I0615 16:04:45.123209  5063 solver.cpp:473] Iteration 7590, lr = 0.0001
I0615 16:04:45.881057  5063 solver.cpp:213] Iteration 7600, loss = 3.57369
I0615 16:04:45.881115  5063 solver.cpp:228]     Train net output #0: softmax = 3.57369 (* 1 = 3.57369 loss)
I0615 16:04:45.881130  5063 solver.cpp:473] Iteration 7600, lr = 0.0001
I0615 16:04:46.639686  5063 solver.cpp:213] Iteration 7610, loss = 3.55719
I0615 16:04:46.639708  5063 solver.cpp:228]     Train net output #0: softmax = 3.55719 (* 1 = 3.55719 loss)
I0615 16:04:46.639713  5063 solver.cpp:473] Iteration 7610, lr = 0.0001
I0615 16:04:47.398296  5063 solver.cpp:213] Iteration 7620, loss = 3.39711
I0615 16:04:47.398329  5063 solver.cpp:228]     Train net output #0: softmax = 3.39711 (* 1 = 3.39711 loss)
I0615 16:04:47.398336  5063 solver.cpp:473] Iteration 7620, lr = 0.0001
I0615 16:04:48.157390  5063 solver.cpp:213] Iteration 7630, loss = 3.64132
I0615 16:04:48.157413  5063 solver.cpp:228]     Train net output #0: softmax = 3.64132 (* 1 = 3.64132 loss)
I0615 16:04:48.157418  5063 solver.cpp:473] Iteration 7630, lr = 0.0001
I0615 16:04:48.915750  5063 solver.cpp:213] Iteration 7640, loss = 3.59354
I0615 16:04:48.915777  5063 solver.cpp:228]     Train net output #0: softmax = 3.59354 (* 1 = 3.59354 loss)
I0615 16:04:48.915782  5063 solver.cpp:473] Iteration 7640, lr = 0.0001
I0615 16:04:49.674129  5063 solver.cpp:213] Iteration 7650, loss = 3.62272
I0615 16:04:49.674149  5063 solver.cpp:228]     Train net output #0: softmax = 3.62272 (* 1 = 3.62272 loss)
I0615 16:04:49.674154  5063 solver.cpp:473] Iteration 7650, lr = 0.0001
I0615 16:04:50.432961  5063 solver.cpp:213] Iteration 7660, loss = 3.69867
I0615 16:04:50.432981  5063 solver.cpp:228]     Train net output #0: softmax = 3.69867 (* 1 = 3.69867 loss)
I0615 16:04:50.432986  5063 solver.cpp:473] Iteration 7660, lr = 0.0001
I0615 16:04:51.192191  5063 solver.cpp:213] Iteration 7670, loss = 3.46938
I0615 16:04:51.192210  5063 solver.cpp:228]     Train net output #0: softmax = 3.46938 (* 1 = 3.46938 loss)
I0615 16:04:51.192215  5063 solver.cpp:473] Iteration 7670, lr = 0.0001
I0615 16:04:51.950006  5063 solver.cpp:213] Iteration 7680, loss = 3.59212
I0615 16:04:51.950026  5063 solver.cpp:228]     Train net output #0: softmax = 3.59212 (* 1 = 3.59212 loss)
I0615 16:04:51.950031  5063 solver.cpp:473] Iteration 7680, lr = 0.0001
I0615 16:04:52.708652  5063 solver.cpp:213] Iteration 7690, loss = 3.60865
I0615 16:04:52.708675  5063 solver.cpp:228]     Train net output #0: softmax = 3.60865 (* 1 = 3.60865 loss)
I0615 16:04:52.708680  5063 solver.cpp:473] Iteration 7690, lr = 0.0001
I0615 16:04:53.466987  5063 solver.cpp:213] Iteration 7700, loss = 3.5868
I0615 16:04:53.467007  5063 solver.cpp:228]     Train net output #0: softmax = 3.5868 (* 1 = 3.5868 loss)
I0615 16:04:53.467012  5063 solver.cpp:473] Iteration 7700, lr = 0.0001
I0615 16:04:54.225867  5063 solver.cpp:213] Iteration 7710, loss = 3.75977
I0615 16:04:54.225886  5063 solver.cpp:228]     Train net output #0: softmax = 3.75977 (* 1 = 3.75977 loss)
I0615 16:04:54.225891  5063 solver.cpp:473] Iteration 7710, lr = 0.0001
I0615 16:04:54.985028  5063 solver.cpp:213] Iteration 7720, loss = 3.54045
I0615 16:04:54.985047  5063 solver.cpp:228]     Train net output #0: softmax = 3.54045 (* 1 = 3.54045 loss)
I0615 16:04:54.985051  5063 solver.cpp:473] Iteration 7720, lr = 0.0001
I0615 16:04:55.743640  5063 solver.cpp:213] Iteration 7730, loss = 3.77999
I0615 16:04:55.743659  5063 solver.cpp:228]     Train net output #0: softmax = 3.77999 (* 1 = 3.77999 loss)
I0615 16:04:55.743664  5063 solver.cpp:473] Iteration 7730, lr = 0.0001
I0615 16:04:56.502468  5063 solver.cpp:213] Iteration 7740, loss = 3.62461
I0615 16:04:56.502486  5063 solver.cpp:228]     Train net output #0: softmax = 3.62461 (* 1 = 3.62461 loss)
I0615 16:04:56.502491  5063 solver.cpp:473] Iteration 7740, lr = 0.0001
I0615 16:04:57.252645  5063 solver.cpp:213] Iteration 7750, loss = 3.70999
I0615 16:04:57.252668  5063 solver.cpp:228]     Train net output #0: softmax = 3.70999 (* 1 = 3.70999 loss)
I0615 16:04:57.252674  5063 solver.cpp:473] Iteration 7750, lr = 0.0001
I0615 16:04:58.011009  5063 solver.cpp:213] Iteration 7760, loss = 3.61547
I0615 16:04:58.011041  5063 solver.cpp:228]     Train net output #0: softmax = 3.61547 (* 1 = 3.61547 loss)
I0615 16:04:58.011183  5063 solver.cpp:473] Iteration 7760, lr = 0.0001
I0615 16:04:58.769840  5063 solver.cpp:213] Iteration 7770, loss = 3.58999
I0615 16:04:58.769861  5063 solver.cpp:228]     Train net output #0: softmax = 3.58999 (* 1 = 3.58999 loss)
I0615 16:04:58.769866  5063 solver.cpp:473] Iteration 7770, lr = 0.0001
I0615 16:04:59.528100  5063 solver.cpp:213] Iteration 7780, loss = 3.66423
I0615 16:04:59.528120  5063 solver.cpp:228]     Train net output #0: softmax = 3.66423 (* 1 = 3.66423 loss)
I0615 16:04:59.528125  5063 solver.cpp:473] Iteration 7780, lr = 0.0001
I0615 16:05:00.286934  5063 solver.cpp:213] Iteration 7790, loss = 3.80943
I0615 16:05:00.286957  5063 solver.cpp:228]     Train net output #0: softmax = 3.80943 (* 1 = 3.80943 loss)
I0615 16:05:00.286962  5063 solver.cpp:473] Iteration 7790, lr = 0.0001
I0615 16:05:01.045094  5063 solver.cpp:213] Iteration 7800, loss = 3.61223
I0615 16:05:01.045115  5063 solver.cpp:228]     Train net output #0: softmax = 3.61223 (* 1 = 3.61223 loss)
I0615 16:05:01.045127  5063 solver.cpp:473] Iteration 7800, lr = 0.0001
I0615 16:05:01.803164  5063 solver.cpp:213] Iteration 7810, loss = 3.75737
I0615 16:05:01.803184  5063 solver.cpp:228]     Train net output #0: softmax = 3.75737 (* 1 = 3.75737 loss)
I0615 16:05:01.803189  5063 solver.cpp:473] Iteration 7810, lr = 0.0001
I0615 16:05:02.561323  5063 solver.cpp:213] Iteration 7820, loss = 3.62773
I0615 16:05:02.561344  5063 solver.cpp:228]     Train net output #0: softmax = 3.62773 (* 1 = 3.62773 loss)
I0615 16:05:02.561349  5063 solver.cpp:473] Iteration 7820, lr = 0.0001
I0615 16:05:03.319258  5063 solver.cpp:213] Iteration 7830, loss = 3.42992
I0615 16:05:03.319288  5063 solver.cpp:228]     Train net output #0: softmax = 3.42992 (* 1 = 3.42992 loss)
I0615 16:05:03.319437  5063 solver.cpp:473] Iteration 7830, lr = 0.0001
I0615 16:05:04.076988  5063 solver.cpp:213] Iteration 7840, loss = 3.7646
I0615 16:05:04.077009  5063 solver.cpp:228]     Train net output #0: softmax = 3.7646 (* 1 = 3.7646 loss)
I0615 16:05:04.077014  5063 solver.cpp:473] Iteration 7840, lr = 0.0001
I0615 16:05:04.834739  5063 solver.cpp:213] Iteration 7850, loss = 3.67383
I0615 16:05:04.834759  5063 solver.cpp:228]     Train net output #0: softmax = 3.67383 (* 1 = 3.67383 loss)
I0615 16:05:04.834764  5063 solver.cpp:473] Iteration 7850, lr = 0.0001
I0615 16:05:05.592574  5063 solver.cpp:213] Iteration 7860, loss = 3.48111
I0615 16:05:05.592595  5063 solver.cpp:228]     Train net output #0: softmax = 3.48111 (* 1 = 3.48111 loss)
I0615 16:05:05.592600  5063 solver.cpp:473] Iteration 7860, lr = 0.0001
I0615 16:05:06.350980  5063 solver.cpp:213] Iteration 7870, loss = 3.6444
I0615 16:05:06.351001  5063 solver.cpp:228]     Train net output #0: softmax = 3.6444 (* 1 = 3.6444 loss)
I0615 16:05:06.351006  5063 solver.cpp:473] Iteration 7870, lr = 0.0001
I0615 16:05:07.109091  5063 solver.cpp:213] Iteration 7880, loss = 3.59491
I0615 16:05:07.109110  5063 solver.cpp:228]     Train net output #0: softmax = 3.59491 (* 1 = 3.59491 loss)
I0615 16:05:07.109115  5063 solver.cpp:473] Iteration 7880, lr = 0.0001
I0615 16:05:07.867100  5063 solver.cpp:213] Iteration 7890, loss = 3.768
I0615 16:05:07.867120  5063 solver.cpp:228]     Train net output #0: softmax = 3.768 (* 1 = 3.768 loss)
I0615 16:05:07.867125  5063 solver.cpp:473] Iteration 7890, lr = 0.0001
I0615 16:05:08.625218  5063 solver.cpp:213] Iteration 7900, loss = 3.44462
I0615 16:05:08.625241  5063 solver.cpp:228]     Train net output #0: softmax = 3.44462 (* 1 = 3.44462 loss)
I0615 16:05:08.625247  5063 solver.cpp:473] Iteration 7900, lr = 0.0001
I0615 16:05:09.383549  5063 solver.cpp:213] Iteration 7910, loss = 3.63082
I0615 16:05:09.383569  5063 solver.cpp:228]     Train net output #0: softmax = 3.63082 (* 1 = 3.63082 loss)
I0615 16:05:09.383574  5063 solver.cpp:473] Iteration 7910, lr = 0.0001
I0615 16:05:10.141597  5063 solver.cpp:213] Iteration 7920, loss = 3.55614
I0615 16:05:10.141618  5063 solver.cpp:228]     Train net output #0: softmax = 3.55614 (* 1 = 3.55614 loss)
I0615 16:05:10.141641  5063 solver.cpp:473] Iteration 7920, lr = 0.0001
I0615 16:05:10.899624  5063 solver.cpp:213] Iteration 7930, loss = 3.74787
I0615 16:05:10.899646  5063 solver.cpp:228]     Train net output #0: softmax = 3.74787 (* 1 = 3.74787 loss)
I0615 16:05:10.899649  5063 solver.cpp:473] Iteration 7930, lr = 0.0001
I0615 16:05:11.658088  5063 solver.cpp:213] Iteration 7940, loss = 3.54545
I0615 16:05:11.658110  5063 solver.cpp:228]     Train net output #0: softmax = 3.54545 (* 1 = 3.54545 loss)
I0615 16:05:11.658115  5063 solver.cpp:473] Iteration 7940, lr = 0.0001
I0615 16:05:12.416146  5063 solver.cpp:213] Iteration 7950, loss = 3.59138
I0615 16:05:12.416167  5063 solver.cpp:228]     Train net output #0: softmax = 3.59138 (* 1 = 3.59138 loss)
I0615 16:05:12.416172  5063 solver.cpp:473] Iteration 7950, lr = 0.0001
I0615 16:05:13.174141  5063 solver.cpp:213] Iteration 7960, loss = 3.71704
I0615 16:05:13.174162  5063 solver.cpp:228]     Train net output #0: softmax = 3.71704 (* 1 = 3.71704 loss)
I0615 16:05:13.174173  5063 solver.cpp:473] Iteration 7960, lr = 0.0001
I0615 16:05:13.932054  5063 solver.cpp:213] Iteration 7970, loss = 3.40296
I0615 16:05:13.932078  5063 solver.cpp:228]     Train net output #0: softmax = 3.40296 (* 1 = 3.40296 loss)
I0615 16:05:13.932210  5063 solver.cpp:473] Iteration 7970, lr = 0.0001
I0615 16:05:14.690012  5063 solver.cpp:213] Iteration 7980, loss = 3.61212
I0615 16:05:14.690032  5063 solver.cpp:228]     Train net output #0: softmax = 3.61212 (* 1 = 3.61212 loss)
I0615 16:05:14.690037  5063 solver.cpp:473] Iteration 7980, lr = 0.0001
I0615 16:05:15.448257  5063 solver.cpp:213] Iteration 7990, loss = 3.61375
I0615 16:05:15.448279  5063 solver.cpp:228]     Train net output #0: softmax = 3.61375 (* 1 = 3.61375 loss)
I0615 16:05:15.448284  5063 solver.cpp:473] Iteration 7990, lr = 0.0001
I0615 16:05:16.153100  5063 solver.cpp:362] Snapshotting to snapshots/16-06-15_15h49m18s_0_11_pretrainClassification_iter_8000.caffemodel
I0615 16:05:16.153825  5063 solver.cpp:370] Snapshotting solver state to snapshots/16-06-15_15h49m18s_0_11_pretrainClassification_iter_8000.solverstate
I0615 16:05:16.154201  5063 solver.cpp:291] Iteration 8000, Testing net (#0)
I0615 16:05:16.248595  5063 solver.cpp:342]     Test net output #0: accuracy_top_1 = 0.1875
I0615 16:05:16.248611  5063 solver.cpp:342]     Test net output #1: accuracy_top_5 = 0.41875
I0615 16:05:16.248618  5063 solver.cpp:342]     Test net output #2: softmax = 3.52558 (* 1 = 3.52558 loss)
I0615 16:05:16.302197  5063 solver.cpp:213] Iteration 8000, loss = 3.57119
I0615 16:05:16.302211  5063 solver.cpp:228]     Train net output #0: softmax = 3.57119 (* 1 = 3.57119 loss)
I0615 16:05:16.302217  5063 solver.cpp:473] Iteration 8000, lr = 0.0001
I0615 16:05:17.060166  5063 solver.cpp:213] Iteration 8010, loss = 3.4343
I0615 16:05:17.060186  5063 solver.cpp:228]     Train net output #0: softmax = 3.4343 (* 1 = 3.4343 loss)
I0615 16:05:17.060190  5063 solver.cpp:473] Iteration 8010, lr = 0.0001
I0615 16:05:17.818099  5063 solver.cpp:213] Iteration 8020, loss = 3.40134
I0615 16:05:17.818120  5063 solver.cpp:228]     Train net output #0: softmax = 3.40134 (* 1 = 3.40134 loss)
I0615 16:05:17.818125  5063 solver.cpp:473] Iteration 8020, lr = 0.0001
I0615 16:05:18.576009  5063 solver.cpp:213] Iteration 8030, loss = 3.48921
I0615 16:05:18.576031  5063 solver.cpp:228]     Train net output #0: softmax = 3.48921 (* 1 = 3.48921 loss)
I0615 16:05:18.576036  5063 solver.cpp:473] Iteration 8030, lr = 0.0001
I0615 16:05:19.333513  5063 solver.cpp:213] Iteration 8040, loss = 3.79588
I0615 16:05:19.333537  5063 solver.cpp:228]     Train net output #0: softmax = 3.79588 (* 1 = 3.79588 loss)
I0615 16:05:19.333739  5063 solver.cpp:473] Iteration 8040, lr = 0.0001
I0615 16:05:20.091200  5063 solver.cpp:213] Iteration 8050, loss = 3.5324
I0615 16:05:20.091222  5063 solver.cpp:228]     Train net output #0: softmax = 3.5324 (* 1 = 3.5324 loss)
I0615 16:05:20.091226  5063 solver.cpp:473] Iteration 8050, lr = 0.0001
I0615 16:05:20.849088  5063 solver.cpp:213] Iteration 8060, loss = 3.37472
I0615 16:05:20.849109  5063 solver.cpp:228]     Train net output #0: softmax = 3.37472 (* 1 = 3.37472 loss)
I0615 16:05:20.849114  5063 solver.cpp:473] Iteration 8060, lr = 0.0001
I0615 16:05:21.606916  5063 solver.cpp:213] Iteration 8070, loss = 3.75348
I0615 16:05:21.606936  5063 solver.cpp:228]     Train net output #0: softmax = 3.75348 (* 1 = 3.75348 loss)
I0615 16:05:21.606941  5063 solver.cpp:473] Iteration 8070, lr = 0.0001
I0615 16:05:22.364897  5063 solver.cpp:213] Iteration 8080, loss = 3.64859
I0615 16:05:22.364917  5063 solver.cpp:228]     Train net output #0: softmax = 3.64859 (* 1 = 3.64859 loss)
I0615 16:05:22.364923  5063 solver.cpp:473] Iteration 8080, lr = 0.0001
I0615 16:05:23.122792  5063 solver.cpp:213] Iteration 8090, loss = 3.52651
I0615 16:05:23.122813  5063 solver.cpp:228]     Train net output #0: softmax = 3.52651 (* 1 = 3.52651 loss)
I0615 16:05:23.122817  5063 solver.cpp:473] Iteration 8090, lr = 0.0001
I0615 16:05:23.880849  5063 solver.cpp:213] Iteration 8100, loss = 3.56245
I0615 16:05:23.880872  5063 solver.cpp:228]     Train net output #0: softmax = 3.56245 (* 1 = 3.56245 loss)
I0615 16:05:23.880877  5063 solver.cpp:473] Iteration 8100, lr = 0.0001
I0615 16:05:24.638998  5063 solver.cpp:213] Iteration 8110, loss = 3.50419
I0615 16:05:24.639020  5063 solver.cpp:228]     Train net output #0: softmax = 3.50419 (* 1 = 3.50419 loss)
I0615 16:05:24.639025  5063 solver.cpp:473] Iteration 8110, lr = 0.0001
I0615 16:05:25.397332  5063 solver.cpp:213] Iteration 8120, loss = 3.62333
I0615 16:05:25.397354  5063 solver.cpp:228]     Train net output #0: softmax = 3.62333 (* 1 = 3.62333 loss)
I0615 16:05:25.397359  5063 solver.cpp:473] Iteration 8120, lr = 0.0001
I0615 16:05:26.155594  5063 solver.cpp:213] Iteration 8130, loss = 3.5274
I0615 16:05:26.155614  5063 solver.cpp:228]     Train net output #0: softmax = 3.5274 (* 1 = 3.5274 loss)
I0615 16:05:26.155618  5063 solver.cpp:473] Iteration 8130, lr = 0.0001
I0615 16:05:26.913483  5063 solver.cpp:213] Iteration 8140, loss = 3.79517
I0615 16:05:26.913522  5063 solver.cpp:228]     Train net output #0: softmax = 3.79517 (* 1 = 3.79517 loss)
I0615 16:05:26.913527  5063 solver.cpp:473] Iteration 8140, lr = 0.0001
I0615 16:05:27.672199  5063 solver.cpp:213] Iteration 8150, loss = 3.58849
I0615 16:05:27.672222  5063 solver.cpp:228]     Train net output #0: softmax = 3.58849 (* 1 = 3.58849 loss)
I0615 16:05:27.672227  5063 solver.cpp:473] Iteration 8150, lr = 0.0001
I0615 16:05:28.430663  5063 solver.cpp:213] Iteration 8160, loss = 3.48435
I0615 16:05:28.430685  5063 solver.cpp:228]     Train net output #0: softmax = 3.48435 (* 1 = 3.48435 loss)
I0615 16:05:28.430691  5063 solver.cpp:473] Iteration 8160, lr = 0.0001
I0615 16:05:29.189105  5063 solver.cpp:213] Iteration 8170, loss = 3.63105
I0615 16:05:29.189126  5063 solver.cpp:228]     Train net output #0: softmax = 3.63105 (* 1 = 3.63105 loss)
I0615 16:05:29.189131  5063 solver.cpp:473] Iteration 8170, lr = 0.0001
I0615 16:05:29.945719  5063 solver.cpp:213] Iteration 8180, loss = 3.66848
I0615 16:05:29.945742  5063 solver.cpp:228]     Train net output #0: softmax = 3.66848 (* 1 = 3.66848 loss)
I0615 16:05:29.945878  5063 solver.cpp:473] Iteration 8180, lr = 0.0001
I0615 16:05:30.702971  5063 solver.cpp:213] Iteration 8190, loss = 3.70758
I0615 16:05:30.703002  5063 solver.cpp:228]     Train net output #0: softmax = 3.70758 (* 1 = 3.70758 loss)
I0615 16:05:30.703007  5063 solver.cpp:473] Iteration 8190, lr = 0.0001
I0615 16:05:31.460279  5063 solver.cpp:213] Iteration 8200, loss = 3.67935
I0615 16:05:31.460301  5063 solver.cpp:228]     Train net output #0: softmax = 3.67935 (* 1 = 3.67935 loss)
I0615 16:05:31.460305  5063 solver.cpp:473] Iteration 8200, lr = 0.0001
I0615 16:05:32.218374  5063 solver.cpp:213] Iteration 8210, loss = 3.54608
I0615 16:05:32.218400  5063 solver.cpp:228]     Train net output #0: softmax = 3.54608 (* 1 = 3.54608 loss)
I0615 16:05:32.218405  5063 solver.cpp:473] Iteration 8210, lr = 0.0001
I0615 16:05:32.976533  5063 solver.cpp:213] Iteration 8220, loss = 3.54643
I0615 16:05:32.976554  5063 solver.cpp:228]     Train net output #0: softmax = 3.54643 (* 1 = 3.54643 loss)
I0615 16:05:32.976558  5063 solver.cpp:473] Iteration 8220, lr = 0.0001
I0615 16:05:33.734925  5063 solver.cpp:213] Iteration 8230, loss = 3.74091
I0615 16:05:33.734951  5063 solver.cpp:228]     Train net output #0: softmax = 3.74091 (* 1 = 3.74091 loss)
I0615 16:05:33.734956  5063 solver.cpp:473] Iteration 8230, lr = 0.0001
I0615 16:05:34.493530  5063 solver.cpp:213] Iteration 8240, loss = 3.70711
I0615 16:05:34.493551  5063 solver.cpp:228]     Train net output #0: softmax = 3.70711 (* 1 = 3.70711 loss)
I0615 16:05:34.493554  5063 solver.cpp:473] Iteration 8240, lr = 0.0001
I0615 16:05:35.251847  5063 solver.cpp:213] Iteration 8250, loss = 3.46417
I0615 16:05:35.251873  5063 solver.cpp:228]     Train net output #0: softmax = 3.46417 (* 1 = 3.46417 loss)
I0615 16:05:35.252022  5063 solver.cpp:473] Iteration 8250, lr = 0.0001
I0615 16:05:36.009886  5063 solver.cpp:213] Iteration 8260, loss = 3.76067
I0615 16:05:36.009913  5063 solver.cpp:228]     Train net output #0: softmax = 3.76067 (* 1 = 3.76067 loss)
I0615 16:05:36.009919  5063 solver.cpp:473] Iteration 8260, lr = 0.0001
I0615 16:05:36.767521  5063 solver.cpp:213] Iteration 8270, loss = 3.67444
I0615 16:05:36.767542  5063 solver.cpp:228]     Train net output #0: softmax = 3.67444 (* 1 = 3.67444 loss)
I0615 16:05:36.767547  5063 solver.cpp:473] Iteration 8270, lr = 0.0001
I0615 16:05:37.526052  5063 solver.cpp:213] Iteration 8280, loss = 3.46632
I0615 16:05:37.526073  5063 solver.cpp:228]     Train net output #0: softmax = 3.46632 (* 1 = 3.46632 loss)
I0615 16:05:37.526077  5063 solver.cpp:473] Iteration 8280, lr = 0.0001
I0615 16:05:38.284231  5063 solver.cpp:213] Iteration 8290, loss = 3.47278
I0615 16:05:38.284253  5063 solver.cpp:228]     Train net output #0: softmax = 3.47278 (* 1 = 3.47278 loss)
I0615 16:05:38.284258  5063 solver.cpp:473] Iteration 8290, lr = 0.0001
I0615 16:05:39.042747  5063 solver.cpp:213] Iteration 8300, loss = 3.76362
I0615 16:05:39.042785  5063 solver.cpp:228]     Train net output #0: softmax = 3.76362 (* 1 = 3.76362 loss)
I0615 16:05:39.042790  5063 solver.cpp:473] Iteration 8300, lr = 0.0001
I0615 16:05:39.801090  5063 solver.cpp:213] Iteration 8310, loss = 3.74966
I0615 16:05:39.801110  5063 solver.cpp:228]     Train net output #0: softmax = 3.74966 (* 1 = 3.74966 loss)
I0615 16:05:39.801115  5063 solver.cpp:473] Iteration 8310, lr = 0.0001
I0615 16:05:40.559527  5063 solver.cpp:213] Iteration 8320, loss = 3.62689
I0615 16:05:40.559548  5063 solver.cpp:228]     Train net output #0: softmax = 3.62689 (* 1 = 3.62689 loss)
I0615 16:05:40.559553  5063 solver.cpp:473] Iteration 8320, lr = 0.0001
I0615 16:05:41.317432  5063 solver.cpp:213] Iteration 8330, loss = 3.68383
I0615 16:05:41.317456  5063 solver.cpp:228]     Train net output #0: softmax = 3.68383 (* 1 = 3.68383 loss)
I0615 16:05:41.317461  5063 solver.cpp:473] Iteration 8330, lr = 0.0001
I0615 16:05:42.075592  5063 solver.cpp:213] Iteration 8340, loss = 3.41297
I0615 16:05:42.075614  5063 solver.cpp:228]     Train net output #0: softmax = 3.41297 (* 1 = 3.41297 loss)
I0615 16:05:42.075619  5063 solver.cpp:473] Iteration 8340, lr = 0.0001
I0615 16:05:42.834013  5063 solver.cpp:213] Iteration 8350, loss = 3.63815
I0615 16:05:42.834034  5063 solver.cpp:228]     Train net output #0: softmax = 3.63815 (* 1 = 3.63815 loss)
I0615 16:05:42.834039  5063 solver.cpp:473] Iteration 8350, lr = 0.0001
I0615 16:05:43.590243  5063 solver.cpp:213] Iteration 8360, loss = 3.39805
I0615 16:05:43.590267  5063 solver.cpp:228]     Train net output #0: softmax = 3.39805 (* 1 = 3.39805 loss)
I0615 16:05:43.590272  5063 solver.cpp:473] Iteration 8360, lr = 0.0001
I0615 16:05:44.348501  5063 solver.cpp:213] Iteration 8370, loss = 3.64967
I0615 16:05:44.348521  5063 solver.cpp:228]     Train net output #0: softmax = 3.64967 (* 1 = 3.64967 loss)
I0615 16:05:44.348526  5063 solver.cpp:473] Iteration 8370, lr = 0.0001
I0615 16:05:45.106884  5063 solver.cpp:213] Iteration 8380, loss = 3.6321
I0615 16:05:45.106906  5063 solver.cpp:228]     Train net output #0: softmax = 3.6321 (* 1 = 3.6321 loss)
I0615 16:05:45.106911  5063 solver.cpp:473] Iteration 8380, lr = 0.0001
I0615 16:05:45.865488  5063 solver.cpp:213] Iteration 8390, loss = 3.50773
I0615 16:05:45.865514  5063 solver.cpp:228]     Train net output #0: softmax = 3.50773 (* 1 = 3.50773 loss)
I0615 16:05:45.865636  5063 solver.cpp:473] Iteration 8390, lr = 0.0001
I0615 16:05:46.624300  5063 solver.cpp:213] Iteration 8400, loss = 3.29585
I0615 16:05:46.624339  5063 solver.cpp:228]     Train net output #0: softmax = 3.29585 (* 1 = 3.29585 loss)
I0615 16:05:46.624346  5063 solver.cpp:473] Iteration 8400, lr = 0.0001
I0615 16:05:47.382187  5063 solver.cpp:213] Iteration 8410, loss = 3.23242
I0615 16:05:47.382207  5063 solver.cpp:228]     Train net output #0: softmax = 3.23242 (* 1 = 3.23242 loss)
I0615 16:05:47.382212  5063 solver.cpp:473] Iteration 8410, lr = 0.0001
I0615 16:05:48.140382  5063 solver.cpp:213] Iteration 8420, loss = 3.47078
I0615 16:05:48.140408  5063 solver.cpp:228]     Train net output #0: softmax = 3.47078 (* 1 = 3.47078 loss)
I0615 16:05:48.140413  5063 solver.cpp:473] Iteration 8420, lr = 0.0001
I0615 16:05:48.898351  5063 solver.cpp:213] Iteration 8430, loss = 3.84969
I0615 16:05:48.898376  5063 solver.cpp:228]     Train net output #0: softmax = 3.84969 (* 1 = 3.84969 loss)
I0615 16:05:48.898381  5063 solver.cpp:473] Iteration 8430, lr = 0.0001
I0615 16:05:49.657063  5063 solver.cpp:213] Iteration 8440, loss = 3.57558
I0615 16:05:49.657083  5063 solver.cpp:228]     Train net output #0: softmax = 3.57558 (* 1 = 3.57558 loss)
I0615 16:05:49.657088  5063 solver.cpp:473] Iteration 8440, lr = 0.0001
I0615 16:05:50.415710  5063 solver.cpp:213] Iteration 8450, loss = 3.3874
I0615 16:05:50.415730  5063 solver.cpp:228]     Train net output #0: softmax = 3.3874 (* 1 = 3.3874 loss)
I0615 16:05:50.415735  5063 solver.cpp:473] Iteration 8450, lr = 0.0001
I0615 16:05:51.174649  5063 solver.cpp:213] Iteration 8460, loss = 3.76462
I0615 16:05:51.174674  5063 solver.cpp:228]     Train net output #0: softmax = 3.76462 (* 1 = 3.76462 loss)
I0615 16:05:51.174684  5063 solver.cpp:473] Iteration 8460, lr = 0.0001
I0615 16:05:51.933599  5063 solver.cpp:213] Iteration 8470, loss = 3.66104
I0615 16:05:51.933624  5063 solver.cpp:228]     Train net output #0: softmax = 3.66104 (* 1 = 3.66104 loss)
I0615 16:05:51.933627  5063 solver.cpp:473] Iteration 8470, lr = 0.0001
I0615 16:05:52.692144  5063 solver.cpp:213] Iteration 8480, loss = 3.38806
I0615 16:05:52.692165  5063 solver.cpp:228]     Train net output #0: softmax = 3.38806 (* 1 = 3.38806 loss)
I0615 16:05:52.692170  5063 solver.cpp:473] Iteration 8480, lr = 0.0001
I0615 16:05:53.450508  5063 solver.cpp:213] Iteration 8490, loss = 3.58241
I0615 16:05:53.450528  5063 solver.cpp:228]     Train net output #0: softmax = 3.58241 (* 1 = 3.58241 loss)
I0615 16:05:53.450533  5063 solver.cpp:473] Iteration 8490, lr = 0.0001
I0615 16:05:54.209373  5063 solver.cpp:213] Iteration 8500, loss = 3.51077
I0615 16:05:54.209394  5063 solver.cpp:228]     Train net output #0: softmax = 3.51077 (* 1 = 3.51077 loss)
I0615 16:05:54.209399  5063 solver.cpp:473] Iteration 8500, lr = 0.0001
I0615 16:05:54.968111  5063 solver.cpp:213] Iteration 8510, loss = 3.85925
I0615 16:05:54.968135  5063 solver.cpp:228]     Train net output #0: softmax = 3.85925 (* 1 = 3.85925 loss)
I0615 16:05:54.968142  5063 solver.cpp:473] Iteration 8510, lr = 0.0001
I0615 16:05:55.727355  5063 solver.cpp:213] Iteration 8520, loss = 3.74418
I0615 16:05:55.727376  5063 solver.cpp:228]     Train net output #0: softmax = 3.74418 (* 1 = 3.74418 loss)
I0615 16:05:55.727381  5063 solver.cpp:473] Iteration 8520, lr = 0.0001
I0615 16:05:56.486445  5063 solver.cpp:213] Iteration 8530, loss = 3.74186
I0615 16:05:56.486466  5063 solver.cpp:228]     Train net output #0: softmax = 3.74186 (* 1 = 3.74186 loss)
I0615 16:05:56.486471  5063 solver.cpp:473] Iteration 8530, lr = 0.0001
I0615 16:05:57.245092  5063 solver.cpp:213] Iteration 8540, loss = 3.46266
I0615 16:05:57.245115  5063 solver.cpp:228]     Train net output #0: softmax = 3.46266 (* 1 = 3.46266 loss)
I0615 16:05:57.245118  5063 solver.cpp:473] Iteration 8540, lr = 0.0001
I0615 16:05:58.004014  5063 solver.cpp:213] Iteration 8550, loss = 3.5674
I0615 16:05:58.004040  5063 solver.cpp:228]     Train net output #0: softmax = 3.5674 (* 1 = 3.5674 loss)
I0615 16:05:58.004045  5063 solver.cpp:473] Iteration 8550, lr = 0.0001
I0615 16:05:58.760380  5063 solver.cpp:213] Iteration 8560, loss = 3.52154
I0615 16:05:58.760401  5063 solver.cpp:228]     Train net output #0: softmax = 3.52154 (* 1 = 3.52154 loss)
I0615 16:05:58.760421  5063 solver.cpp:473] Iteration 8560, lr = 0.0001
I0615 16:05:59.518609  5063 solver.cpp:213] Iteration 8570, loss = 3.59066
I0615 16:05:59.518630  5063 solver.cpp:228]     Train net output #0: softmax = 3.59066 (* 1 = 3.59066 loss)
I0615 16:05:59.518635  5063 solver.cpp:473] Iteration 8570, lr = 0.0001
I0615 16:06:00.277140  5063 solver.cpp:213] Iteration 8580, loss = 3.56505
I0615 16:06:00.277170  5063 solver.cpp:228]     Train net output #0: softmax = 3.56505 (* 1 = 3.56505 loss)
I0615 16:06:00.277175  5063 solver.cpp:473] Iteration 8580, lr = 0.0001
I0615 16:06:01.035629  5063 solver.cpp:213] Iteration 8590, loss = 3.50707
I0615 16:06:01.035648  5063 solver.cpp:228]     Train net output #0: softmax = 3.50707 (* 1 = 3.50707 loss)
I0615 16:06:01.035653  5063 solver.cpp:473] Iteration 8590, lr = 0.0001
I0615 16:06:01.794447  5063 solver.cpp:213] Iteration 8600, loss = 3.54226
I0615 16:06:01.794471  5063 solver.cpp:228]     Train net output #0: softmax = 3.54226 (* 1 = 3.54226 loss)
I0615 16:06:01.794610  5063 solver.cpp:473] Iteration 8600, lr = 0.0001
I0615 16:06:02.553196  5063 solver.cpp:213] Iteration 8610, loss = 3.49548
I0615 16:06:02.553217  5063 solver.cpp:228]     Train net output #0: softmax = 3.49548 (* 1 = 3.49548 loss)
I0615 16:06:02.553222  5063 solver.cpp:473] Iteration 8610, lr = 0.0001
I0615 16:06:03.311951  5063 solver.cpp:213] Iteration 8620, loss = 3.50931
I0615 16:06:03.311976  5063 solver.cpp:228]     Train net output #0: softmax = 3.50931 (* 1 = 3.50931 loss)
I0615 16:06:03.311981  5063 solver.cpp:473] Iteration 8620, lr = 0.0001
I0615 16:06:04.070837  5063 solver.cpp:213] Iteration 8630, loss = 3.71242
I0615 16:06:04.070858  5063 solver.cpp:228]     Train net output #0: softmax = 3.71242 (* 1 = 3.71242 loss)
I0615 16:06:04.070861  5063 solver.cpp:473] Iteration 8630, lr = 0.0001
I0615 16:06:04.828794  5063 solver.cpp:213] Iteration 8640, loss = 3.47132
I0615 16:06:04.828815  5063 solver.cpp:228]     Train net output #0: softmax = 3.47132 (* 1 = 3.47132 loss)
I0615 16:06:04.828820  5063 solver.cpp:473] Iteration 8640, lr = 0.0001
I0615 16:06:05.586678  5063 solver.cpp:213] Iteration 8650, loss = 3.53939
I0615 16:06:05.586699  5063 solver.cpp:228]     Train net output #0: softmax = 3.53939 (* 1 = 3.53939 loss)
I0615 16:06:05.586704  5063 solver.cpp:473] Iteration 8650, lr = 0.0001
I0615 16:06:06.345137  5063 solver.cpp:213] Iteration 8660, loss = 3.50097
I0615 16:06:06.345158  5063 solver.cpp:228]     Train net output #0: softmax = 3.50097 (* 1 = 3.50097 loss)
I0615 16:06:06.345163  5063 solver.cpp:473] Iteration 8660, lr = 0.0001
I0615 16:06:07.103813  5063 solver.cpp:213] Iteration 8670, loss = 3.56361
I0615 16:06:07.103837  5063 solver.cpp:228]     Train net output #0: softmax = 3.56361 (* 1 = 3.56361 loss)
I0615 16:06:07.103847  5063 solver.cpp:473] Iteration 8670, lr = 0.0001
I0615 16:06:07.863000  5063 solver.cpp:213] Iteration 8680, loss = 3.3573
I0615 16:06:07.863021  5063 solver.cpp:228]     Train net output #0: softmax = 3.3573 (* 1 = 3.3573 loss)
I0615 16:06:07.863026  5063 solver.cpp:473] Iteration 8680, lr = 0.0001
I0615 16:06:08.621274  5063 solver.cpp:213] Iteration 8690, loss = 3.67103
I0615 16:06:08.621296  5063 solver.cpp:228]     Train net output #0: softmax = 3.67103 (* 1 = 3.67103 loss)
I0615 16:06:08.621301  5063 solver.cpp:473] Iteration 8690, lr = 0.0001
I0615 16:06:09.380481  5063 solver.cpp:213] Iteration 8700, loss = 3.71013
I0615 16:06:09.380502  5063 solver.cpp:228]     Train net output #0: softmax = 3.71013 (* 1 = 3.71013 loss)
I0615 16:06:09.380506  5063 solver.cpp:473] Iteration 8700, lr = 0.0001
I0615 16:06:10.139542  5063 solver.cpp:213] Iteration 8710, loss = 3.52454
I0615 16:06:10.139569  5063 solver.cpp:228]     Train net output #0: softmax = 3.52454 (* 1 = 3.52454 loss)
I0615 16:06:10.139572  5063 solver.cpp:473] Iteration 8710, lr = 0.0001
I0615 16:06:10.898722  5063 solver.cpp:213] Iteration 8720, loss = 3.69553
I0615 16:06:10.898743  5063 solver.cpp:228]     Train net output #0: softmax = 3.69553 (* 1 = 3.69553 loss)
I0615 16:06:10.898767  5063 solver.cpp:473] Iteration 8720, lr = 0.0001
I0615 16:06:11.657826  5063 solver.cpp:213] Iteration 8730, loss = 3.30849
I0615 16:06:11.657847  5063 solver.cpp:228]     Train net output #0: softmax = 3.30849 (* 1 = 3.30849 loss)
I0615 16:06:11.657852  5063 solver.cpp:473] Iteration 8730, lr = 0.0001
I0615 16:06:12.417078  5063 solver.cpp:213] Iteration 8740, loss = 3.47302
I0615 16:06:12.417106  5063 solver.cpp:228]     Train net output #0: softmax = 3.47302 (* 1 = 3.47302 loss)
I0615 16:06:12.417111  5063 solver.cpp:473] Iteration 8740, lr = 0.0001
I0615 16:06:13.175596  5063 solver.cpp:213] Iteration 8750, loss = 3.46951
I0615 16:06:13.175616  5063 solver.cpp:228]     Train net output #0: softmax = 3.46951 (* 1 = 3.46951 loss)
I0615 16:06:13.175621  5063 solver.cpp:473] Iteration 8750, lr = 0.0001
I0615 16:06:13.934417  5063 solver.cpp:213] Iteration 8760, loss = 3.5914
I0615 16:06:13.934437  5063 solver.cpp:228]     Train net output #0: softmax = 3.5914 (* 1 = 3.5914 loss)
I0615 16:06:13.934442  5063 solver.cpp:473] Iteration 8760, lr = 0.0001
I0615 16:06:14.693794  5063 solver.cpp:213] Iteration 8770, loss = 3.53027
I0615 16:06:14.693814  5063 solver.cpp:228]     Train net output #0: softmax = 3.53027 (* 1 = 3.53027 loss)
I0615 16:06:14.693819  5063 solver.cpp:473] Iteration 8770, lr = 0.0001
I0615 16:06:15.453791  5063 solver.cpp:213] Iteration 8780, loss = 3.503
I0615 16:06:15.453814  5063 solver.cpp:228]     Train net output #0: softmax = 3.503 (* 1 = 3.503 loss)
I0615 16:06:15.453819  5063 solver.cpp:473] Iteration 8780, lr = 0.0001
I0615 16:06:16.212569  5063 solver.cpp:213] Iteration 8790, loss = 3.35027
I0615 16:06:16.212589  5063 solver.cpp:228]     Train net output #0: softmax = 3.35027 (* 1 = 3.35027 loss)
I0615 16:06:16.212594  5063 solver.cpp:473] Iteration 8790, lr = 0.0001
I0615 16:06:16.970816  5063 solver.cpp:213] Iteration 8800, loss = 3.44834
I0615 16:06:16.970861  5063 solver.cpp:228]     Train net output #0: softmax = 3.44834 (* 1 = 3.44834 loss)
I0615 16:06:16.970866  5063 solver.cpp:473] Iteration 8800, lr = 0.0001
I0615 16:06:17.729967  5063 solver.cpp:213] Iteration 8810, loss = 3.4113
I0615 16:06:17.729996  5063 solver.cpp:228]     Train net output #0: softmax = 3.4113 (* 1 = 3.4113 loss)
I0615 16:06:17.730146  5063 solver.cpp:473] Iteration 8810, lr = 0.0001
I0615 16:06:18.489598  5063 solver.cpp:213] Iteration 8820, loss = 3.75073
I0615 16:06:18.489619  5063 solver.cpp:228]     Train net output #0: softmax = 3.75073 (* 1 = 3.75073 loss)
I0615 16:06:18.489624  5063 solver.cpp:473] Iteration 8820, lr = 0.0001
I0615 16:06:19.249217  5063 solver.cpp:213] Iteration 8830, loss = 3.5117
I0615 16:06:19.249238  5063 solver.cpp:228]     Train net output #0: softmax = 3.5117 (* 1 = 3.5117 loss)
I0615 16:06:19.249243  5063 solver.cpp:473] Iteration 8830, lr = 0.0001
I0615 16:06:20.008544  5063 solver.cpp:213] Iteration 8840, loss = 3.41626
I0615 16:06:20.008565  5063 solver.cpp:228]     Train net output #0: softmax = 3.41626 (* 1 = 3.41626 loss)
I0615 16:06:20.008570  5063 solver.cpp:473] Iteration 8840, lr = 0.0001
I0615 16:06:20.767660  5063 solver.cpp:213] Iteration 8850, loss = 3.62829
I0615 16:06:20.767680  5063 solver.cpp:228]     Train net output #0: softmax = 3.62829 (* 1 = 3.62829 loss)
I0615 16:06:20.767685  5063 solver.cpp:473] Iteration 8850, lr = 0.0001
I0615 16:06:21.526435  5063 solver.cpp:213] Iteration 8860, loss = 3.57184
I0615 16:06:21.526456  5063 solver.cpp:228]     Train net output #0: softmax = 3.57184 (* 1 = 3.57184 loss)
I0615 16:06:21.526461  5063 solver.cpp:473] Iteration 8860, lr = 0.0001
I0615 16:06:22.285779  5063 solver.cpp:213] Iteration 8870, loss = 3.52589
I0615 16:06:22.285800  5063 solver.cpp:228]     Train net output #0: softmax = 3.52589 (* 1 = 3.52589 loss)
I0615 16:06:22.285804  5063 solver.cpp:473] Iteration 8870, lr = 0.0001
I0615 16:06:23.044653  5063 solver.cpp:213] Iteration 8880, loss = 3.50206
I0615 16:06:23.044677  5063 solver.cpp:228]     Train net output #0: softmax = 3.50206 (* 1 = 3.50206 loss)
I0615 16:06:23.044801  5063 solver.cpp:473] Iteration 8880, lr = 0.0001
I0615 16:06:23.803586  5063 solver.cpp:213] Iteration 8890, loss = 3.68939
I0615 16:06:23.803606  5063 solver.cpp:228]     Train net output #0: softmax = 3.68939 (* 1 = 3.68939 loss)
I0615 16:06:23.803611  5063 solver.cpp:473] Iteration 8890, lr = 0.0001
I0615 16:06:24.563254  5063 solver.cpp:213] Iteration 8900, loss = 3.84293
I0615 16:06:24.563276  5063 solver.cpp:228]     Train net output #0: softmax = 3.84293 (* 1 = 3.84293 loss)
I0615 16:06:24.563287  5063 solver.cpp:473] Iteration 8900, lr = 0.0001
I0615 16:06:25.322387  5063 solver.cpp:213] Iteration 8910, loss = 3.96424
I0615 16:06:25.322407  5063 solver.cpp:228]     Train net output #0: softmax = 3.96424 (* 1 = 3.96424 loss)
I0615 16:06:25.322412  5063 solver.cpp:473] Iteration 8910, lr = 0.0001
I0615 16:06:26.081877  5063 solver.cpp:213] Iteration 8920, loss = 3.49249
I0615 16:06:26.081897  5063 solver.cpp:228]     Train net output #0: softmax = 3.49249 (* 1 = 3.49249 loss)
I0615 16:06:26.081902  5063 solver.cpp:473] Iteration 8920, lr = 0.0001
I0615 16:06:26.840961  5063 solver.cpp:213] Iteration 8930, loss = 3.51556
I0615 16:06:26.840982  5063 solver.cpp:228]     Train net output #0: softmax = 3.51556 (* 1 = 3.51556 loss)
I0615 16:06:26.840987  5063 solver.cpp:473] Iteration 8930, lr = 0.0001
I0615 16:06:27.600179  5063 solver.cpp:213] Iteration 8940, loss = 3.47977
I0615 16:06:27.600199  5063 solver.cpp:228]     Train net output #0: softmax = 3.47977 (* 1 = 3.47977 loss)
I0615 16:06:27.600204  5063 solver.cpp:473] Iteration 8940, lr = 0.0001
I0615 16:06:28.359045  5063 solver.cpp:213] Iteration 8950, loss = 3.58631
I0615 16:06:28.359072  5063 solver.cpp:228]     Train net output #0: softmax = 3.58631 (* 1 = 3.58631 loss)
I0615 16:06:28.359215  5063 solver.cpp:473] Iteration 8950, lr = 0.0001
I0615 16:06:29.118026  5063 solver.cpp:213] Iteration 8960, loss = 3.53801
I0615 16:06:29.118046  5063 solver.cpp:228]     Train net output #0: softmax = 3.53801 (* 1 = 3.53801 loss)
I0615 16:06:29.118068  5063 solver.cpp:473] Iteration 8960, lr = 0.0001
I0615 16:06:29.876824  5063 solver.cpp:213] Iteration 8970, loss = 3.62571
I0615 16:06:29.876845  5063 solver.cpp:228]     Train net output #0: softmax = 3.62571 (* 1 = 3.62571 loss)
I0615 16:06:29.876850  5063 solver.cpp:473] Iteration 8970, lr = 0.0001
I0615 16:06:30.635653  5063 solver.cpp:213] Iteration 8980, loss = 3.49518
I0615 16:06:30.635674  5063 solver.cpp:228]     Train net output #0: softmax = 3.49518 (* 1 = 3.49518 loss)
I0615 16:06:30.635679  5063 solver.cpp:473] Iteration 8980, lr = 0.0001
I0615 16:06:31.394783  5063 solver.cpp:213] Iteration 8990, loss = 3.62017
I0615 16:06:31.394804  5063 solver.cpp:228]     Train net output #0: softmax = 3.62017 (* 1 = 3.62017 loss)
I0615 16:06:31.394809  5063 solver.cpp:473] Iteration 8990, lr = 0.0001
I0615 16:06:32.100018  5063 solver.cpp:362] Snapshotting to snapshots/16-06-15_15h49m18s_0_11_pretrainClassification_iter_9000.caffemodel
I0615 16:06:32.100723  5063 solver.cpp:370] Snapshotting solver state to snapshots/16-06-15_15h49m18s_0_11_pretrainClassification_iter_9000.solverstate
I0615 16:06:32.101102  5063 solver.cpp:291] Iteration 9000, Testing net (#0)
I0615 16:06:32.195554  5063 solver.cpp:342]     Test net output #0: accuracy_top_1 = 0.167187
I0615 16:06:32.195570  5063 solver.cpp:342]     Test net output #1: accuracy_top_5 = 0.421875
I0615 16:06:32.195576  5063 solver.cpp:342]     Test net output #2: softmax = 3.52053 (* 1 = 3.52053 loss)
I0615 16:06:32.249191  5063 solver.cpp:213] Iteration 9000, loss = 3.35495
I0615 16:06:32.249203  5063 solver.cpp:228]     Train net output #0: softmax = 3.35495 (* 1 = 3.35495 loss)
I0615 16:06:32.249208  5063 solver.cpp:473] Iteration 9000, lr = 0.0001
I0615 16:06:33.007555  5063 solver.cpp:213] Iteration 9010, loss = 3.45456
I0615 16:06:33.007575  5063 solver.cpp:228]     Train net output #0: softmax = 3.45456 (* 1 = 3.45456 loss)
I0615 16:06:33.007580  5063 solver.cpp:473] Iteration 9010, lr = 0.0001
I0615 16:06:33.766772  5063 solver.cpp:213] Iteration 9020, loss = 3.55955
I0615 16:06:33.766796  5063 solver.cpp:228]     Train net output #0: softmax = 3.55955 (* 1 = 3.55955 loss)
I0615 16:06:33.766998  5063 solver.cpp:473] Iteration 9020, lr = 0.0001
I0615 16:06:34.525462  5063 solver.cpp:213] Iteration 9030, loss = 3.44988
I0615 16:06:34.525485  5063 solver.cpp:228]     Train net output #0: softmax = 3.44988 (* 1 = 3.44988 loss)
I0615 16:06:34.525490  5063 solver.cpp:473] Iteration 9030, lr = 0.0001
I0615 16:06:35.284466  5063 solver.cpp:213] Iteration 9040, loss = 3.50721
I0615 16:06:35.284487  5063 solver.cpp:228]     Train net output #0: softmax = 3.50721 (* 1 = 3.50721 loss)
I0615 16:06:35.284492  5063 solver.cpp:473] Iteration 9040, lr = 0.0001
I0615 16:06:36.043118  5063 solver.cpp:213] Iteration 9050, loss = 3.48943
I0615 16:06:36.043138  5063 solver.cpp:228]     Train net output #0: softmax = 3.48943 (* 1 = 3.48943 loss)
I0615 16:06:36.043143  5063 solver.cpp:473] Iteration 9050, lr = 0.0001
I0615 16:06:36.801925  5063 solver.cpp:213] Iteration 9060, loss = 3.54657
I0615 16:06:36.801946  5063 solver.cpp:228]     Train net output #0: softmax = 3.54657 (* 1 = 3.54657 loss)
I0615 16:06:36.801951  5063 solver.cpp:473] Iteration 9060, lr = 0.0001
I0615 16:06:37.559918  5063 solver.cpp:213] Iteration 9070, loss = 3.61685
I0615 16:06:37.559939  5063 solver.cpp:228]     Train net output #0: softmax = 3.61685 (* 1 = 3.61685 loss)
I0615 16:06:37.559944  5063 solver.cpp:473] Iteration 9070, lr = 0.0001
I0615 16:06:38.318115  5063 solver.cpp:213] Iteration 9080, loss = 3.69278
I0615 16:06:38.318135  5063 solver.cpp:228]     Train net output #0: softmax = 3.69278 (* 1 = 3.69278 loss)
I0615 16:06:38.318140  5063 solver.cpp:473] Iteration 9080, lr = 0.0001
I0615 16:06:39.076793  5063 solver.cpp:213] Iteration 9090, loss = 3.20564
I0615 16:06:39.076818  5063 solver.cpp:228]     Train net output #0: softmax = 3.20564 (* 1 = 3.20564 loss)
I0615 16:06:39.076941  5063 solver.cpp:473] Iteration 9090, lr = 0.0001
I0615 16:06:39.835477  5063 solver.cpp:213] Iteration 9100, loss = 3.38283
I0615 16:06:39.835497  5063 solver.cpp:228]     Train net output #0: softmax = 3.38283 (* 1 = 3.38283 loss)
I0615 16:06:39.835502  5063 solver.cpp:473] Iteration 9100, lr = 0.0001
I0615 16:06:40.593818  5063 solver.cpp:213] Iteration 9110, loss = 3.54225
I0615 16:06:40.593839  5063 solver.cpp:228]     Train net output #0: softmax = 3.54225 (* 1 = 3.54225 loss)
I0615 16:06:40.593844  5063 solver.cpp:473] Iteration 9110, lr = 0.0001
I0615 16:06:41.351994  5063 solver.cpp:213] Iteration 9120, loss = 3.49462
I0615 16:06:41.352015  5063 solver.cpp:228]     Train net output #0: softmax = 3.49462 (* 1 = 3.49462 loss)
I0615 16:06:41.352020  5063 solver.cpp:473] Iteration 9120, lr = 0.0001
I0615 16:06:42.110908  5063 solver.cpp:213] Iteration 9130, loss = 3.55325
I0615 16:06:42.110930  5063 solver.cpp:228]     Train net output #0: softmax = 3.55325 (* 1 = 3.55325 loss)
I0615 16:06:42.110935  5063 solver.cpp:473] Iteration 9130, lr = 0.0001
I0615 16:06:42.869642  5063 solver.cpp:213] Iteration 9140, loss = 3.64513
I0615 16:06:42.869666  5063 solver.cpp:228]     Train net output #0: softmax = 3.64513 (* 1 = 3.64513 loss)
I0615 16:06:42.869671  5063 solver.cpp:473] Iteration 9140, lr = 0.0001
I0615 16:06:43.628913  5063 solver.cpp:213] Iteration 9150, loss = 3.45481
I0615 16:06:43.628933  5063 solver.cpp:228]     Train net output #0: softmax = 3.45481 (* 1 = 3.45481 loss)
I0615 16:06:43.628938  5063 solver.cpp:473] Iteration 9150, lr = 0.0001
I0615 16:06:44.387207  5063 solver.cpp:213] Iteration 9160, loss = 3.51939
I0615 16:06:44.387228  5063 solver.cpp:228]     Train net output #0: softmax = 3.51939 (* 1 = 3.51939 loss)
I0615 16:06:44.387233  5063 solver.cpp:473] Iteration 9160, lr = 0.0001
I0615 16:06:45.145016  5063 solver.cpp:213] Iteration 9170, loss = 3.50252
I0615 16:06:45.145037  5063 solver.cpp:228]     Train net output #0: softmax = 3.50252 (* 1 = 3.50252 loss)
I0615 16:06:45.145042  5063 solver.cpp:473] Iteration 9170, lr = 0.0001
I0615 16:06:45.902910  5063 solver.cpp:213] Iteration 9180, loss = 3.49822
I0615 16:06:45.902931  5063 solver.cpp:228]     Train net output #0: softmax = 3.49822 (* 1 = 3.49822 loss)
I0615 16:06:45.902936  5063 solver.cpp:473] Iteration 9180, lr = 0.0001
I0615 16:06:46.660794  5063 solver.cpp:213] Iteration 9190, loss = 3.60281
I0615 16:06:46.660815  5063 solver.cpp:228]     Train net output #0: softmax = 3.60281 (* 1 = 3.60281 loss)
I0615 16:06:46.660820  5063 solver.cpp:473] Iteration 9190, lr = 0.0001
I0615 16:06:47.419263  5063 solver.cpp:213] Iteration 9200, loss = 3.41742
I0615 16:06:47.419312  5063 solver.cpp:228]     Train net output #0: softmax = 3.41742 (* 1 = 3.41742 loss)
I0615 16:06:47.419318  5063 solver.cpp:473] Iteration 9200, lr = 0.0001
I0615 16:06:48.177984  5063 solver.cpp:213] Iteration 9210, loss = 3.70196
I0615 16:06:48.178004  5063 solver.cpp:228]     Train net output #0: softmax = 3.70196 (* 1 = 3.70196 loss)
I0615 16:06:48.178009  5063 solver.cpp:473] Iteration 9210, lr = 0.0001
I0615 16:06:48.936769  5063 solver.cpp:213] Iteration 9220, loss = 3.43781
I0615 16:06:48.936789  5063 solver.cpp:228]     Train net output #0: softmax = 3.43781 (* 1 = 3.43781 loss)
I0615 16:06:48.936794  5063 solver.cpp:473] Iteration 9220, lr = 0.0001
I0615 16:06:49.693696  5063 solver.cpp:213] Iteration 9230, loss = 3.43424
I0615 16:06:49.693724  5063 solver.cpp:228]     Train net output #0: softmax = 3.43424 (* 1 = 3.43424 loss)
I0615 16:06:49.693874  5063 solver.cpp:473] Iteration 9230, lr = 0.0001
I0615 16:06:50.452607  5063 solver.cpp:213] Iteration 9240, loss = 3.53562
I0615 16:06:50.452627  5063 solver.cpp:228]     Train net output #0: softmax = 3.53562 (* 1 = 3.53562 loss)
I0615 16:06:50.452631  5063 solver.cpp:473] Iteration 9240, lr = 0.0001
I0615 16:06:51.211527  5063 solver.cpp:213] Iteration 9250, loss = 3.66514
I0615 16:06:51.211549  5063 solver.cpp:228]     Train net output #0: softmax = 3.66514 (* 1 = 3.66514 loss)
I0615 16:06:51.211554  5063 solver.cpp:473] Iteration 9250, lr = 0.0001
I0615 16:06:51.969835  5063 solver.cpp:213] Iteration 9260, loss = 3.39111
I0615 16:06:51.969856  5063 solver.cpp:228]     Train net output #0: softmax = 3.39111 (* 1 = 3.39111 loss)
I0615 16:06:51.969861  5063 solver.cpp:473] Iteration 9260, lr = 0.0001
I0615 16:06:52.728536  5063 solver.cpp:213] Iteration 9270, loss = 3.46238
I0615 16:06:52.728557  5063 solver.cpp:228]     Train net output #0: softmax = 3.46238 (* 1 = 3.46238 loss)
I0615 16:06:52.728562  5063 solver.cpp:473] Iteration 9270, lr = 0.0001
I0615 16:06:53.487354  5063 solver.cpp:213] Iteration 9280, loss = 3.50984
I0615 16:06:53.487375  5063 solver.cpp:228]     Train net output #0: softmax = 3.50984 (* 1 = 3.50984 loss)
I0615 16:06:53.487380  5063 solver.cpp:473] Iteration 9280, lr = 0.0001
I0615 16:06:54.246337  5063 solver.cpp:213] Iteration 9290, loss = 3.49513
I0615 16:06:54.246357  5063 solver.cpp:228]     Train net output #0: softmax = 3.49513 (* 1 = 3.49513 loss)
I0615 16:06:54.246362  5063 solver.cpp:473] Iteration 9290, lr = 0.0001
I0615 16:06:55.005111  5063 solver.cpp:213] Iteration 9300, loss = 3.73757
I0615 16:06:55.005134  5063 solver.cpp:228]     Train net output #0: softmax = 3.73757 (* 1 = 3.73757 loss)
I0615 16:06:55.005261  5063 solver.cpp:473] Iteration 9300, lr = 0.0001
I0615 16:06:55.763366  5063 solver.cpp:213] Iteration 9310, loss = 3.53815
I0615 16:06:55.763387  5063 solver.cpp:228]     Train net output #0: softmax = 3.53815 (* 1 = 3.53815 loss)
I0615 16:06:55.763391  5063 solver.cpp:473] Iteration 9310, lr = 0.0001
I0615 16:06:56.522603  5063 solver.cpp:213] Iteration 9320, loss = 3.42347
I0615 16:06:56.522622  5063 solver.cpp:228]     Train net output #0: softmax = 3.42347 (* 1 = 3.42347 loss)
I0615 16:06:56.522627  5063 solver.cpp:473] Iteration 9320, lr = 0.0001
I0615 16:06:57.281500  5063 solver.cpp:213] Iteration 9330, loss = 3.44649
I0615 16:06:57.281525  5063 solver.cpp:228]     Train net output #0: softmax = 3.44649 (* 1 = 3.44649 loss)
I0615 16:06:57.281529  5063 solver.cpp:473] Iteration 9330, lr = 0.0001
I0615 16:06:58.040953  5063 solver.cpp:213] Iteration 9340, loss = 3.73414
I0615 16:06:58.040973  5063 solver.cpp:228]     Train net output #0: softmax = 3.73414 (* 1 = 3.73414 loss)
I0615 16:06:58.040978  5063 solver.cpp:473] Iteration 9340, lr = 0.0001
I0615 16:06:58.799499  5063 solver.cpp:213] Iteration 9350, loss = 3.53771
I0615 16:06:58.799520  5063 solver.cpp:228]     Train net output #0: softmax = 3.53771 (* 1 = 3.53771 loss)
I0615 16:06:58.799523  5063 solver.cpp:473] Iteration 9350, lr = 0.0001
I0615 16:06:59.558486  5063 solver.cpp:213] Iteration 9360, loss = 3.4228
I0615 16:06:59.558507  5063 solver.cpp:228]     Train net output #0: softmax = 3.4228 (* 1 = 3.4228 loss)
I0615 16:06:59.558529  5063 solver.cpp:473] Iteration 9360, lr = 0.0001
I0615 16:07:00.316929  5063 solver.cpp:213] Iteration 9370, loss = 3.54218
I0615 16:07:00.316953  5063 solver.cpp:228]     Train net output #0: softmax = 3.54218 (* 1 = 3.54218 loss)
I0615 16:07:00.317090  5063 solver.cpp:473] Iteration 9370, lr = 0.0001
I0615 16:07:01.074620  5063 solver.cpp:213] Iteration 9380, loss = 3.76907
I0615 16:07:01.074640  5063 solver.cpp:228]     Train net output #0: softmax = 3.76907 (* 1 = 3.76907 loss)
I0615 16:07:01.074645  5063 solver.cpp:473] Iteration 9380, lr = 0.0001
I0615 16:07:01.831974  5063 solver.cpp:213] Iteration 9390, loss = 3.53491
I0615 16:07:01.831995  5063 solver.cpp:228]     Train net output #0: softmax = 3.53491 (* 1 = 3.53491 loss)
I0615 16:07:01.832000  5063 solver.cpp:473] Iteration 9390, lr = 0.0001
I0615 16:07:02.590587  5063 solver.cpp:213] Iteration 9400, loss = 3.46371
I0615 16:07:02.590608  5063 solver.cpp:228]     Train net output #0: softmax = 3.46371 (* 1 = 3.46371 loss)
I0615 16:07:02.590613  5063 solver.cpp:473] Iteration 9400, lr = 0.0001
I0615 16:07:03.348950  5063 solver.cpp:213] Iteration 9410, loss = 3.63948
I0615 16:07:03.348970  5063 solver.cpp:228]     Train net output #0: softmax = 3.63948 (* 1 = 3.63948 loss)
I0615 16:07:03.348975  5063 solver.cpp:473] Iteration 9410, lr = 0.0001
I0615 16:07:04.107635  5063 solver.cpp:213] Iteration 9420, loss = 3.50077
I0615 16:07:04.107656  5063 solver.cpp:228]     Train net output #0: softmax = 3.50077 (* 1 = 3.50077 loss)
I0615 16:07:04.107659  5063 solver.cpp:473] Iteration 9420, lr = 0.0001
I0615 16:07:04.865815  5063 solver.cpp:213] Iteration 9430, loss = 3.4163
I0615 16:07:04.865838  5063 solver.cpp:228]     Train net output #0: softmax = 3.4163 (* 1 = 3.4163 loss)
I0615 16:07:04.865842  5063 solver.cpp:473] Iteration 9430, lr = 0.0001
I0615 16:07:05.624582  5063 solver.cpp:213] Iteration 9440, loss = 3.52648
I0615 16:07:05.624604  5063 solver.cpp:228]     Train net output #0: softmax = 3.52648 (* 1 = 3.52648 loss)
I0615 16:07:05.624609  5063 solver.cpp:473] Iteration 9440, lr = 0.0001
I0615 16:07:06.382386  5063 solver.cpp:213] Iteration 9450, loss = 3.54723
I0615 16:07:06.382412  5063 solver.cpp:228]     Train net output #0: softmax = 3.54723 (* 1 = 3.54723 loss)
I0615 16:07:06.382417  5063 solver.cpp:473] Iteration 9450, lr = 0.0001
I0615 16:07:07.140418  5063 solver.cpp:213] Iteration 9460, loss = 3.5766
I0615 16:07:07.140439  5063 solver.cpp:228]     Train net output #0: softmax = 3.5766 (* 1 = 3.5766 loss)
I0615 16:07:07.140444  5063 solver.cpp:473] Iteration 9460, lr = 0.0001
I0615 16:07:07.899189  5063 solver.cpp:213] Iteration 9470, loss = 3.61019
I0615 16:07:07.899210  5063 solver.cpp:228]     Train net output #0: softmax = 3.61019 (* 1 = 3.61019 loss)
I0615 16:07:07.899215  5063 solver.cpp:473] Iteration 9470, lr = 0.0001
I0615 16:07:08.657558  5063 solver.cpp:213] Iteration 9480, loss = 3.43773
I0615 16:07:08.657580  5063 solver.cpp:228]     Train net output #0: softmax = 3.43773 (* 1 = 3.43773 loss)
I0615 16:07:08.657585  5063 solver.cpp:473] Iteration 9480, lr = 0.0001
I0615 16:07:09.416188  5063 solver.cpp:213] Iteration 9490, loss = 3.43048
I0615 16:07:09.416208  5063 solver.cpp:228]     Train net output #0: softmax = 3.43048 (* 1 = 3.43048 loss)
I0615 16:07:09.416213  5063 solver.cpp:473] Iteration 9490, lr = 0.0001
I0615 16:07:10.173810  5063 solver.cpp:213] Iteration 9500, loss = 3.75985
I0615 16:07:10.173832  5063 solver.cpp:228]     Train net output #0: softmax = 3.75985 (* 1 = 3.75985 loss)
I0615 16:07:10.173837  5063 solver.cpp:473] Iteration 9500, lr = 0.0001
I0615 16:07:10.931769  5063 solver.cpp:213] Iteration 9510, loss = 3.61147
I0615 16:07:10.931793  5063 solver.cpp:228]     Train net output #0: softmax = 3.61147 (* 1 = 3.61147 loss)
I0615 16:07:10.931916  5063 solver.cpp:473] Iteration 9510, lr = 0.0001
I0615 16:07:11.689805  5063 solver.cpp:213] Iteration 9520, loss = 3.56682
I0615 16:07:11.689832  5063 solver.cpp:228]     Train net output #0: softmax = 3.56682 (* 1 = 3.56682 loss)
I0615 16:07:11.689849  5063 solver.cpp:473] Iteration 9520, lr = 0.0001
I0615 16:07:12.448077  5063 solver.cpp:213] Iteration 9530, loss = 3.51926
I0615 16:07:12.448097  5063 solver.cpp:228]     Train net output #0: softmax = 3.51926 (* 1 = 3.51926 loss)
I0615 16:07:12.448102  5063 solver.cpp:473] Iteration 9530, lr = 0.0001
I0615 16:07:13.206476  5063 solver.cpp:213] Iteration 9540, loss = 3.37865
I0615 16:07:13.206496  5063 solver.cpp:228]     Train net output #0: softmax = 3.37865 (* 1 = 3.37865 loss)
I0615 16:07:13.206501  5063 solver.cpp:473] Iteration 9540, lr = 0.0001
I0615 16:07:13.965541  5063 solver.cpp:213] Iteration 9550, loss = 3.59504
I0615 16:07:13.965561  5063 solver.cpp:228]     Train net output #0: softmax = 3.59504 (* 1 = 3.59504 loss)
I0615 16:07:13.965566  5063 solver.cpp:473] Iteration 9550, lr = 0.0001
I0615 16:07:14.724046  5063 solver.cpp:213] Iteration 9560, loss = 3.52698
I0615 16:07:14.724066  5063 solver.cpp:228]     Train net output #0: softmax = 3.52698 (* 1 = 3.52698 loss)
I0615 16:07:14.724071  5063 solver.cpp:473] Iteration 9560, lr = 0.0001
I0615 16:07:15.482844  5063 solver.cpp:213] Iteration 9570, loss = 3.56652
I0615 16:07:15.482869  5063 solver.cpp:228]     Train net output #0: softmax = 3.56652 (* 1 = 3.56652 loss)
I0615 16:07:15.482874  5063 solver.cpp:473] Iteration 9570, lr = 0.0001
I0615 16:07:16.241044  5063 solver.cpp:213] Iteration 9580, loss = 3.39858
I0615 16:07:16.241070  5063 solver.cpp:228]     Train net output #0: softmax = 3.39858 (* 1 = 3.39858 loss)
I0615 16:07:16.241204  5063 solver.cpp:473] Iteration 9580, lr = 0.0001
I0615 16:07:16.997743  5063 solver.cpp:213] Iteration 9590, loss = 3.40812
I0615 16:07:16.997774  5063 solver.cpp:228]     Train net output #0: softmax = 3.40812 (* 1 = 3.40812 loss)
I0615 16:07:16.997779  5063 solver.cpp:473] Iteration 9590, lr = 0.0001
I0615 16:07:17.756940  5063 solver.cpp:213] Iteration 9600, loss = 3.28304
I0615 16:07:17.756980  5063 solver.cpp:228]     Train net output #0: softmax = 3.28304 (* 1 = 3.28304 loss)
I0615 16:07:17.756986  5063 solver.cpp:473] Iteration 9600, lr = 0.0001
I0615 16:07:18.515440  5063 solver.cpp:213] Iteration 9610, loss = 3.44474
I0615 16:07:18.515460  5063 solver.cpp:228]     Train net output #0: softmax = 3.44474 (* 1 = 3.44474 loss)
I0615 16:07:18.515465  5063 solver.cpp:473] Iteration 9610, lr = 0.0001
I0615 16:07:19.274247  5063 solver.cpp:213] Iteration 9620, loss = 3.44245
I0615 16:07:19.274269  5063 solver.cpp:228]     Train net output #0: softmax = 3.44245 (* 1 = 3.44245 loss)
I0615 16:07:19.274273  5063 solver.cpp:473] Iteration 9620, lr = 0.0001
I0615 16:07:20.032683  5063 solver.cpp:213] Iteration 9630, loss = 3.59947
I0615 16:07:20.032703  5063 solver.cpp:228]     Train net output #0: softmax = 3.59947 (* 1 = 3.59947 loss)
I0615 16:07:20.032708  5063 solver.cpp:473] Iteration 9630, lr = 0.0001
I0615 16:07:20.791214  5063 solver.cpp:213] Iteration 9640, loss = 3.4801
I0615 16:07:20.791236  5063 solver.cpp:228]     Train net output #0: softmax = 3.4801 (* 1 = 3.4801 loss)
I0615 16:07:20.791240  5063 solver.cpp:473] Iteration 9640, lr = 0.0001
I0615 16:07:21.550333  5063 solver.cpp:213] Iteration 9650, loss = 3.45697
I0615 16:07:21.550354  5063 solver.cpp:228]     Train net output #0: softmax = 3.45697 (* 1 = 3.45697 loss)
I0615 16:07:21.550359  5063 solver.cpp:473] Iteration 9650, lr = 0.0001
I0615 16:07:22.309092  5063 solver.cpp:213] Iteration 9660, loss = 3.48395
I0615 16:07:22.309114  5063 solver.cpp:228]     Train net output #0: softmax = 3.48395 (* 1 = 3.48395 loss)
I0615 16:07:22.309118  5063 solver.cpp:473] Iteration 9660, lr = 0.0001
I0615 16:07:23.068089  5063 solver.cpp:213] Iteration 9670, loss = 3.41804
I0615 16:07:23.068109  5063 solver.cpp:228]     Train net output #0: softmax = 3.41804 (* 1 = 3.41804 loss)
I0615 16:07:23.068114  5063 solver.cpp:473] Iteration 9670, lr = 0.0001
I0615 16:07:23.824659  5063 solver.cpp:213] Iteration 9680, loss = 3.49631
I0615 16:07:23.824688  5063 solver.cpp:228]     Train net output #0: softmax = 3.49631 (* 1 = 3.49631 loss)
I0615 16:07:23.824693  5063 solver.cpp:473] Iteration 9680, lr = 0.0001
I0615 16:07:24.583140  5063 solver.cpp:213] Iteration 9690, loss = 3.49247
I0615 16:07:24.583163  5063 solver.cpp:228]     Train net output #0: softmax = 3.49247 (* 1 = 3.49247 loss)
I0615 16:07:24.583168  5063 solver.cpp:473] Iteration 9690, lr = 0.0001
I0615 16:07:25.341379  5063 solver.cpp:213] Iteration 9700, loss = 3.48661
I0615 16:07:25.341399  5063 solver.cpp:228]     Train net output #0: softmax = 3.48661 (* 1 = 3.48661 loss)
I0615 16:07:25.341404  5063 solver.cpp:473] Iteration 9700, lr = 0.0001
I0615 16:07:26.099723  5063 solver.cpp:213] Iteration 9710, loss = 3.5825
I0615 16:07:26.099745  5063 solver.cpp:228]     Train net output #0: softmax = 3.5825 (* 1 = 3.5825 loss)
I0615 16:07:26.099750  5063 solver.cpp:473] Iteration 9710, lr = 0.0001
I0615 16:07:26.858971  5063 solver.cpp:213] Iteration 9720, loss = 3.50642
I0615 16:07:26.858996  5063 solver.cpp:228]     Train net output #0: softmax = 3.50642 (* 1 = 3.50642 loss)
I0615 16:07:26.859122  5063 solver.cpp:473] Iteration 9720, lr = 0.0001
I0615 16:07:27.617887  5063 solver.cpp:213] Iteration 9730, loss = 3.35988
I0615 16:07:27.617907  5063 solver.cpp:228]     Train net output #0: softmax = 3.35988 (* 1 = 3.35988 loss)
I0615 16:07:27.617911  5063 solver.cpp:473] Iteration 9730, lr = 0.0001
I0615 16:07:28.376296  5063 solver.cpp:213] Iteration 9740, loss = 3.50835
I0615 16:07:28.376318  5063 solver.cpp:228]     Train net output #0: softmax = 3.50835 (* 1 = 3.50835 loss)
I0615 16:07:28.376322  5063 solver.cpp:473] Iteration 9740, lr = 0.0001
I0615 16:07:29.133080  5063 solver.cpp:213] Iteration 9750, loss = 3.53691
I0615 16:07:29.133100  5063 solver.cpp:228]     Train net output #0: softmax = 3.53691 (* 1 = 3.53691 loss)
I0615 16:07:29.133105  5063 solver.cpp:473] Iteration 9750, lr = 0.0001
I0615 16:07:29.892159  5063 solver.cpp:213] Iteration 9760, loss = 3.45059
I0615 16:07:29.892180  5063 solver.cpp:228]     Train net output #0: softmax = 3.45059 (* 1 = 3.45059 loss)
I0615 16:07:29.892202  5063 solver.cpp:473] Iteration 9760, lr = 0.0001
I0615 16:07:30.650663  5063 solver.cpp:213] Iteration 9770, loss = 3.74224
I0615 16:07:30.650684  5063 solver.cpp:228]     Train net output #0: softmax = 3.74224 (* 1 = 3.74224 loss)
I0615 16:07:30.650688  5063 solver.cpp:473] Iteration 9770, lr = 0.0001
I0615 16:07:31.409909  5063 solver.cpp:213] Iteration 9780, loss = 3.54392
I0615 16:07:31.409942  5063 solver.cpp:228]     Train net output #0: softmax = 3.54392 (* 1 = 3.54392 loss)
I0615 16:07:31.409948  5063 solver.cpp:473] Iteration 9780, lr = 0.0001
I0615 16:07:32.168278  5063 solver.cpp:213] Iteration 9790, loss = 3.29893
I0615 16:07:32.168303  5063 solver.cpp:228]     Train net output #0: softmax = 3.29893 (* 1 = 3.29893 loss)
I0615 16:07:32.168437  5063 solver.cpp:473] Iteration 9790, lr = 0.0001
I0615 16:07:32.926604  5063 solver.cpp:213] Iteration 9800, loss = 3.51255
I0615 16:07:32.926623  5063 solver.cpp:228]     Train net output #0: softmax = 3.51255 (* 1 = 3.51255 loss)
I0615 16:07:32.926628  5063 solver.cpp:473] Iteration 9800, lr = 0.0001
I0615 16:07:33.684223  5063 solver.cpp:213] Iteration 9810, loss = 3.58379
I0615 16:07:33.684243  5063 solver.cpp:228]     Train net output #0: softmax = 3.58379 (* 1 = 3.58379 loss)
I0615 16:07:33.684248  5063 solver.cpp:473] Iteration 9810, lr = 0.0001
I0615 16:07:34.443568  5063 solver.cpp:213] Iteration 9820, loss = 3.38853
I0615 16:07:34.443589  5063 solver.cpp:228]     Train net output #0: softmax = 3.38853 (* 1 = 3.38853 loss)
I0615 16:07:34.443594  5063 solver.cpp:473] Iteration 9820, lr = 0.0001
I0615 16:07:35.202567  5063 solver.cpp:213] Iteration 9830, loss = 3.41673
I0615 16:07:35.202589  5063 solver.cpp:228]     Train net output #0: softmax = 3.41673 (* 1 = 3.41673 loss)
I0615 16:07:35.202594  5063 solver.cpp:473] Iteration 9830, lr = 0.0001
I0615 16:07:35.961696  5063 solver.cpp:213] Iteration 9840, loss = 3.52966
I0615 16:07:35.961724  5063 solver.cpp:228]     Train net output #0: softmax = 3.52966 (* 1 = 3.52966 loss)
I0615 16:07:35.961730  5063 solver.cpp:473] Iteration 9840, lr = 0.0001
I0615 16:07:36.720311  5063 solver.cpp:213] Iteration 9850, loss = 3.48494
I0615 16:07:36.720340  5063 solver.cpp:228]     Train net output #0: softmax = 3.48494 (* 1 = 3.48494 loss)
I0615 16:07:36.720345  5063 solver.cpp:473] Iteration 9850, lr = 0.0001
I0615 16:07:37.478368  5063 solver.cpp:213] Iteration 9860, loss = 3.65887
I0615 16:07:37.478390  5063 solver.cpp:228]     Train net output #0: softmax = 3.65887 (* 1 = 3.65887 loss)
I0615 16:07:37.478395  5063 solver.cpp:473] Iteration 9860, lr = 0.0001
I0615 16:07:38.236032  5063 solver.cpp:213] Iteration 9870, loss = 3.71211
I0615 16:07:38.236052  5063 solver.cpp:228]     Train net output #0: softmax = 3.71211 (* 1 = 3.71211 loss)
I0615 16:07:38.236057  5063 solver.cpp:473] Iteration 9870, lr = 0.0001
I0615 16:07:38.992025  5063 solver.cpp:213] Iteration 9880, loss = 3.54504
I0615 16:07:38.992045  5063 solver.cpp:228]     Train net output #0: softmax = 3.54504 (* 1 = 3.54504 loss)
I0615 16:07:38.992050  5063 solver.cpp:473] Iteration 9880, lr = 0.0001
I0615 16:07:39.750562  5063 solver.cpp:213] Iteration 9890, loss = 3.52115
I0615 16:07:39.750583  5063 solver.cpp:228]     Train net output #0: softmax = 3.52115 (* 1 = 3.52115 loss)
I0615 16:07:39.750588  5063 solver.cpp:473] Iteration 9890, lr = 0.0001
I0615 16:07:40.509433  5063 solver.cpp:213] Iteration 9900, loss = 3.70448
I0615 16:07:40.509454  5063 solver.cpp:228]     Train net output #0: softmax = 3.70448 (* 1 = 3.70448 loss)
I0615 16:07:40.509459  5063 solver.cpp:473] Iteration 9900, lr = 0.0001
I0615 16:07:41.268537  5063 solver.cpp:213] Iteration 9910, loss = 3.52637
I0615 16:07:41.268558  5063 solver.cpp:228]     Train net output #0: softmax = 3.52637 (* 1 = 3.52637 loss)
I0615 16:07:41.268563  5063 solver.cpp:473] Iteration 9910, lr = 0.0001
I0615 16:07:42.027334  5063 solver.cpp:213] Iteration 9920, loss = 3.35605
I0615 16:07:42.027360  5063 solver.cpp:228]     Train net output #0: softmax = 3.35605 (* 1 = 3.35605 loss)
I0615 16:07:42.027384  5063 solver.cpp:473] Iteration 9920, lr = 0.0001
I0615 16:07:42.784662  5063 solver.cpp:213] Iteration 9930, loss = 3.56258
I0615 16:07:42.784687  5063 solver.cpp:228]     Train net output #0: softmax = 3.56258 (* 1 = 3.56258 loss)
I0615 16:07:42.784842  5063 solver.cpp:473] Iteration 9930, lr = 0.0001
I0615 16:07:43.542816  5063 solver.cpp:213] Iteration 9940, loss = 3.62347
I0615 16:07:43.542840  5063 solver.cpp:228]     Train net output #0: softmax = 3.62347 (* 1 = 3.62347 loss)
I0615 16:07:43.542845  5063 solver.cpp:473] Iteration 9940, lr = 0.0001
I0615 16:07:44.301259  5063 solver.cpp:213] Iteration 9950, loss = 3.61763
I0615 16:07:44.301280  5063 solver.cpp:228]     Train net output #0: softmax = 3.61763 (* 1 = 3.61763 loss)
I0615 16:07:44.301286  5063 solver.cpp:473] Iteration 9950, lr = 0.0001
I0615 16:07:45.060276  5063 solver.cpp:213] Iteration 9960, loss = 3.41401
I0615 16:07:45.060297  5063 solver.cpp:228]     Train net output #0: softmax = 3.41401 (* 1 = 3.41401 loss)
I0615 16:07:45.060302  5063 solver.cpp:473] Iteration 9960, lr = 0.0001
I0615 16:07:45.819362  5063 solver.cpp:213] Iteration 9970, loss = 3.37995
I0615 16:07:45.819383  5063 solver.cpp:228]     Train net output #0: softmax = 3.37995 (* 1 = 3.37995 loss)
I0615 16:07:45.819388  5063 solver.cpp:473] Iteration 9970, lr = 0.0001
I0615 16:07:46.578109  5063 solver.cpp:213] Iteration 9980, loss = 3.35917
I0615 16:07:46.578131  5063 solver.cpp:228]     Train net output #0: softmax = 3.35917 (* 1 = 3.35917 loss)
I0615 16:07:46.578136  5063 solver.cpp:473] Iteration 9980, lr = 0.0001
I0615 16:07:47.336460  5063 solver.cpp:213] Iteration 9990, loss = 3.41544
I0615 16:07:47.336488  5063 solver.cpp:228]     Train net output #0: softmax = 3.41544 (* 1 = 3.41544 loss)
I0615 16:07:47.336493  5063 solver.cpp:473] Iteration 9990, lr = 0.0001
I0615 16:07:48.042104  5063 solver.cpp:362] Snapshotting to snapshots/16-06-15_15h49m18s_0_11_pretrainClassification_iter_10000.caffemodel
I0615 16:07:48.042814  5063 solver.cpp:370] Snapshotting solver state to snapshots/16-06-15_15h49m18s_0_11_pretrainClassification_iter_10000.solverstate
I0615 16:07:48.043191  5063 solver.cpp:291] Iteration 10000, Testing net (#0)
I0615 16:07:48.137415  5063 solver.cpp:342]     Test net output #0: accuracy_top_1 = 0.178125
I0615 16:07:48.137431  5063 solver.cpp:342]     Test net output #1: accuracy_top_5 = 0.415625
I0615 16:07:48.137437  5063 solver.cpp:342]     Test net output #2: softmax = 3.54261 (* 1 = 3.54261 loss)
I0615 16:07:48.191035  5063 solver.cpp:213] Iteration 10000, loss = 3.55803
I0615 16:07:48.191047  5063 solver.cpp:228]     Train net output #0: softmax = 3.55803 (* 1 = 3.55803 loss)
I0615 16:07:48.191052  5063 solver.cpp:473] Iteration 10000, lr = 0.0001
I0615 16:07:48.949311  5063 solver.cpp:213] Iteration 10010, loss = 3.39586
I0615 16:07:48.949332  5063 solver.cpp:228]     Train net output #0: softmax = 3.39586 (* 1 = 3.39586 loss)
I0615 16:07:48.949337  5063 solver.cpp:473] Iteration 10010, lr = 0.0001
I0615 16:07:49.708130  5063 solver.cpp:213] Iteration 10020, loss = 3.35216
I0615 16:07:49.708150  5063 solver.cpp:228]     Train net output #0: softmax = 3.35216 (* 1 = 3.35216 loss)
I0615 16:07:49.708155  5063 solver.cpp:473] Iteration 10020, lr = 0.0001
I0615 16:07:50.467064  5063 solver.cpp:213] Iteration 10030, loss = 3.51856
I0615 16:07:50.467085  5063 solver.cpp:228]     Train net output #0: softmax = 3.51856 (* 1 = 3.51856 loss)
I0615 16:07:50.467090  5063 solver.cpp:473] Iteration 10030, lr = 0.0001
I0615 16:07:51.226090  5063 solver.cpp:213] Iteration 10040, loss = 3.44858
I0615 16:07:51.226111  5063 solver.cpp:228]     Train net output #0: softmax = 3.44858 (* 1 = 3.44858 loss)
I0615 16:07:51.226116  5063 solver.cpp:473] Iteration 10040, lr = 0.0001
I0615 16:07:51.985076  5063 solver.cpp:213] Iteration 10050, loss = 3.55126
I0615 16:07:51.985097  5063 solver.cpp:228]     Train net output #0: softmax = 3.55126 (* 1 = 3.55126 loss)
I0615 16:07:51.985102  5063 solver.cpp:473] Iteration 10050, lr = 0.0001
I0615 16:07:52.743007  5063 solver.cpp:213] Iteration 10060, loss = 3.47249
I0615 16:07:52.743028  5063 solver.cpp:228]     Train net output #0: softmax = 3.47249 (* 1 = 3.47249 loss)
I0615 16:07:52.743033  5063 solver.cpp:473] Iteration 10060, lr = 0.0001
I0615 16:07:53.500963  5063 solver.cpp:213] Iteration 10070, loss = 3.45707
I0615 16:07:53.500984  5063 solver.cpp:228]     Train net output #0: softmax = 3.45707 (* 1 = 3.45707 loss)
I0615 16:07:53.500989  5063 solver.cpp:473] Iteration 10070, lr = 0.0001
I0615 16:07:54.259773  5063 solver.cpp:213] Iteration 10080, loss = 3.59617
I0615 16:07:54.259788  5063 solver.cpp:228]     Train net output #0: softmax = 3.59617 (* 1 = 3.59617 loss)
I0615 16:07:54.259793  5063 solver.cpp:473] Iteration 10080, lr = 0.0001
I0615 16:07:55.018515  5063 solver.cpp:213] Iteration 10090, loss = 3.56983
I0615 16:07:55.018530  5063 solver.cpp:228]     Train net output #0: softmax = 3.56983 (* 1 = 3.56983 loss)
I0615 16:07:55.018535  5063 solver.cpp:473] Iteration 10090, lr = 0.0001
I0615 16:07:55.777449  5063 solver.cpp:213] Iteration 10100, loss = 3.56509
I0615 16:07:55.777463  5063 solver.cpp:228]     Train net output #0: softmax = 3.56509 (* 1 = 3.56509 loss)
I0615 16:07:55.777468  5063 solver.cpp:473] Iteration 10100, lr = 0.0001
I0615 16:07:56.535923  5063 solver.cpp:213] Iteration 10110, loss = 3.40181
I0615 16:07:56.535943  5063 solver.cpp:228]     Train net output #0: softmax = 3.40181 (* 1 = 3.40181 loss)
I0615 16:07:56.535948  5063 solver.cpp:473] Iteration 10110, lr = 0.0001
I0615 16:07:57.294366  5063 solver.cpp:213] Iteration 10120, loss = 3.45778
I0615 16:07:57.294386  5063 solver.cpp:228]     Train net output #0: softmax = 3.45778 (* 1 = 3.45778 loss)
I0615 16:07:57.294390  5063 solver.cpp:473] Iteration 10120, lr = 0.0001
I0615 16:07:58.052968  5063 solver.cpp:213] Iteration 10130, loss = 3.41849
I0615 16:07:58.052986  5063 solver.cpp:228]     Train net output #0: softmax = 3.41849 (* 1 = 3.41849 loss)
I0615 16:07:58.053004  5063 solver.cpp:473] Iteration 10130, lr = 0.0001
I0615 16:07:58.811906  5063 solver.cpp:213] Iteration 10140, loss = 3.49663
I0615 16:07:58.811923  5063 solver.cpp:228]     Train net output #0: softmax = 3.49663 (* 1 = 3.49663 loss)
I0615 16:07:58.811928  5063 solver.cpp:473] Iteration 10140, lr = 0.0001
I0615 16:07:59.570840  5063 solver.cpp:213] Iteration 10150, loss = 3.44652
I0615 16:07:59.570857  5063 solver.cpp:228]     Train net output #0: softmax = 3.44652 (* 1 = 3.44652 loss)
I0615 16:07:59.570860  5063 solver.cpp:473] Iteration 10150, lr = 0.0001
I0615 16:08:00.329403  5063 solver.cpp:213] Iteration 10160, loss = 3.81848
I0615 16:08:00.329418  5063 solver.cpp:228]     Train net output #0: softmax = 3.81848 (* 1 = 3.81848 loss)
I0615 16:08:00.329423  5063 solver.cpp:473] Iteration 10160, lr = 0.0001
I0615 16:08:01.088302  5063 solver.cpp:213] Iteration 10170, loss = 3.50212
I0615 16:08:01.088317  5063 solver.cpp:228]     Train net output #0: softmax = 3.50212 (* 1 = 3.50212 loss)
I0615 16:08:01.088322  5063 solver.cpp:473] Iteration 10170, lr = 0.0001
I0615 16:08:01.847069  5063 solver.cpp:213] Iteration 10180, loss = 3.64332
I0615 16:08:01.847085  5063 solver.cpp:228]     Train net output #0: softmax = 3.64332 (* 1 = 3.64332 loss)
I0615 16:08:01.847090  5063 solver.cpp:473] Iteration 10180, lr = 0.0001
I0615 16:08:02.605623  5063 solver.cpp:213] Iteration 10190, loss = 3.66816
I0615 16:08:02.605639  5063 solver.cpp:228]     Train net output #0: softmax = 3.66816 (* 1 = 3.66816 loss)
I0615 16:08:02.605643  5063 solver.cpp:473] Iteration 10190, lr = 0.0001
I0615 16:08:03.364364  5063 solver.cpp:213] Iteration 10200, loss = 3.35183
I0615 16:08:03.364382  5063 solver.cpp:228]     Train net output #0: softmax = 3.35183 (* 1 = 3.35183 loss)
I0615 16:08:03.364522  5063 solver.cpp:473] Iteration 10200, lr = 0.0001
I0615 16:08:04.123141  5063 solver.cpp:213] Iteration 10210, loss = 3.55992
I0615 16:08:04.123155  5063 solver.cpp:228]     Train net output #0: softmax = 3.55992 (* 1 = 3.55992 loss)
I0615 16:08:04.123159  5063 solver.cpp:473] Iteration 10210, lr = 0.0001
I0615 16:08:04.881855  5063 solver.cpp:213] Iteration 10220, loss = 3.44393
I0615 16:08:04.881868  5063 solver.cpp:228]     Train net output #0: softmax = 3.44393 (* 1 = 3.44393 loss)
I0615 16:08:04.881873  5063 solver.cpp:473] Iteration 10220, lr = 0.0001
I0615 16:08:05.640120  5063 solver.cpp:213] Iteration 10230, loss = 3.5974
I0615 16:08:05.640138  5063 solver.cpp:228]     Train net output #0: softmax = 3.5974 (* 1 = 3.5974 loss)
I0615 16:08:05.640143  5063 solver.cpp:473] Iteration 10230, lr = 0.0001
I0615 16:08:06.399114  5063 solver.cpp:213] Iteration 10240, loss = 3.6184
I0615 16:08:06.399128  5063 solver.cpp:228]     Train net output #0: softmax = 3.6184 (* 1 = 3.6184 loss)
I0615 16:08:06.399132  5063 solver.cpp:473] Iteration 10240, lr = 0.0001
I0615 16:08:07.157815  5063 solver.cpp:213] Iteration 10250, loss = 3.47723
I0615 16:08:07.157830  5063 solver.cpp:228]     Train net output #0: softmax = 3.47723 (* 1 = 3.47723 loss)
I0615 16:08:07.157835  5063 solver.cpp:473] Iteration 10250, lr = 0.0001
I0615 16:08:07.916533  5063 solver.cpp:213] Iteration 10260, loss = 3.74625
I0615 16:08:07.916548  5063 solver.cpp:228]     Train net output #0: softmax = 3.74625 (* 1 = 3.74625 loss)
I0615 16:08:07.916553  5063 solver.cpp:473] Iteration 10260, lr = 0.0001
I0615 16:08:08.673182  5063 solver.cpp:213] Iteration 10270, loss = 3.59897
I0615 16:08:08.673205  5063 solver.cpp:228]     Train net output #0: softmax = 3.59897 (* 1 = 3.59897 loss)
I0615 16:08:08.673328  5063 solver.cpp:473] Iteration 10270, lr = 0.0001
I0615 16:08:09.431318  5063 solver.cpp:213] Iteration 10280, loss = 3.49265
I0615 16:08:09.431336  5063 solver.cpp:228]     Train net output #0: softmax = 3.49265 (* 1 = 3.49265 loss)
I0615 16:08:09.431340  5063 solver.cpp:473] Iteration 10280, lr = 0.0001
I0615 16:08:10.189680  5063 solver.cpp:213] Iteration 10290, loss = 3.52177
I0615 16:08:10.189695  5063 solver.cpp:228]     Train net output #0: softmax = 3.52177 (* 1 = 3.52177 loss)
I0615 16:08:10.189707  5063 solver.cpp:473] Iteration 10290, lr = 0.0001
I0615 16:08:10.948338  5063 solver.cpp:213] Iteration 10300, loss = 3.50047
I0615 16:08:10.948353  5063 solver.cpp:228]     Train net output #0: softmax = 3.50047 (* 1 = 3.50047 loss)
I0615 16:08:10.948356  5063 solver.cpp:473] Iteration 10300, lr = 0.0001
I0615 16:08:11.706887  5063 solver.cpp:213] Iteration 10310, loss = 3.61269
I0615 16:08:11.706902  5063 solver.cpp:228]     Train net output #0: softmax = 3.61269 (* 1 = 3.61269 loss)
I0615 16:08:11.706905  5063 solver.cpp:473] Iteration 10310, lr = 0.0001
I0615 16:08:12.464915  5063 solver.cpp:213] Iteration 10320, loss = 3.29562
I0615 16:08:12.464932  5063 solver.cpp:228]     Train net output #0: softmax = 3.29562 (* 1 = 3.29562 loss)
I0615 16:08:12.464936  5063 solver.cpp:473] Iteration 10320, lr = 0.0001
I0615 16:08:13.222723  5063 solver.cpp:213] Iteration 10330, loss = 3.39186
I0615 16:08:13.222741  5063 solver.cpp:228]     Train net output #0: softmax = 3.39186 (* 1 = 3.39186 loss)
I0615 16:08:13.222745  5063 solver.cpp:473] Iteration 10330, lr = 0.0001
I0615 16:08:13.979255  5063 solver.cpp:213] Iteration 10340, loss = 3.65353
I0615 16:08:13.979275  5063 solver.cpp:228]     Train net output #0: softmax = 3.65353 (* 1 = 3.65353 loss)
I0615 16:08:13.979411  5063 solver.cpp:473] Iteration 10340, lr = 0.0001
I0615 16:08:14.737303  5063 solver.cpp:213] Iteration 10350, loss = 3.56916
I0615 16:08:14.737318  5063 solver.cpp:228]     Train net output #0: softmax = 3.56916 (* 1 = 3.56916 loss)
I0615 16:08:14.737323  5063 solver.cpp:473] Iteration 10350, lr = 0.0001
I0615 16:08:15.495671  5063 solver.cpp:213] Iteration 10360, loss = 3.49028
I0615 16:08:15.495687  5063 solver.cpp:228]     Train net output #0: softmax = 3.49028 (* 1 = 3.49028 loss)
I0615 16:08:15.495690  5063 solver.cpp:473] Iteration 10360, lr = 0.0001
I0615 16:08:16.254432  5063 solver.cpp:213] Iteration 10370, loss = 3.64205
I0615 16:08:16.254447  5063 solver.cpp:228]     Train net output #0: softmax = 3.64205 (* 1 = 3.64205 loss)
I0615 16:08:16.254452  5063 solver.cpp:473] Iteration 10370, lr = 0.0001
I0615 16:08:17.012339  5063 solver.cpp:213] Iteration 10380, loss = 3.61406
I0615 16:08:17.012358  5063 solver.cpp:228]     Train net output #0: softmax = 3.61406 (* 1 = 3.61406 loss)
I0615 16:08:17.012362  5063 solver.cpp:473] Iteration 10380, lr = 0.0001
I0615 16:08:17.770669  5063 solver.cpp:213] Iteration 10390, loss = 3.62368
I0615 16:08:17.770685  5063 solver.cpp:228]     Train net output #0: softmax = 3.62368 (* 1 = 3.62368 loss)
I0615 16:08:17.770690  5063 solver.cpp:473] Iteration 10390, lr = 0.0001
I0615 16:08:18.529414  5063 solver.cpp:213] Iteration 10400, loss = 3.41473
I0615 16:08:18.529464  5063 solver.cpp:228]     Train net output #0: softmax = 3.41473 (* 1 = 3.41473 loss)
I0615 16:08:18.529470  5063 solver.cpp:473] Iteration 10400, lr = 0.0001
I0615 16:08:19.287775  5063 solver.cpp:213] Iteration 10410, loss = 3.59258
I0615 16:08:19.287794  5063 solver.cpp:228]     Train net output #0: softmax = 3.59258 (* 1 = 3.59258 loss)
I0615 16:08:19.287925  5063 solver.cpp:473] Iteration 10410, lr = 0.0001
I0615 16:08:20.046695  5063 solver.cpp:213] Iteration 10420, loss = 3.67958
I0615 16:08:20.046710  5063 solver.cpp:228]     Train net output #0: softmax = 3.67958 (* 1 = 3.67958 loss)
I0615 16:08:20.046715  5063 solver.cpp:473] Iteration 10420, lr = 0.0001
I0615 16:08:20.804668  5063 solver.cpp:213] Iteration 10430, loss = 3.72314
I0615 16:08:20.804687  5063 solver.cpp:228]     Train net output #0: softmax = 3.72314 (* 1 = 3.72314 loss)
I0615 16:08:20.804692  5063 solver.cpp:473] Iteration 10430, lr = 0.0001
I0615 16:08:21.563189  5063 solver.cpp:213] Iteration 10440, loss = 3.6227
I0615 16:08:21.563205  5063 solver.cpp:228]     Train net output #0: softmax = 3.6227 (* 1 = 3.6227 loss)
I0615 16:08:21.563208  5063 solver.cpp:473] Iteration 10440, lr = 0.0001
I0615 16:08:22.321595  5063 solver.cpp:213] Iteration 10450, loss = 3.60166
I0615 16:08:22.321611  5063 solver.cpp:228]     Train net output #0: softmax = 3.60166 (* 1 = 3.60166 loss)
I0615 16:08:22.321624  5063 solver.cpp:473] Iteration 10450, lr = 0.0001
I0615 16:08:23.080471  5063 solver.cpp:213] Iteration 10460, loss = 3.46641
I0615 16:08:23.080487  5063 solver.cpp:228]     Train net output #0: softmax = 3.46641 (* 1 = 3.46641 loss)
I0615 16:08:23.080490  5063 solver.cpp:473] Iteration 10460, lr = 0.0001
I0615 16:08:23.839587  5063 solver.cpp:213] Iteration 10470, loss = 3.53385
I0615 16:08:23.839604  5063 solver.cpp:228]     Train net output #0: softmax = 3.53385 (* 1 = 3.53385 loss)
I0615 16:08:23.839609  5063 solver.cpp:473] Iteration 10470, lr = 0.0001
I0615 16:08:24.598633  5063 solver.cpp:213] Iteration 10480, loss = 3.42224
I0615 16:08:24.598650  5063 solver.cpp:228]     Train net output #0: softmax = 3.42224 (* 1 = 3.42224 loss)
I0615 16:08:24.598655  5063 solver.cpp:473] Iteration 10480, lr = 0.0001
I0615 16:08:25.357661  5063 solver.cpp:213] Iteration 10490, loss = 3.3914
I0615 16:08:25.357677  5063 solver.cpp:228]     Train net output #0: softmax = 3.3914 (* 1 = 3.3914 loss)
I0615 16:08:25.357681  5063 solver.cpp:473] Iteration 10490, lr = 0.0001
I0615 16:08:26.116652  5063 solver.cpp:213] Iteration 10500, loss = 3.29389
I0615 16:08:26.116669  5063 solver.cpp:228]     Train net output #0: softmax = 3.29389 (* 1 = 3.29389 loss)
I0615 16:08:26.116673  5063 solver.cpp:473] Iteration 10500, lr = 0.0001
I0615 16:08:26.875039  5063 solver.cpp:213] Iteration 10510, loss = 3.57128
I0615 16:08:26.875054  5063 solver.cpp:228]     Train net output #0: softmax = 3.57128 (* 1 = 3.57128 loss)
I0615 16:08:26.875058  5063 solver.cpp:473] Iteration 10510, lr = 0.0001
I0615 16:08:27.633451  5063 solver.cpp:213] Iteration 10520, loss = 3.31985
I0615 16:08:27.633466  5063 solver.cpp:228]     Train net output #0: softmax = 3.31985 (* 1 = 3.31985 loss)
I0615 16:08:27.633471  5063 solver.cpp:473] Iteration 10520, lr = 0.0001
I0615 16:08:28.391685  5063 solver.cpp:213] Iteration 10530, loss = 3.51494
I0615 16:08:28.391701  5063 solver.cpp:228]     Train net output #0: softmax = 3.51494 (* 1 = 3.51494 loss)
I0615 16:08:28.391706  5063 solver.cpp:473] Iteration 10530, lr = 0.0001
I0615 16:08:29.150213  5063 solver.cpp:213] Iteration 10540, loss = 3.53918
I0615 16:08:29.150229  5063 solver.cpp:228]     Train net output #0: softmax = 3.53918 (* 1 = 3.53918 loss)
I0615 16:08:29.150234  5063 solver.cpp:473] Iteration 10540, lr = 0.0001
I0615 16:08:29.908918  5063 solver.cpp:213] Iteration 10550, loss = 3.54098
I0615 16:08:29.908936  5063 solver.cpp:228]     Train net output #0: softmax = 3.54098 (* 1 = 3.54098 loss)
I0615 16:08:29.908946  5063 solver.cpp:473] Iteration 10550, lr = 0.0001
I0615 16:08:30.667743  5063 solver.cpp:213] Iteration 10560, loss = 3.33134
I0615 16:08:30.667771  5063 solver.cpp:228]     Train net output #0: softmax = 3.33134 (* 1 = 3.33134 loss)
I0615 16:08:30.667776  5063 solver.cpp:473] Iteration 10560, lr = 0.0001
I0615 16:08:31.426525  5063 solver.cpp:213] Iteration 10570, loss = 3.65896
I0615 16:08:31.426542  5063 solver.cpp:228]     Train net output #0: softmax = 3.65896 (* 1 = 3.65896 loss)
I0615 16:08:31.426547  5063 solver.cpp:473] Iteration 10570, lr = 0.0001
I0615 16:08:32.185173  5063 solver.cpp:213] Iteration 10580, loss = 3.52713
I0615 16:08:32.185189  5063 solver.cpp:228]     Train net output #0: softmax = 3.52713 (* 1 = 3.52713 loss)
I0615 16:08:32.185194  5063 solver.cpp:473] Iteration 10580, lr = 0.0001
I0615 16:08:32.943006  5063 solver.cpp:213] Iteration 10590, loss = 3.3137
I0615 16:08:32.943023  5063 solver.cpp:228]     Train net output #0: softmax = 3.3137 (* 1 = 3.3137 loss)
I0615 16:08:32.943028  5063 solver.cpp:473] Iteration 10590, lr = 0.0001
I0615 16:08:33.701947  5063 solver.cpp:213] Iteration 10600, loss = 3.49581
I0615 16:08:33.701962  5063 solver.cpp:228]     Train net output #0: softmax = 3.49581 (* 1 = 3.49581 loss)
I0615 16:08:33.701967  5063 solver.cpp:473] Iteration 10600, lr = 0.0001
I0615 16:08:34.457401  5063 solver.cpp:213] Iteration 10610, loss = 3.42529
I0615 16:08:34.457419  5063 solver.cpp:228]     Train net output #0: softmax = 3.42529 (* 1 = 3.42529 loss)
I0615 16:08:34.457432  5063 solver.cpp:473] Iteration 10610, lr = 0.0001
I0615 16:08:35.215896  5063 solver.cpp:213] Iteration 10620, loss = 3.3656
I0615 16:08:35.215917  5063 solver.cpp:228]     Train net output #0: softmax = 3.3656 (* 1 = 3.3656 loss)
I0615 16:08:35.216048  5063 solver.cpp:473] Iteration 10620, lr = 0.0001
I0615 16:08:35.974642  5063 solver.cpp:213] Iteration 10630, loss = 3.62295
I0615 16:08:35.974658  5063 solver.cpp:228]     Train net output #0: softmax = 3.62295 (* 1 = 3.62295 loss)
I0615 16:08:35.974663  5063 solver.cpp:473] Iteration 10630, lr = 0.0001
I0615 16:08:36.733305  5063 solver.cpp:213] Iteration 10640, loss = 3.43419
I0615 16:08:36.733324  5063 solver.cpp:228]     Train net output #0: softmax = 3.43419 (* 1 = 3.43419 loss)
I0615 16:08:36.733327  5063 solver.cpp:473] Iteration 10640, lr = 0.0001
I0615 16:08:37.492055  5063 solver.cpp:213] Iteration 10650, loss = 3.60131
I0615 16:08:37.492071  5063 solver.cpp:228]     Train net output #0: softmax = 3.60131 (* 1 = 3.60131 loss)
I0615 16:08:37.492076  5063 solver.cpp:473] Iteration 10650, lr = 0.0001
I0615 16:08:38.250573  5063 solver.cpp:213] Iteration 10660, loss = 3.54447
I0615 16:08:38.250592  5063 solver.cpp:228]     Train net output #0: softmax = 3.54447 (* 1 = 3.54447 loss)
I0615 16:08:38.250597  5063 solver.cpp:473] Iteration 10660, lr = 0.0001
I0615 16:08:39.009439  5063 solver.cpp:213] Iteration 10670, loss = 3.49832
I0615 16:08:39.009454  5063 solver.cpp:228]     Train net output #0: softmax = 3.49832 (* 1 = 3.49832 loss)
I0615 16:08:39.009459  5063 solver.cpp:473] Iteration 10670, lr = 0.0001
I0615 16:08:39.768633  5063 solver.cpp:213] Iteration 10680, loss = 3.4712
I0615 16:08:39.768647  5063 solver.cpp:228]     Train net output #0: softmax = 3.4712 (* 1 = 3.4712 loss)
I0615 16:08:39.768652  5063 solver.cpp:473] Iteration 10680, lr = 0.0001
I0615 16:08:40.527480  5063 solver.cpp:213] Iteration 10690, loss = 3.49402
I0615 16:08:40.527500  5063 solver.cpp:228]     Train net output #0: softmax = 3.49402 (* 1 = 3.49402 loss)
I0615 16:08:40.527504  5063 solver.cpp:473] Iteration 10690, lr = 0.0001
I0615 16:08:41.285997  5063 solver.cpp:213] Iteration 10700, loss = 3.59262
I0615 16:08:41.286012  5063 solver.cpp:228]     Train net output #0: softmax = 3.59262 (* 1 = 3.59262 loss)
I0615 16:08:41.286016  5063 solver.cpp:473] Iteration 10700, lr = 0.0001
I0615 16:08:42.044963  5063 solver.cpp:213] Iteration 10710, loss = 3.29074
I0615 16:08:42.044978  5063 solver.cpp:228]     Train net output #0: softmax = 3.29074 (* 1 = 3.29074 loss)
I0615 16:08:42.044983  5063 solver.cpp:473] Iteration 10710, lr = 0.0001
I0615 16:08:42.803611  5063 solver.cpp:213] Iteration 10720, loss = 3.41804
I0615 16:08:42.803642  5063 solver.cpp:228]     Train net output #0: softmax = 3.41804 (* 1 = 3.41804 loss)
I0615 16:08:42.803647  5063 solver.cpp:473] Iteration 10720, lr = 0.0001
I0615 16:08:43.562463  5063 solver.cpp:213] Iteration 10730, loss = 3.53861
I0615 16:08:43.562479  5063 solver.cpp:228]     Train net output #0: softmax = 3.53861 (* 1 = 3.53861 loss)
I0615 16:08:43.562484  5063 solver.cpp:473] Iteration 10730, lr = 0.0001
I0615 16:08:44.320907  5063 solver.cpp:213] Iteration 10740, loss = 3.48666
I0615 16:08:44.320924  5063 solver.cpp:228]     Train net output #0: softmax = 3.48666 (* 1 = 3.48666 loss)
I0615 16:08:44.320927  5063 solver.cpp:473] Iteration 10740, lr = 0.0001
I0615 16:08:45.080011  5063 solver.cpp:213] Iteration 10750, loss = 3.41254
I0615 16:08:45.080026  5063 solver.cpp:228]     Train net output #0: softmax = 3.41254 (* 1 = 3.41254 loss)
I0615 16:08:45.080030  5063 solver.cpp:473] Iteration 10750, lr = 0.0001
I0615 16:08:45.839032  5063 solver.cpp:213] Iteration 10760, loss = 3.57314
I0615 16:08:45.839051  5063 solver.cpp:228]     Train net output #0: softmax = 3.57314 (* 1 = 3.57314 loss)
I0615 16:08:45.839182  5063 solver.cpp:473] Iteration 10760, lr = 0.0001
I0615 16:08:46.598253  5063 solver.cpp:213] Iteration 10770, loss = 3.41255
I0615 16:08:46.598266  5063 solver.cpp:228]     Train net output #0: softmax = 3.41255 (* 1 = 3.41255 loss)
I0615 16:08:46.598278  5063 solver.cpp:473] Iteration 10770, lr = 0.0001
I0615 16:08:47.356735  5063 solver.cpp:213] Iteration 10780, loss = 3.38327
I0615 16:08:47.356752  5063 solver.cpp:228]     Train net output #0: softmax = 3.38327 (* 1 = 3.38327 loss)
I0615 16:08:47.356757  5063 solver.cpp:473] Iteration 10780, lr = 0.0001
I0615 16:08:48.115836  5063 solver.cpp:213] Iteration 10790, loss = 3.55985
I0615 16:08:48.115851  5063 solver.cpp:228]     Train net output #0: softmax = 3.55985 (* 1 = 3.55985 loss)
I0615 16:08:48.115856  5063 solver.cpp:473] Iteration 10790, lr = 0.0001
I0615 16:08:48.874379  5063 solver.cpp:213] Iteration 10800, loss = 3.49541
I0615 16:08:48.874438  5063 solver.cpp:228]     Train net output #0: softmax = 3.49541 (* 1 = 3.49541 loss)
I0615 16:08:48.874451  5063 solver.cpp:473] Iteration 10800, lr = 0.0001
I0615 16:08:49.633386  5063 solver.cpp:213] Iteration 10810, loss = 3.54656
I0615 16:08:49.633400  5063 solver.cpp:228]     Train net output #0: softmax = 3.54656 (* 1 = 3.54656 loss)
I0615 16:08:49.633405  5063 solver.cpp:473] Iteration 10810, lr = 0.0001
I0615 16:08:50.391824  5063 solver.cpp:213] Iteration 10820, loss = 3.9001
I0615 16:08:50.391837  5063 solver.cpp:228]     Train net output #0: softmax = 3.9001 (* 1 = 3.9001 loss)
I0615 16:08:50.391842  5063 solver.cpp:473] Iteration 10820, lr = 0.0001
I0615 16:08:51.150368  5063 solver.cpp:213] Iteration 10830, loss = 3.46676
I0615 16:08:51.150387  5063 solver.cpp:228]     Train net output #0: softmax = 3.46676 (* 1 = 3.46676 loss)
I0615 16:08:51.150514  5063 solver.cpp:473] Iteration 10830, lr = 0.0001
I0615 16:08:51.908998  5063 solver.cpp:213] Iteration 10840, loss = 3.40594
I0615 16:08:51.909013  5063 solver.cpp:228]     Train net output #0: softmax = 3.40594 (* 1 = 3.40594 loss)
I0615 16:08:51.909018  5063 solver.cpp:473] Iteration 10840, lr = 0.0001
I0615 16:08:52.667068  5063 solver.cpp:213] Iteration 10850, loss = 3.67272
I0615 16:08:52.667084  5063 solver.cpp:228]     Train net output #0: softmax = 3.67272 (* 1 = 3.67272 loss)
I0615 16:08:52.667089  5063 solver.cpp:473] Iteration 10850, lr = 0.0001
I0615 16:08:53.424624  5063 solver.cpp:213] Iteration 10860, loss = 3.37544
I0615 16:08:53.424640  5063 solver.cpp:228]     Train net output #0: softmax = 3.37544 (* 1 = 3.37544 loss)
I0615 16:08:53.424644  5063 solver.cpp:473] Iteration 10860, lr = 0.0001
I0615 16:08:54.182886  5063 solver.cpp:213] Iteration 10870, loss = 3.40778
I0615 16:08:54.182904  5063 solver.cpp:228]     Train net output #0: softmax = 3.40778 (* 1 = 3.40778 loss)
I0615 16:08:54.182909  5063 solver.cpp:473] Iteration 10870, lr = 0.0001
I0615 16:08:54.941787  5063 solver.cpp:213] Iteration 10880, loss = 3.34648
I0615 16:08:54.941802  5063 solver.cpp:228]     Train net output #0: softmax = 3.34648 (* 1 = 3.34648 loss)
I0615 16:08:54.941807  5063 solver.cpp:473] Iteration 10880, lr = 0.0001
I0615 16:08:55.700860  5063 solver.cpp:213] Iteration 10890, loss = 3.50105
I0615 16:08:55.700875  5063 solver.cpp:228]     Train net output #0: softmax = 3.50105 (* 1 = 3.50105 loss)
I0615 16:08:55.700880  5063 solver.cpp:473] Iteration 10890, lr = 0.0001
I0615 16:08:56.459779  5063 solver.cpp:213] Iteration 10900, loss = 3.55586
I0615 16:08:56.459797  5063 solver.cpp:228]     Train net output #0: softmax = 3.55586 (* 1 = 3.55586 loss)
I0615 16:08:56.459801  5063 solver.cpp:473] Iteration 10900, lr = 0.0001
I0615 16:08:57.218096  5063 solver.cpp:213] Iteration 10910, loss = 3.42085
I0615 16:08:57.218113  5063 solver.cpp:228]     Train net output #0: softmax = 3.42085 (* 1 = 3.42085 loss)
I0615 16:08:57.218118  5063 solver.cpp:473] Iteration 10910, lr = 0.0001
I0615 16:08:57.977826  5063 solver.cpp:213] Iteration 10920, loss = 3.37236
I0615 16:08:57.977840  5063 solver.cpp:228]     Train net output #0: softmax = 3.37236 (* 1 = 3.37236 loss)
I0615 16:08:57.977845  5063 solver.cpp:473] Iteration 10920, lr = 0.0001
I0615 16:08:58.734984  5063 solver.cpp:213] Iteration 10930, loss = 3.53366
I0615 16:08:58.735005  5063 solver.cpp:228]     Train net output #0: softmax = 3.53366 (* 1 = 3.53366 loss)
I0615 16:08:58.735016  5063 solver.cpp:473] Iteration 10930, lr = 0.0001
I0615 16:08:59.493899  5063 solver.cpp:213] Iteration 10940, loss = 3.30121
I0615 16:08:59.493916  5063 solver.cpp:228]     Train net output #0: softmax = 3.30121 (* 1 = 3.30121 loss)
I0615 16:08:59.493921  5063 solver.cpp:473] Iteration 10940, lr = 0.0001
I0615 16:09:00.252279  5063 solver.cpp:213] Iteration 10950, loss = 3.38374
I0615 16:09:00.252295  5063 solver.cpp:228]     Train net output #0: softmax = 3.38374 (* 1 = 3.38374 loss)
I0615 16:09:00.252298  5063 solver.cpp:473] Iteration 10950, lr = 0.0001
I0615 16:09:01.010671  5063 solver.cpp:213] Iteration 10960, loss = 3.39718
I0615 16:09:01.010709  5063 solver.cpp:228]     Train net output #0: softmax = 3.39718 (* 1 = 3.39718 loss)
I0615 16:09:01.010713  5063 solver.cpp:473] Iteration 10960, lr = 0.0001
I0615 16:09:01.769201  5063 solver.cpp:213] Iteration 10970, loss = 3.4106
I0615 16:09:01.769222  5063 solver.cpp:228]     Train net output #0: softmax = 3.4106 (* 1 = 3.4106 loss)
I0615 16:09:01.769356  5063 solver.cpp:473] Iteration 10970, lr = 0.0001
I0615 16:09:02.527957  5063 solver.cpp:213] Iteration 10980, loss = 3.46994
I0615 16:09:02.527976  5063 solver.cpp:228]     Train net output #0: softmax = 3.46994 (* 1 = 3.46994 loss)
I0615 16:09:02.527979  5063 solver.cpp:473] Iteration 10980, lr = 0.0001
I0615 16:09:03.286804  5063 solver.cpp:213] Iteration 10990, loss = 3.3956
I0615 16:09:03.286820  5063 solver.cpp:228]     Train net output #0: softmax = 3.3956 (* 1 = 3.3956 loss)
I0615 16:09:03.286825  5063 solver.cpp:473] Iteration 10990, lr = 0.0001
I0615 16:09:03.991940  5063 solver.cpp:362] Snapshotting to snapshots/16-06-15_15h49m18s_0_11_pretrainClassification_iter_11000.caffemodel
I0615 16:09:03.992704  5063 solver.cpp:370] Snapshotting solver state to snapshots/16-06-15_15h49m18s_0_11_pretrainClassification_iter_11000.solverstate
I0615 16:09:03.993085  5063 solver.cpp:291] Iteration 11000, Testing net (#0)
I0615 16:09:04.087556  5063 solver.cpp:342]     Test net output #0: accuracy_top_1 = 0.164062
I0615 16:09:04.087573  5063 solver.cpp:342]     Test net output #1: accuracy_top_5 = 0.426562
I0615 16:09:04.087579  5063 solver.cpp:342]     Test net output #2: softmax = 3.50025 (* 1 = 3.50025 loss)
I0615 16:09:04.141070  5063 solver.cpp:213] Iteration 11000, loss = 3.52535
I0615 16:09:04.141083  5063 solver.cpp:228]     Train net output #0: softmax = 3.52535 (* 1 = 3.52535 loss)
I0615 16:09:04.141088  5063 solver.cpp:473] Iteration 11000, lr = 0.0001
I0615 16:09:04.899873  5063 solver.cpp:213] Iteration 11010, loss = 3.32205
I0615 16:09:04.899890  5063 solver.cpp:228]     Train net output #0: softmax = 3.32205 (* 1 = 3.32205 loss)
I0615 16:09:04.899894  5063 solver.cpp:473] Iteration 11010, lr = 0.0001
I0615 16:09:05.659118  5063 solver.cpp:213] Iteration 11020, loss = 3.43105
I0615 16:09:05.659135  5063 solver.cpp:228]     Train net output #0: softmax = 3.43105 (* 1 = 3.43105 loss)
I0615 16:09:05.659140  5063 solver.cpp:473] Iteration 11020, lr = 0.0001
I0615 16:09:06.418221  5063 solver.cpp:213] Iteration 11030, loss = 3.3969
I0615 16:09:06.418236  5063 solver.cpp:228]     Train net output #0: softmax = 3.3969 (* 1 = 3.3969 loss)
I0615 16:09:06.418241  5063 solver.cpp:473] Iteration 11030, lr = 0.0001
I0615 16:09:07.177462  5063 solver.cpp:213] Iteration 11040, loss = 3.46385
I0615 16:09:07.177480  5063 solver.cpp:228]     Train net output #0: softmax = 3.46385 (* 1 = 3.46385 loss)
I0615 16:09:07.177691  5063 solver.cpp:473] Iteration 11040, lr = 0.0001
I0615 16:09:07.935995  5063 solver.cpp:213] Iteration 11050, loss = 3.5219
I0615 16:09:07.936012  5063 solver.cpp:228]     Train net output #0: softmax = 3.5219 (* 1 = 3.5219 loss)
I0615 16:09:07.936017  5063 solver.cpp:473] Iteration 11050, lr = 0.0001
I0615 16:09:08.693650  5063 solver.cpp:213] Iteration 11060, loss = 3.7719
I0615 16:09:08.693668  5063 solver.cpp:228]     Train net output #0: softmax = 3.7719 (* 1 = 3.7719 loss)
I0615 16:09:08.693673  5063 solver.cpp:473] Iteration 11060, lr = 0.0001
I0615 16:09:09.451741  5063 solver.cpp:213] Iteration 11070, loss = 3.43715
I0615 16:09:09.451758  5063 solver.cpp:228]     Train net output #0: softmax = 3.43715 (* 1 = 3.43715 loss)
I0615 16:09:09.451763  5063 solver.cpp:473] Iteration 11070, lr = 0.0001
I0615 16:09:10.210508  5063 solver.cpp:213] Iteration 11080, loss = 3.47121
I0615 16:09:10.210523  5063 solver.cpp:228]     Train net output #0: softmax = 3.47121 (* 1 = 3.47121 loss)
I0615 16:09:10.210528  5063 solver.cpp:473] Iteration 11080, lr = 0.0001
I0615 16:09:10.969223  5063 solver.cpp:213] Iteration 11090, loss = 3.59371
I0615 16:09:10.969238  5063 solver.cpp:228]     Train net output #0: softmax = 3.59371 (* 1 = 3.59371 loss)
I0615 16:09:10.969260  5063 solver.cpp:473] Iteration 11090, lr = 0.0001
I0615 16:09:11.727655  5063 solver.cpp:213] Iteration 11100, loss = 3.25108
I0615 16:09:11.727671  5063 solver.cpp:228]     Train net output #0: softmax = 3.25108 (* 1 = 3.25108 loss)
I0615 16:09:11.727676  5063 solver.cpp:473] Iteration 11100, lr = 0.0001
I0615 16:09:12.486315  5063 solver.cpp:213] Iteration 11110, loss = 3.54012
I0615 16:09:12.486331  5063 solver.cpp:228]     Train net output #0: softmax = 3.54012 (* 1 = 3.54012 loss)
I0615 16:09:12.486336  5063 solver.cpp:473] Iteration 11110, lr = 0.0001
I0615 16:09:13.245290  5063 solver.cpp:213] Iteration 11120, loss = 3.44172
I0615 16:09:13.245304  5063 solver.cpp:228]     Train net output #0: softmax = 3.44172 (* 1 = 3.44172 loss)
I0615 16:09:13.245309  5063 solver.cpp:473] Iteration 11120, lr = 0.0001
I0615 16:09:14.004236  5063 solver.cpp:213] Iteration 11130, loss = 3.43736
I0615 16:09:14.004251  5063 solver.cpp:228]     Train net output #0: softmax = 3.43736 (* 1 = 3.43736 loss)
I0615 16:09:14.004256  5063 solver.cpp:473] Iteration 11130, lr = 0.0001
I0615 16:09:14.763130  5063 solver.cpp:213] Iteration 11140, loss = 3.54191
I0615 16:09:14.763149  5063 solver.cpp:228]     Train net output #0: softmax = 3.54191 (* 1 = 3.54191 loss)
I0615 16:09:14.763154  5063 solver.cpp:473] Iteration 11140, lr = 0.0001
I0615 16:09:15.522080  5063 solver.cpp:213] Iteration 11150, loss = 3.44506
I0615 16:09:15.522104  5063 solver.cpp:228]     Train net output #0: softmax = 3.44506 (* 1 = 3.44506 loss)
I0615 16:09:15.522109  5063 solver.cpp:473] Iteration 11150, lr = 0.0001
I0615 16:09:16.281141  5063 solver.cpp:213] Iteration 11160, loss = 3.60248
I0615 16:09:16.281157  5063 solver.cpp:228]     Train net output #0: softmax = 3.60248 (* 1 = 3.60248 loss)
I0615 16:09:16.281160  5063 solver.cpp:473] Iteration 11160, lr = 0.0001
I0615 16:09:17.037282  5063 solver.cpp:213] Iteration 11170, loss = 3.37417
I0615 16:09:17.037299  5063 solver.cpp:228]     Train net output #0: softmax = 3.37417 (* 1 = 3.37417 loss)
I0615 16:09:17.037304  5063 solver.cpp:473] Iteration 11170, lr = 0.0001
I0615 16:09:17.796005  5063 solver.cpp:213] Iteration 11180, loss = 3.51171
I0615 16:09:17.796022  5063 solver.cpp:228]     Train net output #0: softmax = 3.51171 (* 1 = 3.51171 loss)
I0615 16:09:17.796150  5063 solver.cpp:473] Iteration 11180, lr = 0.0001
I0615 16:09:18.555363  5063 solver.cpp:213] Iteration 11190, loss = 3.54243
I0615 16:09:18.555378  5063 solver.cpp:228]     Train net output #0: softmax = 3.54243 (* 1 = 3.54243 loss)
I0615 16:09:18.555383  5063 solver.cpp:473] Iteration 11190, lr = 0.0001
I0615 16:09:19.314441  5063 solver.cpp:213] Iteration 11200, loss = 3.44889
I0615 16:09:19.314472  5063 solver.cpp:228]     Train net output #0: softmax = 3.44889 (* 1 = 3.44889 loss)
I0615 16:09:19.314477  5063 solver.cpp:473] Iteration 11200, lr = 0.0001
I0615 16:09:20.073402  5063 solver.cpp:213] Iteration 11210, loss = 3.64489
I0615 16:09:20.073416  5063 solver.cpp:228]     Train net output #0: softmax = 3.64489 (* 1 = 3.64489 loss)
I0615 16:09:20.073421  5063 solver.cpp:473] Iteration 11210, lr = 0.0001
I0615 16:09:20.829985  5063 solver.cpp:213] Iteration 11220, loss = 3.45274
I0615 16:09:20.830000  5063 solver.cpp:228]     Train net output #0: softmax = 3.45274 (* 1 = 3.45274 loss)
I0615 16:09:20.830004  5063 solver.cpp:473] Iteration 11220, lr = 0.0001
I0615 16:09:21.588598  5063 solver.cpp:213] Iteration 11230, loss = 3.60462
I0615 16:09:21.588611  5063 solver.cpp:228]     Train net output #0: softmax = 3.60462 (* 1 = 3.60462 loss)
I0615 16:09:21.588616  5063 solver.cpp:473] Iteration 11230, lr = 0.0001
I0615 16:09:22.346468  5063 solver.cpp:213] Iteration 11240, loss = 3.60209
I0615 16:09:22.346484  5063 solver.cpp:228]     Train net output #0: softmax = 3.60209 (* 1 = 3.60209 loss)
I0615 16:09:22.346489  5063 solver.cpp:473] Iteration 11240, lr = 0.0001
I0615 16:09:23.105479  5063 solver.cpp:213] Iteration 11250, loss = 3.5078
I0615 16:09:23.105497  5063 solver.cpp:228]     Train net output #0: softmax = 3.5078 (* 1 = 3.5078 loss)
I0615 16:09:23.105630  5063 solver.cpp:473] Iteration 11250, lr = 0.0001
I0615 16:09:23.864624  5063 solver.cpp:213] Iteration 11260, loss = 3.47055
I0615 16:09:23.864639  5063 solver.cpp:228]     Train net output #0: softmax = 3.47055 (* 1 = 3.47055 loss)
I0615 16:09:23.864644  5063 solver.cpp:473] Iteration 11260, lr = 0.0001
I0615 16:09:24.622877  5063 solver.cpp:213] Iteration 11270, loss = 3.49265
I0615 16:09:24.622896  5063 solver.cpp:228]     Train net output #0: softmax = 3.49265 (* 1 = 3.49265 loss)
I0615 16:09:24.622901  5063 solver.cpp:473] Iteration 11270, lr = 0.0001
I0615 16:09:25.381779  5063 solver.cpp:213] Iteration 11280, loss = 3.69966
I0615 16:09:25.381794  5063 solver.cpp:228]     Train net output #0: softmax = 3.69966 (* 1 = 3.69966 loss)
I0615 16:09:25.381799  5063 solver.cpp:473] Iteration 11280, lr = 0.0001
I0615 16:09:26.140699  5063 solver.cpp:213] Iteration 11290, loss = 3.41364
I0615 16:09:26.140713  5063 solver.cpp:228]     Train net output #0: softmax = 3.41364 (* 1 = 3.41364 loss)
I0615 16:09:26.140718  5063 solver.cpp:473] Iteration 11290, lr = 0.0001
I0615 16:09:26.899601  5063 solver.cpp:213] Iteration 11300, loss = 3.66227
I0615 16:09:26.899617  5063 solver.cpp:228]     Train net output #0: softmax = 3.66227 (* 1 = 3.66227 loss)
I0615 16:09:26.899622  5063 solver.cpp:473] Iteration 11300, lr = 0.0001
I0615 16:09:27.658277  5063 solver.cpp:213] Iteration 11310, loss = 3.48496
I0615 16:09:27.658295  5063 solver.cpp:228]     Train net output #0: softmax = 3.48496 (* 1 = 3.48496 loss)
I0615 16:09:27.658299  5063 solver.cpp:473] Iteration 11310, lr = 0.0001
I0615 16:09:28.417023  5063 solver.cpp:213] Iteration 11320, loss = 3.35013
I0615 16:09:28.417039  5063 solver.cpp:228]     Train net output #0: softmax = 3.35013 (* 1 = 3.35013 loss)
I0615 16:09:28.417044  5063 solver.cpp:473] Iteration 11320, lr = 0.0001
I0615 16:09:29.175846  5063 solver.cpp:213] Iteration 11330, loss = 3.39919
I0615 16:09:29.175861  5063 solver.cpp:228]     Train net output #0: softmax = 3.39919 (* 1 = 3.39919 loss)
I0615 16:09:29.175865  5063 solver.cpp:473] Iteration 11330, lr = 0.0001
I0615 16:09:29.934563  5063 solver.cpp:213] Iteration 11340, loss = 3.39423
I0615 16:09:29.934577  5063 solver.cpp:228]     Train net output #0: softmax = 3.39423 (* 1 = 3.39423 loss)
I0615 16:09:29.934582  5063 solver.cpp:473] Iteration 11340, lr = 0.0001
I0615 16:09:30.692764  5063 solver.cpp:213] Iteration 11350, loss = 3.27736
I0615 16:09:30.692780  5063 solver.cpp:228]     Train net output #0: softmax = 3.27736 (* 1 = 3.27736 loss)
I0615 16:09:30.692783  5063 solver.cpp:473] Iteration 11350, lr = 0.0001
I0615 16:09:31.451827  5063 solver.cpp:213] Iteration 11360, loss = 3.41417
I0615 16:09:31.451858  5063 solver.cpp:228]     Train net output #0: softmax = 3.41417 (* 1 = 3.41417 loss)
I0615 16:09:31.451861  5063 solver.cpp:473] Iteration 11360, lr = 0.0001
I0615 16:09:32.210403  5063 solver.cpp:213] Iteration 11370, loss = 3.53779
I0615 16:09:32.210419  5063 solver.cpp:228]     Train net output #0: softmax = 3.53779 (* 1 = 3.53779 loss)
I0615 16:09:32.210423  5063 solver.cpp:473] Iteration 11370, lr = 0.0001
I0615 16:09:32.968482  5063 solver.cpp:213] Iteration 11380, loss = 3.54683
I0615 16:09:32.968508  5063 solver.cpp:228]     Train net output #0: softmax = 3.54683 (* 1 = 3.54683 loss)
I0615 16:09:32.968519  5063 solver.cpp:473] Iteration 11380, lr = 0.0001
I0615 16:09:33.726902  5063 solver.cpp:213] Iteration 11390, loss = 3.44146
I0615 16:09:33.726920  5063 solver.cpp:228]     Train net output #0: softmax = 3.44146 (* 1 = 3.44146 loss)
I0615 16:09:33.726929  5063 solver.cpp:473] Iteration 11390, lr = 0.0001
I0615 16:09:34.485613  5063 solver.cpp:213] Iteration 11400, loss = 3.51845
I0615 16:09:34.485630  5063 solver.cpp:228]     Train net output #0: softmax = 3.51845 (* 1 = 3.51845 loss)
I0615 16:09:34.485635  5063 solver.cpp:473] Iteration 11400, lr = 0.0001
I0615 16:09:35.244240  5063 solver.cpp:213] Iteration 11410, loss = 3.43748
I0615 16:09:35.244256  5063 solver.cpp:228]     Train net output #0: softmax = 3.43748 (* 1 = 3.43748 loss)
I0615 16:09:35.244261  5063 solver.cpp:473] Iteration 11410, lr = 0.0001
I0615 16:09:36.002488  5063 solver.cpp:213] Iteration 11420, loss = 3.33471
I0615 16:09:36.002507  5063 solver.cpp:228]     Train net output #0: softmax = 3.33471 (* 1 = 3.33471 loss)
I0615 16:09:36.002512  5063 solver.cpp:473] Iteration 11420, lr = 0.0001
I0615 16:09:36.760996  5063 solver.cpp:213] Iteration 11430, loss = 3.28299
I0615 16:09:36.761016  5063 solver.cpp:228]     Train net output #0: softmax = 3.28299 (* 1 = 3.28299 loss)
I0615 16:09:36.761021  5063 solver.cpp:473] Iteration 11430, lr = 0.0001
I0615 16:09:37.519600  5063 solver.cpp:213] Iteration 11440, loss = 3.58197
I0615 16:09:37.519619  5063 solver.cpp:228]     Train net output #0: softmax = 3.58197 (* 1 = 3.58197 loss)
I0615 16:09:37.519624  5063 solver.cpp:473] Iteration 11440, lr = 0.0001
I0615 16:09:38.277670  5063 solver.cpp:213] Iteration 11450, loss = 3.65732
I0615 16:09:38.277690  5063 solver.cpp:228]     Train net output #0: softmax = 3.65732 (* 1 = 3.65732 loss)
I0615 16:09:38.277695  5063 solver.cpp:473] Iteration 11450, lr = 0.0001
I0615 16:09:39.035789  5063 solver.cpp:213] Iteration 11460, loss = 3.35824
I0615 16:09:39.035809  5063 solver.cpp:228]     Train net output #0: softmax = 3.35824 (* 1 = 3.35824 loss)
I0615 16:09:39.035974  5063 solver.cpp:473] Iteration 11460, lr = 0.0001
I0615 16:09:39.794456  5063 solver.cpp:213] Iteration 11470, loss = 3.32362
I0615 16:09:39.794472  5063 solver.cpp:228]     Train net output #0: softmax = 3.32362 (* 1 = 3.32362 loss)
I0615 16:09:39.794477  5063 solver.cpp:473] Iteration 11470, lr = 0.0001
I0615 16:09:40.552561  5063 solver.cpp:213] Iteration 11480, loss = 3.63119
I0615 16:09:40.552577  5063 solver.cpp:228]     Train net output #0: softmax = 3.63119 (* 1 = 3.63119 loss)
I0615 16:09:40.552582  5063 solver.cpp:473] Iteration 11480, lr = 0.0001
I0615 16:09:41.311528  5063 solver.cpp:213] Iteration 11490, loss = 3.30426
I0615 16:09:41.311544  5063 solver.cpp:228]     Train net output #0: softmax = 3.30426 (* 1 = 3.30426 loss)
I0615 16:09:41.311548  5063 solver.cpp:473] Iteration 11490, lr = 0.0001
I0615 16:09:42.070267  5063 solver.cpp:213] Iteration 11500, loss = 3.47234
I0615 16:09:42.070288  5063 solver.cpp:228]     Train net output #0: softmax = 3.47234 (* 1 = 3.47234 loss)
I0615 16:09:42.070293  5063 solver.cpp:473] Iteration 11500, lr = 0.0001
I0615 16:09:42.827957  5063 solver.cpp:213] Iteration 11510, loss = 3.63982
I0615 16:09:42.827973  5063 solver.cpp:228]     Train net output #0: softmax = 3.63982 (* 1 = 3.63982 loss)
I0615 16:09:42.827978  5063 solver.cpp:473] Iteration 11510, lr = 0.0001
I0615 16:09:43.585644  5063 solver.cpp:213] Iteration 11520, loss = 3.45773
I0615 16:09:43.585683  5063 solver.cpp:228]     Train net output #0: softmax = 3.45773 (* 1 = 3.45773 loss)
I0615 16:09:43.585690  5063 solver.cpp:473] Iteration 11520, lr = 0.0001
I0615 16:09:44.343406  5063 solver.cpp:213] Iteration 11530, loss = 3.40793
I0615 16:09:44.343431  5063 solver.cpp:228]     Train net output #0: softmax = 3.40793 (* 1 = 3.40793 loss)
I0615 16:09:44.343581  5063 solver.cpp:473] Iteration 11530, lr = 0.0001
I0615 16:09:45.101622  5063 solver.cpp:213] Iteration 11540, loss = 3.43211
I0615 16:09:45.101640  5063 solver.cpp:228]     Train net output #0: softmax = 3.43211 (* 1 = 3.43211 loss)
I0615 16:09:45.101650  5063 solver.cpp:473] Iteration 11540, lr = 0.0001
I0615 16:09:45.859642  5063 solver.cpp:213] Iteration 11550, loss = 3.46176
I0615 16:09:45.859657  5063 solver.cpp:228]     Train net output #0: softmax = 3.46176 (* 1 = 3.46176 loss)
I0615 16:09:45.859661  5063 solver.cpp:473] Iteration 11550, lr = 0.0001
I0615 16:09:46.617627  5063 solver.cpp:213] Iteration 11560, loss = 3.19262
I0615 16:09:46.617642  5063 solver.cpp:228]     Train net output #0: softmax = 3.19262 (* 1 = 3.19262 loss)
I0615 16:09:46.617647  5063 solver.cpp:473] Iteration 11560, lr = 0.0001
I0615 16:09:47.376458  5063 solver.cpp:213] Iteration 11570, loss = 3.51063
I0615 16:09:47.376473  5063 solver.cpp:228]     Train net output #0: softmax = 3.51063 (* 1 = 3.51063 loss)
I0615 16:09:47.376478  5063 solver.cpp:473] Iteration 11570, lr = 0.0001
I0615 16:09:48.134910  5063 solver.cpp:213] Iteration 11580, loss = 3.47904
I0615 16:09:48.134924  5063 solver.cpp:228]     Train net output #0: softmax = 3.47904 (* 1 = 3.47904 loss)
I0615 16:09:48.134929  5063 solver.cpp:473] Iteration 11580, lr = 0.0001
I0615 16:09:48.890020  5063 solver.cpp:213] Iteration 11590, loss = 3.56788
I0615 16:09:48.890038  5063 solver.cpp:228]     Train net output #0: softmax = 3.56788 (* 1 = 3.56788 loss)
I0615 16:09:48.890043  5063 solver.cpp:473] Iteration 11590, lr = 0.0001
I0615 16:09:49.648437  5063 solver.cpp:213] Iteration 11600, loss = 3.62696
I0615 16:09:49.648484  5063 solver.cpp:228]     Train net output #0: softmax = 3.62696 (* 1 = 3.62696 loss)
I0615 16:09:49.648490  5063 solver.cpp:473] Iteration 11600, lr = 0.0001
I0615 16:09:50.407186  5063 solver.cpp:213] Iteration 11610, loss = 3.36132
I0615 16:09:50.407202  5063 solver.cpp:228]     Train net output #0: softmax = 3.36132 (* 1 = 3.36132 loss)
I0615 16:09:50.407207  5063 solver.cpp:473] Iteration 11610, lr = 0.0001
I0615 16:09:51.165948  5063 solver.cpp:213] Iteration 11620, loss = 3.28294
I0615 16:09:51.165963  5063 solver.cpp:228]     Train net output #0: softmax = 3.28294 (* 1 = 3.28294 loss)
I0615 16:09:51.165968  5063 solver.cpp:473] Iteration 11620, lr = 0.0001
I0615 16:09:51.924696  5063 solver.cpp:213] Iteration 11630, loss = 3.63171
I0615 16:09:51.924713  5063 solver.cpp:228]     Train net output #0: softmax = 3.63171 (* 1 = 3.63171 loss)
I0615 16:09:51.924718  5063 solver.cpp:473] Iteration 11630, lr = 0.0001
I0615 16:09:52.683063  5063 solver.cpp:213] Iteration 11640, loss = 3.25179
I0615 16:09:52.683079  5063 solver.cpp:228]     Train net output #0: softmax = 3.25179 (* 1 = 3.25179 loss)
I0615 16:09:52.683084  5063 solver.cpp:473] Iteration 11640, lr = 0.0001
I0615 16:09:53.441473  5063 solver.cpp:213] Iteration 11650, loss = 3.40338
I0615 16:09:53.441488  5063 solver.cpp:228]     Train net output #0: softmax = 3.40338 (* 1 = 3.40338 loss)
I0615 16:09:53.441493  5063 solver.cpp:473] Iteration 11650, lr = 0.0001
I0615 16:09:54.200402  5063 solver.cpp:213] Iteration 11660, loss = 3.44118
I0615 16:09:54.200417  5063 solver.cpp:228]     Train net output #0: softmax = 3.44118 (* 1 = 3.44118 loss)
I0615 16:09:54.200422  5063 solver.cpp:473] Iteration 11660, lr = 0.0001
I0615 16:09:54.959193  5063 solver.cpp:213] Iteration 11670, loss = 3.53108
I0615 16:09:54.959213  5063 solver.cpp:228]     Train net output #0: softmax = 3.53108 (* 1 = 3.53108 loss)
I0615 16:09:54.959347  5063 solver.cpp:473] Iteration 11670, lr = 0.0001
I0615 16:09:55.717545  5063 solver.cpp:213] Iteration 11680, loss = 3.47109
I0615 16:09:55.717561  5063 solver.cpp:228]     Train net output #0: softmax = 3.47109 (* 1 = 3.47109 loss)
I0615 16:09:55.717566  5063 solver.cpp:473] Iteration 11680, lr = 0.0001
I0615 16:09:56.476014  5063 solver.cpp:213] Iteration 11690, loss = 3.42756
I0615 16:09:56.476032  5063 solver.cpp:228]     Train net output #0: softmax = 3.42756 (* 1 = 3.42756 loss)
I0615 16:09:56.476035  5063 solver.cpp:473] Iteration 11690, lr = 0.0001
I0615 16:09:57.234712  5063 solver.cpp:213] Iteration 11700, loss = 3.42288
I0615 16:09:57.234730  5063 solver.cpp:228]     Train net output #0: softmax = 3.42288 (* 1 = 3.42288 loss)
I0615 16:09:57.234743  5063 solver.cpp:473] Iteration 11700, lr = 0.0001
I0615 16:09:57.993497  5063 solver.cpp:213] Iteration 11710, loss = 3.48717
I0615 16:09:57.993513  5063 solver.cpp:228]     Train net output #0: softmax = 3.48717 (* 1 = 3.48717 loss)
I0615 16:09:57.993518  5063 solver.cpp:473] Iteration 11710, lr = 0.0001
I0615 16:09:58.752359  5063 solver.cpp:213] Iteration 11720, loss = 3.65957
I0615 16:09:58.752377  5063 solver.cpp:228]     Train net output #0: softmax = 3.65957 (* 1 = 3.65957 loss)
I0615 16:09:58.752380  5063 solver.cpp:473] Iteration 11720, lr = 0.0001
I0615 16:09:59.511029  5063 solver.cpp:213] Iteration 11730, loss = 3.34739
I0615 16:09:59.511044  5063 solver.cpp:228]     Train net output #0: softmax = 3.34739 (* 1 = 3.34739 loss)
I0615 16:09:59.511049  5063 solver.cpp:473] Iteration 11730, lr = 0.0001
I0615 16:10:00.268764  5063 solver.cpp:213] Iteration 11740, loss = 3.17928
I0615 16:10:00.268784  5063 solver.cpp:228]     Train net output #0: softmax = 3.17928 (* 1 = 3.17928 loss)
I0615 16:10:00.268915  5063 solver.cpp:473] Iteration 11740, lr = 0.0001
I0615 16:10:01.026762  5063 solver.cpp:213] Iteration 11750, loss = 3.58492
I0615 16:10:01.026779  5063 solver.cpp:228]     Train net output #0: softmax = 3.58492 (* 1 = 3.58492 loss)
I0615 16:10:01.026783  5063 solver.cpp:473] Iteration 11750, lr = 0.0001
I0615 16:10:01.785604  5063 solver.cpp:213] Iteration 11760, loss = 3.37556
I0615 16:10:01.785637  5063 solver.cpp:228]     Train net output #0: softmax = 3.37556 (* 1 = 3.37556 loss)
I0615 16:10:01.785642  5063 solver.cpp:473] Iteration 11760, lr = 0.0001
I0615 16:10:02.543965  5063 solver.cpp:213] Iteration 11770, loss = 3.4472
I0615 16:10:02.543980  5063 solver.cpp:228]     Train net output #0: softmax = 3.4472 (* 1 = 3.4472 loss)
I0615 16:10:02.543984  5063 solver.cpp:473] Iteration 11770, lr = 0.0001
I0615 16:10:03.302602  5063 solver.cpp:213] Iteration 11780, loss = 3.42365
I0615 16:10:03.302616  5063 solver.cpp:228]     Train net output #0: softmax = 3.42365 (* 1 = 3.42365 loss)
I0615 16:10:03.302620  5063 solver.cpp:473] Iteration 11780, lr = 0.0001
I0615 16:10:04.061357  5063 solver.cpp:213] Iteration 11790, loss = 3.36386
I0615 16:10:04.061372  5063 solver.cpp:228]     Train net output #0: softmax = 3.36386 (* 1 = 3.36386 loss)
I0615 16:10:04.061378  5063 solver.cpp:473] Iteration 11790, lr = 0.0001
I0615 16:10:04.819175  5063 solver.cpp:213] Iteration 11800, loss = 3.24398
I0615 16:10:04.819193  5063 solver.cpp:228]     Train net output #0: softmax = 3.24398 (* 1 = 3.24398 loss)
I0615 16:10:04.819198  5063 solver.cpp:473] Iteration 11800, lr = 0.0001
I0615 16:10:05.577311  5063 solver.cpp:213] Iteration 11810, loss = 3.34635
I0615 16:10:05.577332  5063 solver.cpp:228]     Train net output #0: softmax = 3.34635 (* 1 = 3.34635 loss)
I0615 16:10:05.577337  5063 solver.cpp:473] Iteration 11810, lr = 0.0001
I0615 16:10:06.335388  5063 solver.cpp:213] Iteration 11820, loss = 3.37038
I0615 16:10:06.335405  5063 solver.cpp:228]     Train net output #0: softmax = 3.37038 (* 1 = 3.37038 loss)
I0615 16:10:06.335410  5063 solver.cpp:473] Iteration 11820, lr = 0.0001
I0615 16:10:07.093480  5063 solver.cpp:213] Iteration 11830, loss = 3.4236
I0615 16:10:07.093499  5063 solver.cpp:228]     Train net output #0: softmax = 3.4236 (* 1 = 3.4236 loss)
I0615 16:10:07.093503  5063 solver.cpp:473] Iteration 11830, lr = 0.0001
I0615 16:10:07.852094  5063 solver.cpp:213] Iteration 11840, loss = 3.34751
I0615 16:10:07.852110  5063 solver.cpp:228]     Train net output #0: softmax = 3.34751 (* 1 = 3.34751 loss)
I0615 16:10:07.852115  5063 solver.cpp:473] Iteration 11840, lr = 0.0001
I0615 16:10:08.610069  5063 solver.cpp:213] Iteration 11850, loss = 3.39203
I0615 16:10:08.610088  5063 solver.cpp:228]     Train net output #0: softmax = 3.39203 (* 1 = 3.39203 loss)
I0615 16:10:08.610092  5063 solver.cpp:473] Iteration 11850, lr = 0.0001
I0615 16:10:09.368654  5063 solver.cpp:213] Iteration 11860, loss = 3.61016
I0615 16:10:09.368674  5063 solver.cpp:228]     Train net output #0: softmax = 3.61016 (* 1 = 3.61016 loss)
I0615 16:10:09.368685  5063 solver.cpp:473] Iteration 11860, lr = 0.0001
I0615 16:10:10.126978  5063 solver.cpp:213] Iteration 11870, loss = 3.48846
I0615 16:10:10.126996  5063 solver.cpp:228]     Train net output #0: softmax = 3.48846 (* 1 = 3.48846 loss)
I0615 16:10:10.127001  5063 solver.cpp:473] Iteration 11870, lr = 0.0001
I0615 16:10:10.885896  5063 solver.cpp:213] Iteration 11880, loss = 3.21595
I0615 16:10:10.885915  5063 solver.cpp:228]     Train net output #0: softmax = 3.21595 (* 1 = 3.21595 loss)
I0615 16:10:10.885923  5063 solver.cpp:473] Iteration 11880, lr = 0.0001
I0615 16:10:11.644219  5063 solver.cpp:213] Iteration 11890, loss = 3.40583
I0615 16:10:11.644239  5063 solver.cpp:228]     Train net output #0: softmax = 3.40583 (* 1 = 3.40583 loss)
I0615 16:10:11.644243  5063 solver.cpp:473] Iteration 11890, lr = 0.0001
I0615 16:10:12.402946  5063 solver.cpp:213] Iteration 11900, loss = 3.5361
I0615 16:10:12.402961  5063 solver.cpp:228]     Train net output #0: softmax = 3.5361 (* 1 = 3.5361 loss)
I0615 16:10:12.402966  5063 solver.cpp:473] Iteration 11900, lr = 0.0001
I0615 16:10:13.161134  5063 solver.cpp:213] Iteration 11910, loss = 3.55871
I0615 16:10:13.161149  5063 solver.cpp:228]     Train net output #0: softmax = 3.55871 (* 1 = 3.55871 loss)
I0615 16:10:13.161154  5063 solver.cpp:473] Iteration 11910, lr = 0.0001
I0615 16:10:13.919845  5063 solver.cpp:213] Iteration 11920, loss = 3.52779
I0615 16:10:13.919875  5063 solver.cpp:228]     Train net output #0: softmax = 3.52779 (* 1 = 3.52779 loss)
I0615 16:10:13.919880  5063 solver.cpp:473] Iteration 11920, lr = 0.0001
I0615 16:10:14.678660  5063 solver.cpp:213] Iteration 11930, loss = 3.32092
I0615 16:10:14.678678  5063 solver.cpp:228]     Train net output #0: softmax = 3.32092 (* 1 = 3.32092 loss)
I0615 16:10:14.678683  5063 solver.cpp:473] Iteration 11930, lr = 0.0001
I0615 16:10:15.437250  5063 solver.cpp:213] Iteration 11940, loss = 3.46701
I0615 16:10:15.437266  5063 solver.cpp:228]     Train net output #0: softmax = 3.46701 (* 1 = 3.46701 loss)
I0615 16:10:15.437270  5063 solver.cpp:473] Iteration 11940, lr = 0.0001
I0615 16:10:16.195945  5063 solver.cpp:213] Iteration 11950, loss = 3.26223
I0615 16:10:16.195963  5063 solver.cpp:228]     Train net output #0: softmax = 3.26223 (* 1 = 3.26223 loss)
I0615 16:10:16.196112  5063 solver.cpp:473] Iteration 11950, lr = 0.0001
I0615 16:10:16.954357  5063 solver.cpp:213] Iteration 11960, loss = 3.49258
I0615 16:10:16.954375  5063 solver.cpp:228]     Train net output #0: softmax = 3.49258 (* 1 = 3.49258 loss)
I0615 16:10:16.954380  5063 solver.cpp:473] Iteration 11960, lr = 0.0001
I0615 16:10:17.713007  5063 solver.cpp:213] Iteration 11970, loss = 3.33681
I0615 16:10:17.713023  5063 solver.cpp:228]     Train net output #0: softmax = 3.33681 (* 1 = 3.33681 loss)
I0615 16:10:17.713027  5063 solver.cpp:473] Iteration 11970, lr = 0.0001
I0615 16:10:18.471928  5063 solver.cpp:213] Iteration 11980, loss = 3.40231
I0615 16:10:18.471942  5063 solver.cpp:228]     Train net output #0: softmax = 3.40231 (* 1 = 3.40231 loss)
I0615 16:10:18.471947  5063 solver.cpp:473] Iteration 11980, lr = 0.0001
I0615 16:10:19.230615  5063 solver.cpp:213] Iteration 11990, loss = 3.31279
I0615 16:10:19.230631  5063 solver.cpp:228]     Train net output #0: softmax = 3.31279 (* 1 = 3.31279 loss)
I0615 16:10:19.230636  5063 solver.cpp:473] Iteration 11990, lr = 0.0001
I0615 16:10:19.935775  5063 solver.cpp:362] Snapshotting to snapshots/16-06-15_15h49m18s_0_11_pretrainClassification_iter_12000.caffemodel
I0615 16:10:19.936522  5063 solver.cpp:370] Snapshotting solver state to snapshots/16-06-15_15h49m18s_0_11_pretrainClassification_iter_12000.solverstate
I0615 16:10:19.936898  5063 solver.cpp:291] Iteration 12000, Testing net (#0)
I0615 16:10:20.031373  5063 solver.cpp:342]     Test net output #0: accuracy_top_1 = 0.178125
I0615 16:10:20.031388  5063 solver.cpp:342]     Test net output #1: accuracy_top_5 = 0.435937
I0615 16:10:20.031394  5063 solver.cpp:342]     Test net output #2: softmax = 3.52898 (* 1 = 3.52898 loss)
I0615 16:10:20.084970  5063 solver.cpp:213] Iteration 12000, loss = 3.11616
I0615 16:10:20.084985  5063 solver.cpp:228]     Train net output #0: softmax = 3.11616 (* 1 = 3.11616 loss)
I0615 16:10:20.084990  5063 solver.cpp:473] Iteration 12000, lr = 0.0001
I0615 16:10:20.843077  5063 solver.cpp:213] Iteration 12010, loss = 3.21264
I0615 16:10:20.843098  5063 solver.cpp:228]     Train net output #0: softmax = 3.21264 (* 1 = 3.21264 loss)
I0615 16:10:20.843102  5063 solver.cpp:473] Iteration 12010, lr = 0.0001
I0615 16:10:21.602321  5063 solver.cpp:213] Iteration 12020, loss = 3.65709
I0615 16:10:21.602339  5063 solver.cpp:228]     Train net output #0: softmax = 3.65709 (* 1 = 3.65709 loss)
I0615 16:10:21.602344  5063 solver.cpp:473] Iteration 12020, lr = 0.0001
I0615 16:10:22.361351  5063 solver.cpp:213] Iteration 12030, loss = 3.25434
I0615 16:10:22.361364  5063 solver.cpp:228]     Train net output #0: softmax = 3.25434 (* 1 = 3.25434 loss)
I0615 16:10:22.361369  5063 solver.cpp:473] Iteration 12030, lr = 0.0001
I0615 16:10:23.120558  5063 solver.cpp:213] Iteration 12040, loss = 3.41143
I0615 16:10:23.120573  5063 solver.cpp:228]     Train net output #0: softmax = 3.41143 (* 1 = 3.41143 loss)
I0615 16:10:23.120579  5063 solver.cpp:473] Iteration 12040, lr = 0.0001
I0615 16:10:23.879441  5063 solver.cpp:213] Iteration 12050, loss = 3.58036
I0615 16:10:23.879456  5063 solver.cpp:228]     Train net output #0: softmax = 3.58036 (* 1 = 3.58036 loss)
I0615 16:10:23.879462  5063 solver.cpp:473] Iteration 12050, lr = 0.0001
I0615 16:10:24.638131  5063 solver.cpp:213] Iteration 12060, loss = 3.43337
I0615 16:10:24.638146  5063 solver.cpp:228]     Train net output #0: softmax = 3.43337 (* 1 = 3.43337 loss)
I0615 16:10:24.638150  5063 solver.cpp:473] Iteration 12060, lr = 0.0001
I0615 16:10:25.396683  5063 solver.cpp:213] Iteration 12070, loss = 3.24484
I0615 16:10:25.396705  5063 solver.cpp:228]     Train net output #0: softmax = 3.24484 (* 1 = 3.24484 loss)
I0615 16:10:25.396710  5063 solver.cpp:473] Iteration 12070, lr = 0.0001
I0615 16:10:26.155755  5063 solver.cpp:213] Iteration 12080, loss = 3.41559
I0615 16:10:26.155769  5063 solver.cpp:228]     Train net output #0: softmax = 3.41559 (* 1 = 3.41559 loss)
I0615 16:10:26.155774  5063 solver.cpp:473] Iteration 12080, lr = 0.0001
I0615 16:10:26.914619  5063 solver.cpp:213] Iteration 12090, loss = 3.29728
I0615 16:10:26.914636  5063 solver.cpp:228]     Train net output #0: softmax = 3.29728 (* 1 = 3.29728 loss)
I0615 16:10:26.914762  5063 solver.cpp:473] Iteration 12090, lr = 0.0001
I0615 16:10:27.673416  5063 solver.cpp:213] Iteration 12100, loss = 3.46692
I0615 16:10:27.673432  5063 solver.cpp:228]     Train net output #0: softmax = 3.46692 (* 1 = 3.46692 loss)
I0615 16:10:27.673436  5063 solver.cpp:473] Iteration 12100, lr = 0.0001
I0615 16:10:28.432001  5063 solver.cpp:213] Iteration 12110, loss = 3.5525
I0615 16:10:28.432018  5063 solver.cpp:228]     Train net output #0: softmax = 3.5525 (* 1 = 3.5525 loss)
I0615 16:10:28.432024  5063 solver.cpp:473] Iteration 12110, lr = 0.0001
I0615 16:10:29.189909  5063 solver.cpp:213] Iteration 12120, loss = 3.2964
I0615 16:10:29.189929  5063 solver.cpp:228]     Train net output #0: softmax = 3.2964 (* 1 = 3.2964 loss)
I0615 16:10:29.189934  5063 solver.cpp:473] Iteration 12120, lr = 0.0001
I0615 16:10:29.949075  5063 solver.cpp:213] Iteration 12130, loss = 3.24684
I0615 16:10:29.949092  5063 solver.cpp:228]     Train net output #0: softmax = 3.24684 (* 1 = 3.24684 loss)
I0615 16:10:29.949097  5063 solver.cpp:473] Iteration 12130, lr = 0.0001
I0615 16:10:30.708044  5063 solver.cpp:213] Iteration 12140, loss = 3.83848
I0615 16:10:30.708061  5063 solver.cpp:228]     Train net output #0: softmax = 3.83848 (* 1 = 3.83848 loss)
I0615 16:10:30.708066  5063 solver.cpp:473] Iteration 12140, lr = 0.0001
I0615 16:10:31.467164  5063 solver.cpp:213] Iteration 12150, loss = 3.4052
I0615 16:10:31.467180  5063 solver.cpp:228]     Train net output #0: softmax = 3.4052 (* 1 = 3.4052 loss)
I0615 16:10:31.467185  5063 solver.cpp:473] Iteration 12150, lr = 0.0001
I0615 16:10:32.225709  5063 solver.cpp:213] Iteration 12160, loss = 3.52533
I0615 16:10:32.225728  5063 solver.cpp:228]     Train net output #0: softmax = 3.52533 (* 1 = 3.52533 loss)
I0615 16:10:32.225860  5063 solver.cpp:473] Iteration 12160, lr = 0.0001
I0615 16:10:32.982882  5063 solver.cpp:213] Iteration 12170, loss = 3.4732
I0615 16:10:32.982897  5063 solver.cpp:228]     Train net output #0: softmax = 3.4732 (* 1 = 3.4732 loss)
I0615 16:10:32.982902  5063 solver.cpp:473] Iteration 12170, lr = 0.0001
I0615 16:10:33.740651  5063 solver.cpp:213] Iteration 12180, loss = 3.49885
I0615 16:10:33.740665  5063 solver.cpp:228]     Train net output #0: softmax = 3.49885 (* 1 = 3.49885 loss)
I0615 16:10:33.740670  5063 solver.cpp:473] Iteration 12180, lr = 0.0001
I0615 16:10:34.499614  5063 solver.cpp:213] Iteration 12190, loss = 3.47704
I0615 16:10:34.499627  5063 solver.cpp:228]     Train net output #0: softmax = 3.47704 (* 1 = 3.47704 loss)
I0615 16:10:34.499632  5063 solver.cpp:473] Iteration 12190, lr = 0.0001
I0615 16:10:35.258143  5063 solver.cpp:213] Iteration 12200, loss = 3.29143
I0615 16:10:35.258160  5063 solver.cpp:228]     Train net output #0: softmax = 3.29143 (* 1 = 3.29143 loss)
I0615 16:10:35.258165  5063 solver.cpp:473] Iteration 12200, lr = 0.0001
I0615 16:10:36.016865  5063 solver.cpp:213] Iteration 12210, loss = 3.44988
I0615 16:10:36.016880  5063 solver.cpp:228]     Train net output #0: softmax = 3.44988 (* 1 = 3.44988 loss)
I0615 16:10:36.016885  5063 solver.cpp:473] Iteration 12210, lr = 0.0001
I0615 16:10:36.775001  5063 solver.cpp:213] Iteration 12220, loss = 3.44453
I0615 16:10:36.775018  5063 solver.cpp:228]     Train net output #0: softmax = 3.44453 (* 1 = 3.44453 loss)
I0615 16:10:36.775022  5063 solver.cpp:473] Iteration 12220, lr = 0.0001
I0615 16:10:37.533413  5063 solver.cpp:213] Iteration 12230, loss = 3.31795
I0615 16:10:37.533430  5063 solver.cpp:228]     Train net output #0: softmax = 3.31795 (* 1 = 3.31795 loss)
I0615 16:10:37.533435  5063 solver.cpp:473] Iteration 12230, lr = 0.0001
I0615 16:10:38.291625  5063 solver.cpp:213] Iteration 12240, loss = 3.13022
I0615 16:10:38.291641  5063 solver.cpp:228]     Train net output #0: softmax = 3.13022 (* 1 = 3.13022 loss)
I0615 16:10:38.291646  5063 solver.cpp:473] Iteration 12240, lr = 0.0001
I0615 16:10:39.049484  5063 solver.cpp:213] Iteration 12250, loss = 3.45821
I0615 16:10:39.049499  5063 solver.cpp:228]     Train net output #0: softmax = 3.45821 (* 1 = 3.45821 loss)
I0615 16:10:39.049504  5063 solver.cpp:473] Iteration 12250, lr = 0.0001
I0615 16:10:39.807550  5063 solver.cpp:213] Iteration 12260, loss = 3.29756
I0615 16:10:39.807565  5063 solver.cpp:228]     Train net output #0: softmax = 3.29756 (* 1 = 3.29756 loss)
I0615 16:10:39.807570  5063 solver.cpp:473] Iteration 12260, lr = 0.0001
I0615 16:10:40.566241  5063 solver.cpp:213] Iteration 12270, loss = 3.37239
I0615 16:10:40.566256  5063 solver.cpp:228]     Train net output #0: softmax = 3.37239 (* 1 = 3.37239 loss)
I0615 16:10:40.566260  5063 solver.cpp:473] Iteration 12270, lr = 0.0001
I0615 16:10:41.324587  5063 solver.cpp:213] Iteration 12280, loss = 3.63848
I0615 16:10:41.324602  5063 solver.cpp:228]     Train net output #0: softmax = 3.63848 (* 1 = 3.63848 loss)
I0615 16:10:41.324606  5063 solver.cpp:473] Iteration 12280, lr = 0.0001
I0615 16:10:42.082742  5063 solver.cpp:213] Iteration 12290, loss = 3.51621
I0615 16:10:42.082765  5063 solver.cpp:228]     Train net output #0: softmax = 3.51621 (* 1 = 3.51621 loss)
I0615 16:10:42.082770  5063 solver.cpp:473] Iteration 12290, lr = 0.0001
I0615 16:10:42.841091  5063 solver.cpp:213] Iteration 12300, loss = 3.47025
I0615 16:10:42.841112  5063 solver.cpp:228]     Train net output #0: softmax = 3.47025 (* 1 = 3.47025 loss)
I0615 16:10:42.841256  5063 solver.cpp:473] Iteration 12300, lr = 0.0001
I0615 16:10:43.598822  5063 solver.cpp:213] Iteration 12310, loss = 3.38861
I0615 16:10:43.598839  5063 solver.cpp:228]     Train net output #0: softmax = 3.38861 (* 1 = 3.38861 loss)
I0615 16:10:43.598845  5063 solver.cpp:473] Iteration 12310, lr = 0.0001
I0615 16:10:44.356657  5063 solver.cpp:213] Iteration 12320, loss = 3.25777
I0615 16:10:44.356674  5063 solver.cpp:228]     Train net output #0: softmax = 3.25777 (* 1 = 3.25777 loss)
I0615 16:10:44.356679  5063 solver.cpp:473] Iteration 12320, lr = 0.0001
I0615 16:10:45.113577  5063 solver.cpp:213] Iteration 12330, loss = 3.21087
I0615 16:10:45.113597  5063 solver.cpp:228]     Train net output #0: softmax = 3.21087 (* 1 = 3.21087 loss)
I0615 16:10:45.113602  5063 solver.cpp:473] Iteration 12330, lr = 0.0001
I0615 16:10:45.872086  5063 solver.cpp:213] Iteration 12340, loss = 3.32775
I0615 16:10:45.872100  5063 solver.cpp:228]     Train net output #0: softmax = 3.32775 (* 1 = 3.32775 loss)
I0615 16:10:45.872105  5063 solver.cpp:473] Iteration 12340, lr = 0.0001
I0615 16:10:46.630702  5063 solver.cpp:213] Iteration 12350, loss = 3.51587
I0615 16:10:46.630717  5063 solver.cpp:228]     Train net output #0: softmax = 3.51587 (* 1 = 3.51587 loss)
I0615 16:10:46.630722  5063 solver.cpp:473] Iteration 12350, lr = 0.0001
I0615 16:10:47.388708  5063 solver.cpp:213] Iteration 12360, loss = 3.40794
I0615 16:10:47.388723  5063 solver.cpp:228]     Train net output #0: softmax = 3.40794 (* 1 = 3.40794 loss)
I0615 16:10:47.388728  5063 solver.cpp:473] Iteration 12360, lr = 0.0001
I0615 16:10:48.146402  5063 solver.cpp:213] Iteration 12370, loss = 3.46591
I0615 16:10:48.146423  5063 solver.cpp:228]     Train net output #0: softmax = 3.46591 (* 1 = 3.46591 loss)
I0615 16:10:48.146549  5063 solver.cpp:473] Iteration 12370, lr = 0.0001
I0615 16:10:48.904078  5063 solver.cpp:213] Iteration 12380, loss = 3.46782
I0615 16:10:48.904100  5063 solver.cpp:228]     Train net output #0: softmax = 3.46782 (* 1 = 3.46782 loss)
I0615 16:10:48.904105  5063 solver.cpp:473] Iteration 12380, lr = 0.0001
I0615 16:10:49.662091  5063 solver.cpp:213] Iteration 12390, loss = 3.16348
I0615 16:10:49.662106  5063 solver.cpp:228]     Train net output #0: softmax = 3.16348 (* 1 = 3.16348 loss)
I0615 16:10:49.662111  5063 solver.cpp:473] Iteration 12390, lr = 0.0001
I0615 16:10:50.420322  5063 solver.cpp:213] Iteration 12400, loss = 3.38428
I0615 16:10:50.420362  5063 solver.cpp:228]     Train net output #0: softmax = 3.38428 (* 1 = 3.38428 loss)
I0615 16:10:50.420367  5063 solver.cpp:473] Iteration 12400, lr = 0.0001
I0615 16:10:51.178016  5063 solver.cpp:213] Iteration 12410, loss = 3.51452
I0615 16:10:51.178032  5063 solver.cpp:228]     Train net output #0: softmax = 3.51452 (* 1 = 3.51452 loss)
I0615 16:10:51.178037  5063 solver.cpp:473] Iteration 12410, lr = 0.0001
I0615 16:10:51.937013  5063 solver.cpp:213] Iteration 12420, loss = 3.2827
I0615 16:10:51.937029  5063 solver.cpp:228]     Train net output #0: softmax = 3.2827 (* 1 = 3.2827 loss)
I0615 16:10:51.937034  5063 solver.cpp:473] Iteration 12420, lr = 0.0001
I0615 16:10:52.694947  5063 solver.cpp:213] Iteration 12430, loss = 3.47305
I0615 16:10:52.694965  5063 solver.cpp:228]     Train net output #0: softmax = 3.47305 (* 1 = 3.47305 loss)
I0615 16:10:52.694970  5063 solver.cpp:473] Iteration 12430, lr = 0.0001
I0615 16:10:53.453130  5063 solver.cpp:213] Iteration 12440, loss = 3.66829
I0615 16:10:53.453150  5063 solver.cpp:228]     Train net output #0: softmax = 3.66829 (* 1 = 3.66829 loss)
I0615 16:10:53.453155  5063 solver.cpp:473] Iteration 12440, lr = 0.0001
I0615 16:10:54.211756  5063 solver.cpp:213] Iteration 12450, loss = 3.6015
I0615 16:10:54.211774  5063 solver.cpp:228]     Train net output #0: softmax = 3.6015 (* 1 = 3.6015 loss)
I0615 16:10:54.211778  5063 solver.cpp:473] Iteration 12450, lr = 0.0001
I0615 16:10:54.970376  5063 solver.cpp:213] Iteration 12460, loss = 3.218
I0615 16:10:54.970393  5063 solver.cpp:228]     Train net output #0: softmax = 3.218 (* 1 = 3.218 loss)
I0615 16:10:54.970398  5063 solver.cpp:473] Iteration 12460, lr = 0.0001
I0615 16:10:55.728991  5063 solver.cpp:213] Iteration 12470, loss = 3.43091
I0615 16:10:55.729007  5063 solver.cpp:228]     Train net output #0: softmax = 3.43091 (* 1 = 3.43091 loss)
I0615 16:10:55.729012  5063 solver.cpp:473] Iteration 12470, lr = 0.0001
I0615 16:10:56.487545  5063 solver.cpp:213] Iteration 12480, loss = 3.49441
I0615 16:10:56.487560  5063 solver.cpp:228]     Train net output #0: softmax = 3.49441 (* 1 = 3.49441 loss)
I0615 16:10:56.487565  5063 solver.cpp:473] Iteration 12480, lr = 0.0001
I0615 16:10:57.244983  5063 solver.cpp:213] Iteration 12490, loss = 3.41347
I0615 16:10:57.245002  5063 solver.cpp:228]     Train net output #0: softmax = 3.41347 (* 1 = 3.41347 loss)
I0615 16:10:57.245007  5063 solver.cpp:473] Iteration 12490, lr = 0.0001
I0615 16:10:58.003046  5063 solver.cpp:213] Iteration 12500, loss = 3.57023
I0615 16:10:58.003063  5063 solver.cpp:228]     Train net output #0: softmax = 3.57023 (* 1 = 3.57023 loss)
I0615 16:10:58.003067  5063 solver.cpp:473] Iteration 12500, lr = 0.0001
I0615 16:10:58.761515  5063 solver.cpp:213] Iteration 12510, loss = 3.37345
I0615 16:10:58.761536  5063 solver.cpp:228]     Train net output #0: softmax = 3.37345 (* 1 = 3.37345 loss)
I0615 16:10:58.761541  5063 solver.cpp:473] Iteration 12510, lr = 0.0001
I0615 16:10:59.519846  5063 solver.cpp:213] Iteration 12520, loss = 3.57131
I0615 16:10:59.519863  5063 solver.cpp:228]     Train net output #0: softmax = 3.57131 (* 1 = 3.57131 loss)
I0615 16:10:59.519868  5063 solver.cpp:473] Iteration 12520, lr = 0.0001
I0615 16:11:00.277804  5063 solver.cpp:213] Iteration 12530, loss = 3.49842
I0615 16:11:00.277820  5063 solver.cpp:228]     Train net output #0: softmax = 3.49842 (* 1 = 3.49842 loss)
I0615 16:11:00.277825  5063 solver.cpp:473] Iteration 12530, lr = 0.0001
I0615 16:11:01.034837  5063 solver.cpp:213] Iteration 12540, loss = 3.29873
I0615 16:11:01.034855  5063 solver.cpp:228]     Train net output #0: softmax = 3.29873 (* 1 = 3.29873 loss)
I0615 16:11:01.034859  5063 solver.cpp:473] Iteration 12540, lr = 0.0001
I0615 16:11:01.792305  5063 solver.cpp:213] Iteration 12550, loss = 3.64538
I0615 16:11:01.792321  5063 solver.cpp:228]     Train net output #0: softmax = 3.64538 (* 1 = 3.64538 loss)
I0615 16:11:01.792325  5063 solver.cpp:473] Iteration 12550, lr = 0.0001
I0615 16:11:02.549924  5063 solver.cpp:213] Iteration 12560, loss = 3.48183
I0615 16:11:02.549962  5063 solver.cpp:228]     Train net output #0: softmax = 3.48183 (* 1 = 3.48183 loss)
I0615 16:11:02.549965  5063 solver.cpp:473] Iteration 12560, lr = 0.0001
I0615 16:11:03.308029  5063 solver.cpp:213] Iteration 12570, loss = 3.63099
I0615 16:11:03.308043  5063 solver.cpp:228]     Train net output #0: softmax = 3.63099 (* 1 = 3.63099 loss)
I0615 16:11:03.308048  5063 solver.cpp:473] Iteration 12570, lr = 0.0001
I0615 16:11:04.066128  5063 solver.cpp:213] Iteration 12580, loss = 3.48439
I0615 16:11:04.066154  5063 solver.cpp:228]     Train net output #0: softmax = 3.48439 (* 1 = 3.48439 loss)
I0615 16:11:04.066162  5063 solver.cpp:473] Iteration 12580, lr = 0.0001
I0615 16:11:04.824975  5063 solver.cpp:213] Iteration 12590, loss = 3.45253
I0615 16:11:04.824990  5063 solver.cpp:228]     Train net output #0: softmax = 3.45253 (* 1 = 3.45253 loss)
I0615 16:11:04.824995  5063 solver.cpp:473] Iteration 12590, lr = 0.0001
I0615 16:11:05.583086  5063 solver.cpp:213] Iteration 12600, loss = 3.17117
I0615 16:11:05.583104  5063 solver.cpp:228]     Train net output #0: softmax = 3.17117 (* 1 = 3.17117 loss)
I0615 16:11:05.583108  5063 solver.cpp:473] Iteration 12600, lr = 0.0001
I0615 16:11:06.341394  5063 solver.cpp:213] Iteration 12610, loss = 3.4046
I0615 16:11:06.341409  5063 solver.cpp:228]     Train net output #0: softmax = 3.4046 (* 1 = 3.4046 loss)
I0615 16:11:06.341413  5063 solver.cpp:473] Iteration 12610, lr = 0.0001
I0615 16:11:07.099776  5063 solver.cpp:213] Iteration 12620, loss = 3.42543
I0615 16:11:07.099791  5063 solver.cpp:228]     Train net output #0: softmax = 3.42543 (* 1 = 3.42543 loss)
I0615 16:11:07.099795  5063 solver.cpp:473] Iteration 12620, lr = 0.0001
I0615 16:11:07.857915  5063 solver.cpp:213] Iteration 12630, loss = 3.43158
I0615 16:11:07.857930  5063 solver.cpp:228]     Train net output #0: softmax = 3.43158 (* 1 = 3.43158 loss)
I0615 16:11:07.857934  5063 solver.cpp:473] Iteration 12630, lr = 0.0001
I0615 16:11:08.615896  5063 solver.cpp:213] Iteration 12640, loss = 3.70389
I0615 16:11:08.615918  5063 solver.cpp:228]     Train net output #0: softmax = 3.70389 (* 1 = 3.70389 loss)
I0615 16:11:08.615923  5063 solver.cpp:473] Iteration 12640, lr = 0.0001
I0615 16:11:09.374100  5063 solver.cpp:213] Iteration 12650, loss = 3.43949
I0615 16:11:09.374119  5063 solver.cpp:228]     Train net output #0: softmax = 3.43949 (* 1 = 3.43949 loss)
I0615 16:11:09.374124  5063 solver.cpp:473] Iteration 12650, lr = 0.0001
I0615 16:11:10.131711  5063 solver.cpp:213] Iteration 12660, loss = 3.52643
I0615 16:11:10.131727  5063 solver.cpp:228]     Train net output #0: softmax = 3.52643 (* 1 = 3.52643 loss)
I0615 16:11:10.131733  5063 solver.cpp:473] Iteration 12660, lr = 0.0001
I0615 16:11:10.888417  5063 solver.cpp:213] Iteration 12670, loss = 3.64084
I0615 16:11:10.888433  5063 solver.cpp:228]     Train net output #0: softmax = 3.64084 (* 1 = 3.64084 loss)
I0615 16:11:10.888437  5063 solver.cpp:473] Iteration 12670, lr = 0.0001
I0615 16:11:11.646936  5063 solver.cpp:213] Iteration 12680, loss = 3.79802
I0615 16:11:11.646951  5063 solver.cpp:228]     Train net output #0: softmax = 3.79802 (* 1 = 3.79802 loss)
I0615 16:11:11.646955  5063 solver.cpp:473] Iteration 12680, lr = 0.0001
I0615 16:11:12.404718  5063 solver.cpp:213] Iteration 12690, loss = 3.55112
I0615 16:11:12.404736  5063 solver.cpp:228]     Train net output #0: softmax = 3.55112 (* 1 = 3.55112 loss)
I0615 16:11:12.404741  5063 solver.cpp:473] Iteration 12690, lr = 0.0001
I0615 16:11:13.162775  5063 solver.cpp:213] Iteration 12700, loss = 3.30993
I0615 16:11:13.162791  5063 solver.cpp:228]     Train net output #0: softmax = 3.30993 (* 1 = 3.30993 loss)
I0615 16:11:13.162796  5063 solver.cpp:473] Iteration 12700, lr = 0.0001
I0615 16:11:13.920940  5063 solver.cpp:213] Iteration 12710, loss = 3.09418
I0615 16:11:13.920961  5063 solver.cpp:228]     Train net output #0: softmax = 3.09418 (* 1 = 3.09418 loss)
I0615 16:11:13.920965  5063 solver.cpp:473] Iteration 12710, lr = 0.0001
I0615 16:11:14.676179  5063 solver.cpp:213] Iteration 12720, loss = 3.14776
I0615 16:11:14.676344  5063 solver.cpp:228]     Train net output #0: softmax = 3.14776 (* 1 = 3.14776 loss)
I0615 16:11:14.676350  5063 solver.cpp:473] Iteration 12720, lr = 0.0001
I0615 16:11:15.434048  5063 solver.cpp:213] Iteration 12730, loss = 3.5436
I0615 16:11:15.434063  5063 solver.cpp:228]     Train net output #0: softmax = 3.5436 (* 1 = 3.5436 loss)
I0615 16:11:15.434068  5063 solver.cpp:473] Iteration 12730, lr = 0.0001
I0615 16:11:16.192143  5063 solver.cpp:213] Iteration 12740, loss = 3.36535
I0615 16:11:16.192162  5063 solver.cpp:228]     Train net output #0: softmax = 3.36535 (* 1 = 3.36535 loss)
I0615 16:11:16.192167  5063 solver.cpp:473] Iteration 12740, lr = 0.0001
I0615 16:11:16.950227  5063 solver.cpp:213] Iteration 12750, loss = 3.24696
I0615 16:11:16.950244  5063 solver.cpp:228]     Train net output #0: softmax = 3.24696 (* 1 = 3.24696 loss)
I0615 16:11:16.950248  5063 solver.cpp:473] Iteration 12750, lr = 0.0001
I0615 16:11:17.708734  5063 solver.cpp:213] Iteration 12760, loss = 3.28057
I0615 16:11:17.708747  5063 solver.cpp:228]     Train net output #0: softmax = 3.28057 (* 1 = 3.28057 loss)
I0615 16:11:17.708752  5063 solver.cpp:473] Iteration 12760, lr = 0.0001
I0615 16:11:18.465232  5063 solver.cpp:213] Iteration 12770, loss = 3.28065
I0615 16:11:18.465248  5063 solver.cpp:228]     Train net output #0: softmax = 3.28065 (* 1 = 3.28065 loss)
I0615 16:11:18.465252  5063 solver.cpp:473] Iteration 12770, lr = 0.0001
I0615 16:11:19.223884  5063 solver.cpp:213] Iteration 12780, loss = 3.32798
I0615 16:11:19.223901  5063 solver.cpp:228]     Train net output #0: softmax = 3.32798 (* 1 = 3.32798 loss)
I0615 16:11:19.223906  5063 solver.cpp:473] Iteration 12780, lr = 0.0001
I0615 16:11:19.982096  5063 solver.cpp:213] Iteration 12790, loss = 3.44232
I0615 16:11:19.982115  5063 solver.cpp:228]     Train net output #0: softmax = 3.44232 (* 1 = 3.44232 loss)
I0615 16:11:19.982125  5063 solver.cpp:473] Iteration 12790, lr = 0.0001
I0615 16:11:20.739141  5063 solver.cpp:213] Iteration 12800, loss = 3.44185
I0615 16:11:20.739181  5063 solver.cpp:228]     Train net output #0: softmax = 3.44185 (* 1 = 3.44185 loss)
I0615 16:11:20.739187  5063 solver.cpp:473] Iteration 12800, lr = 0.0001
I0615 16:11:21.497680  5063 solver.cpp:213] Iteration 12810, loss = 3.39242
I0615 16:11:21.497701  5063 solver.cpp:228]     Train net output #0: softmax = 3.39242 (* 1 = 3.39242 loss)
I0615 16:11:21.497706  5063 solver.cpp:473] Iteration 12810, lr = 0.0001
I0615 16:11:22.255235  5063 solver.cpp:213] Iteration 12820, loss = 3.33542
I0615 16:11:22.255254  5063 solver.cpp:228]     Train net output #0: softmax = 3.33542 (* 1 = 3.33542 loss)
I0615 16:11:22.255259  5063 solver.cpp:473] Iteration 12820, lr = 0.0001
I0615 16:11:23.013558  5063 solver.cpp:213] Iteration 12830, loss = 3.46014
I0615 16:11:23.013574  5063 solver.cpp:228]     Train net output #0: softmax = 3.46014 (* 1 = 3.46014 loss)
I0615 16:11:23.013579  5063 solver.cpp:473] Iteration 12830, lr = 0.0001
I0615 16:11:23.771416  5063 solver.cpp:213] Iteration 12840, loss = 3.75779
I0615 16:11:23.771431  5063 solver.cpp:228]     Train net output #0: softmax = 3.75779 (* 1 = 3.75779 loss)
I0615 16:11:23.771436  5063 solver.cpp:473] Iteration 12840, lr = 0.0001
I0615 16:11:24.529721  5063 solver.cpp:213] Iteration 12850, loss = 3.26444
I0615 16:11:24.529737  5063 solver.cpp:228]     Train net output #0: softmax = 3.26444 (* 1 = 3.26444 loss)
I0615 16:11:24.529742  5063 solver.cpp:473] Iteration 12850, lr = 0.0001
I0615 16:11:25.287617  5063 solver.cpp:213] Iteration 12860, loss = 3.37283
I0615 16:11:25.287639  5063 solver.cpp:228]     Train net output #0: softmax = 3.37283 (* 1 = 3.37283 loss)
I0615 16:11:25.287789  5063 solver.cpp:473] Iteration 12860, lr = 0.0001
I0615 16:11:26.046119  5063 solver.cpp:213] Iteration 12870, loss = 3.4143
I0615 16:11:26.046134  5063 solver.cpp:228]     Train net output #0: softmax = 3.4143 (* 1 = 3.4143 loss)
I0615 16:11:26.046139  5063 solver.cpp:473] Iteration 12870, lr = 0.0001
I0615 16:11:26.804646  5063 solver.cpp:213] Iteration 12880, loss = 3.52761
I0615 16:11:26.804663  5063 solver.cpp:228]     Train net output #0: softmax = 3.52761 (* 1 = 3.52761 loss)
I0615 16:11:26.804668  5063 solver.cpp:473] Iteration 12880, lr = 0.0001
I0615 16:11:27.563397  5063 solver.cpp:213] Iteration 12890, loss = 3.36749
I0615 16:11:27.563415  5063 solver.cpp:228]     Train net output #0: softmax = 3.36749 (* 1 = 3.36749 loss)
I0615 16:11:27.563418  5063 solver.cpp:473] Iteration 12890, lr = 0.0001
I0615 16:11:28.321564  5063 solver.cpp:213] Iteration 12900, loss = 3.42407
I0615 16:11:28.321580  5063 solver.cpp:228]     Train net output #0: softmax = 3.42407 (* 1 = 3.42407 loss)
I0615 16:11:28.321585  5063 solver.cpp:473] Iteration 12900, lr = 0.0001
I0615 16:11:29.079421  5063 solver.cpp:213] Iteration 12910, loss = 3.45006
I0615 16:11:29.079440  5063 solver.cpp:228]     Train net output #0: softmax = 3.45006 (* 1 = 3.45006 loss)
I0615 16:11:29.079444  5063 solver.cpp:473] Iteration 12910, lr = 0.0001
I0615 16:11:29.837982  5063 solver.cpp:213] Iteration 12920, loss = 3.27048
I0615 16:11:29.837997  5063 solver.cpp:228]     Train net output #0: softmax = 3.27048 (* 1 = 3.27048 loss)
I0615 16:11:29.838002  5063 solver.cpp:473] Iteration 12920, lr = 0.0001
I0615 16:11:30.596197  5063 solver.cpp:213] Iteration 12930, loss = 3.34881
I0615 16:11:30.596215  5063 solver.cpp:228]     Train net output #0: softmax = 3.34881 (* 1 = 3.34881 loss)
I0615 16:11:30.596220  5063 solver.cpp:473] Iteration 12930, lr = 0.0001
I0615 16:11:31.354221  5063 solver.cpp:213] Iteration 12940, loss = 3.85311
I0615 16:11:31.354238  5063 solver.cpp:228]     Train net output #0: softmax = 3.85311 (* 1 = 3.85311 loss)
I0615 16:11:31.354243  5063 solver.cpp:473] Iteration 12940, lr = 0.0001
I0615 16:11:32.112247  5063 solver.cpp:213] Iteration 12950, loss = 3.53582
I0615 16:11:32.112265  5063 solver.cpp:228]     Train net output #0: softmax = 3.53582 (* 1 = 3.53582 loss)
I0615 16:11:32.112277  5063 solver.cpp:473] Iteration 12950, lr = 0.0001
I0615 16:11:32.870638  5063 solver.cpp:213] Iteration 12960, loss = 3.45638
I0615 16:11:32.870671  5063 solver.cpp:228]     Train net output #0: softmax = 3.45638 (* 1 = 3.45638 loss)
I0615 16:11:32.870676  5063 solver.cpp:473] Iteration 12960, lr = 0.0001
I0615 16:11:33.628758  5063 solver.cpp:213] Iteration 12970, loss = 3.41552
I0615 16:11:33.628787  5063 solver.cpp:228]     Train net output #0: softmax = 3.41552 (* 1 = 3.41552 loss)
I0615 16:11:33.628792  5063 solver.cpp:473] Iteration 12970, lr = 0.0001
I0615 16:11:34.387240  5063 solver.cpp:213] Iteration 12980, loss = 3.28603
I0615 16:11:34.387255  5063 solver.cpp:228]     Train net output #0: softmax = 3.28603 (* 1 = 3.28603 loss)
I0615 16:11:34.387259  5063 solver.cpp:473] Iteration 12980, lr = 0.0001
I0615 16:11:35.145794  5063 solver.cpp:213] Iteration 12990, loss = 3.17946
I0615 16:11:35.145812  5063 solver.cpp:228]     Train net output #0: softmax = 3.17946 (* 1 = 3.17946 loss)
I0615 16:11:35.145817  5063 solver.cpp:473] Iteration 12990, lr = 0.0001
I0615 16:11:35.850647  5063 solver.cpp:362] Snapshotting to snapshots/16-06-15_15h49m18s_0_11_pretrainClassification_iter_13000.caffemodel
I0615 16:11:35.851383  5063 solver.cpp:370] Snapshotting solver state to snapshots/16-06-15_15h49m18s_0_11_pretrainClassification_iter_13000.solverstate
I0615 16:11:35.851760  5063 solver.cpp:291] Iteration 13000, Testing net (#0)
I0615 16:11:35.946195  5063 solver.cpp:342]     Test net output #0: accuracy_top_1 = 0.19375
I0615 16:11:35.946209  5063 solver.cpp:342]     Test net output #1: accuracy_top_5 = 0.467187
I0615 16:11:35.946215  5063 solver.cpp:342]     Test net output #2: softmax = 3.34489 (* 1 = 3.34489 loss)
I0615 16:11:35.999800  5063 solver.cpp:213] Iteration 13000, loss = 3.46364
I0615 16:11:35.999814  5063 solver.cpp:228]     Train net output #0: softmax = 3.46364 (* 1 = 3.46364 loss)
I0615 16:11:35.999817  5063 solver.cpp:473] Iteration 13000, lr = 0.0001
I0615 16:11:36.758086  5063 solver.cpp:213] Iteration 13010, loss = 3.31555
I0615 16:11:36.758103  5063 solver.cpp:228]     Train net output #0: softmax = 3.31555 (* 1 = 3.31555 loss)
I0615 16:11:36.758108  5063 solver.cpp:473] Iteration 13010, lr = 0.0001
I0615 16:11:37.516341  5063 solver.cpp:213] Iteration 13020, loss = 3.35542
I0615 16:11:37.516356  5063 solver.cpp:228]     Train net output #0: softmax = 3.35542 (* 1 = 3.35542 loss)
I0615 16:11:37.516360  5063 solver.cpp:473] Iteration 13020, lr = 0.0001
I0615 16:11:38.274884  5063 solver.cpp:213] Iteration 13030, loss = 3.39558
I0615 16:11:38.274900  5063 solver.cpp:228]     Train net output #0: softmax = 3.39558 (* 1 = 3.39558 loss)
I0615 16:11:38.274904  5063 solver.cpp:473] Iteration 13030, lr = 0.0001
I0615 16:11:39.032838  5063 solver.cpp:213] Iteration 13040, loss = 3.24211
I0615 16:11:39.032860  5063 solver.cpp:228]     Train net output #0: softmax = 3.24211 (* 1 = 3.24211 loss)
I0615 16:11:39.032866  5063 solver.cpp:473] Iteration 13040, lr = 0.0001
I0615 16:11:39.791106  5063 solver.cpp:213] Iteration 13050, loss = 3.5872
I0615 16:11:39.791122  5063 solver.cpp:228]     Train net output #0: softmax = 3.5872 (* 1 = 3.5872 loss)
I0615 16:11:39.791127  5063 solver.cpp:473] Iteration 13050, lr = 0.0001
I0615 16:11:40.548930  5063 solver.cpp:213] Iteration 13060, loss = 3.2825
I0615 16:11:40.548949  5063 solver.cpp:228]     Train net output #0: softmax = 3.2825 (* 1 = 3.2825 loss)
I0615 16:11:40.548954  5063 solver.cpp:473] Iteration 13060, lr = 0.0001
I0615 16:11:41.307121  5063 solver.cpp:213] Iteration 13070, loss = 3.40739
I0615 16:11:41.307140  5063 solver.cpp:228]     Train net output #0: softmax = 3.40739 (* 1 = 3.40739 loss)
I0615 16:11:41.307150  5063 solver.cpp:473] Iteration 13070, lr = 0.0001
I0615 16:11:42.065274  5063 solver.cpp:213] Iteration 13080, loss = 3.5486
I0615 16:11:42.065290  5063 solver.cpp:228]     Train net output #0: softmax = 3.5486 (* 1 = 3.5486 loss)
I0615 16:11:42.065295  5063 solver.cpp:473] Iteration 13080, lr = 0.0001
I0615 16:11:42.823189  5063 solver.cpp:213] Iteration 13090, loss = 3.46311
I0615 16:11:42.823213  5063 solver.cpp:228]     Train net output #0: softmax = 3.46311 (* 1 = 3.46311 loss)
I0615 16:11:42.823233  5063 solver.cpp:473] Iteration 13090, lr = 0.0001
I0615 16:11:43.581352  5063 solver.cpp:213] Iteration 13100, loss = 3.26029
I0615 16:11:43.581373  5063 solver.cpp:228]     Train net output #0: softmax = 3.26029 (* 1 = 3.26029 loss)
I0615 16:11:43.581377  5063 solver.cpp:473] Iteration 13100, lr = 0.0001
I0615 16:11:44.339390  5063 solver.cpp:213] Iteration 13110, loss = 3.52468
I0615 16:11:44.339412  5063 solver.cpp:228]     Train net output #0: softmax = 3.52468 (* 1 = 3.52468 loss)
I0615 16:11:44.339416  5063 solver.cpp:473] Iteration 13110, lr = 0.0001
I0615 16:11:45.097784  5063 solver.cpp:213] Iteration 13120, loss = 3.46342
I0615 16:11:45.097800  5063 solver.cpp:228]     Train net output #0: softmax = 3.46342 (* 1 = 3.46342 loss)
I0615 16:11:45.097803  5063 solver.cpp:473] Iteration 13120, lr = 0.0001
I0615 16:11:45.856052  5063 solver.cpp:213] Iteration 13130, loss = 3.63743
I0615 16:11:45.856067  5063 solver.cpp:228]     Train net output #0: softmax = 3.63743 (* 1 = 3.63743 loss)
I0615 16:11:45.856072  5063 solver.cpp:473] Iteration 13130, lr = 0.0001
I0615 16:11:46.614440  5063 solver.cpp:213] Iteration 13140, loss = 3.47371
I0615 16:11:46.614464  5063 solver.cpp:228]     Train net output #0: softmax = 3.47371 (* 1 = 3.47371 loss)
I0615 16:11:46.614470  5063 solver.cpp:473] Iteration 13140, lr = 0.0001
I0615 16:11:47.372501  5063 solver.cpp:213] Iteration 13150, loss = 3.11838
I0615 16:11:47.372517  5063 solver.cpp:228]     Train net output #0: softmax = 3.11838 (* 1 = 3.11838 loss)
I0615 16:11:47.372521  5063 solver.cpp:473] Iteration 13150, lr = 0.0001
I0615 16:11:48.131284  5063 solver.cpp:213] Iteration 13160, loss = 3.27273
I0615 16:11:48.131302  5063 solver.cpp:228]     Train net output #0: softmax = 3.27273 (* 1 = 3.27273 loss)
I0615 16:11:48.131307  5063 solver.cpp:473] Iteration 13160, lr = 0.0001
I0615 16:11:48.889358  5063 solver.cpp:213] Iteration 13170, loss = 3.3301
I0615 16:11:48.889374  5063 solver.cpp:228]     Train net output #0: softmax = 3.3301 (* 1 = 3.3301 loss)
I0615 16:11:48.889379  5063 solver.cpp:473] Iteration 13170, lr = 0.0001
I0615 16:11:49.648097  5063 solver.cpp:213] Iteration 13180, loss = 3.37471
I0615 16:11:49.648115  5063 solver.cpp:228]     Train net output #0: softmax = 3.37471 (* 1 = 3.37471 loss)
I0615 16:11:49.648120  5063 solver.cpp:473] Iteration 13180, lr = 0.0001
I0615 16:11:50.406399  5063 solver.cpp:213] Iteration 13190, loss = 3.53358
I0615 16:11:50.406417  5063 solver.cpp:228]     Train net output #0: softmax = 3.53358 (* 1 = 3.53358 loss)
I0615 16:11:50.406422  5063 solver.cpp:473] Iteration 13190, lr = 0.0001
I0615 16:11:51.164680  5063 solver.cpp:213] Iteration 13200, loss = 3.47145
I0615 16:11:51.164719  5063 solver.cpp:228]     Train net output #0: softmax = 3.47145 (* 1 = 3.47145 loss)
I0615 16:11:51.164726  5063 solver.cpp:473] Iteration 13200, lr = 0.0001
I0615 16:11:51.923524  5063 solver.cpp:213] Iteration 13210, loss = 3.44353
I0615 16:11:51.923544  5063 solver.cpp:228]     Train net output #0: softmax = 3.44353 (* 1 = 3.44353 loss)
I0615 16:11:51.923681  5063 solver.cpp:473] Iteration 13210, lr = 0.0001
I0615 16:11:52.681994  5063 solver.cpp:213] Iteration 13220, loss = 3.13311
I0615 16:11:52.682011  5063 solver.cpp:228]     Train net output #0: softmax = 3.13311 (* 1 = 3.13311 loss)
I0615 16:11:52.682016  5063 solver.cpp:473] Iteration 13220, lr = 0.0001
I0615 16:11:53.440256  5063 solver.cpp:213] Iteration 13230, loss = 3.40903
I0615 16:11:53.440270  5063 solver.cpp:228]     Train net output #0: softmax = 3.40903 (* 1 = 3.40903 loss)
I0615 16:11:53.440275  5063 solver.cpp:473] Iteration 13230, lr = 0.0001
I0615 16:11:54.199272  5063 solver.cpp:213] Iteration 13240, loss = 3.29613
I0615 16:11:54.199288  5063 solver.cpp:228]     Train net output #0: softmax = 3.29613 (* 1 = 3.29613 loss)
I0615 16:11:54.199293  5063 solver.cpp:473] Iteration 13240, lr = 0.0001
I0615 16:11:54.957530  5063 solver.cpp:213] Iteration 13250, loss = 3.33891
I0615 16:11:54.957552  5063 solver.cpp:228]     Train net output #0: softmax = 3.33891 (* 1 = 3.33891 loss)
I0615 16:11:54.957558  5063 solver.cpp:473] Iteration 13250, lr = 0.0001
I0615 16:11:55.715745  5063 solver.cpp:213] Iteration 13260, loss = 3.50252
I0615 16:11:55.715767  5063 solver.cpp:228]     Train net output #0: softmax = 3.50252 (* 1 = 3.50252 loss)
I0615 16:11:55.715772  5063 solver.cpp:473] Iteration 13260, lr = 0.0001
I0615 16:11:56.473776  5063 solver.cpp:213] Iteration 13270, loss = 3.56998
I0615 16:11:56.473793  5063 solver.cpp:228]     Train net output #0: softmax = 3.56998 (* 1 = 3.56998 loss)
I0615 16:11:56.473798  5063 solver.cpp:473] Iteration 13270, lr = 0.0001
I0615 16:11:57.232149  5063 solver.cpp:213] Iteration 13280, loss = 3.23143
I0615 16:11:57.232170  5063 solver.cpp:228]     Train net output #0: softmax = 3.23143 (* 1 = 3.23143 loss)
I0615 16:11:57.232292  5063 solver.cpp:473] Iteration 13280, lr = 0.0001
I0615 16:11:57.988060  5063 solver.cpp:213] Iteration 13290, loss = 3.25372
I0615 16:11:57.988077  5063 solver.cpp:228]     Train net output #0: softmax = 3.25372 (* 1 = 3.25372 loss)
I0615 16:11:57.988081  5063 solver.cpp:473] Iteration 13290, lr = 0.0001
I0615 16:11:58.746429  5063 solver.cpp:213] Iteration 13300, loss = 3.43064
I0615 16:11:58.746445  5063 solver.cpp:228]     Train net output #0: softmax = 3.43064 (* 1 = 3.43064 loss)
I0615 16:11:58.746450  5063 solver.cpp:473] Iteration 13300, lr = 0.0001
I0615 16:11:59.505014  5063 solver.cpp:213] Iteration 13310, loss = 3.30958
I0615 16:11:59.505031  5063 solver.cpp:228]     Train net output #0: softmax = 3.30958 (* 1 = 3.30958 loss)
I0615 16:11:59.505035  5063 solver.cpp:473] Iteration 13310, lr = 0.0001
I0615 16:12:00.262984  5063 solver.cpp:213] Iteration 13320, loss = 3.32914
I0615 16:12:00.263001  5063 solver.cpp:228]     Train net output #0: softmax = 3.32914 (* 1 = 3.32914 loss)
I0615 16:12:00.263005  5063 solver.cpp:473] Iteration 13320, lr = 0.0001
I0615 16:12:01.018422  5063 solver.cpp:213] Iteration 13330, loss = 3.48177
I0615 16:12:01.018440  5063 solver.cpp:228]     Train net output #0: softmax = 3.48177 (* 1 = 3.48177 loss)
I0615 16:12:01.018445  5063 solver.cpp:473] Iteration 13330, lr = 0.0001
I0615 16:12:01.775930  5063 solver.cpp:213] Iteration 13340, loss = 3.40874
I0615 16:12:01.775954  5063 solver.cpp:228]     Train net output #0: softmax = 3.40874 (* 1 = 3.40874 loss)
I0615 16:12:01.775959  5063 solver.cpp:473] Iteration 13340, lr = 0.0001
I0615 16:12:02.534411  5063 solver.cpp:213] Iteration 13350, loss = 3.39814
I0615 16:12:02.534430  5063 solver.cpp:228]     Train net output #0: softmax = 3.39814 (* 1 = 3.39814 loss)
I0615 16:12:02.534435  5063 solver.cpp:473] Iteration 13350, lr = 0.0001
I0615 16:12:03.293156  5063 solver.cpp:213] Iteration 13360, loss = 3.4264
I0615 16:12:03.293189  5063 solver.cpp:228]     Train net output #0: softmax = 3.4264 (* 1 = 3.4264 loss)
I0615 16:12:03.293193  5063 solver.cpp:473] Iteration 13360, lr = 0.0001
I0615 16:12:04.051493  5063 solver.cpp:213] Iteration 13370, loss = 3.36483
I0615 16:12:04.051509  5063 solver.cpp:228]     Train net output #0: softmax = 3.36483 (* 1 = 3.36483 loss)
I0615 16:12:04.051514  5063 solver.cpp:473] Iteration 13370, lr = 0.0001
I0615 16:12:04.807767  5063 solver.cpp:213] Iteration 13380, loss = 3.45013
I0615 16:12:04.807783  5063 solver.cpp:228]     Train net output #0: softmax = 3.45013 (* 1 = 3.45013 loss)
I0615 16:12:04.807788  5063 solver.cpp:473] Iteration 13380, lr = 0.0001
I0615 16:12:05.565909  5063 solver.cpp:213] Iteration 13390, loss = 3.26309
I0615 16:12:05.565927  5063 solver.cpp:228]     Train net output #0: softmax = 3.26309 (* 1 = 3.26309 loss)
I0615 16:12:05.565932  5063 solver.cpp:473] Iteration 13390, lr = 0.0001
I0615 16:12:06.324331  5063 solver.cpp:213] Iteration 13400, loss = 3.54162
I0615 16:12:06.324347  5063 solver.cpp:228]     Train net output #0: softmax = 3.54162 (* 1 = 3.54162 loss)
I0615 16:12:06.324352  5063 solver.cpp:473] Iteration 13400, lr = 0.0001
I0615 16:12:07.082515  5063 solver.cpp:213] Iteration 13410, loss = 3.36371
I0615 16:12:07.082533  5063 solver.cpp:228]     Train net output #0: softmax = 3.36371 (* 1 = 3.36371 loss)
I0615 16:12:07.082538  5063 solver.cpp:473] Iteration 13410, lr = 0.0001
I0615 16:12:07.841009  5063 solver.cpp:213] Iteration 13420, loss = 3.32787
I0615 16:12:07.841027  5063 solver.cpp:228]     Train net output #0: softmax = 3.32787 (* 1 = 3.32787 loss)
I0615 16:12:07.841038  5063 solver.cpp:473] Iteration 13420, lr = 0.0001
I0615 16:12:08.599409  5063 solver.cpp:213] Iteration 13430, loss = 3.36164
I0615 16:12:08.599426  5063 solver.cpp:228]     Train net output #0: softmax = 3.36164 (* 1 = 3.36164 loss)
I0615 16:12:08.599431  5063 solver.cpp:473] Iteration 13430, lr = 0.0001
I0615 16:12:09.358291  5063 solver.cpp:213] Iteration 13440, loss = 3.36973
I0615 16:12:09.358306  5063 solver.cpp:228]     Train net output #0: softmax = 3.36973 (* 1 = 3.36973 loss)
I0615 16:12:09.358311  5063 solver.cpp:473] Iteration 13440, lr = 0.0001
I0615 16:12:10.116093  5063 solver.cpp:213] Iteration 13450, loss = 3.17729
I0615 16:12:10.116111  5063 solver.cpp:228]     Train net output #0: softmax = 3.17729 (* 1 = 3.17729 loss)
I0615 16:12:10.116116  5063 solver.cpp:473] Iteration 13450, lr = 0.0001
I0615 16:12:10.874114  5063 solver.cpp:213] Iteration 13460, loss = 3.32887
I0615 16:12:10.874135  5063 solver.cpp:228]     Train net output #0: softmax = 3.32887 (* 1 = 3.32887 loss)
I0615 16:12:10.874140  5063 solver.cpp:473] Iteration 13460, lr = 0.0001
I0615 16:12:11.633180  5063 solver.cpp:213] Iteration 13470, loss = 3.56877
I0615 16:12:11.633195  5063 solver.cpp:228]     Train net output #0: softmax = 3.56877 (* 1 = 3.56877 loss)
I0615 16:12:11.633200  5063 solver.cpp:473] Iteration 13470, lr = 0.0001
I0615 16:12:12.391858  5063 solver.cpp:213] Iteration 13480, loss = 3.40463
I0615 16:12:12.391873  5063 solver.cpp:228]     Train net output #0: softmax = 3.40463 (* 1 = 3.40463 loss)
I0615 16:12:12.391877  5063 solver.cpp:473] Iteration 13480, lr = 0.0001
I0615 16:12:13.150354  5063 solver.cpp:213] Iteration 13490, loss = 3.45259
I0615 16:12:13.150374  5063 solver.cpp:228]     Train net output #0: softmax = 3.45259 (* 1 = 3.45259 loss)
I0615 16:12:13.150537  5063 solver.cpp:473] Iteration 13490, lr = 0.0001
I0615 16:12:13.908886  5063 solver.cpp:213] Iteration 13500, loss = 3.42674
I0615 16:12:13.908901  5063 solver.cpp:228]     Train net output #0: softmax = 3.42674 (* 1 = 3.42674 loss)
I0615 16:12:13.908906  5063 solver.cpp:473] Iteration 13500, lr = 0.0001
I0615 16:12:14.664343  5063 solver.cpp:213] Iteration 13510, loss = 3.37897
I0615 16:12:14.664358  5063 solver.cpp:228]     Train net output #0: softmax = 3.37897 (* 1 = 3.37897 loss)
I0615 16:12:14.664362  5063 solver.cpp:473] Iteration 13510, lr = 0.0001
I0615 16:12:15.422408  5063 solver.cpp:213] Iteration 13520, loss = 3.56423
I0615 16:12:15.422448  5063 solver.cpp:228]     Train net output #0: softmax = 3.56423 (* 1 = 3.56423 loss)
I0615 16:12:15.422454  5063 solver.cpp:473] Iteration 13520, lr = 0.0001
I0615 16:12:16.180490  5063 solver.cpp:213] Iteration 13530, loss = 3.1922
I0615 16:12:16.180506  5063 solver.cpp:228]     Train net output #0: softmax = 3.1922 (* 1 = 3.1922 loss)
I0615 16:12:16.180511  5063 solver.cpp:473] Iteration 13530, lr = 0.0001
I0615 16:12:16.938210  5063 solver.cpp:213] Iteration 13540, loss = 3.36941
I0615 16:12:16.938226  5063 solver.cpp:228]     Train net output #0: softmax = 3.36941 (* 1 = 3.36941 loss)
I0615 16:12:16.938230  5063 solver.cpp:473] Iteration 13540, lr = 0.0001
I0615 16:12:17.696440  5063 solver.cpp:213] Iteration 13550, loss = 3.34035
I0615 16:12:17.696455  5063 solver.cpp:228]     Train net output #0: softmax = 3.34035 (* 1 = 3.34035 loss)
I0615 16:12:17.696460  5063 solver.cpp:473] Iteration 13550, lr = 0.0001
I0615 16:12:18.454823  5063 solver.cpp:213] Iteration 13560, loss = 3.28252
I0615 16:12:18.454843  5063 solver.cpp:228]     Train net output #0: softmax = 3.28252 (* 1 = 3.28252 loss)
I0615 16:12:18.454849  5063 solver.cpp:473] Iteration 13560, lr = 0.0001
I0615 16:12:19.212882  5063 solver.cpp:213] Iteration 13570, loss = 3.43528
I0615 16:12:19.212900  5063 solver.cpp:228]     Train net output #0: softmax = 3.43528 (* 1 = 3.43528 loss)
I0615 16:12:19.212905  5063 solver.cpp:473] Iteration 13570, lr = 0.0001
I0615 16:12:19.971402  5063 solver.cpp:213] Iteration 13580, loss = 3.38073
I0615 16:12:19.971424  5063 solver.cpp:228]     Train net output #0: softmax = 3.38073 (* 1 = 3.38073 loss)
I0615 16:12:19.971429  5063 solver.cpp:473] Iteration 13580, lr = 0.0001
I0615 16:12:20.729871  5063 solver.cpp:213] Iteration 13590, loss = 3.51105
I0615 16:12:20.729889  5063 solver.cpp:228]     Train net output #0: softmax = 3.51105 (* 1 = 3.51105 loss)
I0615 16:12:20.729894  5063 solver.cpp:473] Iteration 13590, lr = 0.0001
I0615 16:12:21.487962  5063 solver.cpp:213] Iteration 13600, loss = 3.32218
I0615 16:12:21.488003  5063 solver.cpp:228]     Train net output #0: softmax = 3.32218 (* 1 = 3.32218 loss)
I0615 16:12:21.488008  5063 solver.cpp:473] Iteration 13600, lr = 0.0001
I0615 16:12:22.246273  5063 solver.cpp:213] Iteration 13610, loss = 3.19177
I0615 16:12:22.246290  5063 solver.cpp:228]     Train net output #0: softmax = 3.19177 (* 1 = 3.19177 loss)
I0615 16:12:22.246295  5063 solver.cpp:473] Iteration 13610, lr = 0.0001
I0615 16:12:23.004672  5063 solver.cpp:213] Iteration 13620, loss = 3.47049
I0615 16:12:23.004688  5063 solver.cpp:228]     Train net output #0: softmax = 3.47049 (* 1 = 3.47049 loss)
I0615 16:12:23.004693  5063 solver.cpp:473] Iteration 13620, lr = 0.0001
I0615 16:12:23.763303  5063 solver.cpp:213] Iteration 13630, loss = 3.29207
I0615 16:12:23.763321  5063 solver.cpp:228]     Train net output #0: softmax = 3.29207 (* 1 = 3.29207 loss)
I0615 16:12:23.763456  5063 solver.cpp:473] Iteration 13630, lr = 0.0001
I0615 16:12:24.521450  5063 solver.cpp:213] Iteration 13640, loss = 3.44099
I0615 16:12:24.521466  5063 solver.cpp:228]     Train net output #0: softmax = 3.44099 (* 1 = 3.44099 loss)
I0615 16:12:24.521471  5063 solver.cpp:473] Iteration 13640, lr = 0.0001
I0615 16:12:25.279460  5063 solver.cpp:213] Iteration 13650, loss = 3.48057
I0615 16:12:25.279484  5063 solver.cpp:228]     Train net output #0: softmax = 3.48057 (* 1 = 3.48057 loss)
I0615 16:12:25.279489  5063 solver.cpp:473] Iteration 13650, lr = 0.0001
I0615 16:12:26.037559  5063 solver.cpp:213] Iteration 13660, loss = 3.33093
I0615 16:12:26.037576  5063 solver.cpp:228]     Train net output #0: softmax = 3.33093 (* 1 = 3.33093 loss)
I0615 16:12:26.037580  5063 solver.cpp:473] Iteration 13660, lr = 0.0001
I0615 16:12:26.794507  5063 solver.cpp:213] Iteration 13670, loss = 3.50041
I0615 16:12:26.794523  5063 solver.cpp:228]     Train net output #0: softmax = 3.50041 (* 1 = 3.50041 loss)
I0615 16:12:26.794528  5063 solver.cpp:473] Iteration 13670, lr = 0.0001
I0615 16:12:27.552832  5063 solver.cpp:213] Iteration 13680, loss = 3.25567
I0615 16:12:27.552847  5063 solver.cpp:228]     Train net output #0: softmax = 3.25567 (* 1 = 3.25567 loss)
I0615 16:12:27.552851  5063 solver.cpp:473] Iteration 13680, lr = 0.0001
I0615 16:12:28.310539  5063 solver.cpp:213] Iteration 13690, loss = 3.55677
I0615 16:12:28.310556  5063 solver.cpp:228]     Train net output #0: softmax = 3.55677 (* 1 = 3.55677 loss)
I0615 16:12:28.310560  5063 solver.cpp:473] Iteration 13690, lr = 0.0001
I0615 16:12:29.065848  5063 solver.cpp:213] Iteration 13700, loss = 3.50534
I0615 16:12:29.065866  5063 solver.cpp:228]     Train net output #0: softmax = 3.50534 (* 1 = 3.50534 loss)
I0615 16:12:29.065994  5063 solver.cpp:473] Iteration 13700, lr = 0.0001
I0615 16:12:29.824023  5063 solver.cpp:213] Iteration 13710, loss = 3.33914
I0615 16:12:29.824040  5063 solver.cpp:228]     Train net output #0: softmax = 3.33914 (* 1 = 3.33914 loss)
I0615 16:12:29.824045  5063 solver.cpp:473] Iteration 13710, lr = 0.0001
I0615 16:12:30.582665  5063 solver.cpp:213] Iteration 13720, loss = 3.39655
I0615 16:12:30.582681  5063 solver.cpp:228]     Train net output #0: softmax = 3.39655 (* 1 = 3.39655 loss)
I0615 16:12:30.582685  5063 solver.cpp:473] Iteration 13720, lr = 0.0001
I0615 16:12:31.340183  5063 solver.cpp:213] Iteration 13730, loss = 3.38591
I0615 16:12:31.340201  5063 solver.cpp:228]     Train net output #0: softmax = 3.38591 (* 1 = 3.38591 loss)
I0615 16:12:31.340206  5063 solver.cpp:473] Iteration 13730, lr = 0.0001
I0615 16:12:32.098248  5063 solver.cpp:213] Iteration 13740, loss = 3.46153
I0615 16:12:32.098268  5063 solver.cpp:228]     Train net output #0: softmax = 3.46153 (* 1 = 3.46153 loss)
I0615 16:12:32.098273  5063 solver.cpp:473] Iteration 13740, lr = 0.0001
I0615 16:12:32.856192  5063 solver.cpp:213] Iteration 13750, loss = 3.48758
I0615 16:12:32.856209  5063 solver.cpp:228]     Train net output #0: softmax = 3.48758 (* 1 = 3.48758 loss)
I0615 16:12:32.856215  5063 solver.cpp:473] Iteration 13750, lr = 0.0001
I0615 16:12:33.614568  5063 solver.cpp:213] Iteration 13760, loss = 3.05155
I0615 16:12:33.614605  5063 solver.cpp:228]     Train net output #0: softmax = 3.05155 (* 1 = 3.05155 loss)
I0615 16:12:33.614611  5063 solver.cpp:473] Iteration 13760, lr = 0.0001
I0615 16:12:34.373113  5063 solver.cpp:213] Iteration 13770, loss = 3.35287
I0615 16:12:34.373128  5063 solver.cpp:228]     Train net output #0: softmax = 3.35287 (* 1 = 3.35287 loss)
I0615 16:12:34.373133  5063 solver.cpp:473] Iteration 13770, lr = 0.0001
I0615 16:12:35.131568  5063 solver.cpp:213] Iteration 13780, loss = 3.21464
I0615 16:12:35.131584  5063 solver.cpp:228]     Train net output #0: softmax = 3.21464 (* 1 = 3.21464 loss)
I0615 16:12:35.131589  5063 solver.cpp:473] Iteration 13780, lr = 0.0001
I0615 16:12:35.890452  5063 solver.cpp:213] Iteration 13790, loss = 3.59709
I0615 16:12:35.890468  5063 solver.cpp:228]     Train net output #0: softmax = 3.59709 (* 1 = 3.59709 loss)
I0615 16:12:35.890473  5063 solver.cpp:473] Iteration 13790, lr = 0.0001
I0615 16:12:36.648896  5063 solver.cpp:213] Iteration 13800, loss = 3.4689
I0615 16:12:36.648911  5063 solver.cpp:228]     Train net output #0: softmax = 3.4689 (* 1 = 3.4689 loss)
I0615 16:12:36.648916  5063 solver.cpp:473] Iteration 13800, lr = 0.0001
I0615 16:12:37.407080  5063 solver.cpp:213] Iteration 13810, loss = 3.43763
I0615 16:12:37.407096  5063 solver.cpp:228]     Train net output #0: softmax = 3.43763 (* 1 = 3.43763 loss)
I0615 16:12:37.407101  5063 solver.cpp:473] Iteration 13810, lr = 0.0001
I0615 16:12:38.164309  5063 solver.cpp:213] Iteration 13820, loss = 3.52446
I0615 16:12:38.164326  5063 solver.cpp:228]     Train net output #0: softmax = 3.52446 (* 1 = 3.52446 loss)
I0615 16:12:38.164331  5063 solver.cpp:473] Iteration 13820, lr = 0.0001
I0615 16:12:38.922564  5063 solver.cpp:213] Iteration 13830, loss = 3.43164
I0615 16:12:38.922580  5063 solver.cpp:228]     Train net output #0: softmax = 3.43164 (* 1 = 3.43164 loss)
I0615 16:12:38.922585  5063 solver.cpp:473] Iteration 13830, lr = 0.0001
I0615 16:12:39.680709  5063 solver.cpp:213] Iteration 13840, loss = 3.35465
I0615 16:12:39.680727  5063 solver.cpp:228]     Train net output #0: softmax = 3.35465 (* 1 = 3.35465 loss)
I0615 16:12:39.680843  5063 solver.cpp:473] Iteration 13840, lr = 0.0001
I0615 16:12:40.438221  5063 solver.cpp:213] Iteration 13850, loss = 3.257
I0615 16:12:40.438241  5063 solver.cpp:228]     Train net output #0: softmax = 3.257 (* 1 = 3.257 loss)
I0615 16:12:40.438247  5063 solver.cpp:473] Iteration 13850, lr = 0.0001
I0615 16:12:41.195518  5063 solver.cpp:213] Iteration 13860, loss = 3.28586
I0615 16:12:41.195531  5063 solver.cpp:228]     Train net output #0: softmax = 3.28586 (* 1 = 3.28586 loss)
I0615 16:12:41.195536  5063 solver.cpp:473] Iteration 13860, lr = 0.0001
I0615 16:12:41.953796  5063 solver.cpp:213] Iteration 13870, loss = 3.19721
I0615 16:12:41.953812  5063 solver.cpp:228]     Train net output #0: softmax = 3.19721 (* 1 = 3.19721 loss)
I0615 16:12:41.953816  5063 solver.cpp:473] Iteration 13870, lr = 0.0001
I0615 16:12:42.712394  5063 solver.cpp:213] Iteration 13880, loss = 3.30691
I0615 16:12:42.712411  5063 solver.cpp:228]     Train net output #0: softmax = 3.30691 (* 1 = 3.30691 loss)
I0615 16:12:42.712415  5063 solver.cpp:473] Iteration 13880, lr = 0.0001
I0615 16:12:43.470391  5063 solver.cpp:213] Iteration 13890, loss = 3.234
I0615 16:12:43.470409  5063 solver.cpp:228]     Train net output #0: softmax = 3.234 (* 1 = 3.234 loss)
I0615 16:12:43.470413  5063 solver.cpp:473] Iteration 13890, lr = 0.0001
I0615 16:12:44.228515  5063 solver.cpp:213] Iteration 13900, loss = 3.32582
I0615 16:12:44.228533  5063 solver.cpp:228]     Train net output #0: softmax = 3.32582 (* 1 = 3.32582 loss)
I0615 16:12:44.228538  5063 solver.cpp:473] Iteration 13900, lr = 0.0001
I0615 16:12:44.986596  5063 solver.cpp:213] Iteration 13910, loss = 3.53808
I0615 16:12:44.986615  5063 solver.cpp:228]     Train net output #0: softmax = 3.53808 (* 1 = 3.53808 loss)
I0615 16:12:44.986624  5063 solver.cpp:473] Iteration 13910, lr = 0.0001
I0615 16:12:45.744984  5063 solver.cpp:213] Iteration 13920, loss = 3.19801
I0615 16:12:45.745017  5063 solver.cpp:228]     Train net output #0: softmax = 3.19801 (* 1 = 3.19801 loss)
I0615 16:12:45.745021  5063 solver.cpp:473] Iteration 13920, lr = 0.0001
I0615 16:12:46.503598  5063 solver.cpp:213] Iteration 13930, loss = 3.45605
I0615 16:12:46.503614  5063 solver.cpp:228]     Train net output #0: softmax = 3.45605 (* 1 = 3.45605 loss)
I0615 16:12:46.503619  5063 solver.cpp:473] Iteration 13930, lr = 0.0001
I0615 16:12:47.261850  5063 solver.cpp:213] Iteration 13940, loss = 3.34147
I0615 16:12:47.261867  5063 solver.cpp:228]     Train net output #0: softmax = 3.34147 (* 1 = 3.34147 loss)
I0615 16:12:47.261871  5063 solver.cpp:473] Iteration 13940, lr = 0.0001
I0615 16:12:48.020498  5063 solver.cpp:213] Iteration 13950, loss = 3.37496
I0615 16:12:48.020515  5063 solver.cpp:228]     Train net output #0: softmax = 3.37496 (* 1 = 3.37496 loss)
I0615 16:12:48.020519  5063 solver.cpp:473] Iteration 13950, lr = 0.0001
I0615 16:12:48.778990  5063 solver.cpp:213] Iteration 13960, loss = 3.52418
I0615 16:12:48.779006  5063 solver.cpp:228]     Train net output #0: softmax = 3.52418 (* 1 = 3.52418 loss)
I0615 16:12:48.779011  5063 solver.cpp:473] Iteration 13960, lr = 0.0001
I0615 16:12:49.537752  5063 solver.cpp:213] Iteration 13970, loss = 3.20436
I0615 16:12:49.537772  5063 solver.cpp:228]     Train net output #0: softmax = 3.20436 (* 1 = 3.20436 loss)
I0615 16:12:49.537776  5063 solver.cpp:473] Iteration 13970, lr = 0.0001
I0615 16:12:50.296416  5063 solver.cpp:213] Iteration 13980, loss = 3.52828
I0615 16:12:50.296433  5063 solver.cpp:228]     Train net output #0: softmax = 3.52828 (* 1 = 3.52828 loss)
I0615 16:12:50.296579  5063 solver.cpp:473] Iteration 13980, lr = 0.0001
I0615 16:12:51.055022  5063 solver.cpp:213] Iteration 13990, loss = 3.42847
I0615 16:12:51.055037  5063 solver.cpp:228]     Train net output #0: softmax = 3.42847 (* 1 = 3.42847 loss)
I0615 16:12:51.055042  5063 solver.cpp:473] Iteration 13990, lr = 0.0001
I0615 16:12:51.760404  5063 solver.cpp:362] Snapshotting to snapshots/16-06-15_15h49m18s_0_11_pretrainClassification_iter_14000.caffemodel
I0615 16:12:51.761101  5063 solver.cpp:370] Snapshotting solver state to snapshots/16-06-15_15h49m18s_0_11_pretrainClassification_iter_14000.solverstate
I0615 16:12:51.761482  5063 solver.cpp:291] Iteration 14000, Testing net (#0)
I0615 16:12:51.855751  5063 solver.cpp:342]     Test net output #0: accuracy_top_1 = 0.18125
I0615 16:12:51.855767  5063 solver.cpp:342]     Test net output #1: accuracy_top_5 = 0.45
I0615 16:12:51.855772  5063 solver.cpp:342]     Test net output #2: softmax = 3.37241 (* 1 = 3.37241 loss)
I0615 16:12:51.909430  5063 solver.cpp:213] Iteration 14000, loss = 3.53823
I0615 16:12:51.909442  5063 solver.cpp:228]     Train net output #0: softmax = 3.53823 (* 1 = 3.53823 loss)
I0615 16:12:51.909447  5063 solver.cpp:473] Iteration 14000, lr = 0.0001
I0615 16:12:52.667793  5063 solver.cpp:213] Iteration 14010, loss = 3.41093
I0615 16:12:52.667809  5063 solver.cpp:228]     Train net output #0: softmax = 3.41093 (* 1 = 3.41093 loss)
I0615 16:12:52.667814  5063 solver.cpp:473] Iteration 14010, lr = 0.0001
I0615 16:12:53.426117  5063 solver.cpp:213] Iteration 14020, loss = 3.39742
I0615 16:12:53.426147  5063 solver.cpp:228]     Train net output #0: softmax = 3.39742 (* 1 = 3.39742 loss)
I0615 16:12:53.426152  5063 solver.cpp:473] Iteration 14020, lr = 0.0001
I0615 16:12:54.184650  5063 solver.cpp:213] Iteration 14030, loss = 3.39465
I0615 16:12:54.184666  5063 solver.cpp:228]     Train net output #0: softmax = 3.39465 (* 1 = 3.39465 loss)
I0615 16:12:54.184670  5063 solver.cpp:473] Iteration 14030, lr = 0.0001
I0615 16:12:54.943601  5063 solver.cpp:213] Iteration 14040, loss = 3.57773
I0615 16:12:54.943617  5063 solver.cpp:228]     Train net output #0: softmax = 3.57773 (* 1 = 3.57773 loss)
I0615 16:12:54.943621  5063 solver.cpp:473] Iteration 14040, lr = 0.0001
I0615 16:12:55.701786  5063 solver.cpp:213] Iteration 14050, loss = 3.42623
I0615 16:12:55.701805  5063 solver.cpp:228]     Train net output #0: softmax = 3.42623 (* 1 = 3.42623 loss)
I0615 16:12:55.701809  5063 solver.cpp:473] Iteration 14050, lr = 0.0001
I0615 16:12:56.460158  5063 solver.cpp:213] Iteration 14060, loss = 3.53857
I0615 16:12:56.460175  5063 solver.cpp:228]     Train net output #0: softmax = 3.53857 (* 1 = 3.53857 loss)
I0615 16:12:56.460178  5063 solver.cpp:473] Iteration 14060, lr = 0.0001
I0615 16:12:57.217980  5063 solver.cpp:213] Iteration 14070, loss = 3.42823
I0615 16:12:57.218004  5063 solver.cpp:228]     Train net output #0: softmax = 3.42823 (* 1 = 3.42823 loss)
I0615 16:12:57.218009  5063 solver.cpp:473] Iteration 14070, lr = 0.0001
I0615 16:12:57.976516  5063 solver.cpp:213] Iteration 14080, loss = 3.17592
I0615 16:12:57.976532  5063 solver.cpp:228]     Train net output #0: softmax = 3.17592 (* 1 = 3.17592 loss)
I0615 16:12:57.976537  5063 solver.cpp:473] Iteration 14080, lr = 0.0001
I0615 16:12:58.734457  5063 solver.cpp:213] Iteration 14090, loss = 3.57649
I0615 16:12:58.734474  5063 solver.cpp:228]     Train net output #0: softmax = 3.57649 (* 1 = 3.57649 loss)
I0615 16:12:58.734478  5063 solver.cpp:473] Iteration 14090, lr = 0.0001
I0615 16:12:59.492182  5063 solver.cpp:213] Iteration 14100, loss = 3.42947
I0615 16:12:59.492202  5063 solver.cpp:228]     Train net output #0: softmax = 3.42947 (* 1 = 3.42947 loss)
I0615 16:12:59.492207  5063 solver.cpp:473] Iteration 14100, lr = 0.0001
I0615 16:13:00.250092  5063 solver.cpp:213] Iteration 14110, loss = 3.22995
I0615 16:13:00.250113  5063 solver.cpp:228]     Train net output #0: softmax = 3.22995 (* 1 = 3.22995 loss)
I0615 16:13:00.250118  5063 solver.cpp:473] Iteration 14110, lr = 0.0001
I0615 16:13:01.007751  5063 solver.cpp:213] Iteration 14120, loss = 3.3996
I0615 16:13:01.007774  5063 solver.cpp:228]     Train net output #0: softmax = 3.3996 (* 1 = 3.3996 loss)
I0615 16:13:01.007900  5063 solver.cpp:473] Iteration 14120, lr = 0.0001
I0615 16:13:01.765846  5063 solver.cpp:213] Iteration 14130, loss = 3.3617
I0615 16:13:01.765866  5063 solver.cpp:228]     Train net output #0: softmax = 3.3617 (* 1 = 3.3617 loss)
I0615 16:13:01.765871  5063 solver.cpp:473] Iteration 14130, lr = 0.0001
I0615 16:13:02.523654  5063 solver.cpp:213] Iteration 14140, loss = 3.53635
I0615 16:13:02.523675  5063 solver.cpp:228]     Train net output #0: softmax = 3.53635 (* 1 = 3.53635 loss)
I0615 16:13:02.523679  5063 solver.cpp:473] Iteration 14140, lr = 0.0001
I0615 16:13:03.282122  5063 solver.cpp:213] Iteration 14150, loss = 3.26016
I0615 16:13:03.282137  5063 solver.cpp:228]     Train net output #0: softmax = 3.26016 (* 1 = 3.26016 loss)
I0615 16:13:03.282142  5063 solver.cpp:473] Iteration 14150, lr = 0.0001
I0615 16:13:04.040580  5063 solver.cpp:213] Iteration 14160, loss = 3.44816
I0615 16:13:04.040597  5063 solver.cpp:228]     Train net output #0: softmax = 3.44816 (* 1 = 3.44816 loss)
I0615 16:13:04.040601  5063 solver.cpp:473] Iteration 14160, lr = 0.0001
I0615 16:13:04.798615  5063 solver.cpp:213] Iteration 14170, loss = 3.29
I0615 16:13:04.798632  5063 solver.cpp:228]     Train net output #0: softmax = 3.29 (* 1 = 3.29 loss)
I0615 16:13:04.798636  5063 solver.cpp:473] Iteration 14170, lr = 0.0001
I0615 16:13:05.557235  5063 solver.cpp:213] Iteration 14180, loss = 3.71403
I0615 16:13:05.557257  5063 solver.cpp:228]     Train net output #0: softmax = 3.71403 (* 1 = 3.71403 loss)
I0615 16:13:05.557262  5063 solver.cpp:473] Iteration 14180, lr = 0.0001
I0615 16:13:06.316119  5063 solver.cpp:213] Iteration 14190, loss = 3.30479
I0615 16:13:06.316140  5063 solver.cpp:228]     Train net output #0: softmax = 3.30479 (* 1 = 3.30479 loss)
I0615 16:13:06.316278  5063 solver.cpp:473] Iteration 14190, lr = 0.0001
I0615 16:13:07.074441  5063 solver.cpp:213] Iteration 14200, loss = 3.33895
I0615 16:13:07.074461  5063 solver.cpp:228]     Train net output #0: softmax = 3.33895 (* 1 = 3.33895 loss)
I0615 16:13:07.074465  5063 solver.cpp:473] Iteration 14200, lr = 0.0001
I0615 16:13:07.833385  5063 solver.cpp:213] Iteration 14210, loss = 3.51401
I0615 16:13:07.833406  5063 solver.cpp:228]     Train net output #0: softmax = 3.51401 (* 1 = 3.51401 loss)
I0615 16:13:07.833410  5063 solver.cpp:473] Iteration 14210, lr = 0.0001
I0615 16:13:08.592133  5063 solver.cpp:213] Iteration 14220, loss = 3.23173
I0615 16:13:08.592152  5063 solver.cpp:228]     Train net output #0: softmax = 3.23173 (* 1 = 3.23173 loss)
I0615 16:13:08.592157  5063 solver.cpp:473] Iteration 14220, lr = 0.0001
I0615 16:13:09.350677  5063 solver.cpp:213] Iteration 14230, loss = 3.42254
I0615 16:13:09.350700  5063 solver.cpp:228]     Train net output #0: softmax = 3.42254 (* 1 = 3.42254 loss)
I0615 16:13:09.350704  5063 solver.cpp:473] Iteration 14230, lr = 0.0001
I0615 16:13:10.109412  5063 solver.cpp:213] Iteration 14240, loss = 3.40056
I0615 16:13:10.109427  5063 solver.cpp:228]     Train net output #0: softmax = 3.40056 (* 1 = 3.40056 loss)
I0615 16:13:10.109431  5063 solver.cpp:473] Iteration 14240, lr = 0.0001
I0615 16:13:10.867840  5063 solver.cpp:213] Iteration 14250, loss = 3.45717
I0615 16:13:10.867856  5063 solver.cpp:228]     Train net output #0: softmax = 3.45717 (* 1 = 3.45717 loss)
I0615 16:13:10.867861  5063 solver.cpp:473] Iteration 14250, lr = 0.0001
I0615 16:13:11.626502  5063 solver.cpp:213] Iteration 14260, loss = 3.1694
I0615 16:13:11.626520  5063 solver.cpp:228]     Train net output #0: softmax = 3.1694 (* 1 = 3.1694 loss)
I0615 16:13:11.626525  5063 solver.cpp:473] Iteration 14260, lr = 0.0001
I0615 16:13:12.384742  5063 solver.cpp:213] Iteration 14270, loss = 3.2067
I0615 16:13:12.384763  5063 solver.cpp:228]     Train net output #0: softmax = 3.2067 (* 1 = 3.2067 loss)
I0615 16:13:12.384768  5063 solver.cpp:473] Iteration 14270, lr = 0.0001
I0615 16:13:13.142416  5063 solver.cpp:213] Iteration 14280, loss = 3.24496
I0615 16:13:13.142434  5063 solver.cpp:228]     Train net output #0: softmax = 3.24496 (* 1 = 3.24496 loss)
I0615 16:13:13.142438  5063 solver.cpp:473] Iteration 14280, lr = 0.0001
I0615 16:13:13.901234  5063 solver.cpp:213] Iteration 14290, loss = 3.59594
I0615 16:13:13.901252  5063 solver.cpp:228]     Train net output #0: softmax = 3.59594 (* 1 = 3.59594 loss)
I0615 16:13:13.901255  5063 solver.cpp:473] Iteration 14290, lr = 0.0001
I0615 16:13:14.659481  5063 solver.cpp:213] Iteration 14300, loss = 3.30261
I0615 16:13:14.659497  5063 solver.cpp:228]     Train net output #0: softmax = 3.30261 (* 1 = 3.30261 loss)
I0615 16:13:14.659502  5063 solver.cpp:473] Iteration 14300, lr = 0.0001
I0615 16:13:15.417577  5063 solver.cpp:213] Iteration 14310, loss = 3.10863
I0615 16:13:15.417593  5063 solver.cpp:228]     Train net output #0: softmax = 3.10863 (* 1 = 3.10863 loss)
I0615 16:13:15.417598  5063 solver.cpp:473] Iteration 14310, lr = 0.0001
I0615 16:13:16.176147  5063 solver.cpp:213] Iteration 14320, loss = 3.55043
I0615 16:13:16.176174  5063 solver.cpp:228]     Train net output #0: softmax = 3.55043 (* 1 = 3.55043 loss)
I0615 16:13:16.176179  5063 solver.cpp:473] Iteration 14320, lr = 0.0001
I0615 16:13:16.934038  5063 solver.cpp:213] Iteration 14330, loss = 3.27143
I0615 16:13:16.934058  5063 solver.cpp:228]     Train net output #0: softmax = 3.27143 (* 1 = 3.27143 loss)
I0615 16:13:16.934069  5063 solver.cpp:473] Iteration 14330, lr = 0.0001
I0615 16:13:17.692971  5063 solver.cpp:213] Iteration 14340, loss = 3.32565
I0615 16:13:17.692998  5063 solver.cpp:228]     Train net output #0: softmax = 3.32565 (* 1 = 3.32565 loss)
I0615 16:13:17.693003  5063 solver.cpp:473] Iteration 14340, lr = 0.0001
I0615 16:13:18.451905  5063 solver.cpp:213] Iteration 14350, loss = 3.34063
I0615 16:13:18.451921  5063 solver.cpp:228]     Train net output #0: softmax = 3.34063 (* 1 = 3.34063 loss)
I0615 16:13:18.451926  5063 solver.cpp:473] Iteration 14350, lr = 0.0001
I0615 16:13:19.210414  5063 solver.cpp:213] Iteration 14360, loss = 3.2369
I0615 16:13:19.210429  5063 solver.cpp:228]     Train net output #0: softmax = 3.2369 (* 1 = 3.2369 loss)
I0615 16:13:19.210433  5063 solver.cpp:473] Iteration 14360, lr = 0.0001
I0615 16:13:19.969189  5063 solver.cpp:213] Iteration 14370, loss = 3.33186
I0615 16:13:19.969204  5063 solver.cpp:228]     Train net output #0: softmax = 3.33186 (* 1 = 3.33186 loss)
I0615 16:13:19.969208  5063 solver.cpp:473] Iteration 14370, lr = 0.0001
I0615 16:13:20.727879  5063 solver.cpp:213] Iteration 14380, loss = 3.2541
I0615 16:13:20.727893  5063 solver.cpp:228]     Train net output #0: softmax = 3.2541 (* 1 = 3.2541 loss)
I0615 16:13:20.727898  5063 solver.cpp:473] Iteration 14380, lr = 0.0001
I0615 16:13:21.486428  5063 solver.cpp:213] Iteration 14390, loss = 3.62988
I0615 16:13:21.486443  5063 solver.cpp:228]     Train net output #0: softmax = 3.62988 (* 1 = 3.62988 loss)
I0615 16:13:21.486449  5063 solver.cpp:473] Iteration 14390, lr = 0.0001
I0615 16:13:22.243353  5063 solver.cpp:213] Iteration 14400, loss = 3.36174
I0615 16:13:22.243525  5063 solver.cpp:228]     Train net output #0: softmax = 3.36174 (* 1 = 3.36174 loss)
I0615 16:13:22.243532  5063 solver.cpp:473] Iteration 14400, lr = 0.0001
I0615 16:13:23.000802  5063 solver.cpp:213] Iteration 14410, loss = 3.39459
I0615 16:13:23.000819  5063 solver.cpp:228]     Train net output #0: softmax = 3.39459 (* 1 = 3.39459 loss)
I0615 16:13:23.000823  5063 solver.cpp:473] Iteration 14410, lr = 0.0001
I0615 16:13:23.759234  5063 solver.cpp:213] Iteration 14420, loss = 3.44175
I0615 16:13:23.759248  5063 solver.cpp:228]     Train net output #0: softmax = 3.44175 (* 1 = 3.44175 loss)
I0615 16:13:23.759253  5063 solver.cpp:473] Iteration 14420, lr = 0.0001
I0615 16:13:24.517907  5063 solver.cpp:213] Iteration 14430, loss = 3.41127
I0615 16:13:24.517922  5063 solver.cpp:228]     Train net output #0: softmax = 3.41127 (* 1 = 3.41127 loss)
I0615 16:13:24.517926  5063 solver.cpp:473] Iteration 14430, lr = 0.0001
I0615 16:13:25.275887  5063 solver.cpp:213] Iteration 14440, loss = 3.46107
I0615 16:13:25.275903  5063 solver.cpp:228]     Train net output #0: softmax = 3.46107 (* 1 = 3.46107 loss)
I0615 16:13:25.275908  5063 solver.cpp:473] Iteration 14440, lr = 0.0001
I0615 16:13:26.034754  5063 solver.cpp:213] Iteration 14450, loss = 3.56385
I0615 16:13:26.034770  5063 solver.cpp:228]     Train net output #0: softmax = 3.56385 (* 1 = 3.56385 loss)
I0615 16:13:26.034773  5063 solver.cpp:473] Iteration 14450, lr = 0.0001
I0615 16:13:26.792230  5063 solver.cpp:213] Iteration 14460, loss = 3.45341
I0615 16:13:26.792251  5063 solver.cpp:228]     Train net output #0: softmax = 3.45341 (* 1 = 3.45341 loss)
I0615 16:13:26.792255  5063 solver.cpp:473] Iteration 14460, lr = 0.0001
I0615 16:13:27.550577  5063 solver.cpp:213] Iteration 14470, loss = 3.31012
I0615 16:13:27.550596  5063 solver.cpp:228]     Train net output #0: softmax = 3.31012 (* 1 = 3.31012 loss)
I0615 16:13:27.550601  5063 solver.cpp:473] Iteration 14470, lr = 0.0001
I0615 16:13:28.308423  5063 solver.cpp:213] Iteration 14480, loss = 3.57713
I0615 16:13:28.308440  5063 solver.cpp:228]     Train net output #0: softmax = 3.57713 (* 1 = 3.57713 loss)
I0615 16:13:28.308445  5063 solver.cpp:473] Iteration 14480, lr = 0.0001
I0615 16:13:29.066251  5063 solver.cpp:213] Iteration 14490, loss = 3.53259
I0615 16:13:29.066265  5063 solver.cpp:228]     Train net output #0: softmax = 3.53259 (* 1 = 3.53259 loss)
I0615 16:13:29.066270  5063 solver.cpp:473] Iteration 14490, lr = 0.0001
I0615 16:13:29.824723  5063 solver.cpp:213] Iteration 14500, loss = 3.21284
I0615 16:13:29.824744  5063 solver.cpp:228]     Train net output #0: softmax = 3.21284 (* 1 = 3.21284 loss)
I0615 16:13:29.824749  5063 solver.cpp:473] Iteration 14500, lr = 0.0001
I0615 16:13:30.583245  5063 solver.cpp:213] Iteration 14510, loss = 3.58845
I0615 16:13:30.583261  5063 solver.cpp:228]     Train net output #0: softmax = 3.58845 (* 1 = 3.58845 loss)
I0615 16:13:30.583264  5063 solver.cpp:473] Iteration 14510, lr = 0.0001
I0615 16:13:31.341476  5063 solver.cpp:213] Iteration 14520, loss = 3.44048
I0615 16:13:31.341490  5063 solver.cpp:228]     Train net output #0: softmax = 3.44048 (* 1 = 3.44048 loss)
I0615 16:13:31.341495  5063 solver.cpp:473] Iteration 14520, lr = 0.0001
I0615 16:13:32.099275  5063 solver.cpp:213] Iteration 14530, loss = 3.23851
I0615 16:13:32.099295  5063 solver.cpp:228]     Train net output #0: softmax = 3.23851 (* 1 = 3.23851 loss)
I0615 16:13:32.099300  5063 solver.cpp:473] Iteration 14530, lr = 0.0001
I0615 16:13:32.857136  5063 solver.cpp:213] Iteration 14540, loss = 3.30892
I0615 16:13:32.857156  5063 solver.cpp:228]     Train net output #0: softmax = 3.30892 (* 1 = 3.30892 loss)
I0615 16:13:32.857285  5063 solver.cpp:473] Iteration 14540, lr = 0.0001
I0615 16:13:33.615681  5063 solver.cpp:213] Iteration 14550, loss = 3.59142
I0615 16:13:33.615697  5063 solver.cpp:228]     Train net output #0: softmax = 3.59142 (* 1 = 3.59142 loss)
I0615 16:13:33.615702  5063 solver.cpp:473] Iteration 14550, lr = 0.0001
I0615 16:13:34.374150  5063 solver.cpp:213] Iteration 14560, loss = 3.53927
I0615 16:13:34.374210  5063 solver.cpp:228]     Train net output #0: softmax = 3.53927 (* 1 = 3.53927 loss)
I0615 16:13:34.374217  5063 solver.cpp:473] Iteration 14560, lr = 0.0001
I0615 16:13:35.130069  5063 solver.cpp:213] Iteration 14570, loss = 3.54325
I0615 16:13:35.130089  5063 solver.cpp:228]     Train net output #0: softmax = 3.54325 (* 1 = 3.54325 loss)
I0615 16:13:35.130095  5063 solver.cpp:473] Iteration 14570, lr = 0.0001
I0615 16:13:35.886179  5063 solver.cpp:213] Iteration 14580, loss = 3.60488
I0615 16:13:35.886195  5063 solver.cpp:228]     Train net output #0: softmax = 3.60488 (* 1 = 3.60488 loss)
I0615 16:13:35.886199  5063 solver.cpp:473] Iteration 14580, lr = 0.0001
I0615 16:13:36.644397  5063 solver.cpp:213] Iteration 14590, loss = 3.13644
I0615 16:13:36.644410  5063 solver.cpp:228]     Train net output #0: softmax = 3.13644 (* 1 = 3.13644 loss)
I0615 16:13:36.644415  5063 solver.cpp:473] Iteration 14590, lr = 0.0001
I0615 16:13:37.403017  5063 solver.cpp:213] Iteration 14600, loss = 3.33434
I0615 16:13:37.403031  5063 solver.cpp:228]     Train net output #0: softmax = 3.33434 (* 1 = 3.33434 loss)
I0615 16:13:37.403035  5063 solver.cpp:473] Iteration 14600, lr = 0.0001
I0615 16:13:38.161813  5063 solver.cpp:213] Iteration 14610, loss = 3.27689
I0615 16:13:38.161830  5063 solver.cpp:228]     Train net output #0: softmax = 3.27689 (* 1 = 3.27689 loss)
I0615 16:13:38.161964  5063 solver.cpp:473] Iteration 14610, lr = 0.0001
I0615 16:13:38.920534  5063 solver.cpp:213] Iteration 14620, loss = 3.31517
I0615 16:13:38.920549  5063 solver.cpp:228]     Train net output #0: softmax = 3.31517 (* 1 = 3.31517 loss)
I0615 16:13:38.920554  5063 solver.cpp:473] Iteration 14620, lr = 0.0001
I0615 16:13:39.678872  5063 solver.cpp:213] Iteration 14630, loss = 3.46554
I0615 16:13:39.678894  5063 solver.cpp:228]     Train net output #0: softmax = 3.46554 (* 1 = 3.46554 loss)
I0615 16:13:39.678899  5063 solver.cpp:473] Iteration 14630, lr = 0.0001
I0615 16:13:40.437309  5063 solver.cpp:213] Iteration 14640, loss = 3.33101
I0615 16:13:40.437325  5063 solver.cpp:228]     Train net output #0: softmax = 3.33101 (* 1 = 3.33101 loss)
I0615 16:13:40.437330  5063 solver.cpp:473] Iteration 14640, lr = 0.0001
I0615 16:13:41.195623  5063 solver.cpp:213] Iteration 14650, loss = 3.01792
I0615 16:13:41.195638  5063 solver.cpp:228]     Train net output #0: softmax = 3.01792 (* 1 = 3.01792 loss)
I0615 16:13:41.195642  5063 solver.cpp:473] Iteration 14650, lr = 0.0001
I0615 16:13:41.953531  5063 solver.cpp:213] Iteration 14660, loss = 3.07351
I0615 16:13:41.953554  5063 solver.cpp:228]     Train net output #0: softmax = 3.07351 (* 1 = 3.07351 loss)
I0615 16:13:41.953559  5063 solver.cpp:473] Iteration 14660, lr = 0.0001
I0615 16:13:42.710894  5063 solver.cpp:213] Iteration 14670, loss = 3.28664
I0615 16:13:42.710911  5063 solver.cpp:228]     Train net output #0: softmax = 3.28664 (* 1 = 3.28664 loss)
I0615 16:13:42.710914  5063 solver.cpp:473] Iteration 14670, lr = 0.0001
I0615 16:13:43.468896  5063 solver.cpp:213] Iteration 14680, loss = 3.61441
I0615 16:13:43.468914  5063 solver.cpp:228]     Train net output #0: softmax = 3.61441 (* 1 = 3.61441 loss)
I0615 16:13:43.468919  5063 solver.cpp:473] Iteration 14680, lr = 0.0001
I0615 16:13:44.226866  5063 solver.cpp:213] Iteration 14690, loss = 3.37877
I0615 16:13:44.226882  5063 solver.cpp:228]     Train net output #0: softmax = 3.37877 (* 1 = 3.37877 loss)
I0615 16:13:44.226886  5063 solver.cpp:473] Iteration 14690, lr = 0.0001
I0615 16:13:44.984421  5063 solver.cpp:213] Iteration 14700, loss = 3.14261
I0615 16:13:44.984438  5063 solver.cpp:228]     Train net output #0: softmax = 3.14261 (* 1 = 3.14261 loss)
I0615 16:13:44.984442  5063 solver.cpp:473] Iteration 14700, lr = 0.0001
I0615 16:13:45.742672  5063 solver.cpp:213] Iteration 14710, loss = 3.44759
I0615 16:13:45.742687  5063 solver.cpp:228]     Train net output #0: softmax = 3.44759 (* 1 = 3.44759 loss)
I0615 16:13:45.742691  5063 solver.cpp:473] Iteration 14710, lr = 0.0001
I0615 16:13:46.500768  5063 solver.cpp:213] Iteration 14720, loss = 3.36514
I0615 16:13:46.500799  5063 solver.cpp:228]     Train net output #0: softmax = 3.36514 (* 1 = 3.36514 loss)
I0615 16:13:46.500804  5063 solver.cpp:473] Iteration 14720, lr = 0.0001
I0615 16:13:47.258891  5063 solver.cpp:213] Iteration 14730, loss = 3.29172
I0615 16:13:47.258916  5063 solver.cpp:228]     Train net output #0: softmax = 3.29172 (* 1 = 3.29172 loss)
I0615 16:13:47.258920  5063 solver.cpp:473] Iteration 14730, lr = 0.0001
I0615 16:13:48.016830  5063 solver.cpp:213] Iteration 14740, loss = 3.38953
I0615 16:13:48.016847  5063 solver.cpp:228]     Train net output #0: softmax = 3.38953 (* 1 = 3.38953 loss)
I0615 16:13:48.016852  5063 solver.cpp:473] Iteration 14740, lr = 0.0001
I0615 16:13:48.773990  5063 solver.cpp:213] Iteration 14750, loss = 3.37709
I0615 16:13:48.774016  5063 solver.cpp:228]     Train net output #0: softmax = 3.37709 (* 1 = 3.37709 loss)
I0615 16:13:48.774152  5063 solver.cpp:473] Iteration 14750, lr = 0.0001
I0615 16:13:49.532073  5063 solver.cpp:213] Iteration 14760, loss = 3.61046
I0615 16:13:49.532088  5063 solver.cpp:228]     Train net output #0: softmax = 3.61046 (* 1 = 3.61046 loss)
I0615 16:13:49.532094  5063 solver.cpp:473] Iteration 14760, lr = 0.0001
I0615 16:13:50.290278  5063 solver.cpp:213] Iteration 14770, loss = 3.51522
I0615 16:13:50.290294  5063 solver.cpp:228]     Train net output #0: softmax = 3.51522 (* 1 = 3.51522 loss)
I0615 16:13:50.290298  5063 solver.cpp:473] Iteration 14770, lr = 0.0001
I0615 16:13:51.048486  5063 solver.cpp:213] Iteration 14780, loss = 3.47693
I0615 16:13:51.048501  5063 solver.cpp:228]     Train net output #0: softmax = 3.47693 (* 1 = 3.47693 loss)
I0615 16:13:51.048504  5063 solver.cpp:473] Iteration 14780, lr = 0.0001
I0615 16:13:51.806258  5063 solver.cpp:213] Iteration 14790, loss = 3.27975
I0615 16:13:51.806282  5063 solver.cpp:228]     Train net output #0: softmax = 3.27975 (* 1 = 3.27975 loss)
I0615 16:13:51.806287  5063 solver.cpp:473] Iteration 14790, lr = 0.0001
I0615 16:13:52.563730  5063 solver.cpp:213] Iteration 14800, loss = 3.25275
I0615 16:13:52.563771  5063 solver.cpp:228]     Train net output #0: softmax = 3.25275 (* 1 = 3.25275 loss)
I0615 16:13:52.563776  5063 solver.cpp:473] Iteration 14800, lr = 0.0001
I0615 16:13:53.322072  5063 solver.cpp:213] Iteration 14810, loss = 3.25324
I0615 16:13:53.322093  5063 solver.cpp:228]     Train net output #0: softmax = 3.25324 (* 1 = 3.25324 loss)
I0615 16:13:53.322098  5063 solver.cpp:473] Iteration 14810, lr = 0.0001
I0615 16:13:54.080296  5063 solver.cpp:213] Iteration 14820, loss = 3.46363
I0615 16:13:54.080451  5063 solver.cpp:228]     Train net output #0: softmax = 3.46363 (* 1 = 3.46363 loss)
I0615 16:13:54.080458  5063 solver.cpp:473] Iteration 14820, lr = 0.0001
I0615 16:13:54.838266  5063 solver.cpp:213] Iteration 14830, loss = 3.34022
I0615 16:13:54.838281  5063 solver.cpp:228]     Train net output #0: softmax = 3.34022 (* 1 = 3.34022 loss)
I0615 16:13:54.838285  5063 solver.cpp:473] Iteration 14830, lr = 0.0001
I0615 16:13:55.596282  5063 solver.cpp:213] Iteration 14840, loss = 3.44735
I0615 16:13:55.596298  5063 solver.cpp:228]     Train net output #0: softmax = 3.44735 (* 1 = 3.44735 loss)
I0615 16:13:55.596302  5063 solver.cpp:473] Iteration 14840, lr = 0.0001
I0615 16:13:56.353930  5063 solver.cpp:213] Iteration 14850, loss = 3.37762
I0615 16:13:56.353948  5063 solver.cpp:228]     Train net output #0: softmax = 3.37762 (* 1 = 3.37762 loss)
I0615 16:13:56.353952  5063 solver.cpp:473] Iteration 14850, lr = 0.0001
I0615 16:13:57.111440  5063 solver.cpp:213] Iteration 14860, loss = 3.20036
I0615 16:13:57.111457  5063 solver.cpp:228]     Train net output #0: softmax = 3.20036 (* 1 = 3.20036 loss)
I0615 16:13:57.111461  5063 solver.cpp:473] Iteration 14860, lr = 0.0001
I0615 16:13:57.869854  5063 solver.cpp:213] Iteration 14870, loss = 3.36513
I0615 16:13:57.869869  5063 solver.cpp:228]     Train net output #0: softmax = 3.36513 (* 1 = 3.36513 loss)
I0615 16:13:57.869874  5063 solver.cpp:473] Iteration 14870, lr = 0.0001
I0615 16:13:58.626828  5063 solver.cpp:213] Iteration 14880, loss = 3.54975
I0615 16:13:58.626849  5063 solver.cpp:228]     Train net output #0: softmax = 3.54975 (* 1 = 3.54975 loss)
I0615 16:13:58.626853  5063 solver.cpp:473] Iteration 14880, lr = 0.0001
I0615 16:13:59.385188  5063 solver.cpp:213] Iteration 14890, loss = 3.33004
I0615 16:13:59.385205  5063 solver.cpp:228]     Train net output #0: softmax = 3.33004 (* 1 = 3.33004 loss)
I0615 16:13:59.385210  5063 solver.cpp:473] Iteration 14890, lr = 0.0001
I0615 16:14:00.143260  5063 solver.cpp:213] Iteration 14900, loss = 3.30663
I0615 16:14:00.143276  5063 solver.cpp:228]     Train net output #0: softmax = 3.30663 (* 1 = 3.30663 loss)
I0615 16:14:00.143280  5063 solver.cpp:473] Iteration 14900, lr = 0.0001
I0615 16:14:00.900558  5063 solver.cpp:213] Iteration 14910, loss = 3.30764
I0615 16:14:00.900578  5063 solver.cpp:228]     Train net output #0: softmax = 3.30764 (* 1 = 3.30764 loss)
I0615 16:14:00.900583  5063 solver.cpp:473] Iteration 14910, lr = 0.0001
I0615 16:14:01.658408  5063 solver.cpp:213] Iteration 14920, loss = 3.54674
I0615 16:14:01.658424  5063 solver.cpp:228]     Train net output #0: softmax = 3.54674 (* 1 = 3.54674 loss)
I0615 16:14:01.658429  5063 solver.cpp:473] Iteration 14920, lr = 0.0001
I0615 16:14:02.416208  5063 solver.cpp:213] Iteration 14930, loss = 3.10181
I0615 16:14:02.416225  5063 solver.cpp:228]     Train net output #0: softmax = 3.10181 (* 1 = 3.10181 loss)
I0615 16:14:02.416229  5063 solver.cpp:473] Iteration 14930, lr = 0.0001
I0615 16:14:03.174211  5063 solver.cpp:213] Iteration 14940, loss = 3.45827
I0615 16:14:03.174224  5063 solver.cpp:228]     Train net output #0: softmax = 3.45827 (* 1 = 3.45827 loss)
I0615 16:14:03.174229  5063 solver.cpp:473] Iteration 14940, lr = 0.0001
I0615 16:14:03.932719  5063 solver.cpp:213] Iteration 14950, loss = 3.4279
I0615 16:14:03.932742  5063 solver.cpp:228]     Train net output #0: softmax = 3.4279 (* 1 = 3.4279 loss)
I0615 16:14:03.932747  5063 solver.cpp:473] Iteration 14950, lr = 0.0001
I0615 16:14:04.690404  5063 solver.cpp:213] Iteration 14960, loss = 3.42293
I0615 16:14:04.690572  5063 solver.cpp:228]     Train net output #0: softmax = 3.42293 (* 1 = 3.42293 loss)
I0615 16:14:04.690578  5063 solver.cpp:473] Iteration 14960, lr = 0.0001
I0615 16:14:05.448253  5063 solver.cpp:213] Iteration 14970, loss = 3.43079
I0615 16:14:05.448271  5063 solver.cpp:228]     Train net output #0: softmax = 3.43079 (* 1 = 3.43079 loss)
I0615 16:14:05.448276  5063 solver.cpp:473] Iteration 14970, lr = 0.0001
I0615 16:14:06.203981  5063 solver.cpp:213] Iteration 14980, loss = 3.07883
I0615 16:14:06.204000  5063 solver.cpp:228]     Train net output #0: softmax = 3.07883 (* 1 = 3.07883 loss)
I0615 16:14:06.204006  5063 solver.cpp:473] Iteration 14980, lr = 0.0001
I0615 16:14:06.960422  5063 solver.cpp:213] Iteration 14990, loss = 3.43795
I0615 16:14:06.960440  5063 solver.cpp:228]     Train net output #0: softmax = 3.43795 (* 1 = 3.43795 loss)
I0615 16:14:06.960445  5063 solver.cpp:473] Iteration 14990, lr = 0.0001
I0615 16:14:07.664499  5063 solver.cpp:362] Snapshotting to snapshots/16-06-15_15h49m18s_0_11_pretrainClassification_iter_15000.caffemodel
I0615 16:14:07.665220  5063 solver.cpp:370] Snapshotting solver state to snapshots/16-06-15_15h49m18s_0_11_pretrainClassification_iter_15000.solverstate
I0615 16:14:07.665606  5063 solver.cpp:291] Iteration 15000, Testing net (#0)
I0615 16:14:07.759944  5063 solver.cpp:342]     Test net output #0: accuracy_top_1 = 0.21875
I0615 16:14:07.759958  5063 solver.cpp:342]     Test net output #1: accuracy_top_5 = 0.482812
I0615 16:14:07.759964  5063 solver.cpp:342]     Test net output #2: softmax = 3.30386 (* 1 = 3.30386 loss)
I0615 16:14:07.813567  5063 solver.cpp:213] Iteration 15000, loss = 3.34032
I0615 16:14:07.813581  5063 solver.cpp:228]     Train net output #0: softmax = 3.34032 (* 1 = 3.34032 loss)
I0615 16:14:07.813585  5063 solver.cpp:473] Iteration 15000, lr = 0.0001
I0615 16:14:08.570051  5063 solver.cpp:213] Iteration 15010, loss = 3.25566
I0615 16:14:08.570075  5063 solver.cpp:228]     Train net output #0: softmax = 3.25566 (* 1 = 3.25566 loss)
I0615 16:14:08.570078  5063 solver.cpp:473] Iteration 15010, lr = 0.0001
I0615 16:14:09.328277  5063 solver.cpp:213] Iteration 15020, loss = 3.33188
I0615 16:14:09.328295  5063 solver.cpp:228]     Train net output #0: softmax = 3.33188 (* 1 = 3.33188 loss)
I0615 16:14:09.328300  5063 solver.cpp:473] Iteration 15020, lr = 0.0001
I0615 16:14:10.086561  5063 solver.cpp:213] Iteration 15030, loss = 3.39246
I0615 16:14:10.086581  5063 solver.cpp:228]     Train net output #0: softmax = 3.39246 (* 1 = 3.39246 loss)
I0615 16:14:10.086791  5063 solver.cpp:473] Iteration 15030, lr = 0.0001
I0615 16:14:10.844750  5063 solver.cpp:213] Iteration 15040, loss = 3.07976
I0615 16:14:10.844768  5063 solver.cpp:228]     Train net output #0: softmax = 3.07976 (* 1 = 3.07976 loss)
I0615 16:14:10.844772  5063 solver.cpp:473] Iteration 15040, lr = 0.0001
I0615 16:14:11.603112  5063 solver.cpp:213] Iteration 15050, loss = 3.28249
I0615 16:14:11.603127  5063 solver.cpp:228]     Train net output #0: softmax = 3.28249 (* 1 = 3.28249 loss)
I0615 16:14:11.603132  5063 solver.cpp:473] Iteration 15050, lr = 0.0001
I0615 16:14:12.361027  5063 solver.cpp:213] Iteration 15060, loss = 3.22778
I0615 16:14:12.361043  5063 solver.cpp:228]     Train net output #0: softmax = 3.22778 (* 1 = 3.22778 loss)
I0615 16:14:12.361048  5063 solver.cpp:473] Iteration 15060, lr = 0.0001
I0615 16:14:13.118582  5063 solver.cpp:213] Iteration 15070, loss = 3.52926
I0615 16:14:13.118600  5063 solver.cpp:228]     Train net output #0: softmax = 3.52926 (* 1 = 3.52926 loss)
I0615 16:14:13.118605  5063 solver.cpp:473] Iteration 15070, lr = 0.0001
I0615 16:14:13.876495  5063 solver.cpp:213] Iteration 15080, loss = 3.32958
I0615 16:14:13.876516  5063 solver.cpp:228]     Train net output #0: softmax = 3.32958 (* 1 = 3.32958 loss)
I0615 16:14:13.876520  5063 solver.cpp:473] Iteration 15080, lr = 0.0001
I0615 16:14:14.633669  5063 solver.cpp:213] Iteration 15090, loss = 3.1932
I0615 16:14:14.633687  5063 solver.cpp:228]     Train net output #0: softmax = 3.1932 (* 1 = 3.1932 loss)
I0615 16:14:14.633709  5063 solver.cpp:473] Iteration 15090, lr = 0.0001
I0615 16:14:15.391726  5063 solver.cpp:213] Iteration 15100, loss = 3.38441
I0615 16:14:15.391741  5063 solver.cpp:228]     Train net output #0: softmax = 3.38441 (* 1 = 3.38441 loss)
I0615 16:14:15.391746  5063 solver.cpp:473] Iteration 15100, lr = 0.0001
I0615 16:14:16.150348  5063 solver.cpp:213] Iteration 15110, loss = 3.36463
I0615 16:14:16.150363  5063 solver.cpp:228]     Train net output #0: softmax = 3.36463 (* 1 = 3.36463 loss)
I0615 16:14:16.150372  5063 solver.cpp:473] Iteration 15110, lr = 0.0001
I0615 16:14:16.908152  5063 solver.cpp:213] Iteration 15120, loss = 3.13028
I0615 16:14:16.908167  5063 solver.cpp:228]     Train net output #0: softmax = 3.13028 (* 1 = 3.13028 loss)
I0615 16:14:16.908172  5063 solver.cpp:473] Iteration 15120, lr = 0.0001
I0615 16:14:17.666205  5063 solver.cpp:213] Iteration 15130, loss = 3.26734
I0615 16:14:17.666219  5063 solver.cpp:228]     Train net output #0: softmax = 3.26734 (* 1 = 3.26734 loss)
I0615 16:14:17.666224  5063 solver.cpp:473] Iteration 15130, lr = 0.0001
I0615 16:14:18.423704  5063 solver.cpp:213] Iteration 15140, loss = 3.38035
I0615 16:14:18.423722  5063 solver.cpp:228]     Train net output #0: softmax = 3.38035 (* 1 = 3.38035 loss)
I0615 16:14:18.423725  5063 solver.cpp:473] Iteration 15140, lr = 0.0001
I0615 16:14:19.181885  5063 solver.cpp:213] Iteration 15150, loss = 3.5998
I0615 16:14:19.181908  5063 solver.cpp:228]     Train net output #0: softmax = 3.5998 (* 1 = 3.5998 loss)
I0615 16:14:19.181913  5063 solver.cpp:473] Iteration 15150, lr = 0.0001
I0615 16:14:19.940187  5063 solver.cpp:213] Iteration 15160, loss = 3.66318
I0615 16:14:19.940201  5063 solver.cpp:228]     Train net output #0: softmax = 3.66318 (* 1 = 3.66318 loss)
I0615 16:14:19.940206  5063 solver.cpp:473] Iteration 15160, lr = 0.0001
I0615 16:14:20.697895  5063 solver.cpp:213] Iteration 15170, loss = 3.35382
I0615 16:14:20.697914  5063 solver.cpp:228]     Train net output #0: softmax = 3.35382 (* 1 = 3.35382 loss)
I0615 16:14:20.698041  5063 solver.cpp:473] Iteration 15170, lr = 0.0001
I0615 16:14:21.456171  5063 solver.cpp:213] Iteration 15180, loss = 3.34228
I0615 16:14:21.456184  5063 solver.cpp:228]     Train net output #0: softmax = 3.34228 (* 1 = 3.34228 loss)
I0615 16:14:21.456189  5063 solver.cpp:473] Iteration 15180, lr = 0.0001
I0615 16:14:22.214308  5063 solver.cpp:213] Iteration 15190, loss = 3.20395
I0615 16:14:22.214323  5063 solver.cpp:228]     Train net output #0: softmax = 3.20395 (* 1 = 3.20395 loss)
I0615 16:14:22.214328  5063 solver.cpp:473] Iteration 15190, lr = 0.0001
I0615 16:14:22.972043  5063 solver.cpp:213] Iteration 15200, loss = 3.3901
I0615 16:14:22.972084  5063 solver.cpp:228]     Train net output #0: softmax = 3.3901 (* 1 = 3.3901 loss)
I0615 16:14:22.972089  5063 solver.cpp:473] Iteration 15200, lr = 0.0001
I0615 16:14:23.730389  5063 solver.cpp:213] Iteration 15210, loss = 3.36465
I0615 16:14:23.730411  5063 solver.cpp:228]     Train net output #0: softmax = 3.36465 (* 1 = 3.36465 loss)
I0615 16:14:23.730415  5063 solver.cpp:473] Iteration 15210, lr = 0.0001
I0615 16:14:24.488111  5063 solver.cpp:213] Iteration 15220, loss = 3.36004
I0615 16:14:24.488132  5063 solver.cpp:228]     Train net output #0: softmax = 3.36004 (* 1 = 3.36004 loss)
I0615 16:14:24.488137  5063 solver.cpp:473] Iteration 15220, lr = 0.0001
I0615 16:14:25.244621  5063 solver.cpp:213] Iteration 15230, loss = 3.24102
I0615 16:14:25.244637  5063 solver.cpp:228]     Train net output #0: softmax = 3.24102 (* 1 = 3.24102 loss)
I0615 16:14:25.244642  5063 solver.cpp:473] Iteration 15230, lr = 0.0001
I0615 16:14:26.003001  5063 solver.cpp:213] Iteration 15240, loss = 3.41296
I0615 16:14:26.003021  5063 solver.cpp:228]     Train net output #0: softmax = 3.41296 (* 1 = 3.41296 loss)
I0615 16:14:26.003157  5063 solver.cpp:473] Iteration 15240, lr = 0.0001
I0615 16:14:26.760622  5063 solver.cpp:213] Iteration 15250, loss = 3.20136
I0615 16:14:26.760638  5063 solver.cpp:228]     Train net output #0: softmax = 3.20136 (* 1 = 3.20136 loss)
I0615 16:14:26.760643  5063 solver.cpp:473] Iteration 15250, lr = 0.0001
I0615 16:14:27.518667  5063 solver.cpp:213] Iteration 15260, loss = 3.31833
I0615 16:14:27.518684  5063 solver.cpp:228]     Train net output #0: softmax = 3.31833 (* 1 = 3.31833 loss)
I0615 16:14:27.518688  5063 solver.cpp:473] Iteration 15260, lr = 0.0001
I0615 16:14:28.277107  5063 solver.cpp:213] Iteration 15270, loss = 3.33494
I0615 16:14:28.277130  5063 solver.cpp:228]     Train net output #0: softmax = 3.33494 (* 1 = 3.33494 loss)
I0615 16:14:28.277135  5063 solver.cpp:473] Iteration 15270, lr = 0.0001
I0615 16:14:29.034834  5063 solver.cpp:213] Iteration 15280, loss = 3.25623
I0615 16:14:29.034860  5063 solver.cpp:228]     Train net output #0: softmax = 3.25623 (* 1 = 3.25623 loss)
I0615 16:14:29.034865  5063 solver.cpp:473] Iteration 15280, lr = 0.0001
I0615 16:14:29.792490  5063 solver.cpp:213] Iteration 15290, loss = 3.19814
I0615 16:14:29.792506  5063 solver.cpp:228]     Train net output #0: softmax = 3.19814 (* 1 = 3.19814 loss)
I0615 16:14:29.792511  5063 solver.cpp:473] Iteration 15290, lr = 0.0001
I0615 16:14:30.550602  5063 solver.cpp:213] Iteration 15300, loss = 3.29875
I0615 16:14:30.550621  5063 solver.cpp:228]     Train net output #0: softmax = 3.29875 (* 1 = 3.29875 loss)
I0615 16:14:30.550626  5063 solver.cpp:473] Iteration 15300, lr = 0.0001
I0615 16:14:31.308718  5063 solver.cpp:213] Iteration 15310, loss = 3.50789
I0615 16:14:31.308737  5063 solver.cpp:228]     Train net output #0: softmax = 3.50789 (* 1 = 3.50789 loss)
I0615 16:14:31.308862  5063 solver.cpp:473] Iteration 15310, lr = 0.0001
I0615 16:14:32.067018  5063 solver.cpp:213] Iteration 15320, loss = 3.43147
I0615 16:14:32.067035  5063 solver.cpp:228]     Train net output #0: softmax = 3.43147 (* 1 = 3.43147 loss)
I0615 16:14:32.067040  5063 solver.cpp:473] Iteration 15320, lr = 0.0001
I0615 16:14:32.824039  5063 solver.cpp:213] Iteration 15330, loss = 3.55743
I0615 16:14:32.824059  5063 solver.cpp:228]     Train net output #0: softmax = 3.55743 (* 1 = 3.55743 loss)
I0615 16:14:32.824062  5063 solver.cpp:473] Iteration 15330, lr = 0.0001
I0615 16:14:33.583001  5063 solver.cpp:213] Iteration 15340, loss = 2.98435
I0615 16:14:33.583021  5063 solver.cpp:228]     Train net output #0: softmax = 2.98435 (* 1 = 2.98435 loss)
I0615 16:14:33.583025  5063 solver.cpp:473] Iteration 15340, lr = 0.0001
I0615 16:14:34.341536  5063 solver.cpp:213] Iteration 15350, loss = 3.17255
I0615 16:14:34.341554  5063 solver.cpp:228]     Train net output #0: softmax = 3.17255 (* 1 = 3.17255 loss)
I0615 16:14:34.341559  5063 solver.cpp:473] Iteration 15350, lr = 0.0001
I0615 16:14:35.100256  5063 solver.cpp:213] Iteration 15360, loss = 3.37104
I0615 16:14:35.100286  5063 solver.cpp:228]     Train net output #0: softmax = 3.37104 (* 1 = 3.37104 loss)
I0615 16:14:35.100291  5063 solver.cpp:473] Iteration 15360, lr = 0.0001
I0615 16:14:35.858701  5063 solver.cpp:213] Iteration 15370, loss = 3.25752
I0615 16:14:35.858717  5063 solver.cpp:228]     Train net output #0: softmax = 3.25752 (* 1 = 3.25752 loss)
I0615 16:14:35.858721  5063 solver.cpp:473] Iteration 15370, lr = 0.0001
I0615 16:14:36.617231  5063 solver.cpp:213] Iteration 15380, loss = 3.33812
I0615 16:14:36.617249  5063 solver.cpp:228]     Train net output #0: softmax = 3.33812 (* 1 = 3.33812 loss)
I0615 16:14:36.617254  5063 solver.cpp:473] Iteration 15380, lr = 0.0001
I0615 16:14:37.376070  5063 solver.cpp:213] Iteration 15390, loss = 3.46099
I0615 16:14:37.376086  5063 solver.cpp:228]     Train net output #0: softmax = 3.46099 (* 1 = 3.46099 loss)
I0615 16:14:37.376091  5063 solver.cpp:473] Iteration 15390, lr = 0.0001
I0615 16:14:38.134444  5063 solver.cpp:213] Iteration 15400, loss = 3.29079
I0615 16:14:38.134459  5063 solver.cpp:228]     Train net output #0: softmax = 3.29079 (* 1 = 3.29079 loss)
I0615 16:14:38.134464  5063 solver.cpp:473] Iteration 15400, lr = 0.0001
I0615 16:14:38.893352  5063 solver.cpp:213] Iteration 15410, loss = 3.1981
I0615 16:14:38.893373  5063 solver.cpp:228]     Train net output #0: softmax = 3.1981 (* 1 = 3.1981 loss)
I0615 16:14:38.893378  5063 solver.cpp:473] Iteration 15410, lr = 0.0001
I0615 16:14:39.651695  5063 solver.cpp:213] Iteration 15420, loss = 3.25281
I0615 16:14:39.651712  5063 solver.cpp:228]     Train net output #0: softmax = 3.25281 (* 1 = 3.25281 loss)
I0615 16:14:39.651716  5063 solver.cpp:473] Iteration 15420, lr = 0.0001
I0615 16:14:40.409786  5063 solver.cpp:213] Iteration 15430, loss = 3.3078
I0615 16:14:40.409811  5063 solver.cpp:228]     Train net output #0: softmax = 3.3078 (* 1 = 3.3078 loss)
I0615 16:14:40.409816  5063 solver.cpp:473] Iteration 15430, lr = 0.0001
I0615 16:14:41.167155  5063 solver.cpp:213] Iteration 15440, loss = 3.35774
I0615 16:14:41.167171  5063 solver.cpp:228]     Train net output #0: softmax = 3.35774 (* 1 = 3.35774 loss)
I0615 16:14:41.167176  5063 solver.cpp:473] Iteration 15440, lr = 0.0001
I0615 16:14:41.925758  5063 solver.cpp:213] Iteration 15450, loss = 3.2139
I0615 16:14:41.925776  5063 solver.cpp:228]     Train net output #0: softmax = 3.2139 (* 1 = 3.2139 loss)
I0615 16:14:41.925950  5063 solver.cpp:473] Iteration 15450, lr = 0.0001
I0615 16:14:42.682790  5063 solver.cpp:213] Iteration 15460, loss = 3.47997
I0615 16:14:42.682806  5063 solver.cpp:228]     Train net output #0: softmax = 3.47997 (* 1 = 3.47997 loss)
I0615 16:14:42.682811  5063 solver.cpp:473] Iteration 15460, lr = 0.0001
I0615 16:14:43.440842  5063 solver.cpp:213] Iteration 15470, loss = 3.31549
I0615 16:14:43.440860  5063 solver.cpp:228]     Train net output #0: softmax = 3.31549 (* 1 = 3.31549 loss)
I0615 16:14:43.440865  5063 solver.cpp:473] Iteration 15470, lr = 0.0001
I0615 16:14:44.198915  5063 solver.cpp:213] Iteration 15480, loss = 3.27852
I0615 16:14:44.198932  5063 solver.cpp:228]     Train net output #0: softmax = 3.27852 (* 1 = 3.27852 loss)
I0615 16:14:44.198936  5063 solver.cpp:473] Iteration 15480, lr = 0.0001
I0615 16:14:44.957080  5063 solver.cpp:213] Iteration 15490, loss = 3.33167
I0615 16:14:44.957094  5063 solver.cpp:228]     Train net output #0: softmax = 3.33167 (* 1 = 3.33167 loss)
I0615 16:14:44.957099  5063 solver.cpp:473] Iteration 15490, lr = 0.0001
I0615 16:14:45.715193  5063 solver.cpp:213] Iteration 15500, loss = 3.42992
I0615 16:14:45.715214  5063 solver.cpp:228]     Train net output #0: softmax = 3.42992 (* 1 = 3.42992 loss)
I0615 16:14:45.715219  5063 solver.cpp:473] Iteration 15500, lr = 0.0001
I0615 16:14:46.473309  5063 solver.cpp:213] Iteration 15510, loss = 3.1164
I0615 16:14:46.473325  5063 solver.cpp:228]     Train net output #0: softmax = 3.1164 (* 1 = 3.1164 loss)
I0615 16:14:46.473328  5063 solver.cpp:473] Iteration 15510, lr = 0.0001
I0615 16:14:47.231348  5063 solver.cpp:213] Iteration 15520, loss = 3.25646
I0615 16:14:47.231515  5063 solver.cpp:228]     Train net output #0: softmax = 3.25646 (* 1 = 3.25646 loss)
I0615 16:14:47.231521  5063 solver.cpp:473] Iteration 15520, lr = 0.0001
I0615 16:14:47.988756  5063 solver.cpp:213] Iteration 15530, loss = 3.29227
I0615 16:14:47.988771  5063 solver.cpp:228]     Train net output #0: softmax = 3.29227 (* 1 = 3.29227 loss)
I0615 16:14:47.988776  5063 solver.cpp:473] Iteration 15530, lr = 0.0001
I0615 16:14:48.747232  5063 solver.cpp:213] Iteration 15540, loss = 3.19239
I0615 16:14:48.747249  5063 solver.cpp:228]     Train net output #0: softmax = 3.19239 (* 1 = 3.19239 loss)
I0615 16:14:48.747254  5063 solver.cpp:473] Iteration 15540, lr = 0.0001
I0615 16:14:49.505825  5063 solver.cpp:213] Iteration 15550, loss = 3.53778
I0615 16:14:49.505839  5063 solver.cpp:228]     Train net output #0: softmax = 3.53778 (* 1 = 3.53778 loss)
I0615 16:14:49.505844  5063 solver.cpp:473] Iteration 15550, lr = 0.0001
I0615 16:14:50.264119  5063 solver.cpp:213] Iteration 15560, loss = 3.2521
I0615 16:14:50.264150  5063 solver.cpp:228]     Train net output #0: softmax = 3.2521 (* 1 = 3.2521 loss)
I0615 16:14:50.264155  5063 solver.cpp:473] Iteration 15560, lr = 0.0001
I0615 16:14:51.022040  5063 solver.cpp:213] Iteration 15570, loss = 3.1521
I0615 16:14:51.022059  5063 solver.cpp:228]     Train net output #0: softmax = 3.1521 (* 1 = 3.1521 loss)
I0615 16:14:51.022064  5063 solver.cpp:473] Iteration 15570, lr = 0.0001
I0615 16:14:51.780768  5063 solver.cpp:213] Iteration 15580, loss = 3.10304
I0615 16:14:51.780784  5063 solver.cpp:228]     Train net output #0: softmax = 3.10304 (* 1 = 3.10304 loss)
I0615 16:14:51.780788  5063 solver.cpp:473] Iteration 15580, lr = 0.0001
I0615 16:14:52.538514  5063 solver.cpp:213] Iteration 15590, loss = 3.31079
I0615 16:14:52.538538  5063 solver.cpp:228]     Train net output #0: softmax = 3.31079 (* 1 = 3.31079 loss)
I0615 16:14:52.538542  5063 solver.cpp:473] Iteration 15590, lr = 0.0001
I0615 16:14:53.296645  5063 solver.cpp:213] Iteration 15600, loss = 3.43298
I0615 16:14:53.296686  5063 solver.cpp:228]     Train net output #0: softmax = 3.43298 (* 1 = 3.43298 loss)
I0615 16:14:53.296692  5063 solver.cpp:473] Iteration 15600, lr = 0.0001
I0615 16:14:54.055133  5063 solver.cpp:213] Iteration 15610, loss = 3.20057
I0615 16:14:54.055148  5063 solver.cpp:228]     Train net output #0: softmax = 3.20057 (* 1 = 3.20057 loss)
I0615 16:14:54.055153  5063 solver.cpp:473] Iteration 15610, lr = 0.0001
I0615 16:14:54.811764  5063 solver.cpp:213] Iteration 15620, loss = 3.40313
I0615 16:14:54.811779  5063 solver.cpp:228]     Train net output #0: softmax = 3.40313 (* 1 = 3.40313 loss)
I0615 16:14:54.811784  5063 solver.cpp:473] Iteration 15620, lr = 0.0001
I0615 16:14:55.569742  5063 solver.cpp:213] Iteration 15630, loss = 3.67781
I0615 16:14:55.569758  5063 solver.cpp:228]     Train net output #0: softmax = 3.67781 (* 1 = 3.67781 loss)
I0615 16:14:55.569762  5063 solver.cpp:473] Iteration 15630, lr = 0.0001
I0615 16:14:56.327847  5063 solver.cpp:213] Iteration 15640, loss = 3.43289
I0615 16:14:56.327863  5063 solver.cpp:228]     Train net output #0: softmax = 3.43289 (* 1 = 3.43289 loss)
I0615 16:14:56.327867  5063 solver.cpp:473] Iteration 15640, lr = 0.0001
I0615 16:14:57.086014  5063 solver.cpp:213] Iteration 15650, loss = 3.45755
I0615 16:14:57.086035  5063 solver.cpp:228]     Train net output #0: softmax = 3.45755 (* 1 = 3.45755 loss)
I0615 16:14:57.086040  5063 solver.cpp:473] Iteration 15650, lr = 0.0001
I0615 16:14:57.844007  5063 solver.cpp:213] Iteration 15660, loss = 3.37844
I0615 16:14:57.844027  5063 solver.cpp:228]     Train net output #0: softmax = 3.37844 (* 1 = 3.37844 loss)
I0615 16:14:57.844163  5063 solver.cpp:473] Iteration 15660, lr = 0.0001
I0615 16:14:58.602733  5063 solver.cpp:213] Iteration 15670, loss = 3.35213
I0615 16:14:58.602751  5063 solver.cpp:228]     Train net output #0: softmax = 3.35213 (* 1 = 3.35213 loss)
I0615 16:14:58.602756  5063 solver.cpp:473] Iteration 15670, lr = 0.0001
I0615 16:14:59.361237  5063 solver.cpp:213] Iteration 15680, loss = 3.25592
I0615 16:14:59.361253  5063 solver.cpp:228]     Train net output #0: softmax = 3.25592 (* 1 = 3.25592 loss)
I0615 16:14:59.361256  5063 solver.cpp:473] Iteration 15680, lr = 0.0001
I0615 16:15:00.119544  5063 solver.cpp:213] Iteration 15690, loss = 3.23213
I0615 16:15:00.119560  5063 solver.cpp:228]     Train net output #0: softmax = 3.23213 (* 1 = 3.23213 loss)
I0615 16:15:00.119565  5063 solver.cpp:473] Iteration 15690, lr = 0.0001
I0615 16:15:00.877575  5063 solver.cpp:213] Iteration 15700, loss = 3.31699
I0615 16:15:00.877593  5063 solver.cpp:228]     Train net output #0: softmax = 3.31699 (* 1 = 3.31699 loss)
I0615 16:15:00.877596  5063 solver.cpp:473] Iteration 15700, lr = 0.0001
I0615 16:15:01.636164  5063 solver.cpp:213] Iteration 15710, loss = 3.50104
I0615 16:15:01.636179  5063 solver.cpp:228]     Train net output #0: softmax = 3.50104 (* 1 = 3.50104 loss)
I0615 16:15:01.636183  5063 solver.cpp:473] Iteration 15710, lr = 0.0001
I0615 16:15:02.394873  5063 solver.cpp:213] Iteration 15720, loss = 3.46782
I0615 16:15:02.394893  5063 solver.cpp:228]     Train net output #0: softmax = 3.46782 (* 1 = 3.46782 loss)
I0615 16:15:02.394898  5063 solver.cpp:473] Iteration 15720, lr = 0.0001
I0615 16:15:03.152711  5063 solver.cpp:213] Iteration 15730, loss = 3.16914
I0615 16:15:03.152729  5063 solver.cpp:228]     Train net output #0: softmax = 3.16914 (* 1 = 3.16914 loss)
I0615 16:15:03.152854  5063 solver.cpp:473] Iteration 15730, lr = 0.0001
I0615 16:15:03.910624  5063 solver.cpp:213] Iteration 15740, loss = 3.26072
I0615 16:15:03.910640  5063 solver.cpp:228]     Train net output #0: softmax = 3.26072 (* 1 = 3.26072 loss)
I0615 16:15:03.910645  5063 solver.cpp:473] Iteration 15740, lr = 0.0001
I0615 16:15:04.667858  5063 solver.cpp:213] Iteration 15750, loss = 3.50484
I0615 16:15:04.667884  5063 solver.cpp:228]     Train net output #0: softmax = 3.50484 (* 1 = 3.50484 loss)
I0615 16:15:04.667891  5063 solver.cpp:473] Iteration 15750, lr = 0.0001
I0615 16:15:05.426105  5063 solver.cpp:213] Iteration 15760, loss = 3.24373
I0615 16:15:05.426136  5063 solver.cpp:228]     Train net output #0: softmax = 3.24373 (* 1 = 3.24373 loss)
I0615 16:15:05.426141  5063 solver.cpp:473] Iteration 15760, lr = 0.0001
I0615 16:15:06.184548  5063 solver.cpp:213] Iteration 15770, loss = 3.3103
I0615 16:15:06.184562  5063 solver.cpp:228]     Train net output #0: softmax = 3.3103 (* 1 = 3.3103 loss)
I0615 16:15:06.184566  5063 solver.cpp:473] Iteration 15770, lr = 0.0001
I0615 16:15:06.942654  5063 solver.cpp:213] Iteration 15780, loss = 3.31148
I0615 16:15:06.942669  5063 solver.cpp:228]     Train net output #0: softmax = 3.31148 (* 1 = 3.31148 loss)
I0615 16:15:06.942673  5063 solver.cpp:473] Iteration 15780, lr = 0.0001
I0615 16:15:07.701187  5063 solver.cpp:213] Iteration 15790, loss = 3.20718
I0615 16:15:07.701201  5063 solver.cpp:228]     Train net output #0: softmax = 3.20718 (* 1 = 3.20718 loss)
I0615 16:15:07.701205  5063 solver.cpp:473] Iteration 15790, lr = 0.0001
I0615 16:15:08.459116  5063 solver.cpp:213] Iteration 15800, loss = 3.4029
I0615 16:15:08.459136  5063 solver.cpp:228]     Train net output #0: softmax = 3.4029 (* 1 = 3.4029 loss)
I0615 16:15:08.459141  5063 solver.cpp:473] Iteration 15800, lr = 0.0001
I0615 16:15:09.216492  5063 solver.cpp:213] Iteration 15810, loss = 3.28795
I0615 16:15:09.216509  5063 solver.cpp:228]     Train net output #0: softmax = 3.28795 (* 1 = 3.28795 loss)
I0615 16:15:09.216513  5063 solver.cpp:473] Iteration 15810, lr = 0.0001
I0615 16:15:09.974695  5063 solver.cpp:213] Iteration 15820, loss = 3.38257
I0615 16:15:09.974712  5063 solver.cpp:228]     Train net output #0: softmax = 3.38257 (* 1 = 3.38257 loss)
I0615 16:15:09.974716  5063 solver.cpp:473] Iteration 15820, lr = 0.0001
I0615 16:15:10.733186  5063 solver.cpp:213] Iteration 15830, loss = 3.19936
I0615 16:15:10.733201  5063 solver.cpp:228]     Train net output #0: softmax = 3.19936 (* 1 = 3.19936 loss)
I0615 16:15:10.733206  5063 solver.cpp:473] Iteration 15830, lr = 0.0001
I0615 16:15:11.491602  5063 solver.cpp:213] Iteration 15840, loss = 3.24804
I0615 16:15:11.491616  5063 solver.cpp:228]     Train net output #0: softmax = 3.24804 (* 1 = 3.24804 loss)
I0615 16:15:11.491621  5063 solver.cpp:473] Iteration 15840, lr = 0.0001
I0615 16:15:12.249517  5063 solver.cpp:213] Iteration 15850, loss = 3.14143
I0615 16:15:12.249532  5063 solver.cpp:228]     Train net output #0: softmax = 3.14143 (* 1 = 3.14143 loss)
I0615 16:15:12.249536  5063 solver.cpp:473] Iteration 15850, lr = 0.0001
I0615 16:15:13.007699  5063 solver.cpp:213] Iteration 15860, loss = 3.28262
I0615 16:15:13.007721  5063 solver.cpp:228]     Train net output #0: softmax = 3.28262 (* 1 = 3.28262 loss)
I0615 16:15:13.007725  5063 solver.cpp:473] Iteration 15860, lr = 0.0001
I0615 16:15:13.765974  5063 solver.cpp:213] Iteration 15870, loss = 3.13661
I0615 16:15:13.765995  5063 solver.cpp:228]     Train net output #0: softmax = 3.13661 (* 1 = 3.13661 loss)
I0615 16:15:13.766114  5063 solver.cpp:473] Iteration 15870, lr = 0.0001
I0615 16:15:14.524137  5063 solver.cpp:213] Iteration 15880, loss = 3.31991
I0615 16:15:14.524150  5063 solver.cpp:228]     Train net output #0: softmax = 3.31991 (* 1 = 3.31991 loss)
I0615 16:15:14.524155  5063 solver.cpp:473] Iteration 15880, lr = 0.0001
I0615 16:15:15.282728  5063 solver.cpp:213] Iteration 15890, loss = 3.27164
I0615 16:15:15.282744  5063 solver.cpp:228]     Train net output #0: softmax = 3.27164 (* 1 = 3.27164 loss)
I0615 16:15:15.282748  5063 solver.cpp:473] Iteration 15890, lr = 0.0001
I0615 16:15:16.040609  5063 solver.cpp:213] Iteration 15900, loss = 3.26651
I0615 16:15:16.040628  5063 solver.cpp:228]     Train net output #0: softmax = 3.26651 (* 1 = 3.26651 loss)
I0615 16:15:16.040633  5063 solver.cpp:473] Iteration 15900, lr = 0.0001
I0615 16:15:16.799351  5063 solver.cpp:213] Iteration 15910, loss = 3.23299
I0615 16:15:16.799376  5063 solver.cpp:228]     Train net output #0: softmax = 3.23299 (* 1 = 3.23299 loss)
I0615 16:15:16.799381  5063 solver.cpp:473] Iteration 15910, lr = 0.0001
I0615 16:15:17.558521  5063 solver.cpp:213] Iteration 15920, loss = 3.16853
I0615 16:15:17.558552  5063 solver.cpp:228]     Train net output #0: softmax = 3.16853 (* 1 = 3.16853 loss)
I0615 16:15:17.558555  5063 solver.cpp:473] Iteration 15920, lr = 0.0001
I0615 16:15:18.316773  5063 solver.cpp:213] Iteration 15930, loss = 3.27487
I0615 16:15:18.316795  5063 solver.cpp:228]     Train net output #0: softmax = 3.27487 (* 1 = 3.27487 loss)
I0615 16:15:18.316800  5063 solver.cpp:473] Iteration 15930, lr = 0.0001
I0615 16:15:19.075588  5063 solver.cpp:213] Iteration 15940, loss = 3.3681
I0615 16:15:19.075605  5063 solver.cpp:228]     Train net output #0: softmax = 3.3681 (* 1 = 3.3681 loss)
I0615 16:15:19.075610  5063 solver.cpp:473] Iteration 15940, lr = 0.0001
I0615 16:15:19.832545  5063 solver.cpp:213] Iteration 15950, loss = 3.3052
I0615 16:15:19.832562  5063 solver.cpp:228]     Train net output #0: softmax = 3.3052 (* 1 = 3.3052 loss)
I0615 16:15:19.832567  5063 solver.cpp:473] Iteration 15950, lr = 0.0001
I0615 16:15:20.591047  5063 solver.cpp:213] Iteration 15960, loss = 3.44791
I0615 16:15:20.591061  5063 solver.cpp:228]     Train net output #0: softmax = 3.44791 (* 1 = 3.44791 loss)
I0615 16:15:20.591066  5063 solver.cpp:473] Iteration 15960, lr = 0.0001
I0615 16:15:21.349298  5063 solver.cpp:213] Iteration 15970, loss = 3.33559
I0615 16:15:21.349313  5063 solver.cpp:228]     Train net output #0: softmax = 3.33559 (* 1 = 3.33559 loss)
I0615 16:15:21.349316  5063 solver.cpp:473] Iteration 15970, lr = 0.0001
I0615 16:15:22.107204  5063 solver.cpp:213] Iteration 15980, loss = 3.10001
I0615 16:15:22.107223  5063 solver.cpp:228]     Train net output #0: softmax = 3.10001 (* 1 = 3.10001 loss)
I0615 16:15:22.107228  5063 solver.cpp:473] Iteration 15980, lr = 0.0001
I0615 16:15:22.864049  5063 solver.cpp:213] Iteration 15990, loss = 3.29877
I0615 16:15:22.864066  5063 solver.cpp:228]     Train net output #0: softmax = 3.29877 (* 1 = 3.29877 loss)
I0615 16:15:22.864071  5063 solver.cpp:473] Iteration 15990, lr = 0.0001
I0615 16:15:23.568374  5063 solver.cpp:362] Snapshotting to snapshots/16-06-15_15h49m18s_0_11_pretrainClassification_iter_16000.caffemodel
I0615 16:15:23.569105  5063 solver.cpp:370] Snapshotting solver state to snapshots/16-06-15_15h49m18s_0_11_pretrainClassification_iter_16000.solverstate
I0615 16:15:23.569489  5063 solver.cpp:291] Iteration 16000, Testing net (#0)
I0615 16:15:23.663863  5063 solver.cpp:342]     Test net output #0: accuracy_top_1 = 0.201562
I0615 16:15:23.663878  5063 solver.cpp:342]     Test net output #1: accuracy_top_5 = 0.451562
I0615 16:15:23.663884  5063 solver.cpp:342]     Test net output #2: softmax = 3.4281 (* 1 = 3.4281 loss)
I0615 16:15:23.717628  5063 solver.cpp:213] Iteration 16000, loss = 3.26457
I0615 16:15:23.717641  5063 solver.cpp:228]     Train net output #0: softmax = 3.26457 (* 1 = 3.26457 loss)
I0615 16:15:23.717646  5063 solver.cpp:473] Iteration 16000, lr = 0.0001
I0615 16:15:24.475986  5063 solver.cpp:213] Iteration 16010, loss = 3.26093
I0615 16:15:24.476004  5063 solver.cpp:228]     Train net output #0: softmax = 3.26093 (* 1 = 3.26093 loss)
I0615 16:15:24.476008  5063 solver.cpp:473] Iteration 16010, lr = 0.0001
I0615 16:15:25.234666  5063 solver.cpp:213] Iteration 16020, loss = 3.5816
I0615 16:15:25.234683  5063 solver.cpp:228]     Train net output #0: softmax = 3.5816 (* 1 = 3.5816 loss)
I0615 16:15:25.234688  5063 solver.cpp:473] Iteration 16020, lr = 0.0001
I0615 16:15:25.992456  5063 solver.cpp:213] Iteration 16030, loss = 3.31562
I0615 16:15:25.992475  5063 solver.cpp:228]     Train net output #0: softmax = 3.31562 (* 1 = 3.31562 loss)
I0615 16:15:25.992480  5063 solver.cpp:473] Iteration 16030, lr = 0.0001
I0615 16:15:26.750285  5063 solver.cpp:213] Iteration 16040, loss = 3.16737
I0615 16:15:26.750303  5063 solver.cpp:228]     Train net output #0: softmax = 3.16737 (* 1 = 3.16737 loss)
I0615 16:15:26.750313  5063 solver.cpp:473] Iteration 16040, lr = 0.0001
I0615 16:15:27.508563  5063 solver.cpp:213] Iteration 16050, loss = 3.23203
I0615 16:15:27.508579  5063 solver.cpp:228]     Train net output #0: softmax = 3.23203 (* 1 = 3.23203 loss)
I0615 16:15:27.508582  5063 solver.cpp:473] Iteration 16050, lr = 0.0001
I0615 16:15:28.267566  5063 solver.cpp:213] Iteration 16060, loss = 3.45549
I0615 16:15:28.267587  5063 solver.cpp:228]     Train net output #0: softmax = 3.45549 (* 1 = 3.45549 loss)
I0615 16:15:28.267592  5063 solver.cpp:473] Iteration 16060, lr = 0.0001
I0615 16:15:29.026002  5063 solver.cpp:213] Iteration 16070, loss = 3.27194
I0615 16:15:29.026023  5063 solver.cpp:228]     Train net output #0: softmax = 3.27194 (* 1 = 3.27194 loss)
I0615 16:15:29.026147  5063 solver.cpp:473] Iteration 16070, lr = 0.0001
I0615 16:15:29.784479  5063 solver.cpp:213] Iteration 16080, loss = 3.28433
I0615 16:15:29.784495  5063 solver.cpp:228]     Train net output #0: softmax = 3.28433 (* 1 = 3.28433 loss)
I0615 16:15:29.784499  5063 solver.cpp:473] Iteration 16080, lr = 0.0001
I0615 16:15:30.542765  5063 solver.cpp:213] Iteration 16090, loss = 3.33276
I0615 16:15:30.542781  5063 solver.cpp:228]     Train net output #0: softmax = 3.33276 (* 1 = 3.33276 loss)
I0615 16:15:30.542785  5063 solver.cpp:473] Iteration 16090, lr = 0.0001
I0615 16:15:31.301899  5063 solver.cpp:213] Iteration 16100, loss = 3.32467
I0615 16:15:31.301915  5063 solver.cpp:228]     Train net output #0: softmax = 3.32467 (* 1 = 3.32467 loss)
I0615 16:15:31.301919  5063 solver.cpp:473] Iteration 16100, lr = 0.0001
I0615 16:15:32.061064  5063 solver.cpp:213] Iteration 16110, loss = 3.48419
I0615 16:15:32.061080  5063 solver.cpp:228]     Train net output #0: softmax = 3.48419 (* 1 = 3.48419 loss)
I0615 16:15:32.061085  5063 solver.cpp:473] Iteration 16110, lr = 0.0001
I0615 16:15:32.819298  5063 solver.cpp:213] Iteration 16120, loss = 3.53839
I0615 16:15:32.819314  5063 solver.cpp:228]     Train net output #0: softmax = 3.53839 (* 1 = 3.53839 loss)
I0615 16:15:32.819319  5063 solver.cpp:473] Iteration 16120, lr = 0.0001
I0615 16:15:33.578639  5063 solver.cpp:213] Iteration 16130, loss = 3.30318
I0615 16:15:33.578654  5063 solver.cpp:228]     Train net output #0: softmax = 3.30318 (* 1 = 3.30318 loss)
I0615 16:15:33.578657  5063 solver.cpp:473] Iteration 16130, lr = 0.0001
I0615 16:15:34.337580  5063 solver.cpp:213] Iteration 16140, loss = 3.16419
I0615 16:15:34.337605  5063 solver.cpp:228]     Train net output #0: softmax = 3.16419 (* 1 = 3.16419 loss)
I0615 16:15:34.337739  5063 solver.cpp:473] Iteration 16140, lr = 0.0001
I0615 16:15:35.095630  5063 solver.cpp:213] Iteration 16150, loss = 3.34925
I0615 16:15:35.095648  5063 solver.cpp:228]     Train net output #0: softmax = 3.34925 (* 1 = 3.34925 loss)
I0615 16:15:35.095651  5063 solver.cpp:473] Iteration 16150, lr = 0.0001
I0615 16:15:35.854202  5063 solver.cpp:213] Iteration 16160, loss = 3.3626
I0615 16:15:35.854229  5063 solver.cpp:228]     Train net output #0: softmax = 3.3626 (* 1 = 3.3626 loss)
I0615 16:15:35.854234  5063 solver.cpp:473] Iteration 16160, lr = 0.0001
I0615 16:15:36.612059  5063 solver.cpp:213] Iteration 16170, loss = 3.24514
I0615 16:15:36.612079  5063 solver.cpp:228]     Train net output #0: softmax = 3.24514 (* 1 = 3.24514 loss)
I0615 16:15:36.612083  5063 solver.cpp:473] Iteration 16170, lr = 0.0001
I0615 16:15:37.370517  5063 solver.cpp:213] Iteration 16180, loss = 3.22432
I0615 16:15:37.370532  5063 solver.cpp:228]     Train net output #0: softmax = 3.22432 (* 1 = 3.22432 loss)
I0615 16:15:37.370537  5063 solver.cpp:473] Iteration 16180, lr = 0.0001
I0615 16:15:38.128991  5063 solver.cpp:213] Iteration 16190, loss = 3.36175
I0615 16:15:38.129009  5063 solver.cpp:228]     Train net output #0: softmax = 3.36175 (* 1 = 3.36175 loss)
I0615 16:15:38.129014  5063 solver.cpp:473] Iteration 16190, lr = 0.0001
I0615 16:15:38.887620  5063 solver.cpp:213] Iteration 16200, loss = 3.36796
I0615 16:15:38.887637  5063 solver.cpp:228]     Train net output #0: softmax = 3.36796 (* 1 = 3.36796 loss)
I0615 16:15:38.887650  5063 solver.cpp:473] Iteration 16200, lr = 0.0001
I0615 16:15:39.646312  5063 solver.cpp:213] Iteration 16210, loss = 3.19226
I0615 16:15:39.646334  5063 solver.cpp:228]     Train net output #0: softmax = 3.19226 (* 1 = 3.19226 loss)
I0615 16:15:39.646338  5063 solver.cpp:473] Iteration 16210, lr = 0.0001
I0615 16:15:40.404531  5063 solver.cpp:213] Iteration 16220, loss = 3.2388
I0615 16:15:40.404546  5063 solver.cpp:228]     Train net output #0: softmax = 3.2388 (* 1 = 3.2388 loss)
I0615 16:15:40.404551  5063 solver.cpp:473] Iteration 16220, lr = 0.0001
I0615 16:15:41.163231  5063 solver.cpp:213] Iteration 16230, loss = 3.0684
I0615 16:15:41.163247  5063 solver.cpp:228]     Train net output #0: softmax = 3.0684 (* 1 = 3.0684 loss)
I0615 16:15:41.163251  5063 solver.cpp:473] Iteration 16230, lr = 0.0001
I0615 16:15:41.921922  5063 solver.cpp:213] Iteration 16240, loss = 3.37114
I0615 16:15:41.921937  5063 solver.cpp:228]     Train net output #0: softmax = 3.37114 (* 1 = 3.37114 loss)
I0615 16:15:41.921942  5063 solver.cpp:473] Iteration 16240, lr = 0.0001
I0615 16:15:42.679862  5063 solver.cpp:213] Iteration 16250, loss = 3.35317
I0615 16:15:42.679878  5063 solver.cpp:228]     Train net output #0: softmax = 3.35317 (* 1 = 3.35317 loss)
I0615 16:15:42.679883  5063 solver.cpp:473] Iteration 16250, lr = 0.0001
I0615 16:15:43.438112  5063 solver.cpp:213] Iteration 16260, loss = 3.20119
I0615 16:15:43.438129  5063 solver.cpp:228]     Train net output #0: softmax = 3.20119 (* 1 = 3.20119 loss)
I0615 16:15:43.438134  5063 solver.cpp:473] Iteration 16260, lr = 0.0001
I0615 16:15:44.196714  5063 solver.cpp:213] Iteration 16270, loss = 3.20516
I0615 16:15:44.196732  5063 solver.cpp:228]     Train net output #0: softmax = 3.20516 (* 1 = 3.20516 loss)
I0615 16:15:44.196735  5063 solver.cpp:473] Iteration 16270, lr = 0.0001
I0615 16:15:44.954926  5063 solver.cpp:213] Iteration 16280, loss = 3.27903
I0615 16:15:44.954946  5063 solver.cpp:228]     Train net output #0: softmax = 3.27903 (* 1 = 3.27903 loss)
I0615 16:15:44.955070  5063 solver.cpp:473] Iteration 16280, lr = 0.0001
I0615 16:15:45.713454  5063 solver.cpp:213] Iteration 16290, loss = 3.16576
I0615 16:15:45.713472  5063 solver.cpp:228]     Train net output #0: softmax = 3.16576 (* 1 = 3.16576 loss)
I0615 16:15:45.713477  5063 solver.cpp:473] Iteration 16290, lr = 0.0001
I0615 16:15:46.469770  5063 solver.cpp:213] Iteration 16300, loss = 3.41475
I0615 16:15:46.469784  5063 solver.cpp:228]     Train net output #0: softmax = 3.41475 (* 1 = 3.41475 loss)
I0615 16:15:46.469789  5063 solver.cpp:473] Iteration 16300, lr = 0.0001
I0615 16:15:47.228198  5063 solver.cpp:213] Iteration 16310, loss = 3.33453
I0615 16:15:47.228214  5063 solver.cpp:228]     Train net output #0: softmax = 3.33453 (* 1 = 3.33453 loss)
I0615 16:15:47.228217  5063 solver.cpp:473] Iteration 16310, lr = 0.0001
I0615 16:15:47.986933  5063 solver.cpp:213] Iteration 16320, loss = 3.27132
I0615 16:15:47.986956  5063 solver.cpp:228]     Train net output #0: softmax = 3.27132 (* 1 = 3.27132 loss)
I0615 16:15:47.986961  5063 solver.cpp:473] Iteration 16320, lr = 0.0001
I0615 16:15:48.745425  5063 solver.cpp:213] Iteration 16330, loss = 3.36206
I0615 16:15:48.745443  5063 solver.cpp:228]     Train net output #0: softmax = 3.36206 (* 1 = 3.36206 loss)
I0615 16:15:48.745448  5063 solver.cpp:473] Iteration 16330, lr = 0.0001
I0615 16:15:49.504045  5063 solver.cpp:213] Iteration 16340, loss = 3.37094
I0615 16:15:49.504063  5063 solver.cpp:228]     Train net output #0: softmax = 3.37094 (* 1 = 3.37094 loss)
I0615 16:15:49.504067  5063 solver.cpp:473] Iteration 16340, lr = 0.0001
I0615 16:15:50.262946  5063 solver.cpp:213] Iteration 16350, loss = 3.35605
I0615 16:15:50.262965  5063 solver.cpp:228]     Train net output #0: softmax = 3.35605 (* 1 = 3.35605 loss)
I0615 16:15:50.263101  5063 solver.cpp:473] Iteration 16350, lr = 0.0001
I0615 16:15:51.020470  5063 solver.cpp:213] Iteration 16360, loss = 3.16946
I0615 16:15:51.020486  5063 solver.cpp:228]     Train net output #0: softmax = 3.16946 (* 1 = 3.16946 loss)
I0615 16:15:51.020498  5063 solver.cpp:473] Iteration 16360, lr = 0.0001
I0615 16:15:51.779326  5063 solver.cpp:213] Iteration 16370, loss = 3.25498
I0615 16:15:51.779343  5063 solver.cpp:228]     Train net output #0: softmax = 3.25498 (* 1 = 3.25498 loss)
I0615 16:15:51.779347  5063 solver.cpp:473] Iteration 16370, lr = 0.0001
I0615 16:15:52.537190  5063 solver.cpp:213] Iteration 16380, loss = 3.25932
I0615 16:15:52.537209  5063 solver.cpp:228]     Train net output #0: softmax = 3.25932 (* 1 = 3.25932 loss)
I0615 16:15:52.537214  5063 solver.cpp:473] Iteration 16380, lr = 0.0001
I0615 16:15:53.296000  5063 solver.cpp:213] Iteration 16390, loss = 3.33356
I0615 16:15:53.296022  5063 solver.cpp:228]     Train net output #0: softmax = 3.33356 (* 1 = 3.33356 loss)
I0615 16:15:53.296027  5063 solver.cpp:473] Iteration 16390, lr = 0.0001
I0615 16:15:54.054620  5063 solver.cpp:213] Iteration 16400, loss = 3.35104
I0615 16:15:54.054657  5063 solver.cpp:228]     Train net output #0: softmax = 3.35104 (* 1 = 3.35104 loss)
I0615 16:15:54.054663  5063 solver.cpp:473] Iteration 16400, lr = 0.0001
I0615 16:15:54.811501  5063 solver.cpp:213] Iteration 16410, loss = 3.57952
I0615 16:15:54.811516  5063 solver.cpp:228]     Train net output #0: softmax = 3.57952 (* 1 = 3.57952 loss)
I0615 16:15:54.811520  5063 solver.cpp:473] Iteration 16410, lr = 0.0001
I0615 16:15:55.570119  5063 solver.cpp:213] Iteration 16420, loss = 3.30014
I0615 16:15:55.570137  5063 solver.cpp:228]     Train net output #0: softmax = 3.30014 (* 1 = 3.30014 loss)
I0615 16:15:55.570142  5063 solver.cpp:473] Iteration 16420, lr = 0.0001
I0615 16:15:56.328866  5063 solver.cpp:213] Iteration 16430, loss = 3.43433
I0615 16:15:56.328881  5063 solver.cpp:228]     Train net output #0: softmax = 3.43433 (* 1 = 3.43433 loss)
I0615 16:15:56.328886  5063 solver.cpp:473] Iteration 16430, lr = 0.0001
I0615 16:15:57.086542  5063 solver.cpp:213] Iteration 16440, loss = 3.41732
I0615 16:15:57.086565  5063 solver.cpp:228]     Train net output #0: softmax = 3.41732 (* 1 = 3.41732 loss)
I0615 16:15:57.086570  5063 solver.cpp:473] Iteration 16440, lr = 0.0001
I0615 16:15:57.844349  5063 solver.cpp:213] Iteration 16450, loss = 3.16646
I0615 16:15:57.844370  5063 solver.cpp:228]     Train net output #0: softmax = 3.16646 (* 1 = 3.16646 loss)
I0615 16:15:57.844375  5063 solver.cpp:473] Iteration 16450, lr = 0.0001
I0615 16:15:58.601811  5063 solver.cpp:213] Iteration 16460, loss = 3.46108
I0615 16:15:58.601835  5063 solver.cpp:228]     Train net output #0: softmax = 3.46108 (* 1 = 3.46108 loss)
I0615 16:15:58.601840  5063 solver.cpp:473] Iteration 16460, lr = 0.0001
I0615 16:15:59.359688  5063 solver.cpp:213] Iteration 16470, loss = 3.26815
I0615 16:15:59.359704  5063 solver.cpp:228]     Train net output #0: softmax = 3.26815 (* 1 = 3.26815 loss)
I0615 16:15:59.359709  5063 solver.cpp:473] Iteration 16470, lr = 0.0001
I0615 16:16:00.118262  5063 solver.cpp:213] Iteration 16480, loss = 3.42425
I0615 16:16:00.118278  5063 solver.cpp:228]     Train net output #0: softmax = 3.42425 (* 1 = 3.42425 loss)
I0615 16:16:00.118283  5063 solver.cpp:473] Iteration 16480, lr = 0.0001
I0615 16:16:00.875907  5063 solver.cpp:213] Iteration 16490, loss = 3.31674
I0615 16:16:00.875931  5063 solver.cpp:228]     Train net output #0: softmax = 3.31674 (* 1 = 3.31674 loss)
I0615 16:16:00.930150  5063 solver.cpp:473] Iteration 16490, lr = 0.0001
I0615 16:16:01.666086  5063 solver.cpp:213] Iteration 16500, loss = 3.2731
I0615 16:16:01.666102  5063 solver.cpp:228]     Train net output #0: softmax = 3.2731 (* 1 = 3.2731 loss)
I0615 16:16:01.666106  5063 solver.cpp:473] Iteration 16500, lr = 0.0001
I0615 16:16:02.424507  5063 solver.cpp:213] Iteration 16510, loss = 3.49463
I0615 16:16:02.424522  5063 solver.cpp:228]     Train net output #0: softmax = 3.49463 (* 1 = 3.49463 loss)
I0615 16:16:02.424527  5063 solver.cpp:473] Iteration 16510, lr = 0.0001
I0615 16:16:03.183138  5063 solver.cpp:213] Iteration 16520, loss = 3.3718
I0615 16:16:03.183153  5063 solver.cpp:228]     Train net output #0: softmax = 3.3718 (* 1 = 3.3718 loss)
I0615 16:16:03.183166  5063 solver.cpp:473] Iteration 16520, lr = 0.0001
I0615 16:16:03.940286  5063 solver.cpp:213] Iteration 16530, loss = 3.22431
I0615 16:16:03.940307  5063 solver.cpp:228]     Train net output #0: softmax = 3.22431 (* 1 = 3.22431 loss)
I0615 16:16:03.940311  5063 solver.cpp:473] Iteration 16530, lr = 0.0001
I0615 16:16:04.697103  5063 solver.cpp:213] Iteration 16540, loss = 3.37104
I0615 16:16:04.697124  5063 solver.cpp:228]     Train net output #0: softmax = 3.37104 (* 1 = 3.37104 loss)
I0615 16:16:04.697131  5063 solver.cpp:473] Iteration 16540, lr = 0.0001
I0615 16:16:05.454641  5063 solver.cpp:213] Iteration 16550, loss = 3.24722
I0615 16:16:05.454658  5063 solver.cpp:228]     Train net output #0: softmax = 3.24722 (* 1 = 3.24722 loss)
I0615 16:16:05.454661  5063 solver.cpp:473] Iteration 16550, lr = 0.0001
I0615 16:16:06.212695  5063 solver.cpp:213] Iteration 16560, loss = 3.33361
I0615 16:16:06.212882  5063 solver.cpp:228]     Train net output #0: softmax = 3.33361 (* 1 = 3.33361 loss)
I0615 16:16:06.212888  5063 solver.cpp:473] Iteration 16560, lr = 0.0001
I0615 16:16:06.970356  5063 solver.cpp:213] Iteration 16570, loss = 3.14282
I0615 16:16:06.970371  5063 solver.cpp:228]     Train net output #0: softmax = 3.14282 (* 1 = 3.14282 loss)
I0615 16:16:06.970376  5063 solver.cpp:473] Iteration 16570, lr = 0.0001
I0615 16:16:07.728807  5063 solver.cpp:213] Iteration 16580, loss = 3.19014
I0615 16:16:07.728823  5063 solver.cpp:228]     Train net output #0: softmax = 3.19014 (* 1 = 3.19014 loss)
I0615 16:16:07.728827  5063 solver.cpp:473] Iteration 16580, lr = 0.0001
I0615 16:16:08.487471  5063 solver.cpp:213] Iteration 16590, loss = 3.53884
I0615 16:16:08.487491  5063 solver.cpp:228]     Train net output #0: softmax = 3.53884 (* 1 = 3.53884 loss)
I0615 16:16:08.487496  5063 solver.cpp:473] Iteration 16590, lr = 0.0001
I0615 16:16:09.245564  5063 solver.cpp:213] Iteration 16600, loss = 3.34479
I0615 16:16:09.245582  5063 solver.cpp:228]     Train net output #0: softmax = 3.34479 (* 1 = 3.34479 loss)
I0615 16:16:09.245587  5063 solver.cpp:473] Iteration 16600, lr = 0.0001
I0615 16:16:10.003865  5063 solver.cpp:213] Iteration 16610, loss = 3.32189
I0615 16:16:10.003881  5063 solver.cpp:228]     Train net output #0: softmax = 3.32189 (* 1 = 3.32189 loss)
I0615 16:16:10.003885  5063 solver.cpp:473] Iteration 16610, lr = 0.0001
I0615 16:16:10.761979  5063 solver.cpp:213] Iteration 16620, loss = 3.35335
I0615 16:16:10.761996  5063 solver.cpp:228]     Train net output #0: softmax = 3.35335 (* 1 = 3.35335 loss)
I0615 16:16:10.761999  5063 solver.cpp:473] Iteration 16620, lr = 0.0001
I0615 16:16:11.520490  5063 solver.cpp:213] Iteration 16630, loss = 3.56146
I0615 16:16:11.520509  5063 solver.cpp:228]     Train net output #0: softmax = 3.56146 (* 1 = 3.56146 loss)
I0615 16:16:11.520514  5063 solver.cpp:473] Iteration 16630, lr = 0.0001
I0615 16:16:12.278904  5063 solver.cpp:213] Iteration 16640, loss = 3.4061
I0615 16:16:12.278920  5063 solver.cpp:228]     Train net output #0: softmax = 3.4061 (* 1 = 3.4061 loss)
I0615 16:16:12.278924  5063 solver.cpp:473] Iteration 16640, lr = 0.0001
I0615 16:16:13.037382  5063 solver.cpp:213] Iteration 16650, loss = 3.33561
I0615 16:16:13.037400  5063 solver.cpp:228]     Train net output #0: softmax = 3.33561 (* 1 = 3.33561 loss)
I0615 16:16:13.037405  5063 solver.cpp:473] Iteration 16650, lr = 0.0001
I0615 16:16:13.795992  5063 solver.cpp:213] Iteration 16660, loss = 3.45838
I0615 16:16:13.796012  5063 solver.cpp:228]     Train net output #0: softmax = 3.45838 (* 1 = 3.45838 loss)
I0615 16:16:13.796017  5063 solver.cpp:473] Iteration 16660, lr = 0.0001
I0615 16:16:14.554219  5063 solver.cpp:213] Iteration 16670, loss = 3.50329
I0615 16:16:14.554235  5063 solver.cpp:228]     Train net output #0: softmax = 3.50329 (* 1 = 3.50329 loss)
I0615 16:16:14.554240  5063 solver.cpp:473] Iteration 16670, lr = 0.0001
I0615 16:16:15.312655  5063 solver.cpp:213] Iteration 16680, loss = 3.50606
I0615 16:16:15.312670  5063 solver.cpp:228]     Train net output #0: softmax = 3.50606 (* 1 = 3.50606 loss)
I0615 16:16:15.312681  5063 solver.cpp:473] Iteration 16680, lr = 0.0001
I0615 16:16:16.071257  5063 solver.cpp:213] Iteration 16690, loss = 3.54105
I0615 16:16:16.071274  5063 solver.cpp:228]     Train net output #0: softmax = 3.54105 (* 1 = 3.54105 loss)
I0615 16:16:16.071279  5063 solver.cpp:473] Iteration 16690, lr = 0.0001
I0615 16:16:16.828712  5063 solver.cpp:213] Iteration 16700, loss = 3.39296
I0615 16:16:16.828730  5063 solver.cpp:228]     Train net output #0: softmax = 3.39296 (* 1 = 3.39296 loss)
I0615 16:16:16.828855  5063 solver.cpp:473] Iteration 16700, lr = 0.0001
I0615 16:16:17.587362  5063 solver.cpp:213] Iteration 16710, loss = 3.22619
I0615 16:16:17.587375  5063 solver.cpp:228]     Train net output #0: softmax = 3.22619 (* 1 = 3.22619 loss)
I0615 16:16:17.587379  5063 solver.cpp:473] Iteration 16710, lr = 0.0001
I0615 16:16:18.345799  5063 solver.cpp:213] Iteration 16720, loss = 3.26193
I0615 16:16:18.345829  5063 solver.cpp:228]     Train net output #0: softmax = 3.26193 (* 1 = 3.26193 loss)
I0615 16:16:18.345834  5063 solver.cpp:473] Iteration 16720, lr = 0.0001
I0615 16:16:19.104315  5063 solver.cpp:213] Iteration 16730, loss = 3.11642
I0615 16:16:19.104331  5063 solver.cpp:228]     Train net output #0: softmax = 3.11642 (* 1 = 3.11642 loss)
I0615 16:16:19.104336  5063 solver.cpp:473] Iteration 16730, lr = 0.0001
I0615 16:16:19.862975  5063 solver.cpp:213] Iteration 16740, loss = 3.19494
I0615 16:16:19.862996  5063 solver.cpp:228]     Train net output #0: softmax = 3.19494 (* 1 = 3.19494 loss)
I0615 16:16:19.863001  5063 solver.cpp:473] Iteration 16740, lr = 0.0001
I0615 16:16:20.621251  5063 solver.cpp:213] Iteration 16750, loss = 3.24254
I0615 16:16:20.621268  5063 solver.cpp:228]     Train net output #0: softmax = 3.24254 (* 1 = 3.24254 loss)
I0615 16:16:20.621273  5063 solver.cpp:473] Iteration 16750, lr = 0.0001
I0615 16:16:21.379858  5063 solver.cpp:213] Iteration 16760, loss = 3.35132
I0615 16:16:21.379873  5063 solver.cpp:228]     Train net output #0: softmax = 3.35132 (* 1 = 3.35132 loss)
I0615 16:16:21.379876  5063 solver.cpp:473] Iteration 16760, lr = 0.0001
I0615 16:16:22.138249  5063 solver.cpp:213] Iteration 16770, loss = 3.29701
I0615 16:16:22.138269  5063 solver.cpp:228]     Train net output #0: softmax = 3.29701 (* 1 = 3.29701 loss)
I0615 16:16:22.138401  5063 solver.cpp:473] Iteration 16770, lr = 0.0001
I0615 16:16:22.896939  5063 solver.cpp:213] Iteration 16780, loss = 3.36627
I0615 16:16:22.896955  5063 solver.cpp:228]     Train net output #0: softmax = 3.36627 (* 1 = 3.36627 loss)
I0615 16:16:22.896958  5063 solver.cpp:473] Iteration 16780, lr = 0.0001
I0615 16:16:23.655823  5063 solver.cpp:213] Iteration 16790, loss = 3.44795
I0615 16:16:23.655838  5063 solver.cpp:228]     Train net output #0: softmax = 3.44795 (* 1 = 3.44795 loss)
I0615 16:16:23.655843  5063 solver.cpp:473] Iteration 16790, lr = 0.0001
I0615 16:16:24.413424  5063 solver.cpp:213] Iteration 16800, loss = 3.42501
I0615 16:16:24.413462  5063 solver.cpp:228]     Train net output #0: softmax = 3.42501 (* 1 = 3.42501 loss)
I0615 16:16:24.413467  5063 solver.cpp:473] Iteration 16800, lr = 0.0001
I0615 16:16:25.172083  5063 solver.cpp:213] Iteration 16810, loss = 3.26731
I0615 16:16:25.172106  5063 solver.cpp:228]     Train net output #0: softmax = 3.26731 (* 1 = 3.26731 loss)
I0615 16:16:25.172111  5063 solver.cpp:473] Iteration 16810, lr = 0.0001
I0615 16:16:25.930670  5063 solver.cpp:213] Iteration 16820, loss = 3.52979
I0615 16:16:25.930685  5063 solver.cpp:228]     Train net output #0: softmax = 3.52979 (* 1 = 3.52979 loss)
I0615 16:16:25.930691  5063 solver.cpp:473] Iteration 16820, lr = 0.0001
I0615 16:16:26.688942  5063 solver.cpp:213] Iteration 16830, loss = 3.35157
I0615 16:16:26.688959  5063 solver.cpp:228]     Train net output #0: softmax = 3.35157 (* 1 = 3.35157 loss)
I0615 16:16:26.688963  5063 solver.cpp:473] Iteration 16830, lr = 0.0001
I0615 16:16:27.448197  5063 solver.cpp:213] Iteration 16840, loss = 3.20986
I0615 16:16:27.448221  5063 solver.cpp:228]     Train net output #0: softmax = 3.20986 (* 1 = 3.20986 loss)
I0615 16:16:27.448226  5063 solver.cpp:473] Iteration 16840, lr = 0.0001
I0615 16:16:28.207362  5063 solver.cpp:213] Iteration 16850, loss = 3.29699
I0615 16:16:28.207377  5063 solver.cpp:228]     Train net output #0: softmax = 3.29699 (* 1 = 3.29699 loss)
I0615 16:16:28.207381  5063 solver.cpp:473] Iteration 16850, lr = 0.0001
I0615 16:16:28.966110  5063 solver.cpp:213] Iteration 16860, loss = 3.28524
I0615 16:16:28.966127  5063 solver.cpp:228]     Train net output #0: softmax = 3.28524 (* 1 = 3.28524 loss)
I0615 16:16:28.966131  5063 solver.cpp:473] Iteration 16860, lr = 0.0001
I0615 16:16:29.725217  5063 solver.cpp:213] Iteration 16870, loss = 3.13218
I0615 16:16:29.725237  5063 solver.cpp:228]     Train net output #0: softmax = 3.13218 (* 1 = 3.13218 loss)
I0615 16:16:29.725241  5063 solver.cpp:473] Iteration 16870, lr = 0.0001
I0615 16:16:30.484217  5063 solver.cpp:213] Iteration 16880, loss = 3.45184
I0615 16:16:30.484235  5063 solver.cpp:228]     Train net output #0: softmax = 3.45184 (* 1 = 3.45184 loss)
I0615 16:16:30.484239  5063 solver.cpp:473] Iteration 16880, lr = 0.0001
I0615 16:16:31.243310  5063 solver.cpp:213] Iteration 16890, loss = 3.36114
I0615 16:16:31.243326  5063 solver.cpp:228]     Train net output #0: softmax = 3.36114 (* 1 = 3.36114 loss)
I0615 16:16:31.243331  5063 solver.cpp:473] Iteration 16890, lr = 0.0001
I0615 16:16:32.001796  5063 solver.cpp:213] Iteration 16900, loss = 3.43584
I0615 16:16:32.001814  5063 solver.cpp:228]     Train net output #0: softmax = 3.43584 (* 1 = 3.43584 loss)
I0615 16:16:32.001819  5063 solver.cpp:473] Iteration 16900, lr = 0.0001
I0615 16:16:32.760572  5063 solver.cpp:213] Iteration 16910, loss = 3.31227
I0615 16:16:32.760591  5063 solver.cpp:228]     Train net output #0: softmax = 3.31227 (* 1 = 3.31227 loss)
I0615 16:16:32.760752  5063 solver.cpp:473] Iteration 16910, lr = 0.0001
I0615 16:16:33.519582  5063 solver.cpp:213] Iteration 16920, loss = 3.31544
I0615 16:16:33.519598  5063 solver.cpp:228]     Train net output #0: softmax = 3.31544 (* 1 = 3.31544 loss)
I0615 16:16:33.519603  5063 solver.cpp:473] Iteration 16920, lr = 0.0001
I0615 16:16:34.277143  5063 solver.cpp:213] Iteration 16930, loss = 3.24958
I0615 16:16:34.277158  5063 solver.cpp:228]     Train net output #0: softmax = 3.24958 (* 1 = 3.24958 loss)
I0615 16:16:34.277163  5063 solver.cpp:473] Iteration 16930, lr = 0.0001
I0615 16:16:35.035323  5063 solver.cpp:213] Iteration 16940, loss = 3.24025
I0615 16:16:35.035339  5063 solver.cpp:228]     Train net output #0: softmax = 3.24025 (* 1 = 3.24025 loss)
I0615 16:16:35.035344  5063 solver.cpp:473] Iteration 16940, lr = 0.0001
I0615 16:16:35.793653  5063 solver.cpp:213] Iteration 16950, loss = 3.45673
I0615 16:16:35.793668  5063 solver.cpp:228]     Train net output #0: softmax = 3.45673 (* 1 = 3.45673 loss)
I0615 16:16:35.793673  5063 solver.cpp:473] Iteration 16950, lr = 0.0001
I0615 16:16:36.551969  5063 solver.cpp:213] Iteration 16960, loss = 3.0812
I0615 16:16:36.552003  5063 solver.cpp:228]     Train net output #0: softmax = 3.0812 (* 1 = 3.0812 loss)
I0615 16:16:36.552008  5063 solver.cpp:473] Iteration 16960, lr = 0.0001
I0615 16:16:37.309664  5063 solver.cpp:213] Iteration 16970, loss = 3.2561
I0615 16:16:37.309680  5063 solver.cpp:228]     Train net output #0: softmax = 3.2561 (* 1 = 3.2561 loss)
I0615 16:16:37.309685  5063 solver.cpp:473] Iteration 16970, lr = 0.0001
I0615 16:16:38.067662  5063 solver.cpp:213] Iteration 16980, loss = 3.41482
I0615 16:16:38.067682  5063 solver.cpp:228]     Train net output #0: softmax = 3.41482 (* 1 = 3.41482 loss)
I0615 16:16:38.067816  5063 solver.cpp:473] Iteration 16980, lr = 0.0001
I0615 16:16:38.826288  5063 solver.cpp:213] Iteration 16990, loss = 3.35365
I0615 16:16:38.826305  5063 solver.cpp:228]     Train net output #0: softmax = 3.35365 (* 1 = 3.35365 loss)
I0615 16:16:38.826309  5063 solver.cpp:473] Iteration 16990, lr = 0.0001
I0615 16:16:39.531409  5063 solver.cpp:362] Snapshotting to snapshots/16-06-15_15h49m18s_0_11_pretrainClassification_iter_17000.caffemodel
I0615 16:16:39.532096  5063 solver.cpp:370] Snapshotting solver state to snapshots/16-06-15_15h49m18s_0_11_pretrainClassification_iter_17000.solverstate
I0615 16:16:39.532476  5063 solver.cpp:291] Iteration 17000, Testing net (#0)
I0615 16:16:39.626834  5063 solver.cpp:342]     Test net output #0: accuracy_top_1 = 0.20625
I0615 16:16:39.626849  5063 solver.cpp:342]     Test net output #1: accuracy_top_5 = 0.45
I0615 16:16:39.626855  5063 solver.cpp:342]     Test net output #2: softmax = 3.38333 (* 1 = 3.38333 loss)
I0615 16:16:39.680408  5063 solver.cpp:213] Iteration 17000, loss = 3.33925
I0615 16:16:39.680421  5063 solver.cpp:228]     Train net output #0: softmax = 3.33925 (* 1 = 3.33925 loss)
I0615 16:16:39.680426  5063 solver.cpp:473] Iteration 17000, lr = 0.0001
I0615 16:16:40.439312  5063 solver.cpp:213] Iteration 17010, loss = 3.32549
I0615 16:16:40.439337  5063 solver.cpp:228]     Train net output #0: softmax = 3.32549 (* 1 = 3.32549 loss)
I0615 16:16:40.439342  5063 solver.cpp:473] Iteration 17010, lr = 0.0001
I0615 16:16:41.195849  5063 solver.cpp:213] Iteration 17020, loss = 3.23167
I0615 16:16:41.195863  5063 solver.cpp:228]     Train net output #0: softmax = 3.23167 (* 1 = 3.23167 loss)
I0615 16:16:41.195868  5063 solver.cpp:473] Iteration 17020, lr = 0.0001
I0615 16:16:41.954041  5063 solver.cpp:213] Iteration 17030, loss = 3.15686
I0615 16:16:41.954056  5063 solver.cpp:228]     Train net output #0: softmax = 3.15686 (* 1 = 3.15686 loss)
I0615 16:16:41.954061  5063 solver.cpp:473] Iteration 17030, lr = 0.0001
I0615 16:16:42.712976  5063 solver.cpp:213] Iteration 17040, loss = 3.31383
I0615 16:16:42.712992  5063 solver.cpp:228]     Train net output #0: softmax = 3.31383 (* 1 = 3.31383 loss)
I0615 16:16:42.712997  5063 solver.cpp:473] Iteration 17040, lr = 0.0001
I0615 16:16:43.471416  5063 solver.cpp:213] Iteration 17050, loss = 3.22854
I0615 16:16:43.471444  5063 solver.cpp:228]     Train net output #0: softmax = 3.22854 (* 1 = 3.22854 loss)
I0615 16:16:43.471449  5063 solver.cpp:473] Iteration 17050, lr = 0.0001
I0615 16:16:44.230391  5063 solver.cpp:213] Iteration 17060, loss = 3.43062
I0615 16:16:44.230406  5063 solver.cpp:228]     Train net output #0: softmax = 3.43062 (* 1 = 3.43062 loss)
I0615 16:16:44.230411  5063 solver.cpp:473] Iteration 17060, lr = 0.0001
I0615 16:16:44.987776  5063 solver.cpp:213] Iteration 17070, loss = 3.61918
I0615 16:16:44.987795  5063 solver.cpp:228]     Train net output #0: softmax = 3.61918 (* 1 = 3.61918 loss)
I0615 16:16:44.987800  5063 solver.cpp:473] Iteration 17070, lr = 0.0001
I0615 16:16:45.746307  5063 solver.cpp:213] Iteration 17080, loss = 3.3791
I0615 16:16:45.746326  5063 solver.cpp:228]     Train net output #0: softmax = 3.3791 (* 1 = 3.3791 loss)
I0615 16:16:45.746330  5063 solver.cpp:473] Iteration 17080, lr = 0.0001
I0615 16:16:46.504997  5063 solver.cpp:213] Iteration 17090, loss = 3.23701
I0615 16:16:46.505014  5063 solver.cpp:228]     Train net output #0: softmax = 3.23701 (* 1 = 3.23701 loss)
I0615 16:16:46.505038  5063 solver.cpp:473] Iteration 17090, lr = 0.0001
I0615 16:16:47.263638  5063 solver.cpp:213] Iteration 17100, loss = 3.55485
I0615 16:16:47.263655  5063 solver.cpp:228]     Train net output #0: softmax = 3.55485 (* 1 = 3.55485 loss)
I0615 16:16:47.263660  5063 solver.cpp:473] Iteration 17100, lr = 0.0001
I0615 16:16:48.021678  5063 solver.cpp:213] Iteration 17110, loss = 3.12311
I0615 16:16:48.021694  5063 solver.cpp:228]     Train net output #0: softmax = 3.12311 (* 1 = 3.12311 loss)
I0615 16:16:48.021699  5063 solver.cpp:473] Iteration 17110, lr = 0.0001
I0615 16:16:48.780231  5063 solver.cpp:213] Iteration 17120, loss = 3.18825
I0615 16:16:48.780251  5063 solver.cpp:228]     Train net output #0: softmax = 3.18825 (* 1 = 3.18825 loss)
I0615 16:16:48.780388  5063 solver.cpp:473] Iteration 17120, lr = 0.0001
I0615 16:16:49.538853  5063 solver.cpp:213] Iteration 17130, loss = 3.20262
I0615 16:16:49.538872  5063 solver.cpp:228]     Train net output #0: softmax = 3.20262 (* 1 = 3.20262 loss)
I0615 16:16:49.538882  5063 solver.cpp:473] Iteration 17130, lr = 0.0001
I0615 16:16:50.296715  5063 solver.cpp:213] Iteration 17140, loss = 3.46086
I0615 16:16:50.296736  5063 solver.cpp:228]     Train net output #0: softmax = 3.46086 (* 1 = 3.46086 loss)
I0615 16:16:50.296741  5063 solver.cpp:473] Iteration 17140, lr = 0.0001
I0615 16:16:51.055390  5063 solver.cpp:213] Iteration 17150, loss = 3.28016
I0615 16:16:51.055407  5063 solver.cpp:228]     Train net output #0: softmax = 3.28016 (* 1 = 3.28016 loss)
I0615 16:16:51.055411  5063 solver.cpp:473] Iteration 17150, lr = 0.0001
I0615 16:16:51.813851  5063 solver.cpp:213] Iteration 17160, loss = 3.33444
I0615 16:16:51.813866  5063 solver.cpp:228]     Train net output #0: softmax = 3.33444 (* 1 = 3.33444 loss)
I0615 16:16:51.813871  5063 solver.cpp:473] Iteration 17160, lr = 0.0001
I0615 16:16:52.571755  5063 solver.cpp:213] Iteration 17170, loss = 3.07145
I0615 16:16:52.571773  5063 solver.cpp:228]     Train net output #0: softmax = 3.07145 (* 1 = 3.07145 loss)
I0615 16:16:52.571777  5063 solver.cpp:473] Iteration 17170, lr = 0.0001
I0615 16:16:53.329403  5063 solver.cpp:213] Iteration 17180, loss = 3.23486
I0615 16:16:53.329421  5063 solver.cpp:228]     Train net output #0: softmax = 3.23486 (* 1 = 3.23486 loss)
I0615 16:16:53.329424  5063 solver.cpp:473] Iteration 17180, lr = 0.0001
I0615 16:16:54.087012  5063 solver.cpp:213] Iteration 17190, loss = 3.03962
I0615 16:16:54.087033  5063 solver.cpp:228]     Train net output #0: softmax = 3.03962 (* 1 = 3.03962 loss)
I0615 16:16:54.087155  5063 solver.cpp:473] Iteration 17190, lr = 0.0001
I0615 16:16:54.845623  5063 solver.cpp:213] Iteration 17200, loss = 3.35919
I0615 16:16:54.845667  5063 solver.cpp:228]     Train net output #0: softmax = 3.35919 (* 1 = 3.35919 loss)
I0615 16:16:54.845674  5063 solver.cpp:473] Iteration 17200, lr = 0.0001
I0615 16:16:55.604377  5063 solver.cpp:213] Iteration 17210, loss = 3.28491
I0615 16:16:55.604393  5063 solver.cpp:228]     Train net output #0: softmax = 3.28491 (* 1 = 3.28491 loss)
I0615 16:16:55.604398  5063 solver.cpp:473] Iteration 17210, lr = 0.0001
I0615 16:16:56.363123  5063 solver.cpp:213] Iteration 17220, loss = 3.35917
I0615 16:16:56.363138  5063 solver.cpp:228]     Train net output #0: softmax = 3.35917 (* 1 = 3.35917 loss)
I0615 16:16:56.363142  5063 solver.cpp:473] Iteration 17220, lr = 0.0001
I0615 16:16:57.121539  5063 solver.cpp:213] Iteration 17230, loss = 3.36522
I0615 16:16:57.121562  5063 solver.cpp:228]     Train net output #0: softmax = 3.36522 (* 1 = 3.36522 loss)
I0615 16:16:57.121565  5063 solver.cpp:473] Iteration 17230, lr = 0.0001
I0615 16:16:57.879400  5063 solver.cpp:213] Iteration 17240, loss = 3.1516
I0615 16:16:57.879417  5063 solver.cpp:228]     Train net output #0: softmax = 3.1516 (* 1 = 3.1516 loss)
I0615 16:16:57.879421  5063 solver.cpp:473] Iteration 17240, lr = 0.0001
I0615 16:16:58.637573  5063 solver.cpp:213] Iteration 17250, loss = 3.32253
I0615 16:16:58.637589  5063 solver.cpp:228]     Train net output #0: softmax = 3.32253 (* 1 = 3.32253 loss)
I0615 16:16:58.637593  5063 solver.cpp:473] Iteration 17250, lr = 0.0001
I0615 16:16:59.396350  5063 solver.cpp:213] Iteration 17260, loss = 3.18278
I0615 16:16:59.396365  5063 solver.cpp:228]     Train net output #0: softmax = 3.18278 (* 1 = 3.18278 loss)
I0615 16:16:59.396370  5063 solver.cpp:473] Iteration 17260, lr = 0.0001
I0615 16:17:00.155411  5063 solver.cpp:213] Iteration 17270, loss = 3.27286
I0615 16:17:00.155426  5063 solver.cpp:228]     Train net output #0: softmax = 3.27286 (* 1 = 3.27286 loss)
I0615 16:17:00.155431  5063 solver.cpp:473] Iteration 17270, lr = 0.0001
I0615 16:17:00.913714  5063 solver.cpp:213] Iteration 17280, loss = 3.29159
I0615 16:17:00.913732  5063 solver.cpp:228]     Train net output #0: softmax = 3.29159 (* 1 = 3.29159 loss)
I0615 16:17:00.913736  5063 solver.cpp:473] Iteration 17280, lr = 0.0001
I0615 16:17:01.671210  5063 solver.cpp:213] Iteration 17290, loss = 3.27459
I0615 16:17:01.671227  5063 solver.cpp:228]     Train net output #0: softmax = 3.27459 (* 1 = 3.27459 loss)
I0615 16:17:01.671238  5063 solver.cpp:473] Iteration 17290, lr = 0.0001
I0615 16:17:02.429343  5063 solver.cpp:213] Iteration 17300, loss = 3.24806
I0615 16:17:02.429368  5063 solver.cpp:228]     Train net output #0: softmax = 3.24806 (* 1 = 3.24806 loss)
I0615 16:17:02.429373  5063 solver.cpp:473] Iteration 17300, lr = 0.0001
I0615 16:17:03.187686  5063 solver.cpp:213] Iteration 17310, loss = 3.64344
I0615 16:17:03.187705  5063 solver.cpp:228]     Train net output #0: softmax = 3.64344 (* 1 = 3.64344 loss)
I0615 16:17:03.187710  5063 solver.cpp:473] Iteration 17310, lr = 0.0001
I0615 16:17:03.945972  5063 solver.cpp:213] Iteration 17320, loss = 3.24561
I0615 16:17:03.945989  5063 solver.cpp:228]     Train net output #0: softmax = 3.24561 (* 1 = 3.24561 loss)
I0615 16:17:03.945993  5063 solver.cpp:473] Iteration 17320, lr = 0.0001
I0615 16:17:04.704213  5063 solver.cpp:213] Iteration 17330, loss = 3.29149
I0615 16:17:04.704236  5063 solver.cpp:228]     Train net output #0: softmax = 3.29149 (* 1 = 3.29149 loss)
I0615 16:17:04.704438  5063 solver.cpp:473] Iteration 17330, lr = 0.0001
I0615 16:17:05.462661  5063 solver.cpp:213] Iteration 17340, loss = 3.43922
I0615 16:17:05.462677  5063 solver.cpp:228]     Train net output #0: softmax = 3.43922 (* 1 = 3.43922 loss)
I0615 16:17:05.462682  5063 solver.cpp:473] Iteration 17340, lr = 0.0001
I0615 16:17:06.221562  5063 solver.cpp:213] Iteration 17350, loss = 3.04527
I0615 16:17:06.221585  5063 solver.cpp:228]     Train net output #0: softmax = 3.04527 (* 1 = 3.04527 loss)
I0615 16:17:06.221590  5063 solver.cpp:473] Iteration 17350, lr = 0.0001
I0615 16:17:06.979648  5063 solver.cpp:213] Iteration 17360, loss = 3.40063
I0615 16:17:06.979682  5063 solver.cpp:228]     Train net output #0: softmax = 3.40063 (* 1 = 3.40063 loss)
I0615 16:17:06.979687  5063 solver.cpp:473] Iteration 17360, lr = 0.0001
I0615 16:17:07.738101  5063 solver.cpp:213] Iteration 17370, loss = 3.16357
I0615 16:17:07.738116  5063 solver.cpp:228]     Train net output #0: softmax = 3.16357 (* 1 = 3.16357 loss)
I0615 16:17:07.738121  5063 solver.cpp:473] Iteration 17370, lr = 0.0001
I0615 16:17:08.496018  5063 solver.cpp:213] Iteration 17380, loss = 3.34034
I0615 16:17:08.496034  5063 solver.cpp:228]     Train net output #0: softmax = 3.34034 (* 1 = 3.34034 loss)
I0615 16:17:08.496038  5063 solver.cpp:473] Iteration 17380, lr = 0.0001
I0615 16:17:09.254570  5063 solver.cpp:213] Iteration 17390, loss = 3.34024
I0615 16:17:09.254585  5063 solver.cpp:228]     Train net output #0: softmax = 3.34024 (* 1 = 3.34024 loss)
I0615 16:17:09.254590  5063 solver.cpp:473] Iteration 17390, lr = 0.0001
I0615 16:17:10.012833  5063 solver.cpp:213] Iteration 17400, loss = 3.32581
I0615 16:17:10.012854  5063 solver.cpp:228]     Train net output #0: softmax = 3.32581 (* 1 = 3.32581 loss)
I0615 16:17:10.012866  5063 solver.cpp:473] Iteration 17400, lr = 0.0001
I0615 16:17:10.770498  5063 solver.cpp:213] Iteration 17410, loss = 3.31543
I0615 16:17:10.770516  5063 solver.cpp:228]     Train net output #0: softmax = 3.31543 (* 1 = 3.31543 loss)
I0615 16:17:10.770521  5063 solver.cpp:473] Iteration 17410, lr = 0.0001
I0615 16:17:11.529281  5063 solver.cpp:213] Iteration 17420, loss = 3.14343
I0615 16:17:11.529297  5063 solver.cpp:228]     Train net output #0: softmax = 3.14343 (* 1 = 3.14343 loss)
I0615 16:17:11.529301  5063 solver.cpp:473] Iteration 17420, lr = 0.0001
I0615 16:17:12.287236  5063 solver.cpp:213] Iteration 17430, loss = 3.37276
I0615 16:17:12.287256  5063 solver.cpp:228]     Train net output #0: softmax = 3.37276 (* 1 = 3.37276 loss)
I0615 16:17:12.287261  5063 solver.cpp:473] Iteration 17430, lr = 0.0001
I0615 16:17:13.045176  5063 solver.cpp:213] Iteration 17440, loss = 3.43597
I0615 16:17:13.045197  5063 solver.cpp:228]     Train net output #0: softmax = 3.43597 (* 1 = 3.43597 loss)
I0615 16:17:13.045202  5063 solver.cpp:473] Iteration 17440, lr = 0.0001
I0615 16:17:13.803444  5063 solver.cpp:213] Iteration 17450, loss = 3.32475
I0615 16:17:13.803462  5063 solver.cpp:228]     Train net output #0: softmax = 3.32475 (* 1 = 3.32475 loss)
I0615 16:17:13.803473  5063 solver.cpp:473] Iteration 17450, lr = 0.0001
I0615 16:17:14.561605  5063 solver.cpp:213] Iteration 17460, loss = 3.36249
I0615 16:17:14.561624  5063 solver.cpp:228]     Train net output #0: softmax = 3.36249 (* 1 = 3.36249 loss)
I0615 16:17:14.561627  5063 solver.cpp:473] Iteration 17460, lr = 0.0001
I0615 16:17:15.319792  5063 solver.cpp:213] Iteration 17470, loss = 3.27835
I0615 16:17:15.319813  5063 solver.cpp:228]     Train net output #0: softmax = 3.27835 (* 1 = 3.27835 loss)
I0615 16:17:15.319823  5063 solver.cpp:473] Iteration 17470, lr = 0.0001
I0615 16:17:16.077407  5063 solver.cpp:213] Iteration 17480, loss = 3.3612
I0615 16:17:16.077425  5063 solver.cpp:228]     Train net output #0: softmax = 3.3612 (* 1 = 3.3612 loss)
I0615 16:17:16.077430  5063 solver.cpp:473] Iteration 17480, lr = 0.0001
I0615 16:17:16.835244  5063 solver.cpp:213] Iteration 17490, loss = 3.49386
I0615 16:17:16.835264  5063 solver.cpp:228]     Train net output #0: softmax = 3.49386 (* 1 = 3.49386 loss)
I0615 16:17:16.835268  5063 solver.cpp:473] Iteration 17490, lr = 0.0001
I0615 16:17:17.594259  5063 solver.cpp:213] Iteration 17500, loss = 3.26707
I0615 16:17:17.594274  5063 solver.cpp:228]     Train net output #0: softmax = 3.26707 (* 1 = 3.26707 loss)
I0615 16:17:17.594279  5063 solver.cpp:473] Iteration 17500, lr = 0.0001
I0615 16:17:18.352531  5063 solver.cpp:213] Iteration 17510, loss = 3.27226
I0615 16:17:18.352552  5063 solver.cpp:228]     Train net output #0: softmax = 3.27226 (* 1 = 3.27226 loss)
I0615 16:17:18.352557  5063 solver.cpp:473] Iteration 17510, lr = 0.0001
I0615 16:17:19.106263  5063 solver.cpp:213] Iteration 17520, loss = 3.21873
I0615 16:17:19.106294  5063 solver.cpp:228]     Train net output #0: softmax = 3.21873 (* 1 = 3.21873 loss)
I0615 16:17:19.106299  5063 solver.cpp:473] Iteration 17520, lr = 0.0001
I0615 16:17:19.865305  5063 solver.cpp:213] Iteration 17530, loss = 3.40343
I0615 16:17:19.865322  5063 solver.cpp:228]     Train net output #0: softmax = 3.40343 (* 1 = 3.40343 loss)
I0615 16:17:19.865326  5063 solver.cpp:473] Iteration 17530, lr = 0.0001
I0615 16:17:20.624397  5063 solver.cpp:213] Iteration 17540, loss = 3.26076
I0615 16:17:20.624415  5063 solver.cpp:228]     Train net output #0: softmax = 3.26076 (* 1 = 3.26076 loss)
I0615 16:17:20.624419  5063 solver.cpp:473] Iteration 17540, lr = 0.0001
I0615 16:17:21.383314  5063 solver.cpp:213] Iteration 17550, loss = 3.47437
I0615 16:17:21.383329  5063 solver.cpp:228]     Train net output #0: softmax = 3.47437 (* 1 = 3.47437 loss)
I0615 16:17:21.383333  5063 solver.cpp:473] Iteration 17550, lr = 0.0001
I0615 16:17:22.142101  5063 solver.cpp:213] Iteration 17560, loss = 3.32936
I0615 16:17:22.142117  5063 solver.cpp:228]     Train net output #0: softmax = 3.32936 (* 1 = 3.32936 loss)
I0615 16:17:22.142122  5063 solver.cpp:473] Iteration 17560, lr = 0.0001
I0615 16:17:22.900717  5063 solver.cpp:213] Iteration 17570, loss = 3.0285
I0615 16:17:22.900730  5063 solver.cpp:228]     Train net output #0: softmax = 3.0285 (* 1 = 3.0285 loss)
I0615 16:17:22.900735  5063 solver.cpp:473] Iteration 17570, lr = 0.0001
I0615 16:17:23.658861  5063 solver.cpp:213] Iteration 17580, loss = 3.21102
I0615 16:17:23.658879  5063 solver.cpp:228]     Train net output #0: softmax = 3.21102 (* 1 = 3.21102 loss)
I0615 16:17:23.658882  5063 solver.cpp:473] Iteration 17580, lr = 0.0001
I0615 16:17:24.416960  5063 solver.cpp:213] Iteration 17590, loss = 3.24602
I0615 16:17:24.416980  5063 solver.cpp:228]     Train net output #0: softmax = 3.24602 (* 1 = 3.24602 loss)
I0615 16:17:24.416985  5063 solver.cpp:473] Iteration 17590, lr = 0.0001
I0615 16:17:25.173858  5063 solver.cpp:213] Iteration 17600, loss = 3.2365
I0615 16:17:25.173908  5063 solver.cpp:228]     Train net output #0: softmax = 3.2365 (* 1 = 3.2365 loss)
I0615 16:17:25.173914  5063 solver.cpp:473] Iteration 17600, lr = 0.0001
I0615 16:17:25.932252  5063 solver.cpp:213] Iteration 17610, loss = 3.28464
I0615 16:17:25.932271  5063 solver.cpp:228]     Train net output #0: softmax = 3.28464 (* 1 = 3.28464 loss)
I0615 16:17:25.932286  5063 solver.cpp:473] Iteration 17610, lr = 0.0001
I0615 16:17:26.691380  5063 solver.cpp:213] Iteration 17620, loss = 3.30059
I0615 16:17:26.691395  5063 solver.cpp:228]     Train net output #0: softmax = 3.30059 (* 1 = 3.30059 loss)
I0615 16:17:26.691398  5063 solver.cpp:473] Iteration 17620, lr = 0.0001
I0615 16:17:27.449491  5063 solver.cpp:213] Iteration 17630, loss = 3.27933
I0615 16:17:27.449506  5063 solver.cpp:228]     Train net output #0: softmax = 3.27933 (* 1 = 3.27933 loss)
I0615 16:17:27.449512  5063 solver.cpp:473] Iteration 17630, lr = 0.0001
I0615 16:17:28.208042  5063 solver.cpp:213] Iteration 17640, loss = 3.18354
I0615 16:17:28.208058  5063 solver.cpp:228]     Train net output #0: softmax = 3.18354 (* 1 = 3.18354 loss)
I0615 16:17:28.208063  5063 solver.cpp:473] Iteration 17640, lr = 0.0001
I0615 16:17:28.966114  5063 solver.cpp:213] Iteration 17650, loss = 3.34496
I0615 16:17:28.966130  5063 solver.cpp:228]     Train net output #0: softmax = 3.34496 (* 1 = 3.34496 loss)
I0615 16:17:28.966135  5063 solver.cpp:473] Iteration 17650, lr = 0.0001
I0615 16:17:29.724563  5063 solver.cpp:213] Iteration 17660, loss = 3.21347
I0615 16:17:29.724584  5063 solver.cpp:228]     Train net output #0: softmax = 3.21347 (* 1 = 3.21347 loss)
I0615 16:17:29.724589  5063 solver.cpp:473] Iteration 17660, lr = 0.0001
I0615 16:17:30.483227  5063 solver.cpp:213] Iteration 17670, loss = 3.23152
I0615 16:17:30.483244  5063 solver.cpp:228]     Train net output #0: softmax = 3.23152 (* 1 = 3.23152 loss)
I0615 16:17:30.483248  5063 solver.cpp:473] Iteration 17670, lr = 0.0001
I0615 16:17:31.241336  5063 solver.cpp:213] Iteration 17680, loss = 3.06631
I0615 16:17:31.241356  5063 solver.cpp:228]     Train net output #0: softmax = 3.06631 (* 1 = 3.06631 loss)
I0615 16:17:31.241482  5063 solver.cpp:473] Iteration 17680, lr = 0.0001
I0615 16:17:31.999534  5063 solver.cpp:213] Iteration 17690, loss = 3.34569
I0615 16:17:31.999552  5063 solver.cpp:228]     Train net output #0: softmax = 3.34569 (* 1 = 3.34569 loss)
I0615 16:17:31.999555  5063 solver.cpp:473] Iteration 17690, lr = 0.0001
I0615 16:17:32.757406  5063 solver.cpp:213] Iteration 17700, loss = 3.43345
I0615 16:17:32.757421  5063 solver.cpp:228]     Train net output #0: softmax = 3.43345 (* 1 = 3.43345 loss)
I0615 16:17:32.757426  5063 solver.cpp:473] Iteration 17700, lr = 0.0001
I0615 16:17:33.515933  5063 solver.cpp:213] Iteration 17710, loss = 3.24793
I0615 16:17:33.515949  5063 solver.cpp:228]     Train net output #0: softmax = 3.24793 (* 1 = 3.24793 loss)
I0615 16:17:33.515954  5063 solver.cpp:473] Iteration 17710, lr = 0.0001
I0615 16:17:34.274616  5063 solver.cpp:213] Iteration 17720, loss = 3.22893
I0615 16:17:34.274633  5063 solver.cpp:228]     Train net output #0: softmax = 3.22893 (* 1 = 3.22893 loss)
I0615 16:17:34.274637  5063 solver.cpp:473] Iteration 17720, lr = 0.0001
I0615 16:17:35.033232  5063 solver.cpp:213] Iteration 17730, loss = 3.40499
I0615 16:17:35.033247  5063 solver.cpp:228]     Train net output #0: softmax = 3.40499 (* 1 = 3.40499 loss)
I0615 16:17:35.033252  5063 solver.cpp:473] Iteration 17730, lr = 0.0001
I0615 16:17:35.791750  5063 solver.cpp:213] Iteration 17740, loss = 3.19631
I0615 16:17:35.791766  5063 solver.cpp:228]     Train net output #0: softmax = 3.19631 (* 1 = 3.19631 loss)
I0615 16:17:35.791771  5063 solver.cpp:473] Iteration 17740, lr = 0.0001
I0615 16:17:36.549741  5063 solver.cpp:213] Iteration 17750, loss = 3.35118
I0615 16:17:36.549772  5063 solver.cpp:228]     Train net output #0: softmax = 3.35118 (* 1 = 3.35118 loss)
I0615 16:17:36.549777  5063 solver.cpp:473] Iteration 17750, lr = 0.0001
I0615 16:17:37.307662  5063 solver.cpp:213] Iteration 17760, loss = 3.36285
I0615 16:17:37.307701  5063 solver.cpp:228]     Train net output #0: softmax = 3.36285 (* 1 = 3.36285 loss)
I0615 16:17:37.307706  5063 solver.cpp:473] Iteration 17760, lr = 0.0001
I0615 16:17:38.066642  5063 solver.cpp:213] Iteration 17770, loss = 3.37718
I0615 16:17:38.066656  5063 solver.cpp:228]     Train net output #0: softmax = 3.37718 (* 1 = 3.37718 loss)
I0615 16:17:38.066665  5063 solver.cpp:473] Iteration 17770, lr = 0.0001
I0615 16:17:38.825695  5063 solver.cpp:213] Iteration 17780, loss = 3.25284
I0615 16:17:38.825711  5063 solver.cpp:228]     Train net output #0: softmax = 3.25284 (* 1 = 3.25284 loss)
I0615 16:17:38.825716  5063 solver.cpp:473] Iteration 17780, lr = 0.0001
I0615 16:17:39.584358  5063 solver.cpp:213] Iteration 17790, loss = 3.19693
I0615 16:17:39.584373  5063 solver.cpp:228]     Train net output #0: softmax = 3.19693 (* 1 = 3.19693 loss)
I0615 16:17:39.584378  5063 solver.cpp:473] Iteration 17790, lr = 0.0001
I0615 16:17:40.342835  5063 solver.cpp:213] Iteration 17800, loss = 3.25982
I0615 16:17:40.342850  5063 solver.cpp:228]     Train net output #0: softmax = 3.25982 (* 1 = 3.25982 loss)
I0615 16:17:40.342855  5063 solver.cpp:473] Iteration 17800, lr = 0.0001
I0615 16:17:41.101208  5063 solver.cpp:213] Iteration 17810, loss = 3.02683
I0615 16:17:41.101225  5063 solver.cpp:228]     Train net output #0: softmax = 3.02683 (* 1 = 3.02683 loss)
I0615 16:17:41.101230  5063 solver.cpp:473] Iteration 17810, lr = 0.0001
I0615 16:17:41.859580  5063 solver.cpp:213] Iteration 17820, loss = 3.27174
I0615 16:17:41.859603  5063 solver.cpp:228]     Train net output #0: softmax = 3.27174 (* 1 = 3.27174 loss)
I0615 16:17:41.859767  5063 solver.cpp:473] Iteration 17820, lr = 0.0001
I0615 16:17:42.617846  5063 solver.cpp:213] Iteration 17830, loss = 3.30754
I0615 16:17:42.617861  5063 solver.cpp:228]     Train net output #0: softmax = 3.30754 (* 1 = 3.30754 loss)
I0615 16:17:42.617866  5063 solver.cpp:473] Iteration 17830, lr = 0.0001
I0615 16:17:43.376215  5063 solver.cpp:213] Iteration 17840, loss = 3.41712
I0615 16:17:43.376235  5063 solver.cpp:228]     Train net output #0: softmax = 3.41712 (* 1 = 3.41712 loss)
I0615 16:17:43.376240  5063 solver.cpp:473] Iteration 17840, lr = 0.0001
I0615 16:17:44.134595  5063 solver.cpp:213] Iteration 17850, loss = 3.34504
I0615 16:17:44.134615  5063 solver.cpp:228]     Train net output #0: softmax = 3.34504 (* 1 = 3.34504 loss)
I0615 16:17:44.134620  5063 solver.cpp:473] Iteration 17850, lr = 0.0001
I0615 16:17:44.892241  5063 solver.cpp:213] Iteration 17860, loss = 3.1869
I0615 16:17:44.892261  5063 solver.cpp:228]     Train net output #0: softmax = 3.1869 (* 1 = 3.1869 loss)
I0615 16:17:44.892266  5063 solver.cpp:473] Iteration 17860, lr = 0.0001
I0615 16:17:45.650744  5063 solver.cpp:213] Iteration 17870, loss = 3.09928
I0615 16:17:45.650760  5063 solver.cpp:228]     Train net output #0: softmax = 3.09928 (* 1 = 3.09928 loss)
I0615 16:17:45.650765  5063 solver.cpp:473] Iteration 17870, lr = 0.0001
I0615 16:17:46.409596  5063 solver.cpp:213] Iteration 17880, loss = 3.44099
I0615 16:17:46.409610  5063 solver.cpp:228]     Train net output #0: softmax = 3.44099 (* 1 = 3.44099 loss)
I0615 16:17:46.409615  5063 solver.cpp:473] Iteration 17880, lr = 0.0001
I0615 16:17:47.168086  5063 solver.cpp:213] Iteration 17890, loss = 3.1228
I0615 16:17:47.168110  5063 solver.cpp:228]     Train net output #0: softmax = 3.1228 (* 1 = 3.1228 loss)
I0615 16:17:47.168237  5063 solver.cpp:473] Iteration 17890, lr = 0.0001
I0615 16:17:47.926709  5063 solver.cpp:213] Iteration 17900, loss = 3.17029
I0615 16:17:47.926726  5063 solver.cpp:228]     Train net output #0: softmax = 3.17029 (* 1 = 3.17029 loss)
I0615 16:17:47.926731  5063 solver.cpp:473] Iteration 17900, lr = 0.0001
I0615 16:17:48.685989  5063 solver.cpp:213] Iteration 17910, loss = 3.19002
I0615 16:17:48.686005  5063 solver.cpp:228]     Train net output #0: softmax = 3.19002 (* 1 = 3.19002 loss)
I0615 16:17:48.686010  5063 solver.cpp:473] Iteration 17910, lr = 0.0001
I0615 16:17:49.444727  5063 solver.cpp:213] Iteration 17920, loss = 3.2319
I0615 16:17:49.444758  5063 solver.cpp:228]     Train net output #0: softmax = 3.2319 (* 1 = 3.2319 loss)
I0615 16:17:49.444763  5063 solver.cpp:473] Iteration 17920, lr = 0.0001
I0615 16:17:50.202816  5063 solver.cpp:213] Iteration 17930, loss = 3.53383
I0615 16:17:50.202831  5063 solver.cpp:228]     Train net output #0: softmax = 3.53383 (* 1 = 3.53383 loss)
I0615 16:17:50.202839  5063 solver.cpp:473] Iteration 17930, lr = 0.0001
I0615 16:17:50.961370  5063 solver.cpp:213] Iteration 17940, loss = 3.25953
I0615 16:17:50.961386  5063 solver.cpp:228]     Train net output #0: softmax = 3.25953 (* 1 = 3.25953 loss)
I0615 16:17:50.961391  5063 solver.cpp:473] Iteration 17940, lr = 0.0001
I0615 16:17:51.719791  5063 solver.cpp:213] Iteration 17950, loss = 3.23483
I0615 16:17:51.719807  5063 solver.cpp:228]     Train net output #0: softmax = 3.23483 (* 1 = 3.23483 loss)
I0615 16:17:51.719812  5063 solver.cpp:473] Iteration 17950, lr = 0.0001
I0615 16:17:52.478314  5063 solver.cpp:213] Iteration 17960, loss = 3.37959
I0615 16:17:52.478338  5063 solver.cpp:228]     Train net output #0: softmax = 3.37959 (* 1 = 3.37959 loss)
I0615 16:17:52.478343  5063 solver.cpp:473] Iteration 17960, lr = 0.0001
I0615 16:17:53.237532  5063 solver.cpp:213] Iteration 17970, loss = 3.4337
I0615 16:17:53.237546  5063 solver.cpp:228]     Train net output #0: softmax = 3.4337 (* 1 = 3.4337 loss)
I0615 16:17:53.237551  5063 solver.cpp:473] Iteration 17970, lr = 0.0001
I0615 16:17:53.994449  5063 solver.cpp:213] Iteration 17980, loss = 3.1727
I0615 16:17:53.994463  5063 solver.cpp:228]     Train net output #0: softmax = 3.1727 (* 1 = 3.1727 loss)
I0615 16:17:53.994468  5063 solver.cpp:473] Iteration 17980, lr = 0.0001
I0615 16:17:54.753424  5063 solver.cpp:213] Iteration 17990, loss = 2.97363
I0615 16:17:54.753439  5063 solver.cpp:228]     Train net output #0: softmax = 2.97363 (* 1 = 2.97363 loss)
I0615 16:17:54.753443  5063 solver.cpp:473] Iteration 17990, lr = 0.0001
I0615 16:17:55.459233  5063 solver.cpp:362] Snapshotting to snapshots/16-06-15_15h49m18s_0_11_pretrainClassification_iter_18000.caffemodel
I0615 16:17:55.459933  5063 solver.cpp:370] Snapshotting solver state to snapshots/16-06-15_15h49m18s_0_11_pretrainClassification_iter_18000.solverstate
I0615 16:17:55.460337  5063 solver.cpp:291] Iteration 18000, Testing net (#0)
I0615 16:17:55.554654  5063 solver.cpp:342]     Test net output #0: accuracy_top_1 = 0.204688
I0615 16:17:55.554669  5063 solver.cpp:342]     Test net output #1: accuracy_top_5 = 0.485938
I0615 16:17:55.554675  5063 solver.cpp:342]     Test net output #2: softmax = 3.34906 (* 1 = 3.34906 loss)
I0615 16:17:55.608247  5063 solver.cpp:213] Iteration 18000, loss = 3.42215
I0615 16:17:55.608260  5063 solver.cpp:228]     Train net output #0: softmax = 3.42215 (* 1 = 3.42215 loss)
I0615 16:17:55.608264  5063 solver.cpp:473] Iteration 18000, lr = 0.0001
I0615 16:17:56.367466  5063 solver.cpp:213] Iteration 18010, loss = 3.23169
I0615 16:17:56.367482  5063 solver.cpp:228]     Train net output #0: softmax = 3.23169 (* 1 = 3.23169 loss)
I0615 16:17:56.367486  5063 solver.cpp:473] Iteration 18010, lr = 0.0001
I0615 16:17:57.125871  5063 solver.cpp:213] Iteration 18020, loss = 3.27692
I0615 16:17:57.125888  5063 solver.cpp:228]     Train net output #0: softmax = 3.27692 (* 1 = 3.27692 loss)
I0615 16:17:57.125892  5063 solver.cpp:473] Iteration 18020, lr = 0.0001
I0615 16:17:57.884018  5063 solver.cpp:213] Iteration 18030, loss = 3.40155
I0615 16:17:57.884037  5063 solver.cpp:228]     Train net output #0: softmax = 3.40155 (* 1 = 3.40155 loss)
I0615 16:17:57.884246  5063 solver.cpp:473] Iteration 18030, lr = 0.0001
I0615 16:17:58.642933  5063 solver.cpp:213] Iteration 18040, loss = 3.19913
I0615 16:17:58.642947  5063 solver.cpp:228]     Train net output #0: softmax = 3.19913 (* 1 = 3.19913 loss)
I0615 16:17:58.642952  5063 solver.cpp:473] Iteration 18040, lr = 0.0001
I0615 16:17:59.401496  5063 solver.cpp:213] Iteration 18050, loss = 3.06922
I0615 16:17:59.401515  5063 solver.cpp:228]     Train net output #0: softmax = 3.06922 (* 1 = 3.06922 loss)
I0615 16:17:59.401520  5063 solver.cpp:473] Iteration 18050, lr = 0.0001
I0615 16:18:00.160100  5063 solver.cpp:213] Iteration 18060, loss = 3.02572
I0615 16:18:00.160117  5063 solver.cpp:228]     Train net output #0: softmax = 3.02572 (* 1 = 3.02572 loss)
I0615 16:18:00.160122  5063 solver.cpp:473] Iteration 18060, lr = 0.0001
I0615 16:18:00.918892  5063 solver.cpp:213] Iteration 18070, loss = 3.17236
I0615 16:18:00.918908  5063 solver.cpp:228]     Train net output #0: softmax = 3.17236 (* 1 = 3.17236 loss)
I0615 16:18:00.918913  5063 solver.cpp:473] Iteration 18070, lr = 0.0001
I0615 16:18:01.677466  5063 solver.cpp:213] Iteration 18080, loss = 3.28371
I0615 16:18:01.677487  5063 solver.cpp:228]     Train net output #0: softmax = 3.28371 (* 1 = 3.28371 loss)
I0615 16:18:01.677492  5063 solver.cpp:473] Iteration 18080, lr = 0.0001
I0615 16:18:02.434695  5063 solver.cpp:213] Iteration 18090, loss = 3.2382
I0615 16:18:02.434710  5063 solver.cpp:228]     Train net output #0: softmax = 3.2382 (* 1 = 3.2382 loss)
I0615 16:18:02.434715  5063 solver.cpp:473] Iteration 18090, lr = 0.0001
I0615 16:18:03.191926  5063 solver.cpp:213] Iteration 18100, loss = 3.14756
I0615 16:18:03.191946  5063 solver.cpp:228]     Train net output #0: softmax = 3.14756 (* 1 = 3.14756 loss)
I0615 16:18:03.192070  5063 solver.cpp:473] Iteration 18100, lr = 0.0001
I0615 16:18:03.948243  5063 solver.cpp:213] Iteration 18110, loss = 3.26987
I0615 16:18:03.948258  5063 solver.cpp:228]     Train net output #0: softmax = 3.26987 (* 1 = 3.26987 loss)
I0615 16:18:03.948262  5063 solver.cpp:473] Iteration 18110, lr = 0.0001
I0615 16:18:04.706743  5063 solver.cpp:213] Iteration 18120, loss = 3.25769
I0615 16:18:04.706764  5063 solver.cpp:228]     Train net output #0: softmax = 3.25769 (* 1 = 3.25769 loss)
I0615 16:18:04.706768  5063 solver.cpp:473] Iteration 18120, lr = 0.0001
I0615 16:18:05.465533  5063 solver.cpp:213] Iteration 18130, loss = 2.98896
I0615 16:18:05.465546  5063 solver.cpp:228]     Train net output #0: softmax = 2.98896 (* 1 = 2.98896 loss)
I0615 16:18:05.465550  5063 solver.cpp:473] Iteration 18130, lr = 0.0001
I0615 16:18:06.224284  5063 solver.cpp:213] Iteration 18140, loss = 3.21712
I0615 16:18:06.224300  5063 solver.cpp:228]     Train net output #0: softmax = 3.21712 (* 1 = 3.21712 loss)
I0615 16:18:06.224304  5063 solver.cpp:473] Iteration 18140, lr = 0.0001
I0615 16:18:06.982908  5063 solver.cpp:213] Iteration 18150, loss = 3.43506
I0615 16:18:06.982923  5063 solver.cpp:228]     Train net output #0: softmax = 3.43506 (* 1 = 3.43506 loss)
I0615 16:18:06.982928  5063 solver.cpp:473] Iteration 18150, lr = 0.0001
I0615 16:18:07.741475  5063 solver.cpp:213] Iteration 18160, loss = 3.42637
I0615 16:18:07.741490  5063 solver.cpp:228]     Train net output #0: softmax = 3.42637 (* 1 = 3.42637 loss)
I0615 16:18:07.741494  5063 solver.cpp:473] Iteration 18160, lr = 0.0001
I0615 16:18:08.500308  5063 solver.cpp:213] Iteration 18170, loss = 3.29031
I0615 16:18:08.500326  5063 solver.cpp:228]     Train net output #0: softmax = 3.29031 (* 1 = 3.29031 loss)
I0615 16:18:08.500331  5063 solver.cpp:473] Iteration 18170, lr = 0.0001
I0615 16:18:09.259253  5063 solver.cpp:213] Iteration 18180, loss = 3.21194
I0615 16:18:09.259274  5063 solver.cpp:228]     Train net output #0: softmax = 3.21194 (* 1 = 3.21194 loss)
I0615 16:18:09.259279  5063 solver.cpp:473] Iteration 18180, lr = 0.0001
I0615 16:18:10.017843  5063 solver.cpp:213] Iteration 18190, loss = 3.13045
I0615 16:18:10.017863  5063 solver.cpp:228]     Train net output #0: softmax = 3.13045 (* 1 = 3.13045 loss)
I0615 16:18:10.017868  5063 solver.cpp:473] Iteration 18190, lr = 0.0001
I0615 16:18:10.777065  5063 solver.cpp:213] Iteration 18200, loss = 2.90894
I0615 16:18:10.777081  5063 solver.cpp:228]     Train net output #0: softmax = 2.90894 (* 1 = 2.90894 loss)
I0615 16:18:10.777084  5063 solver.cpp:473] Iteration 18200, lr = 0.0001
I0615 16:18:11.535548  5063 solver.cpp:213] Iteration 18210, loss = 3.37314
I0615 16:18:11.535567  5063 solver.cpp:228]     Train net output #0: softmax = 3.37314 (* 1 = 3.37314 loss)
I0615 16:18:11.535572  5063 solver.cpp:473] Iteration 18210, lr = 0.0001
I0615 16:18:12.294378  5063 solver.cpp:213] Iteration 18220, loss = 3.13781
I0615 16:18:12.294394  5063 solver.cpp:228]     Train net output #0: softmax = 3.13781 (* 1 = 3.13781 loss)
I0615 16:18:12.294406  5063 solver.cpp:473] Iteration 18220, lr = 0.0001
I0615 16:18:13.053462  5063 solver.cpp:213] Iteration 18230, loss = 3.1474
I0615 16:18:13.053475  5063 solver.cpp:228]     Train net output #0: softmax = 3.1474 (* 1 = 3.1474 loss)
I0615 16:18:13.053480  5063 solver.cpp:473] Iteration 18230, lr = 0.0001
I0615 16:18:13.813096  5063 solver.cpp:213] Iteration 18240, loss = 3.2231
I0615 16:18:13.813113  5063 solver.cpp:228]     Train net output #0: softmax = 3.2231 (* 1 = 3.2231 loss)
I0615 16:18:13.813240  5063 solver.cpp:473] Iteration 18240, lr = 0.0001
I0615 16:18:14.572288  5063 solver.cpp:213] Iteration 18250, loss = 2.96556
I0615 16:18:14.572304  5063 solver.cpp:228]     Train net output #0: softmax = 2.96556 (* 1 = 2.96556 loss)
I0615 16:18:14.572309  5063 solver.cpp:473] Iteration 18250, lr = 0.0001
I0615 16:18:15.331007  5063 solver.cpp:213] Iteration 18260, loss = 3.18956
I0615 16:18:15.331022  5063 solver.cpp:228]     Train net output #0: softmax = 3.18956 (* 1 = 3.18956 loss)
I0615 16:18:15.331025  5063 solver.cpp:473] Iteration 18260, lr = 0.0001
I0615 16:18:16.089200  5063 solver.cpp:213] Iteration 18270, loss = 3.30835
I0615 16:18:16.089220  5063 solver.cpp:228]     Train net output #0: softmax = 3.30835 (* 1 = 3.30835 loss)
I0615 16:18:16.089223  5063 solver.cpp:473] Iteration 18270, lr = 0.0001
I0615 16:18:16.847615  5063 solver.cpp:213] Iteration 18280, loss = 3.25506
I0615 16:18:16.847636  5063 solver.cpp:228]     Train net output #0: softmax = 3.25506 (* 1 = 3.25506 loss)
I0615 16:18:16.847640  5063 solver.cpp:473] Iteration 18280, lr = 0.0001
I0615 16:18:17.606381  5063 solver.cpp:213] Iteration 18290, loss = 3.16632
I0615 16:18:17.606395  5063 solver.cpp:228]     Train net output #0: softmax = 3.16632 (* 1 = 3.16632 loss)
I0615 16:18:17.606400  5063 solver.cpp:473] Iteration 18290, lr = 0.0001
I0615 16:18:18.365401  5063 solver.cpp:213] Iteration 18300, loss = 3.46246
I0615 16:18:18.365417  5063 solver.cpp:228]     Train net output #0: softmax = 3.46246 (* 1 = 3.46246 loss)
I0615 16:18:18.365420  5063 solver.cpp:473] Iteration 18300, lr = 0.0001
I0615 16:18:19.124439  5063 solver.cpp:213] Iteration 18310, loss = 3.26887
I0615 16:18:19.124457  5063 solver.cpp:228]     Train net output #0: softmax = 3.26887 (* 1 = 3.26887 loss)
I0615 16:18:19.124622  5063 solver.cpp:473] Iteration 18310, lr = 0.0001
I0615 16:18:19.882652  5063 solver.cpp:213] Iteration 18320, loss = 3.01076
I0615 16:18:19.882668  5063 solver.cpp:228]     Train net output #0: softmax = 3.01076 (* 1 = 3.01076 loss)
I0615 16:18:19.882671  5063 solver.cpp:473] Iteration 18320, lr = 0.0001
I0615 16:18:20.640820  5063 solver.cpp:213] Iteration 18330, loss = 3.22903
I0615 16:18:20.640839  5063 solver.cpp:228]     Train net output #0: softmax = 3.22903 (* 1 = 3.22903 loss)
I0615 16:18:20.640843  5063 solver.cpp:473] Iteration 18330, lr = 0.0001
I0615 16:18:21.399891  5063 solver.cpp:213] Iteration 18340, loss = 3.17837
I0615 16:18:21.399907  5063 solver.cpp:228]     Train net output #0: softmax = 3.17837 (* 1 = 3.17837 loss)
I0615 16:18:21.399912  5063 solver.cpp:473] Iteration 18340, lr = 0.0001
I0615 16:18:22.158466  5063 solver.cpp:213] Iteration 18350, loss = 3.32915
I0615 16:18:22.158484  5063 solver.cpp:228]     Train net output #0: softmax = 3.32915 (* 1 = 3.32915 loss)
I0615 16:18:22.158489  5063 solver.cpp:473] Iteration 18350, lr = 0.0001
I0615 16:18:22.917429  5063 solver.cpp:213] Iteration 18360, loss = 3.25953
I0615 16:18:22.917444  5063 solver.cpp:228]     Train net output #0: softmax = 3.25953 (* 1 = 3.25953 loss)
I0615 16:18:22.917448  5063 solver.cpp:473] Iteration 18360, lr = 0.0001
I0615 16:18:23.676153  5063 solver.cpp:213] Iteration 18370, loss = 3.14395
I0615 16:18:23.676177  5063 solver.cpp:228]     Train net output #0: softmax = 3.14395 (* 1 = 3.14395 loss)
I0615 16:18:23.676182  5063 solver.cpp:473] Iteration 18370, lr = 0.0001
I0615 16:18:24.434728  5063 solver.cpp:213] Iteration 18380, loss = 3.10683
I0615 16:18:24.434746  5063 solver.cpp:228]     Train net output #0: softmax = 3.10683 (* 1 = 3.10683 loss)
I0615 16:18:24.434756  5063 solver.cpp:473] Iteration 18380, lr = 0.0001
I0615 16:18:25.193339  5063 solver.cpp:213] Iteration 18390, loss = 3.77623
I0615 16:18:25.193356  5063 solver.cpp:228]     Train net output #0: softmax = 3.77623 (* 1 = 3.77623 loss)
I0615 16:18:25.193361  5063 solver.cpp:473] Iteration 18390, lr = 0.0001
I0615 16:18:25.951859  5063 solver.cpp:213] Iteration 18400, loss = 3.23109
I0615 16:18:25.951901  5063 solver.cpp:228]     Train net output #0: softmax = 3.23109 (* 1 = 3.23109 loss)
I0615 16:18:25.951907  5063 solver.cpp:473] Iteration 18400, lr = 0.0001
I0615 16:18:26.710435  5063 solver.cpp:213] Iteration 18410, loss = 3.37455
I0615 16:18:26.710450  5063 solver.cpp:228]     Train net output #0: softmax = 3.37455 (* 1 = 3.37455 loss)
I0615 16:18:26.710455  5063 solver.cpp:473] Iteration 18410, lr = 0.0001
I0615 16:18:27.469267  5063 solver.cpp:213] Iteration 18420, loss = 3.29179
I0615 16:18:27.469281  5063 solver.cpp:228]     Train net output #0: softmax = 3.29179 (* 1 = 3.29179 loss)
I0615 16:18:27.469285  5063 solver.cpp:473] Iteration 18420, lr = 0.0001
I0615 16:18:28.228075  5063 solver.cpp:213] Iteration 18430, loss = 3.261
I0615 16:18:28.228090  5063 solver.cpp:228]     Train net output #0: softmax = 3.261 (* 1 = 3.261 loss)
I0615 16:18:28.228094  5063 solver.cpp:473] Iteration 18430, lr = 0.0001
I0615 16:18:28.987203  5063 solver.cpp:213] Iteration 18440, loss = 3.30514
I0615 16:18:28.987218  5063 solver.cpp:228]     Train net output #0: softmax = 3.30514 (* 1 = 3.30514 loss)
I0615 16:18:28.987223  5063 solver.cpp:473] Iteration 18440, lr = 0.0001
I0615 16:18:29.746274  5063 solver.cpp:213] Iteration 18450, loss = 3.16312
I0615 16:18:29.746291  5063 solver.cpp:228]     Train net output #0: softmax = 3.16312 (* 1 = 3.16312 loss)
I0615 16:18:29.746460  5063 solver.cpp:473] Iteration 18450, lr = 0.0001
I0615 16:18:30.504931  5063 solver.cpp:213] Iteration 18460, loss = 3.33682
I0615 16:18:30.504952  5063 solver.cpp:228]     Train net output #0: softmax = 3.33682 (* 1 = 3.33682 loss)
I0615 16:18:30.504956  5063 solver.cpp:473] Iteration 18460, lr = 0.0001
I0615 16:18:31.263928  5063 solver.cpp:213] Iteration 18470, loss = 3.20966
I0615 16:18:31.263943  5063 solver.cpp:228]     Train net output #0: softmax = 3.20966 (* 1 = 3.20966 loss)
I0615 16:18:31.263947  5063 solver.cpp:473] Iteration 18470, lr = 0.0001
I0615 16:18:32.021951  5063 solver.cpp:213] Iteration 18480, loss = 3.14662
I0615 16:18:32.021966  5063 solver.cpp:228]     Train net output #0: softmax = 3.14662 (* 1 = 3.14662 loss)
I0615 16:18:32.021971  5063 solver.cpp:473] Iteration 18480, lr = 0.0001
I0615 16:18:32.779819  5063 solver.cpp:213] Iteration 18490, loss = 2.97585
I0615 16:18:32.779836  5063 solver.cpp:228]     Train net output #0: softmax = 2.97585 (* 1 = 2.97585 loss)
I0615 16:18:32.779840  5063 solver.cpp:473] Iteration 18490, lr = 0.0001
I0615 16:18:33.538123  5063 solver.cpp:213] Iteration 18500, loss = 3.33401
I0615 16:18:33.538137  5063 solver.cpp:228]     Train net output #0: softmax = 3.33401 (* 1 = 3.33401 loss)
I0615 16:18:33.538141  5063 solver.cpp:473] Iteration 18500, lr = 0.0001
I0615 16:18:34.296636  5063 solver.cpp:213] Iteration 18510, loss = 3.29671
I0615 16:18:34.296653  5063 solver.cpp:228]     Train net output #0: softmax = 3.29671 (* 1 = 3.29671 loss)
I0615 16:18:34.296658  5063 solver.cpp:473] Iteration 18510, lr = 0.0001
I0615 16:18:35.055168  5063 solver.cpp:213] Iteration 18520, loss = 3.3597
I0615 16:18:35.055186  5063 solver.cpp:228]     Train net output #0: softmax = 3.3597 (* 1 = 3.3597 loss)
I0615 16:18:35.055308  5063 solver.cpp:473] Iteration 18520, lr = 0.0001
I0615 16:18:35.813664  5063 solver.cpp:213] Iteration 18530, loss = 3.36243
I0615 16:18:35.813678  5063 solver.cpp:228]     Train net output #0: softmax = 3.36243 (* 1 = 3.36243 loss)
I0615 16:18:35.813683  5063 solver.cpp:473] Iteration 18530, lr = 0.0001
I0615 16:18:36.572222  5063 solver.cpp:213] Iteration 18540, loss = 3.20988
I0615 16:18:36.572239  5063 solver.cpp:228]     Train net output #0: softmax = 3.20988 (* 1 = 3.20988 loss)
I0615 16:18:36.572252  5063 solver.cpp:473] Iteration 18540, lr = 0.0001
I0615 16:18:37.330317  5063 solver.cpp:213] Iteration 18550, loss = 3.37541
I0615 16:18:37.330334  5063 solver.cpp:228]     Train net output #0: softmax = 3.37541 (* 1 = 3.37541 loss)
I0615 16:18:37.330339  5063 solver.cpp:473] Iteration 18550, lr = 0.0001
I0615 16:18:38.088856  5063 solver.cpp:213] Iteration 18560, loss = 3.15369
I0615 16:18:38.088888  5063 solver.cpp:228]     Train net output #0: softmax = 3.15369 (* 1 = 3.15369 loss)
I0615 16:18:38.088893  5063 solver.cpp:473] Iteration 18560, lr = 0.0001
I0615 16:18:38.847190  5063 solver.cpp:213] Iteration 18570, loss = 2.95262
I0615 16:18:38.847208  5063 solver.cpp:228]     Train net output #0: softmax = 2.95262 (* 1 = 2.95262 loss)
I0615 16:18:38.847213  5063 solver.cpp:473] Iteration 18570, lr = 0.0001
I0615 16:18:39.605789  5063 solver.cpp:213] Iteration 18580, loss = 3.12398
I0615 16:18:39.605803  5063 solver.cpp:228]     Train net output #0: softmax = 3.12398 (* 1 = 3.12398 loss)
I0615 16:18:39.605808  5063 solver.cpp:473] Iteration 18580, lr = 0.0001
I0615 16:18:40.363979  5063 solver.cpp:213] Iteration 18590, loss = 3.09768
I0615 16:18:40.363999  5063 solver.cpp:228]     Train net output #0: softmax = 3.09768 (* 1 = 3.09768 loss)
I0615 16:18:40.364135  5063 solver.cpp:473] Iteration 18590, lr = 0.0001
I0615 16:18:41.122905  5063 solver.cpp:213] Iteration 18600, loss = 3.29561
I0615 16:18:41.122925  5063 solver.cpp:228]     Train net output #0: softmax = 3.29561 (* 1 = 3.29561 loss)
I0615 16:18:41.122930  5063 solver.cpp:473] Iteration 18600, lr = 0.0001
I0615 16:18:41.881155  5063 solver.cpp:213] Iteration 18610, loss = 3.18619
I0615 16:18:41.881170  5063 solver.cpp:228]     Train net output #0: softmax = 3.18619 (* 1 = 3.18619 loss)
I0615 16:18:41.881175  5063 solver.cpp:473] Iteration 18610, lr = 0.0001
I0615 16:18:42.640346  5063 solver.cpp:213] Iteration 18620, loss = 3.31144
I0615 16:18:42.640360  5063 solver.cpp:228]     Train net output #0: softmax = 3.31144 (* 1 = 3.31144 loss)
I0615 16:18:42.640364  5063 solver.cpp:473] Iteration 18620, lr = 0.0001
I0615 16:18:43.398735  5063 solver.cpp:213] Iteration 18630, loss = 3.31013
I0615 16:18:43.398751  5063 solver.cpp:228]     Train net output #0: softmax = 3.31013 (* 1 = 3.31013 loss)
I0615 16:18:43.398756  5063 solver.cpp:473] Iteration 18630, lr = 0.0001
I0615 16:18:44.157922  5063 solver.cpp:213] Iteration 18640, loss = 3.07479
I0615 16:18:44.157937  5063 solver.cpp:228]     Train net output #0: softmax = 3.07479 (* 1 = 3.07479 loss)
I0615 16:18:44.157941  5063 solver.cpp:473] Iteration 18640, lr = 0.0001
I0615 16:18:44.916865  5063 solver.cpp:213] Iteration 18650, loss = 3.15601
I0615 16:18:44.916883  5063 solver.cpp:228]     Train net output #0: softmax = 3.15601 (* 1 = 3.15601 loss)
I0615 16:18:44.916888  5063 solver.cpp:473] Iteration 18650, lr = 0.0001
I0615 16:18:45.675882  5063 solver.cpp:213] Iteration 18660, loss = 3.2642
I0615 16:18:45.675901  5063 solver.cpp:228]     Train net output #0: softmax = 3.2642 (* 1 = 3.2642 loss)
I0615 16:18:45.676028  5063 solver.cpp:473] Iteration 18660, lr = 0.0001
I0615 16:18:46.434433  5063 solver.cpp:213] Iteration 18670, loss = 3.37582
I0615 16:18:46.434454  5063 solver.cpp:228]     Train net output #0: softmax = 3.37582 (* 1 = 3.37582 loss)
I0615 16:18:46.434459  5063 solver.cpp:473] Iteration 18670, lr = 0.0001
I0615 16:18:47.192706  5063 solver.cpp:213] Iteration 18680, loss = 3.2558
I0615 16:18:47.192723  5063 solver.cpp:228]     Train net output #0: softmax = 3.2558 (* 1 = 3.2558 loss)
I0615 16:18:47.192728  5063 solver.cpp:473] Iteration 18680, lr = 0.0001
I0615 16:18:47.951400  5063 solver.cpp:213] Iteration 18690, loss = 3.33246
I0615 16:18:47.951414  5063 solver.cpp:228]     Train net output #0: softmax = 3.33246 (* 1 = 3.33246 loss)
I0615 16:18:47.951418  5063 solver.cpp:473] Iteration 18690, lr = 0.0001
I0615 16:18:48.709280  5063 solver.cpp:213] Iteration 18700, loss = 3.50079
I0615 16:18:48.709296  5063 solver.cpp:228]     Train net output #0: softmax = 3.50079 (* 1 = 3.50079 loss)
I0615 16:18:48.709307  5063 solver.cpp:473] Iteration 18700, lr = 0.0001
I0615 16:18:49.468278  5063 solver.cpp:213] Iteration 18710, loss = 3.04906
I0615 16:18:49.468292  5063 solver.cpp:228]     Train net output #0: softmax = 3.04906 (* 1 = 3.04906 loss)
I0615 16:18:49.468297  5063 solver.cpp:473] Iteration 18710, lr = 0.0001
I0615 16:18:50.226928  5063 solver.cpp:213] Iteration 18720, loss = 3.17663
I0615 16:18:50.226956  5063 solver.cpp:228]     Train net output #0: softmax = 3.17663 (* 1 = 3.17663 loss)
I0615 16:18:50.226961  5063 solver.cpp:473] Iteration 18720, lr = 0.0001
I0615 16:18:50.985674  5063 solver.cpp:213] Iteration 18730, loss = 3.22955
I0615 16:18:50.985692  5063 solver.cpp:228]     Train net output #0: softmax = 3.22955 (* 1 = 3.22955 loss)
I0615 16:18:50.985860  5063 solver.cpp:473] Iteration 18730, lr = 0.0001
I0615 16:18:51.744029  5063 solver.cpp:213] Iteration 18740, loss = 3.2833
I0615 16:18:51.744055  5063 solver.cpp:228]     Train net output #0: softmax = 3.2833 (* 1 = 3.2833 loss)
I0615 16:18:51.744060  5063 solver.cpp:473] Iteration 18740, lr = 0.0001
I0615 16:18:52.502106  5063 solver.cpp:213] Iteration 18750, loss = 3.39818
I0615 16:18:52.502120  5063 solver.cpp:228]     Train net output #0: softmax = 3.39818 (* 1 = 3.39818 loss)
I0615 16:18:52.502125  5063 solver.cpp:473] Iteration 18750, lr = 0.0001
I0615 16:18:53.261401  5063 solver.cpp:213] Iteration 18760, loss = 3.25453
I0615 16:18:53.261415  5063 solver.cpp:228]     Train net output #0: softmax = 3.25453 (* 1 = 3.25453 loss)
I0615 16:18:53.261420  5063 solver.cpp:473] Iteration 18760, lr = 0.0001
I0615 16:18:54.020220  5063 solver.cpp:213] Iteration 18770, loss = 3.46059
I0615 16:18:54.020234  5063 solver.cpp:228]     Train net output #0: softmax = 3.46059 (* 1 = 3.46059 loss)
I0615 16:18:54.020238  5063 solver.cpp:473] Iteration 18770, lr = 0.0001
I0615 16:18:54.777045  5063 solver.cpp:213] Iteration 18780, loss = 3.32842
I0615 16:18:54.777060  5063 solver.cpp:228]     Train net output #0: softmax = 3.32842 (* 1 = 3.32842 loss)
I0615 16:18:54.777065  5063 solver.cpp:473] Iteration 18780, lr = 0.0001
I0615 16:18:55.536080  5063 solver.cpp:213] Iteration 18790, loss = 3.22484
I0615 16:18:55.536098  5063 solver.cpp:228]     Train net output #0: softmax = 3.22484 (* 1 = 3.22484 loss)
I0615 16:18:55.536101  5063 solver.cpp:473] Iteration 18790, lr = 0.0001
I0615 16:18:56.294534  5063 solver.cpp:213] Iteration 18800, loss = 3.39274
I0615 16:18:56.294701  5063 solver.cpp:228]     Train net output #0: softmax = 3.39274 (* 1 = 3.39274 loss)
I0615 16:18:56.294709  5063 solver.cpp:473] Iteration 18800, lr = 0.0001
I0615 16:18:57.053109  5063 solver.cpp:213] Iteration 18810, loss = 3.34968
I0615 16:18:57.053124  5063 solver.cpp:228]     Train net output #0: softmax = 3.34968 (* 1 = 3.34968 loss)
I0615 16:18:57.053129  5063 solver.cpp:473] Iteration 18810, lr = 0.0001
I0615 16:18:57.811905  5063 solver.cpp:213] Iteration 18820, loss = 3.38999
I0615 16:18:57.811920  5063 solver.cpp:228]     Train net output #0: softmax = 3.38999 (* 1 = 3.38999 loss)
I0615 16:18:57.811924  5063 solver.cpp:473] Iteration 18820, lr = 0.0001
I0615 16:18:58.571039  5063 solver.cpp:213] Iteration 18830, loss = 3.3216
I0615 16:18:58.571054  5063 solver.cpp:228]     Train net output #0: softmax = 3.3216 (* 1 = 3.3216 loss)
I0615 16:18:58.571058  5063 solver.cpp:473] Iteration 18830, lr = 0.0001
I0615 16:18:59.330091  5063 solver.cpp:213] Iteration 18840, loss = 3.19952
I0615 16:18:59.330109  5063 solver.cpp:228]     Train net output #0: softmax = 3.19952 (* 1 = 3.19952 loss)
I0615 16:18:59.330114  5063 solver.cpp:473] Iteration 18840, lr = 0.0001
I0615 16:19:00.088851  5063 solver.cpp:213] Iteration 18850, loss = 3.12792
I0615 16:19:00.088873  5063 solver.cpp:228]     Train net output #0: softmax = 3.12792 (* 1 = 3.12792 loss)
I0615 16:19:00.088878  5063 solver.cpp:473] Iteration 18850, lr = 0.0001
I0615 16:19:00.846772  5063 solver.cpp:213] Iteration 18860, loss = 3.35462
I0615 16:19:00.846793  5063 solver.cpp:228]     Train net output #0: softmax = 3.35462 (* 1 = 3.35462 loss)
I0615 16:19:00.846804  5063 solver.cpp:473] Iteration 18860, lr = 0.0001
I0615 16:19:01.605614  5063 solver.cpp:213] Iteration 18870, loss = 3.26516
I0615 16:19:01.605630  5063 solver.cpp:228]     Train net output #0: softmax = 3.26516 (* 1 = 3.26516 loss)
I0615 16:19:01.605635  5063 solver.cpp:473] Iteration 18870, lr = 0.0001
I0615 16:19:02.364274  5063 solver.cpp:213] Iteration 18880, loss = 3.22226
I0615 16:19:02.364289  5063 solver.cpp:228]     Train net output #0: softmax = 3.22226 (* 1 = 3.22226 loss)
I0615 16:19:02.364295  5063 solver.cpp:473] Iteration 18880, lr = 0.0001
I0615 16:19:03.123282  5063 solver.cpp:213] Iteration 18890, loss = 3.58916
I0615 16:19:03.123296  5063 solver.cpp:228]     Train net output #0: softmax = 3.58916 (* 1 = 3.58916 loss)
I0615 16:19:03.123301  5063 solver.cpp:473] Iteration 18890, lr = 0.0001
I0615 16:19:03.882274  5063 solver.cpp:213] Iteration 18900, loss = 3.20132
I0615 16:19:03.882289  5063 solver.cpp:228]     Train net output #0: softmax = 3.20132 (* 1 = 3.20132 loss)
I0615 16:19:03.882293  5063 solver.cpp:473] Iteration 18900, lr = 0.0001
I0615 16:19:04.639505  5063 solver.cpp:213] Iteration 18910, loss = 3.44138
I0615 16:19:04.639528  5063 solver.cpp:228]     Train net output #0: softmax = 3.44138 (* 1 = 3.44138 loss)
I0615 16:19:04.639534  5063 solver.cpp:473] Iteration 18910, lr = 0.0001
I0615 16:19:05.398582  5063 solver.cpp:213] Iteration 18920, loss = 3.36678
I0615 16:19:05.398602  5063 solver.cpp:228]     Train net output #0: softmax = 3.36678 (* 1 = 3.36678 loss)
I0615 16:19:05.398607  5063 solver.cpp:473] Iteration 18920, lr = 0.0001
I0615 16:19:06.157443  5063 solver.cpp:213] Iteration 18930, loss = 3.54746
I0615 16:19:06.157456  5063 solver.cpp:228]     Train net output #0: softmax = 3.54746 (* 1 = 3.54746 loss)
I0615 16:19:06.157461  5063 solver.cpp:473] Iteration 18930, lr = 0.0001
I0615 16:19:06.916043  5063 solver.cpp:213] Iteration 18940, loss = 3.42894
I0615 16:19:06.916062  5063 solver.cpp:228]     Train net output #0: softmax = 3.42894 (* 1 = 3.42894 loss)
I0615 16:19:06.916189  5063 solver.cpp:473] Iteration 18940, lr = 0.0001
I0615 16:19:07.675366  5063 solver.cpp:213] Iteration 18950, loss = 3.31891
I0615 16:19:07.675380  5063 solver.cpp:228]     Train net output #0: softmax = 3.31891 (* 1 = 3.31891 loss)
I0615 16:19:07.675384  5063 solver.cpp:473] Iteration 18950, lr = 0.0001
I0615 16:19:08.433585  5063 solver.cpp:213] Iteration 18960, loss = 2.88875
I0615 16:19:08.433617  5063 solver.cpp:228]     Train net output #0: softmax = 2.88875 (* 1 = 2.88875 loss)
I0615 16:19:08.433622  5063 solver.cpp:473] Iteration 18960, lr = 0.0001
I0615 16:19:09.192342  5063 solver.cpp:213] Iteration 18970, loss = 3.13901
I0615 16:19:09.192355  5063 solver.cpp:228]     Train net output #0: softmax = 3.13901 (* 1 = 3.13901 loss)
I0615 16:19:09.192360  5063 solver.cpp:473] Iteration 18970, lr = 0.0001
I0615 16:19:09.950631  5063 solver.cpp:213] Iteration 18980, loss = 3.40372
I0615 16:19:09.950646  5063 solver.cpp:228]     Train net output #0: softmax = 3.40372 (* 1 = 3.40372 loss)
I0615 16:19:09.950650  5063 solver.cpp:473] Iteration 18980, lr = 0.0001
I0615 16:19:10.708663  5063 solver.cpp:213] Iteration 18990, loss = 3.26469
I0615 16:19:10.708678  5063 solver.cpp:228]     Train net output #0: softmax = 3.26469 (* 1 = 3.26469 loss)
I0615 16:19:10.708683  5063 solver.cpp:473] Iteration 18990, lr = 0.0001
I0615 16:19:11.414067  5063 solver.cpp:362] Snapshotting to snapshots/16-06-15_15h49m18s_0_11_pretrainClassification_iter_19000.caffemodel
I0615 16:19:11.414772  5063 solver.cpp:370] Snapshotting solver state to snapshots/16-06-15_15h49m18s_0_11_pretrainClassification_iter_19000.solverstate
I0615 16:19:11.415145  5063 solver.cpp:291] Iteration 19000, Testing net (#0)
I0615 16:19:11.509587  5063 solver.cpp:342]     Test net output #0: accuracy_top_1 = 0.225
I0615 16:19:11.509600  5063 solver.cpp:342]     Test net output #1: accuracy_top_5 = 0.540625
I0615 16:19:11.509606  5063 solver.cpp:342]     Test net output #2: softmax = 3.16633 (* 1 = 3.16633 loss)
I0615 16:19:11.563287  5063 solver.cpp:213] Iteration 19000, loss = 3.13024
I0615 16:19:11.563300  5063 solver.cpp:228]     Train net output #0: softmax = 3.13024 (* 1 = 3.13024 loss)
I0615 16:19:11.563304  5063 solver.cpp:473] Iteration 19000, lr = 0.0001
I0615 16:19:12.321831  5063 solver.cpp:213] Iteration 19010, loss = 3.0753
I0615 16:19:12.321853  5063 solver.cpp:228]     Train net output #0: softmax = 3.0753 (* 1 = 3.0753 loss)
I0615 16:19:12.321863  5063 solver.cpp:473] Iteration 19010, lr = 0.0001
I0615 16:19:13.080814  5063 solver.cpp:213] Iteration 19020, loss = 3.13118
I0615 16:19:13.080829  5063 solver.cpp:228]     Train net output #0: softmax = 3.13118 (* 1 = 3.13118 loss)
I0615 16:19:13.080834  5063 solver.cpp:473] Iteration 19020, lr = 0.0001
I0615 16:19:13.839362  5063 solver.cpp:213] Iteration 19030, loss = 3.19025
I0615 16:19:13.839385  5063 solver.cpp:228]     Train net output #0: softmax = 3.19025 (* 1 = 3.19025 loss)
I0615 16:19:13.839390  5063 solver.cpp:473] Iteration 19030, lr = 0.0001
I0615 16:19:14.598450  5063 solver.cpp:213] Iteration 19040, loss = 3.28566
I0615 16:19:14.598465  5063 solver.cpp:228]     Train net output #0: softmax = 3.28566 (* 1 = 3.28566 loss)
I0615 16:19:14.598470  5063 solver.cpp:473] Iteration 19040, lr = 0.0001
I0615 16:19:15.356439  5063 solver.cpp:213] Iteration 19050, loss = 3.25335
I0615 16:19:15.356457  5063 solver.cpp:228]     Train net output #0: softmax = 3.25335 (* 1 = 3.25335 loss)
I0615 16:19:15.356462  5063 solver.cpp:473] Iteration 19050, lr = 0.0001
I0615 16:19:16.115038  5063 solver.cpp:213] Iteration 19060, loss = 3.16094
I0615 16:19:16.115056  5063 solver.cpp:228]     Train net output #0: softmax = 3.16094 (* 1 = 3.16094 loss)
I0615 16:19:16.115059  5063 solver.cpp:473] Iteration 19060, lr = 0.0001
I0615 16:19:16.873623  5063 solver.cpp:213] Iteration 19070, loss = 3.38022
I0615 16:19:16.873639  5063 solver.cpp:228]     Train net output #0: softmax = 3.38022 (* 1 = 3.38022 loss)
I0615 16:19:16.873643  5063 solver.cpp:473] Iteration 19070, lr = 0.0001
I0615 16:19:17.632330  5063 solver.cpp:213] Iteration 19080, loss = 3.26803
I0615 16:19:17.632349  5063 solver.cpp:228]     Train net output #0: softmax = 3.26803 (* 1 = 3.26803 loss)
I0615 16:19:17.632352  5063 solver.cpp:473] Iteration 19080, lr = 0.0001
I0615 16:19:18.391433  5063 solver.cpp:213] Iteration 19090, loss = 3.60146
I0615 16:19:18.391454  5063 solver.cpp:228]     Train net output #0: softmax = 3.60146 (* 1 = 3.60146 loss)
I0615 16:19:18.391476  5063 solver.cpp:473] Iteration 19090, lr = 0.0001
I0615 16:19:19.149986  5063 solver.cpp:213] Iteration 19100, loss = 3.06592
I0615 16:19:19.150014  5063 solver.cpp:228]     Train net output #0: softmax = 3.06592 (* 1 = 3.06592 loss)
I0615 16:19:19.150019  5063 solver.cpp:473] Iteration 19100, lr = 0.0001
I0615 16:19:19.908752  5063 solver.cpp:213] Iteration 19110, loss = 3.15258
I0615 16:19:19.908767  5063 solver.cpp:228]     Train net output #0: softmax = 3.15258 (* 1 = 3.15258 loss)
I0615 16:19:19.908771  5063 solver.cpp:473] Iteration 19110, lr = 0.0001
I0615 16:19:20.667057  5063 solver.cpp:213] Iteration 19120, loss = 3.31711
I0615 16:19:20.667073  5063 solver.cpp:228]     Train net output #0: softmax = 3.31711 (* 1 = 3.31711 loss)
I0615 16:19:20.667078  5063 solver.cpp:473] Iteration 19120, lr = 0.0001
I0615 16:19:21.426183  5063 solver.cpp:213] Iteration 19130, loss = 3.3432
I0615 16:19:21.426198  5063 solver.cpp:228]     Train net output #0: softmax = 3.3432 (* 1 = 3.3432 loss)
I0615 16:19:21.426201  5063 solver.cpp:473] Iteration 19130, lr = 0.0001
I0615 16:19:22.184911  5063 solver.cpp:213] Iteration 19140, loss = 3.17368
I0615 16:19:22.184926  5063 solver.cpp:228]     Train net output #0: softmax = 3.17368 (* 1 = 3.17368 loss)
I0615 16:19:22.184929  5063 solver.cpp:473] Iteration 19140, lr = 0.0001
I0615 16:19:22.943586  5063 solver.cpp:213] Iteration 19150, loss = 3.21626
I0615 16:19:22.943605  5063 solver.cpp:228]     Train net output #0: softmax = 3.21626 (* 1 = 3.21626 loss)
I0615 16:19:22.943740  5063 solver.cpp:473] Iteration 19150, lr = 0.0001
I0615 16:19:23.702011  5063 solver.cpp:213] Iteration 19160, loss = 3.28852
I0615 16:19:23.702033  5063 solver.cpp:228]     Train net output #0: softmax = 3.28852 (* 1 = 3.28852 loss)
I0615 16:19:23.702038  5063 solver.cpp:473] Iteration 19160, lr = 0.0001
I0615 16:19:24.460176  5063 solver.cpp:213] Iteration 19170, loss = 3.13902
I0615 16:19:24.460194  5063 solver.cpp:228]     Train net output #0: softmax = 3.13902 (* 1 = 3.13902 loss)
I0615 16:19:24.460199  5063 solver.cpp:473] Iteration 19170, lr = 0.0001
I0615 16:19:25.219239  5063 solver.cpp:213] Iteration 19180, loss = 3.27806
I0615 16:19:25.219256  5063 solver.cpp:228]     Train net output #0: softmax = 3.27806 (* 1 = 3.27806 loss)
I0615 16:19:25.219260  5063 solver.cpp:473] Iteration 19180, lr = 0.0001
I0615 16:19:25.978142  5063 solver.cpp:213] Iteration 19190, loss = 3.56868
I0615 16:19:25.978157  5063 solver.cpp:228]     Train net output #0: softmax = 3.56868 (* 1 = 3.56868 loss)
I0615 16:19:25.978163  5063 solver.cpp:473] Iteration 19190, lr = 0.0001
I0615 16:19:26.736155  5063 solver.cpp:213] Iteration 19200, loss = 3.39222
I0615 16:19:26.736197  5063 solver.cpp:228]     Train net output #0: softmax = 3.39222 (* 1 = 3.39222 loss)
I0615 16:19:26.736203  5063 solver.cpp:473] Iteration 19200, lr = 0.0001
I0615 16:19:27.494091  5063 solver.cpp:213] Iteration 19210, loss = 3.25631
I0615 16:19:27.494109  5063 solver.cpp:228]     Train net output #0: softmax = 3.25631 (* 1 = 3.25631 loss)
I0615 16:19:27.494114  5063 solver.cpp:473] Iteration 19210, lr = 0.0001
I0615 16:19:28.253140  5063 solver.cpp:213] Iteration 19220, loss = 3.32849
I0615 16:19:28.253160  5063 solver.cpp:228]     Train net output #0: softmax = 3.32849 (* 1 = 3.32849 loss)
I0615 16:19:28.253320  5063 solver.cpp:473] Iteration 19220, lr = 0.0001
I0615 16:19:29.006794  5063 solver.cpp:213] Iteration 19230, loss = 3.0592
I0615 16:19:29.006814  5063 solver.cpp:228]     Train net output #0: softmax = 3.0592 (* 1 = 3.0592 loss)
I0615 16:19:29.006817  5063 solver.cpp:473] Iteration 19230, lr = 0.0001
I0615 16:19:29.764914  5063 solver.cpp:213] Iteration 19240, loss = 2.90072
I0615 16:19:29.764933  5063 solver.cpp:228]     Train net output #0: softmax = 2.90072 (* 1 = 2.90072 loss)
I0615 16:19:29.764938  5063 solver.cpp:473] Iteration 19240, lr = 0.0001
I0615 16:19:30.523804  5063 solver.cpp:213] Iteration 19250, loss = 3.23786
I0615 16:19:30.523820  5063 solver.cpp:228]     Train net output #0: softmax = 3.23786 (* 1 = 3.23786 loss)
I0615 16:19:30.523825  5063 solver.cpp:473] Iteration 19250, lr = 0.0001
I0615 16:19:31.282896  5063 solver.cpp:213] Iteration 19260, loss = 3.21141
I0615 16:19:31.282912  5063 solver.cpp:228]     Train net output #0: softmax = 3.21141 (* 1 = 3.21141 loss)
I0615 16:19:31.282917  5063 solver.cpp:473] Iteration 19260, lr = 0.0001
I0615 16:19:32.041271  5063 solver.cpp:213] Iteration 19270, loss = 3.24434
I0615 16:19:32.041286  5063 solver.cpp:228]     Train net output #0: softmax = 3.24434 (* 1 = 3.24434 loss)
I0615 16:19:32.041291  5063 solver.cpp:473] Iteration 19270, lr = 0.0001
I0615 16:19:32.798876  5063 solver.cpp:213] Iteration 19280, loss = 3.20867
I0615 16:19:32.798895  5063 solver.cpp:228]     Train net output #0: softmax = 3.20867 (* 1 = 3.20867 loss)
I0615 16:19:32.798900  5063 solver.cpp:473] Iteration 19280, lr = 0.0001
I0615 16:19:33.557606  5063 solver.cpp:213] Iteration 19290, loss = 3.15002
I0615 16:19:33.557624  5063 solver.cpp:228]     Train net output #0: softmax = 3.15002 (* 1 = 3.15002 loss)
I0615 16:19:33.557629  5063 solver.cpp:473] Iteration 19290, lr = 0.0001
I0615 16:19:34.316238  5063 solver.cpp:213] Iteration 19300, loss = 3.47867
I0615 16:19:34.316253  5063 solver.cpp:228]     Train net output #0: softmax = 3.47867 (* 1 = 3.47867 loss)
I0615 16:19:34.316258  5063 solver.cpp:473] Iteration 19300, lr = 0.0001
I0615 16:19:35.074587  5063 solver.cpp:213] Iteration 19310, loss = 3.1404
I0615 16:19:35.074609  5063 solver.cpp:228]     Train net output #0: softmax = 3.1404 (* 1 = 3.1404 loss)
I0615 16:19:35.074614  5063 solver.cpp:473] Iteration 19310, lr = 0.0001
I0615 16:19:35.832823  5063 solver.cpp:213] Iteration 19320, loss = 3.2982
I0615 16:19:35.832839  5063 solver.cpp:228]     Train net output #0: softmax = 3.2982 (* 1 = 3.2982 loss)
I0615 16:19:35.832844  5063 solver.cpp:473] Iteration 19320, lr = 0.0001
I0615 16:19:36.591065  5063 solver.cpp:213] Iteration 19330, loss = 3.426
I0615 16:19:36.591080  5063 solver.cpp:228]     Train net output #0: softmax = 3.426 (* 1 = 3.426 loss)
I0615 16:19:36.591084  5063 solver.cpp:473] Iteration 19330, lr = 0.0001
I0615 16:19:37.349151  5063 solver.cpp:213] Iteration 19340, loss = 3.28089
I0615 16:19:37.349169  5063 solver.cpp:228]     Train net output #0: softmax = 3.28089 (* 1 = 3.28089 loss)
I0615 16:19:37.349174  5063 solver.cpp:473] Iteration 19340, lr = 0.0001
I0615 16:19:38.108050  5063 solver.cpp:213] Iteration 19350, loss = 3.02112
I0615 16:19:38.108079  5063 solver.cpp:228]     Train net output #0: softmax = 3.02112 (* 1 = 3.02112 loss)
I0615 16:19:38.108084  5063 solver.cpp:473] Iteration 19350, lr = 0.0001
I0615 16:19:38.866477  5063 solver.cpp:213] Iteration 19360, loss = 3.32966
I0615 16:19:38.866647  5063 solver.cpp:228]     Train net output #0: softmax = 3.32966 (* 1 = 3.32966 loss)
I0615 16:19:38.866653  5063 solver.cpp:473] Iteration 19360, lr = 0.0001
I0615 16:19:39.625072  5063 solver.cpp:213] Iteration 19370, loss = 3.21142
I0615 16:19:39.625093  5063 solver.cpp:228]     Train net output #0: softmax = 3.21142 (* 1 = 3.21142 loss)
I0615 16:19:39.625098  5063 solver.cpp:473] Iteration 19370, lr = 0.0001
I0615 16:19:40.382963  5063 solver.cpp:213] Iteration 19380, loss = 3.38507
I0615 16:19:40.382983  5063 solver.cpp:228]     Train net output #0: softmax = 3.38507 (* 1 = 3.38507 loss)
I0615 16:19:40.382987  5063 solver.cpp:473] Iteration 19380, lr = 0.0001
I0615 16:19:41.141516  5063 solver.cpp:213] Iteration 19390, loss = 3.40339
I0615 16:19:41.141533  5063 solver.cpp:228]     Train net output #0: softmax = 3.40339 (* 1 = 3.40339 loss)
I0615 16:19:41.141537  5063 solver.cpp:473] Iteration 19390, lr = 0.0001
I0615 16:19:41.900364  5063 solver.cpp:213] Iteration 19400, loss = 2.94194
I0615 16:19:41.900380  5063 solver.cpp:228]     Train net output #0: softmax = 2.94194 (* 1 = 2.94194 loss)
I0615 16:19:41.900384  5063 solver.cpp:473] Iteration 19400, lr = 0.0001
I0615 16:19:42.658772  5063 solver.cpp:213] Iteration 19410, loss = 3.09368
I0615 16:19:42.658787  5063 solver.cpp:228]     Train net output #0: softmax = 3.09368 (* 1 = 3.09368 loss)
I0615 16:19:42.658792  5063 solver.cpp:473] Iteration 19410, lr = 0.0001
I0615 16:19:43.416949  5063 solver.cpp:213] Iteration 19420, loss = 3.1853
I0615 16:19:43.416966  5063 solver.cpp:228]     Train net output #0: softmax = 3.1853 (* 1 = 3.1853 loss)
I0615 16:19:43.416970  5063 solver.cpp:473] Iteration 19420, lr = 0.0001
I0615 16:19:44.175367  5063 solver.cpp:213] Iteration 19430, loss = 3.05792
I0615 16:19:44.175387  5063 solver.cpp:228]     Train net output #0: softmax = 3.05792 (* 1 = 3.05792 loss)
I0615 16:19:44.175523  5063 solver.cpp:473] Iteration 19430, lr = 0.0001
I0615 16:19:44.932226  5063 solver.cpp:213] Iteration 19440, loss = 3.33576
I0615 16:19:44.932250  5063 solver.cpp:228]     Train net output #0: softmax = 3.33576 (* 1 = 3.33576 loss)
I0615 16:19:44.932255  5063 solver.cpp:473] Iteration 19440, lr = 0.0001
I0615 16:19:45.691036  5063 solver.cpp:213] Iteration 19450, loss = 3.27593
I0615 16:19:45.691052  5063 solver.cpp:228]     Train net output #0: softmax = 3.27593 (* 1 = 3.27593 loss)
I0615 16:19:45.691057  5063 solver.cpp:473] Iteration 19450, lr = 0.0001
I0615 16:19:46.449687  5063 solver.cpp:213] Iteration 19460, loss = 3.19295
I0615 16:19:46.449703  5063 solver.cpp:228]     Train net output #0: softmax = 3.19295 (* 1 = 3.19295 loss)
I0615 16:19:46.449707  5063 solver.cpp:473] Iteration 19460, lr = 0.0001
I0615 16:19:47.207984  5063 solver.cpp:213] Iteration 19470, loss = 2.9868
I0615 16:19:47.208000  5063 solver.cpp:228]     Train net output #0: softmax = 2.9868 (* 1 = 2.9868 loss)
I0615 16:19:47.208004  5063 solver.cpp:473] Iteration 19470, lr = 0.0001
I0615 16:19:47.966717  5063 solver.cpp:213] Iteration 19480, loss = 3.31916
I0615 16:19:47.966734  5063 solver.cpp:228]     Train net output #0: softmax = 3.31916 (* 1 = 3.31916 loss)
I0615 16:19:47.966738  5063 solver.cpp:473] Iteration 19480, lr = 0.0001
I0615 16:19:48.724146  5063 solver.cpp:213] Iteration 19490, loss = 3.094
I0615 16:19:48.724164  5063 solver.cpp:228]     Train net output #0: softmax = 3.094 (* 1 = 3.094 loss)
I0615 16:19:48.724167  5063 solver.cpp:473] Iteration 19490, lr = 0.0001
I0615 16:19:49.482632  5063 solver.cpp:213] Iteration 19500, loss = 3.07067
I0615 16:19:49.482648  5063 solver.cpp:228]     Train net output #0: softmax = 3.07067 (* 1 = 3.07067 loss)
I0615 16:19:49.482652  5063 solver.cpp:473] Iteration 19500, lr = 0.0001
I0615 16:19:50.240993  5063 solver.cpp:213] Iteration 19510, loss = 3.23502
I0615 16:19:50.241008  5063 solver.cpp:228]     Train net output #0: softmax = 3.23502 (* 1 = 3.23502 loss)
I0615 16:19:50.241013  5063 solver.cpp:473] Iteration 19510, lr = 0.0001
I0615 16:19:50.999634  5063 solver.cpp:213] Iteration 19520, loss = 3.32303
I0615 16:19:50.999665  5063 solver.cpp:228]     Train net output #0: softmax = 3.32303 (* 1 = 3.32303 loss)
I0615 16:19:50.999668  5063 solver.cpp:473] Iteration 19520, lr = 0.0001
I0615 16:19:51.758363  5063 solver.cpp:213] Iteration 19530, loss = 3.03503
I0615 16:19:51.758379  5063 solver.cpp:228]     Train net output #0: softmax = 3.03503 (* 1 = 3.03503 loss)
I0615 16:19:51.758384  5063 solver.cpp:473] Iteration 19530, lr = 0.0001
I0615 16:19:52.516832  5063 solver.cpp:213] Iteration 19540, loss = 3.10529
I0615 16:19:52.516850  5063 solver.cpp:228]     Train net output #0: softmax = 3.10529 (* 1 = 3.10529 loss)
I0615 16:19:52.516855  5063 solver.cpp:473] Iteration 19540, lr = 0.0001
I0615 16:19:53.275313  5063 solver.cpp:213] Iteration 19550, loss = 3.32309
I0615 16:19:53.275329  5063 solver.cpp:228]     Train net output #0: softmax = 3.32309 (* 1 = 3.32309 loss)
I0615 16:19:53.275333  5063 solver.cpp:473] Iteration 19550, lr = 0.0001
I0615 16:19:54.033959  5063 solver.cpp:213] Iteration 19560, loss = 3.12452
I0615 16:19:54.033974  5063 solver.cpp:228]     Train net output #0: softmax = 3.12452 (* 1 = 3.12452 loss)
I0615 16:19:54.033978  5063 solver.cpp:473] Iteration 19560, lr = 0.0001
I0615 16:19:54.792284  5063 solver.cpp:213] Iteration 19570, loss = 3.14649
I0615 16:19:54.792305  5063 solver.cpp:228]     Train net output #0: softmax = 3.14649 (* 1 = 3.14649 loss)
I0615 16:19:54.792438  5063 solver.cpp:473] Iteration 19570, lr = 0.0001
I0615 16:19:55.550897  5063 solver.cpp:213] Iteration 19580, loss = 3.26183
I0615 16:19:55.550915  5063 solver.cpp:228]     Train net output #0: softmax = 3.26183 (* 1 = 3.26183 loss)
I0615 16:19:55.550920  5063 solver.cpp:473] Iteration 19580, lr = 0.0001
I0615 16:19:56.309479  5063 solver.cpp:213] Iteration 19590, loss = 3.31158
I0615 16:19:56.309495  5063 solver.cpp:228]     Train net output #0: softmax = 3.31158 (* 1 = 3.31158 loss)
I0615 16:19:56.309500  5063 solver.cpp:473] Iteration 19590, lr = 0.0001
I0615 16:19:57.065415  5063 solver.cpp:213] Iteration 19600, loss = 3.08107
I0615 16:19:57.065459  5063 solver.cpp:228]     Train net output #0: softmax = 3.08107 (* 1 = 3.08107 loss)
I0615 16:19:57.065464  5063 solver.cpp:473] Iteration 19600, lr = 0.0001
I0615 16:19:57.822865  5063 solver.cpp:213] Iteration 19610, loss = 3.20267
I0615 16:19:57.822885  5063 solver.cpp:228]     Train net output #0: softmax = 3.20267 (* 1 = 3.20267 loss)
I0615 16:19:57.822890  5063 solver.cpp:473] Iteration 19610, lr = 0.0001
I0615 16:19:58.580881  5063 solver.cpp:213] Iteration 19620, loss = 3.12088
I0615 16:19:58.580895  5063 solver.cpp:228]     Train net output #0: softmax = 3.12088 (* 1 = 3.12088 loss)
I0615 16:19:58.580900  5063 solver.cpp:473] Iteration 19620, lr = 0.0001
I0615 16:19:59.339962  5063 solver.cpp:213] Iteration 19630, loss = 3.24848
I0615 16:19:59.339983  5063 solver.cpp:228]     Train net output #0: softmax = 3.24848 (* 1 = 3.24848 loss)
I0615 16:19:59.339987  5063 solver.cpp:473] Iteration 19630, lr = 0.0001
I0615 16:20:00.098850  5063 solver.cpp:213] Iteration 19640, loss = 3.01105
I0615 16:20:00.098870  5063 solver.cpp:228]     Train net output #0: softmax = 3.01105 (* 1 = 3.01105 loss)
I0615 16:20:00.099005  5063 solver.cpp:473] Iteration 19640, lr = 0.0001
I0615 16:20:00.857887  5063 solver.cpp:213] Iteration 19650, loss = 3.48375
I0615 16:20:00.857903  5063 solver.cpp:228]     Train net output #0: softmax = 3.48375 (* 1 = 3.48375 loss)
I0615 16:20:00.857908  5063 solver.cpp:473] Iteration 19650, lr = 0.0001
I0615 16:20:01.616286  5063 solver.cpp:213] Iteration 19660, loss = 3.25862
I0615 16:20:01.616300  5063 solver.cpp:228]     Train net output #0: softmax = 3.25862 (* 1 = 3.25862 loss)
I0615 16:20:01.616304  5063 solver.cpp:473] Iteration 19660, lr = 0.0001
I0615 16:20:02.375573  5063 solver.cpp:213] Iteration 19670, loss = 3.06991
I0615 16:20:02.375596  5063 solver.cpp:228]     Train net output #0: softmax = 3.06991 (* 1 = 3.06991 loss)
I0615 16:20:02.375600  5063 solver.cpp:473] Iteration 19670, lr = 0.0001
I0615 16:20:03.133927  5063 solver.cpp:213] Iteration 19680, loss = 3.25336
I0615 16:20:03.133944  5063 solver.cpp:228]     Train net output #0: softmax = 3.25336 (* 1 = 3.25336 loss)
I0615 16:20:03.133949  5063 solver.cpp:473] Iteration 19680, lr = 0.0001
I0615 16:20:03.892515  5063 solver.cpp:213] Iteration 19690, loss = 3.28761
I0615 16:20:03.892530  5063 solver.cpp:228]     Train net output #0: softmax = 3.28761 (* 1 = 3.28761 loss)
I0615 16:20:03.892535  5063 solver.cpp:473] Iteration 19690, lr = 0.0001
I0615 16:20:04.650663  5063 solver.cpp:213] Iteration 19700, loss = 3.1504
I0615 16:20:04.650678  5063 solver.cpp:228]     Train net output #0: softmax = 3.1504 (* 1 = 3.1504 loss)
I0615 16:20:04.650683  5063 solver.cpp:473] Iteration 19700, lr = 0.0001
I0615 16:20:05.409404  5063 solver.cpp:213] Iteration 19710, loss = 3.1908
I0615 16:20:05.409420  5063 solver.cpp:228]     Train net output #0: softmax = 3.1908 (* 1 = 3.1908 loss)
I0615 16:20:05.409425  5063 solver.cpp:473] Iteration 19710, lr = 0.0001
I0615 16:20:06.167927  5063 solver.cpp:213] Iteration 19720, loss = 3.33469
I0615 16:20:06.167942  5063 solver.cpp:228]     Train net output #0: softmax = 3.33469 (* 1 = 3.33469 loss)
I0615 16:20:06.167945  5063 solver.cpp:473] Iteration 19720, lr = 0.0001
I0615 16:20:06.926388  5063 solver.cpp:213] Iteration 19730, loss = 3.14616
I0615 16:20:06.926409  5063 solver.cpp:228]     Train net output #0: softmax = 3.14616 (* 1 = 3.14616 loss)
I0615 16:20:06.926414  5063 solver.cpp:473] Iteration 19730, lr = 0.0001
I0615 16:20:07.684907  5063 solver.cpp:213] Iteration 19740, loss = 3.2252
I0615 16:20:07.684923  5063 solver.cpp:228]     Train net output #0: softmax = 3.2252 (* 1 = 3.2252 loss)
I0615 16:20:07.684927  5063 solver.cpp:473] Iteration 19740, lr = 0.0001
I0615 16:20:08.443225  5063 solver.cpp:213] Iteration 19750, loss = 3.23274
I0615 16:20:08.443244  5063 solver.cpp:228]     Train net output #0: softmax = 3.23274 (* 1 = 3.23274 loss)
I0615 16:20:08.443249  5063 solver.cpp:473] Iteration 19750, lr = 0.0001
I0615 16:20:09.199714  5063 solver.cpp:213] Iteration 19760, loss = 3.28703
I0615 16:20:09.199745  5063 solver.cpp:228]     Train net output #0: softmax = 3.28703 (* 1 = 3.28703 loss)
I0615 16:20:09.199750  5063 solver.cpp:473] Iteration 19760, lr = 0.0001
I0615 16:20:09.958894  5063 solver.cpp:213] Iteration 19770, loss = 3.42565
I0615 16:20:09.958911  5063 solver.cpp:228]     Train net output #0: softmax = 3.42565 (* 1 = 3.42565 loss)
I0615 16:20:09.958915  5063 solver.cpp:473] Iteration 19770, lr = 0.0001
I0615 16:20:10.716662  5063 solver.cpp:213] Iteration 19780, loss = 2.98368
I0615 16:20:10.716684  5063 solver.cpp:228]     Train net output #0: softmax = 2.98368 (* 1 = 2.98368 loss)
I0615 16:20:10.716830  5063 solver.cpp:473] Iteration 19780, lr = 0.0001
I0615 16:20:11.475245  5063 solver.cpp:213] Iteration 19790, loss = 3.2066
I0615 16:20:11.475262  5063 solver.cpp:228]     Train net output #0: softmax = 3.2066 (* 1 = 3.2066 loss)
I0615 16:20:11.475266  5063 solver.cpp:473] Iteration 19790, lr = 0.0001
I0615 16:20:12.233759  5063 solver.cpp:213] Iteration 19800, loss = 3.07395
I0615 16:20:12.233784  5063 solver.cpp:228]     Train net output #0: softmax = 3.07395 (* 1 = 3.07395 loss)
I0615 16:20:12.233789  5063 solver.cpp:473] Iteration 19800, lr = 0.0001
I0615 16:20:12.991394  5063 solver.cpp:213] Iteration 19810, loss = 3.17075
I0615 16:20:12.991408  5063 solver.cpp:228]     Train net output #0: softmax = 3.17075 (* 1 = 3.17075 loss)
I0615 16:20:12.991413  5063 solver.cpp:473] Iteration 19810, lr = 0.0001
I0615 16:20:13.747689  5063 solver.cpp:213] Iteration 19820, loss = 3.28416
I0615 16:20:13.747704  5063 solver.cpp:228]     Train net output #0: softmax = 3.28416 (* 1 = 3.28416 loss)
I0615 16:20:13.747707  5063 solver.cpp:473] Iteration 19820, lr = 0.0001
I0615 16:20:14.506425  5063 solver.cpp:213] Iteration 19830, loss = 3.32501
I0615 16:20:14.506440  5063 solver.cpp:228]     Train net output #0: softmax = 3.32501 (* 1 = 3.32501 loss)
I0615 16:20:14.506444  5063 solver.cpp:473] Iteration 19830, lr = 0.0001
I0615 16:20:15.265329  5063 solver.cpp:213] Iteration 19840, loss = 3.43529
I0615 16:20:15.265346  5063 solver.cpp:228]     Train net output #0: softmax = 3.43529 (* 1 = 3.43529 loss)
I0615 16:20:15.265349  5063 solver.cpp:473] Iteration 19840, lr = 0.0001
I0615 16:20:16.024094  5063 solver.cpp:213] Iteration 19850, loss = 3.18911
I0615 16:20:16.024112  5063 solver.cpp:228]     Train net output #0: softmax = 3.18911 (* 1 = 3.18911 loss)
I0615 16:20:16.024282  5063 solver.cpp:473] Iteration 19850, lr = 0.0001
I0615 16:20:16.782806  5063 solver.cpp:213] Iteration 19860, loss = 3.18849
I0615 16:20:16.782827  5063 solver.cpp:228]     Train net output #0: softmax = 3.18849 (* 1 = 3.18849 loss)
I0615 16:20:16.782831  5063 solver.cpp:473] Iteration 19860, lr = 0.0001
I0615 16:20:17.541087  5063 solver.cpp:213] Iteration 19870, loss = 3.24605
I0615 16:20:17.541105  5063 solver.cpp:228]     Train net output #0: softmax = 3.24605 (* 1 = 3.24605 loss)
I0615 16:20:17.541110  5063 solver.cpp:473] Iteration 19870, lr = 0.0001
I0615 16:20:18.299237  5063 solver.cpp:213] Iteration 19880, loss = 3.03996
I0615 16:20:18.299252  5063 solver.cpp:228]     Train net output #0: softmax = 3.03996 (* 1 = 3.03996 loss)
I0615 16:20:18.299257  5063 solver.cpp:473] Iteration 19880, lr = 0.0001
I0615 16:20:19.058131  5063 solver.cpp:213] Iteration 19890, loss = 3.24236
I0615 16:20:19.058146  5063 solver.cpp:228]     Train net output #0: softmax = 3.24236 (* 1 = 3.24236 loss)
I0615 16:20:19.058151  5063 solver.cpp:473] Iteration 19890, lr = 0.0001
I0615 16:20:19.816861  5063 solver.cpp:213] Iteration 19900, loss = 3.31639
I0615 16:20:19.816876  5063 solver.cpp:228]     Train net output #0: softmax = 3.31639 (* 1 = 3.31639 loss)
I0615 16:20:19.816881  5063 solver.cpp:473] Iteration 19900, lr = 0.0001
I0615 16:20:20.576287  5063 solver.cpp:213] Iteration 19910, loss = 3.27098
I0615 16:20:20.576303  5063 solver.cpp:228]     Train net output #0: softmax = 3.27098 (* 1 = 3.27098 loss)
I0615 16:20:20.576308  5063 solver.cpp:473] Iteration 19910, lr = 0.0001
I0615 16:20:21.335216  5063 solver.cpp:213] Iteration 19920, loss = 3.31744
I0615 16:20:21.335373  5063 solver.cpp:228]     Train net output #0: softmax = 3.31744 (* 1 = 3.31744 loss)
I0615 16:20:21.335381  5063 solver.cpp:473] Iteration 19920, lr = 0.0001
I0615 16:20:22.094396  5063 solver.cpp:213] Iteration 19930, loss = 3.157
I0615 16:20:22.094411  5063 solver.cpp:228]     Train net output #0: softmax = 3.157 (* 1 = 3.157 loss)
I0615 16:20:22.094416  5063 solver.cpp:473] Iteration 19930, lr = 0.0001
I0615 16:20:22.853528  5063 solver.cpp:213] Iteration 19940, loss = 3.22359
I0615 16:20:22.853543  5063 solver.cpp:228]     Train net output #0: softmax = 3.22359 (* 1 = 3.22359 loss)
I0615 16:20:22.853548  5063 solver.cpp:473] Iteration 19940, lr = 0.0001
I0615 16:20:23.609987  5063 solver.cpp:213] Iteration 19950, loss = 3.26409
I0615 16:20:23.610000  5063 solver.cpp:228]     Train net output #0: softmax = 3.26409 (* 1 = 3.26409 loss)
I0615 16:20:23.610005  5063 solver.cpp:473] Iteration 19950, lr = 0.0001
I0615 16:20:24.368551  5063 solver.cpp:213] Iteration 19960, loss = 3.23351
I0615 16:20:24.368566  5063 solver.cpp:228]     Train net output #0: softmax = 3.23351 (* 1 = 3.23351 loss)
I0615 16:20:24.368571  5063 solver.cpp:473] Iteration 19960, lr = 0.0001
I0615 16:20:25.124800  5063 solver.cpp:213] Iteration 19970, loss = 3.20094
I0615 16:20:25.124819  5063 solver.cpp:228]     Train net output #0: softmax = 3.20094 (* 1 = 3.20094 loss)
I0615 16:20:25.124822  5063 solver.cpp:473] Iteration 19970, lr = 0.0001
I0615 16:20:25.883108  5063 solver.cpp:213] Iteration 19980, loss = 3.19066
I0615 16:20:25.883123  5063 solver.cpp:228]     Train net output #0: softmax = 3.19066 (* 1 = 3.19066 loss)
I0615 16:20:25.883127  5063 solver.cpp:473] Iteration 19980, lr = 0.0001
I0615 16:20:26.641690  5063 solver.cpp:213] Iteration 19990, loss = 3.1955
I0615 16:20:26.641707  5063 solver.cpp:228]     Train net output #0: softmax = 3.1955 (* 1 = 3.1955 loss)
I0615 16:20:26.641711  5063 solver.cpp:473] Iteration 19990, lr = 0.0001
I0615 16:20:27.346170  5063 solver.cpp:362] Snapshotting to snapshots/16-06-15_15h49m18s_0_11_pretrainClassification_iter_20000.caffemodel
I0615 16:20:27.346856  5063 solver.cpp:370] Snapshotting solver state to snapshots/16-06-15_15h49m18s_0_11_pretrainClassification_iter_20000.solverstate
I0615 16:20:27.365777  5063 solver.cpp:273] Iteration 20000, loss = 3.32602
I0615 16:20:27.365788  5063 solver.cpp:291] Iteration 20000, Testing net (#0)
I0615 16:20:27.460083  5063 solver.cpp:342]     Test net output #0: accuracy_top_1 = 0.195312
I0615 16:20:27.460094  5063 solver.cpp:342]     Test net output #1: accuracy_top_5 = 0.492188
I0615 16:20:27.460100  5063 solver.cpp:342]     Test net output #2: softmax = 3.37371 (* 1 = 3.37371 loss)
I0615 16:20:27.460104  5063 solver.cpp:278] Optimization Done.
I0615 16:20:27.460108  5063 caffe.cpp:121] Optimization Done.
