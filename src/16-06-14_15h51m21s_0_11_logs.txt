libdc1394 error: Failed to initialize libdc1394
I0614 16:00:00.558238  5063 caffe.cpp:99] Use GPU with device ID 0
I0614 16:00:00.676777  5063 caffe.cpp:107] Starting Optimization
I0614 16:00:00.676842  5063 solver.cpp:32] Initializing solver from parameters: 
test_iter: 5
test_interval: 1000
base_lr: 0.0001
display: 10
max_iter: 20000
lr_policy: "fixed"
momentum: 0.9
weight_decay: 0.004
snapshot: 1000
snapshot_prefix: "snapshots/16-06-14_15h51m21s_0_11_pretrainClassification"
solver_mode: GPU
net: "prototxt/16-06-14_15h51m21s_0_11_pretrainClassification_net.sh"
I0614 16:00:00.676861  5063 solver.cpp:70] Creating training net from net file: prototxt/16-06-14_15h51m21s_0_11_pretrainClassification_net.sh
I0614 16:00:00.677278  5063 net.cpp:277] The NetState phase (0) differed from the phase (1) specified by a rule in layer data
I0614 16:00:00.677296  5063 net.cpp:277] The NetState phase (0) differed from the phase (1) specified by a rule in layer accuracy_top_1
I0614 16:00:00.677300  5063 net.cpp:277] The NetState phase (0) differed from the phase (1) specified by a rule in layer accuracy_top_5
I0614 16:00:00.677392  5063 net.cpp:39] Initializing net from parameters: 
layers {
  top: "data"
  top: "label"
  name: "data"
  type: DATA
  data_param {
    source: "/dataset/cifar100_lmdb_lab/cifar100_train_lmdb"
    batch_size: 128
    backend: LMDB
  }
  include {
    phase: TRAIN
  }
  transform_param {
    mean_file: "/dataset/cifar100_lmdb_lab/mean.binaryproto"
  }
}
layers {
  bottom: "data"
  top: "0_0_conv"
  name: "0_0_conv"
  type: CONVOLUTION
  blobs_lr: 1
  blobs_lr: 2
  weight_decay: 0
  weight_decay: 0
  convolution_param {
    num_output: 16
    pad: 1
    kernel_size: 3
    stride: 1
    weight_filler {
      type: "xavier"
    }
    bias_filler {
      type: "constant"
      value: 0
    }
  }
}
layers {
  bottom: "0_0_conv"
  top: "0_0_conv"
  name: "0_0_conv_ReLU"
  type: RELU
}
layers {
  bottom: "0_0_conv"
  top: "0_1_conv"
  name: "0_1_conv"
  type: CONVOLUTION
  blobs_lr: 1
  blobs_lr: 2
  weight_decay: 0
  weight_decay: 0
  convolution_param {
    num_output: 16
    pad: 1
    kernel_size: 3
    stride: 1
    weight_filler {
      type: "xavier"
    }
    bias_filler {
      type: "constant"
      value: 0
    }
  }
}
layers {
  bottom: "0_1_conv"
  top: "0_1_conv"
  name: "0_1_conv_ReLU"
  type: RELU
}
layers {
  bottom: "0_1_conv"
  top: "0_pool"
  name: "0_pool"
  type: POOLING
  pooling_param {
    pool: MAX
    kernel_size: 2
    stride: 2
  }
}
layers {
  bottom: "0_pool"
  top: "1_0_conv"
  name: "1_0_conv"
  type: CONVOLUTION
  blobs_lr: 1
  blobs_lr: 2
  weight_decay: 0
  weight_decay: 0
  convolution_param {
    num_output: 16
    pad: 1
    kernel_size: 3
    stride: 1
    weight_filler {
      type: "xavier"
    }
    bias_filler {
      type: "constant"
      value: 0
    }
  }
}
layers {
  bottom: "1_0_conv"
  top: "1_0_conv"
  name: "1_0_conv_ReLU"
  type: RELU
}
layers {
  bottom: "1_0_conv"
  top: "1_1_conv"
  name: "1_1_conv"
  type: CONVOLUTION
  blobs_lr: 1
  blobs_lr: 2
  weight_decay: 0
  weight_decay: 0
  convolution_param {
    num_output: 16
    pad: 1
    kernel_size: 3
    stride: 1
    weight_filler {
      type: "xavier"
    }
    bias_filler {
      type: "constant"
      value: 0
    }
  }
}
layers {
  bottom: "1_1_conv"
  top: "1_1_conv"
  name: "1_1_conv_ReLU"
  type: RELU
}
layers {
  bottom: "1_1_conv"
  top: "1_pool"
  name: "1_pool"
  type: POOLING
  pooling_param {
    pool: MAX
    kernel_size: 2
    stride: 2
  }
}
layers {
  bottom: "1_pool"
  top: "2_0_conv"
  name: "2_0_conv"
  type: CONVOLUTION
  blobs_lr: 1
  blobs_lr: 2
  weight_decay: 0
  weight_decay: 0
  convolution_param {
    num_output: 16
    pad: 1
    kernel_size: 3
    stride: 1
    weight_filler {
      type: "xavier"
    }
    bias_filler {
      type: "constant"
      value: 0
    }
  }
}
layers {
  bottom: "2_0_conv"
  top: "2_0_conv"
  name: "2_0_conv_ReLU"
  type: RELU
}
layers {
  bottom: "2_0_conv"
  top: "2_1_conv"
  name: "2_1_conv"
  type: CONVOLUTION
  blobs_lr: 1
  blobs_lr: 2
  weight_decay: 0
  weight_decay: 0
  convolution_param {
    num_output: 16
    pad: 1
    kernel_size: 3
    stride: 1
    weight_filler {
      type: "xavier"
    }
    bias_filler {
      type: "constant"
      value: 0
    }
  }
}
layers {
  bottom: "2_1_conv"
  top: "2_1_conv"
  name: "2_1_conv_ReLU"
  type: RELU
}
layers {
  bottom: "2_1_conv"
  top: "2_pool"
  name: "2_pool"
  type: POOLING
  pooling_param {
    pool: MAX
    kernel_size: 2
    stride: 2
  }
}
layers {
  bottom: "2_pool"
  top: "middle_conv"
  name: "middle_conv"
  type: CONVOLUTION
  blobs_lr: 1
  blobs_lr: 2
  weight_decay: 0
  weight_decay: 0
  convolution_param {
    num_output: 50
    pad: 0
    kernel_size: 4
    stride: 1
    weight_filler {
      type: "xavier"
    }
    bias_filler {
      type: "constant"
      value: 0
    }
  }
}
layers {
  bottom: "middle_conv"
  top: "middle_conv"
  name: "middle_conv_ReLU"
  type: RELU
}
layers {
  bottom: "middle_conv"
  top: "fc1"
  name: "fc1"
  type: INNER_PRODUCT
  blobs_lr: 1
  blobs_lr: 2
  weight_decay: 0
  weight_decay: 0
  inner_product_param {
    num_output: 512
    weight_filler {
      type: "xavier"
    }
    bias_filler {
      type: "constant"
      value: 0
    }
  }
}
layers {
  bottom: "fc1"
  top: "fc1"
  name: "fc1_Dropout"
  type: DROPOUT
  dropout_param {
    dropout_ratio: 0.5
  }
  include {
    phase: TRAIN
  }
}
layers {
  bottom: "fc1"
  top: "fc1"
  name: "fc1_Dropout_ReLU"
  type: RELU
}
layers {
  bottom: "fc1"
  top: "fc2"
  name: "fc2"
  type: INNER_PRODUCT
  blobs_lr: 1
  blobs_lr: 2
  weight_decay: 0
  weight_decay: 0
  inner_product_param {
    num_output: 100
    weight_filler {
      type: "xavier"
    }
    bias_filler {
      type: "constant"
      value: 0
    }
  }
}
layers {
  bottom: "fc2"
  bottom: "label"
  top: "softmax"
  name: "softmax"
  type: SOFTMAX_LOSS
}
state {
  phase: TRAIN
}
I0614 16:00:00.677461  5063 layer_factory.hpp:78] Creating layer data
I0614 16:00:00.677475  5063 data_transformer.cpp:25] Loading mean file from/dataset/cifar100_lmdb_lab/mean.binaryproto
I0614 16:00:00.677523  5063 net.cpp:69] Creating Layer data
I0614 16:00:00.677530  5063 net.cpp:358] data -> data
I0614 16:00:00.677538  5063 net.cpp:358] data -> label
I0614 16:00:00.677543  5063 net.cpp:98] Setting up data
I0614 16:00:00.677547  5063 data_layer.cpp:32] Opening dataset /dataset/cifar100_lmdb_lab/cifar100_train_lmdb
I0614 16:00:00.677618  5063 data_layer.cpp:71] output data size: 128,3,32,32
I0614 16:00:00.677933  5063 net.cpp:105] Top shape: 128 3 32 32 (393216)
I0614 16:00:00.677939  5063 net.cpp:105] Top shape: 128 1 1 1 (128)
I0614 16:00:00.677942  5063 layer_factory.hpp:78] Creating layer 0_0_conv
I0614 16:00:00.677958  5063 net.cpp:69] Creating Layer 0_0_conv
I0614 16:00:00.677961  5063 net.cpp:396] 0_0_conv <- data
I0614 16:00:00.677971  5063 net.cpp:358] 0_0_conv -> 0_0_conv
I0614 16:00:00.677978  5063 net.cpp:98] Setting up 0_0_conv
I0614 16:00:00.678246  5063 net.cpp:105] Top shape: 128 16 32 32 (2097152)
I0614 16:00:00.678259  5063 layer_factory.hpp:78] Creating layer 0_0_conv_ReLU
I0614 16:00:00.678266  5063 net.cpp:69] Creating Layer 0_0_conv_ReLU
I0614 16:00:00.678268  5063 net.cpp:396] 0_0_conv_ReLU <- 0_0_conv
I0614 16:00:00.678272  5063 net.cpp:347] 0_0_conv_ReLU -> 0_0_conv (in-place)
I0614 16:00:00.678277  5063 net.cpp:98] Setting up 0_0_conv_ReLU
I0614 16:00:00.678279  5063 net.cpp:105] Top shape: 128 16 32 32 (2097152)
I0614 16:00:00.678283  5063 layer_factory.hpp:78] Creating layer 0_1_conv
I0614 16:00:00.678287  5063 net.cpp:69] Creating Layer 0_1_conv
I0614 16:00:00.678289  5063 net.cpp:396] 0_1_conv <- 0_0_conv
I0614 16:00:00.678295  5063 net.cpp:358] 0_1_conv -> 0_1_conv
I0614 16:00:00.678299  5063 net.cpp:98] Setting up 0_1_conv
I0614 16:00:00.678319  5063 net.cpp:105] Top shape: 128 16 32 32 (2097152)
I0614 16:00:00.678325  5063 layer_factory.hpp:78] Creating layer 0_1_conv_ReLU
I0614 16:00:00.678329  5063 net.cpp:69] Creating Layer 0_1_conv_ReLU
I0614 16:00:00.678331  5063 net.cpp:396] 0_1_conv_ReLU <- 0_1_conv
I0614 16:00:00.678336  5063 net.cpp:347] 0_1_conv_ReLU -> 0_1_conv (in-place)
I0614 16:00:00.678344  5063 net.cpp:98] Setting up 0_1_conv_ReLU
I0614 16:00:00.678346  5063 net.cpp:105] Top shape: 128 16 32 32 (2097152)
I0614 16:00:00.678357  5063 layer_factory.hpp:78] Creating layer 0_pool
I0614 16:00:00.678362  5063 net.cpp:69] Creating Layer 0_pool
I0614 16:00:00.678365  5063 net.cpp:396] 0_pool <- 0_1_conv
I0614 16:00:00.678369  5063 net.cpp:358] 0_pool -> 0_pool
I0614 16:00:00.678374  5063 net.cpp:98] Setting up 0_pool
I0614 16:00:00.678380  5063 net.cpp:105] Top shape: 128 16 16 16 (524288)
I0614 16:00:00.678383  5063 layer_factory.hpp:78] Creating layer 1_0_conv
I0614 16:00:00.678387  5063 net.cpp:69] Creating Layer 1_0_conv
I0614 16:00:00.678390  5063 net.cpp:396] 1_0_conv <- 0_pool
I0614 16:00:00.678395  5063 net.cpp:358] 1_0_conv -> 1_0_conv
I0614 16:00:00.678398  5063 net.cpp:98] Setting up 1_0_conv
I0614 16:00:00.678416  5063 net.cpp:105] Top shape: 128 16 16 16 (524288)
I0614 16:00:00.678422  5063 layer_factory.hpp:78] Creating layer 1_0_conv_ReLU
I0614 16:00:00.678426  5063 net.cpp:69] Creating Layer 1_0_conv_ReLU
I0614 16:00:00.678428  5063 net.cpp:396] 1_0_conv_ReLU <- 1_0_conv
I0614 16:00:00.678433  5063 net.cpp:347] 1_0_conv_ReLU -> 1_0_conv (in-place)
I0614 16:00:00.678437  5063 net.cpp:98] Setting up 1_0_conv_ReLU
I0614 16:00:00.678441  5063 net.cpp:105] Top shape: 128 16 16 16 (524288)
I0614 16:00:00.678442  5063 layer_factory.hpp:78] Creating layer 1_1_conv
I0614 16:00:00.678447  5063 net.cpp:69] Creating Layer 1_1_conv
I0614 16:00:00.678450  5063 net.cpp:396] 1_1_conv <- 1_0_conv
I0614 16:00:00.678455  5063 net.cpp:358] 1_1_conv -> 1_1_conv
I0614 16:00:00.678459  5063 net.cpp:98] Setting up 1_1_conv
I0614 16:00:00.678479  5063 net.cpp:105] Top shape: 128 16 16 16 (524288)
I0614 16:00:00.678484  5063 layer_factory.hpp:78] Creating layer 1_1_conv_ReLU
I0614 16:00:00.678488  5063 net.cpp:69] Creating Layer 1_1_conv_ReLU
I0614 16:00:00.678490  5063 net.cpp:396] 1_1_conv_ReLU <- 1_1_conv
I0614 16:00:00.678494  5063 net.cpp:347] 1_1_conv_ReLU -> 1_1_conv (in-place)
I0614 16:00:00.678498  5063 net.cpp:98] Setting up 1_1_conv_ReLU
I0614 16:00:00.678501  5063 net.cpp:105] Top shape: 128 16 16 16 (524288)
I0614 16:00:00.678504  5063 layer_factory.hpp:78] Creating layer 1_pool
I0614 16:00:00.678508  5063 net.cpp:69] Creating Layer 1_pool
I0614 16:00:00.678510  5063 net.cpp:396] 1_pool <- 1_1_conv
I0614 16:00:00.678514  5063 net.cpp:358] 1_pool -> 1_pool
I0614 16:00:00.678519  5063 net.cpp:98] Setting up 1_pool
I0614 16:00:00.678521  5063 net.cpp:105] Top shape: 128 16 8 8 (131072)
I0614 16:00:00.678524  5063 layer_factory.hpp:78] Creating layer 2_0_conv
I0614 16:00:00.678530  5063 net.cpp:69] Creating Layer 2_0_conv
I0614 16:00:00.678534  5063 net.cpp:396] 2_0_conv <- 1_pool
I0614 16:00:00.678539  5063 net.cpp:358] 2_0_conv -> 2_0_conv
I0614 16:00:00.678544  5063 net.cpp:98] Setting up 2_0_conv
I0614 16:00:00.678562  5063 net.cpp:105] Top shape: 128 16 8 8 (131072)
I0614 16:00:00.678568  5063 layer_factory.hpp:78] Creating layer 2_0_conv_ReLU
I0614 16:00:00.678571  5063 net.cpp:69] Creating Layer 2_0_conv_ReLU
I0614 16:00:00.678575  5063 net.cpp:396] 2_0_conv_ReLU <- 2_0_conv
I0614 16:00:00.678580  5063 net.cpp:347] 2_0_conv_ReLU -> 2_0_conv (in-place)
I0614 16:00:00.678582  5063 net.cpp:98] Setting up 2_0_conv_ReLU
I0614 16:00:00.678586  5063 net.cpp:105] Top shape: 128 16 8 8 (131072)
I0614 16:00:00.678588  5063 layer_factory.hpp:78] Creating layer 2_1_conv
I0614 16:00:00.678592  5063 net.cpp:69] Creating Layer 2_1_conv
I0614 16:00:00.678596  5063 net.cpp:396] 2_1_conv <- 2_0_conv
I0614 16:00:00.678599  5063 net.cpp:358] 2_1_conv -> 2_1_conv
I0614 16:00:00.678603  5063 net.cpp:98] Setting up 2_1_conv
I0614 16:00:00.678622  5063 net.cpp:105] Top shape: 128 16 8 8 (131072)
I0614 16:00:00.678627  5063 layer_factory.hpp:78] Creating layer 2_1_conv_ReLU
I0614 16:00:00.678629  5063 net.cpp:69] Creating Layer 2_1_conv_ReLU
I0614 16:00:00.678632  5063 net.cpp:396] 2_1_conv_ReLU <- 2_1_conv
I0614 16:00:00.678637  5063 net.cpp:347] 2_1_conv_ReLU -> 2_1_conv (in-place)
I0614 16:00:00.678642  5063 net.cpp:98] Setting up 2_1_conv_ReLU
I0614 16:00:00.678645  5063 net.cpp:105] Top shape: 128 16 8 8 (131072)
I0614 16:00:00.678652  5063 layer_factory.hpp:78] Creating layer 2_pool
I0614 16:00:00.678655  5063 net.cpp:69] Creating Layer 2_pool
I0614 16:00:00.678658  5063 net.cpp:396] 2_pool <- 2_1_conv
I0614 16:00:00.678663  5063 net.cpp:358] 2_pool -> 2_pool
I0614 16:00:00.678668  5063 net.cpp:98] Setting up 2_pool
I0614 16:00:00.678670  5063 net.cpp:105] Top shape: 128 16 4 4 (32768)
I0614 16:00:00.678673  5063 layer_factory.hpp:78] Creating layer middle_conv
I0614 16:00:00.678678  5063 net.cpp:69] Creating Layer middle_conv
I0614 16:00:00.678680  5063 net.cpp:396] middle_conv <- 2_pool
I0614 16:00:00.678684  5063 net.cpp:358] middle_conv -> middle_conv
I0614 16:00:00.678689  5063 net.cpp:98] Setting up middle_conv
I0614 16:00:00.678760  5063 net.cpp:105] Top shape: 128 50 1 1 (6400)
I0614 16:00:00.678766  5063 layer_factory.hpp:78] Creating layer middle_conv_ReLU
I0614 16:00:00.678768  5063 net.cpp:69] Creating Layer middle_conv_ReLU
I0614 16:00:00.678771  5063 net.cpp:396] middle_conv_ReLU <- middle_conv
I0614 16:00:00.678776  5063 net.cpp:347] middle_conv_ReLU -> middle_conv (in-place)
I0614 16:00:00.678779  5063 net.cpp:98] Setting up middle_conv_ReLU
I0614 16:00:00.678781  5063 net.cpp:105] Top shape: 128 50 1 1 (6400)
I0614 16:00:00.678784  5063 layer_factory.hpp:78] Creating layer fc1
I0614 16:00:00.678789  5063 net.cpp:69] Creating Layer fc1
I0614 16:00:00.678792  5063 net.cpp:396] fc1 <- middle_conv
I0614 16:00:00.678797  5063 net.cpp:358] fc1 -> fc1
I0614 16:00:00.678802  5063 net.cpp:98] Setting up fc1
I0614 16:00:00.678937  5063 net.cpp:105] Top shape: 128 512 1 1 (65536)
I0614 16:00:00.678942  5063 layer_factory.hpp:78] Creating layer fc1_Dropout
I0614 16:00:00.678946  5063 net.cpp:69] Creating Layer fc1_Dropout
I0614 16:00:00.678949  5063 net.cpp:396] fc1_Dropout <- fc1
I0614 16:00:00.678954  5063 net.cpp:347] fc1_Dropout -> fc1 (in-place)
I0614 16:00:00.678958  5063 net.cpp:98] Setting up fc1_Dropout
I0614 16:00:00.678962  5063 net.cpp:105] Top shape: 128 512 1 1 (65536)
I0614 16:00:00.678966  5063 layer_factory.hpp:78] Creating layer fc1_Dropout_ReLU
I0614 16:00:00.678968  5063 net.cpp:69] Creating Layer fc1_Dropout_ReLU
I0614 16:00:00.678972  5063 net.cpp:396] fc1_Dropout_ReLU <- fc1
I0614 16:00:00.678975  5063 net.cpp:347] fc1_Dropout_ReLU -> fc1 (in-place)
I0614 16:00:00.678978  5063 net.cpp:98] Setting up fc1_Dropout_ReLU
I0614 16:00:00.678982  5063 net.cpp:105] Top shape: 128 512 1 1 (65536)
I0614 16:00:00.678984  5063 layer_factory.hpp:78] Creating layer fc2
I0614 16:00:00.678988  5063 net.cpp:69] Creating Layer fc2
I0614 16:00:00.678990  5063 net.cpp:396] fc2 <- fc1
I0614 16:00:00.678995  5063 net.cpp:358] fc2 -> fc2
I0614 16:00:00.679000  5063 net.cpp:98] Setting up fc2
I0614 16:00:00.679302  5063 net.cpp:105] Top shape: 128 100 1 1 (12800)
I0614 16:00:00.679311  5063 layer_factory.hpp:78] Creating layer softmax
I0614 16:00:00.679318  5063 net.cpp:69] Creating Layer softmax
I0614 16:00:00.679322  5063 net.cpp:396] softmax <- fc2
I0614 16:00:00.679325  5063 net.cpp:396] softmax <- label
I0614 16:00:00.679329  5063 net.cpp:358] softmax -> softmax
I0614 16:00:00.679334  5063 net.cpp:98] Setting up softmax
I0614 16:00:00.679349  5063 net.cpp:105] Top shape: 1 1 1 1 (1)
I0614 16:00:00.679352  5063 net.cpp:111]     with loss weight 1
I0614 16:00:00.679363  5063 net.cpp:172] softmax needs backward computation.
I0614 16:00:00.679366  5063 net.cpp:172] fc2 needs backward computation.
I0614 16:00:00.679369  5063 net.cpp:172] fc1_Dropout_ReLU needs backward computation.
I0614 16:00:00.679373  5063 net.cpp:172] fc1_Dropout needs backward computation.
I0614 16:00:00.679374  5063 net.cpp:172] fc1 needs backward computation.
I0614 16:00:00.679378  5063 net.cpp:172] middle_conv_ReLU needs backward computation.
I0614 16:00:00.679380  5063 net.cpp:172] middle_conv needs backward computation.
I0614 16:00:00.679383  5063 net.cpp:172] 2_pool needs backward computation.
I0614 16:00:00.679386  5063 net.cpp:172] 2_1_conv_ReLU needs backward computation.
I0614 16:00:00.679391  5063 net.cpp:172] 2_1_conv needs backward computation.
I0614 16:00:00.679399  5063 net.cpp:172] 2_0_conv_ReLU needs backward computation.
I0614 16:00:00.679401  5063 net.cpp:172] 2_0_conv needs backward computation.
I0614 16:00:00.679404  5063 net.cpp:172] 1_pool needs backward computation.
I0614 16:00:00.679406  5063 net.cpp:172] 1_1_conv_ReLU needs backward computation.
I0614 16:00:00.679409  5063 net.cpp:172] 1_1_conv needs backward computation.
I0614 16:00:00.679412  5063 net.cpp:172] 1_0_conv_ReLU needs backward computation.
I0614 16:00:00.679415  5063 net.cpp:172] 1_0_conv needs backward computation.
I0614 16:00:00.679419  5063 net.cpp:172] 0_pool needs backward computation.
I0614 16:00:00.679420  5063 net.cpp:172] 0_1_conv_ReLU needs backward computation.
I0614 16:00:00.679424  5063 net.cpp:172] 0_1_conv needs backward computation.
I0614 16:00:00.679426  5063 net.cpp:172] 0_0_conv_ReLU needs backward computation.
I0614 16:00:00.679430  5063 net.cpp:172] 0_0_conv needs backward computation.
I0614 16:00:00.679432  5063 net.cpp:174] data does not need backward computation.
I0614 16:00:00.679435  5063 net.cpp:210] This network produces output softmax
I0614 16:00:00.679447  5063 net.cpp:469] Collecting Learning Rate and Weight Decay.
I0614 16:00:00.679452  5063 net.cpp:221] Network initialization done.
I0614 16:00:00.679455  5063 net.cpp:222] Memory required for data: 49254916
I0614 16:00:00.679869  5063 solver.cpp:154] Creating test net (#0) specified by net file: prototxt/16-06-14_15h51m21s_0_11_pretrainClassification_net.sh
I0614 16:00:00.679894  5063 net.cpp:277] The NetState phase (1) differed from the phase (0) specified by a rule in layer data
I0614 16:00:00.679904  5063 net.cpp:277] The NetState phase (1) differed from the phase (0) specified by a rule in layer fc1_Dropout
I0614 16:00:00.679996  5063 net.cpp:39] Initializing net from parameters: 
layers {
  top: "data"
  top: "label"
  name: "data"
  type: DATA
  data_param {
    source: "/dataset/cifar100_lmdb_lab/cifar100_test_lmdb"
    batch_size: 128
    backend: LMDB
  }
  include {
    phase: TEST
  }
  transform_param {
    mean_file: "/dataset/cifar100_lmdb_lab/mean.binaryproto"
  }
}
layers {
  bottom: "data"
  top: "0_0_conv"
  name: "0_0_conv"
  type: CONVOLUTION
  blobs_lr: 1
  blobs_lr: 2
  weight_decay: 0
  weight_decay: 0
  convolution_param {
    num_output: 16
    pad: 1
    kernel_size: 3
    stride: 1
    weight_filler {
      type: "xavier"
    }
    bias_filler {
      type: "constant"
      value: 0
    }
  }
}
layers {
  bottom: "0_0_conv"
  top: "0_0_conv"
  name: "0_0_conv_ReLU"
  type: RELU
}
layers {
  bottom: "0_0_conv"
  top: "0_1_conv"
  name: "0_1_conv"
  type: CONVOLUTION
  blobs_lr: 1
  blobs_lr: 2
  weight_decay: 0
  weight_decay: 0
  convolution_param {
    num_output: 16
    pad: 1
    kernel_size: 3
    stride: 1
    weight_filler {
      type: "xavier"
    }
    bias_filler {
      type: "constant"
      value: 0
    }
  }
}
layers {
  bottom: "0_1_conv"
  top: "0_1_conv"
  name: "0_1_conv_ReLU"
  type: RELU
}
layers {
  bottom: "0_1_conv"
  top: "0_pool"
  name: "0_pool"
  type: POOLING
  pooling_param {
    pool: MAX
    kernel_size: 2
    stride: 2
  }
}
layers {
  bottom: "0_pool"
  top: "1_0_conv"
  name: "1_0_conv"
  type: CONVOLUTION
  blobs_lr: 1
  blobs_lr: 2
  weight_decay: 0
  weight_decay: 0
  convolution_param {
    num_output: 16
    pad: 1
    kernel_size: 3
    stride: 1
    weight_filler {
      type: "xavier"
    }
    bias_filler {
      type: "constant"
      value: 0
    }
  }
}
layers {
  bottom: "1_0_conv"
  top: "1_0_conv"
  name: "1_0_conv_ReLU"
  type: RELU
}
layers {
  bottom: "1_0_conv"
  top: "1_1_conv"
  name: "1_1_conv"
  type: CONVOLUTION
  blobs_lr: 1
  blobs_lr: 2
  weight_decay: 0
  weight_decay: 0
  convolution_param {
    num_output: 16
    pad: 1
    kernel_size: 3
    stride: 1
    weight_filler {
      type: "xavier"
    }
    bias_filler {
      type: "constant"
      value: 0
    }
  }
}
layers {
  bottom: "1_1_conv"
  top: "1_1_conv"
  name: "1_1_conv_ReLU"
  type: RELU
}
layers {
  bottom: "1_1_conv"
  top: "1_pool"
  name: "1_pool"
  type: POOLING
  pooling_param {
    pool: MAX
    kernel_size: 2
    stride: 2
  }
}
layers {
  bottom: "1_pool"
  top: "2_0_conv"
  name: "2_0_conv"
  type: CONVOLUTION
  blobs_lr: 1
  blobs_lr: 2
  weight_decay: 0
  weight_decay: 0
  convolution_param {
    num_output: 16
    pad: 1
    kernel_size: 3
    stride: 1
    weight_filler {
      type: "xavier"
    }
    bias_filler {
      type: "constant"
      value: 0
    }
  }
}
layers {
  bottom: "2_0_conv"
  top: "2_0_conv"
  name: "2_0_conv_ReLU"
  type: RELU
}
layers {
  bottom: "2_0_conv"
  top: "2_1_conv"
  name: "2_1_conv"
  type: CONVOLUTION
  blobs_lr: 1
  blobs_lr: 2
  weight_decay: 0
  weight_decay: 0
  convolution_param {
    num_output: 16
    pad: 1
    kernel_size: 3
    stride: 1
    weight_filler {
      type: "xavier"
    }
    bias_filler {
      type: "constant"
      value: 0
    }
  }
}
layers {
  bottom: "2_1_conv"
  top: "2_1_conv"
  name: "2_1_conv_ReLU"
  type: RELU
}
layers {
  bottom: "2_1_conv"
  top: "2_pool"
  name: "2_pool"
  type: POOLING
  pooling_param {
    pool: MAX
    kernel_size: 2
    stride: 2
  }
}
layers {
  bottom: "2_pool"
  top: "middle_conv"
  name: "middle_conv"
  type: CONVOLUTION
  blobs_lr: 1
  blobs_lr: 2
  weight_decay: 0
  weight_decay: 0
  convolution_param {
    num_output: 50
    pad: 0
    kernel_size: 4
    stride: 1
    weight_filler {
      type: "xavier"
    }
    bias_filler {
      type: "constant"
      value: 0
    }
  }
}
layers {
  bottom: "middle_conv"
  top: "middle_conv"
  name: "middle_conv_ReLU"
  type: RELU
}
layers {
  bottom: "middle_conv"
  top: "fc1"
  name: "fc1"
  type: INNER_PRODUCT
  blobs_lr: 1
  blobs_lr: 2
  weight_decay: 0
  weight_decay: 0
  inner_product_param {
    num_output: 512
    weight_filler {
      type: "xavier"
    }
    bias_filler {
      type: "constant"
      value: 0
    }
  }
}
layers {
  bottom: "fc1"
  top: "fc1"
  name: "fc1_Dropout_ReLU"
  type: RELU
}
layers {
  bottom: "fc1"
  top: "fc2"
  name: "fc2"
  type: INNER_PRODUCT
  blobs_lr: 1
  blobs_lr: 2
  weight_decay: 0
  weight_decay: 0
  inner_product_param {
    num_output: 100
    weight_filler {
      type: "xavier"
    }
    bias_filler {
      type: "constant"
      value: 0
    }
  }
}
layers {
  bottom: "fc2"
  bottom: "label"
  top: "softmax"
  name: "softmax"
  type: SOFTMAX_LOSS
}
layers {
  bottom: "fc2"
  bottom: "label"
  top: "accuracy_top_1"
  name: "accuracy_top_1"
  type: ACCURACY
  accuracy_param {
    top_k: 1
  }
  include {
    phase: TEST
  }
}
layers {
  bottom: "fc2"
  bottom: "label"
  top: "accuracy_top_5"
  name: "accuracy_top_5"
  type: ACCURACY
  accuracy_param {
    top_k: 5
  }
  include {
    phase: TEST
  }
}
state {
  phase: TEST
}
I0614 16:00:00.680074  5063 layer_factory.hpp:78] Creating layer data
I0614 16:00:00.680081  5063 data_transformer.cpp:25] Loading mean file from/dataset/cifar100_lmdb_lab/mean.binaryproto
I0614 16:00:00.680112  5063 net.cpp:69] Creating Layer data
I0614 16:00:00.680117  5063 net.cpp:358] data -> data
I0614 16:00:00.680124  5063 net.cpp:358] data -> label
I0614 16:00:00.680138  5063 net.cpp:98] Setting up data
I0614 16:00:00.680143  5063 data_layer.cpp:32] Opening dataset /dataset/cifar100_lmdb_lab/cifar100_test_lmdb
I0614 16:00:00.680219  5063 data_layer.cpp:71] output data size: 128,3,32,32
I0614 16:00:00.680626  5063 net.cpp:105] Top shape: 128 3 32 32 (393216)
I0614 16:00:00.680634  5063 net.cpp:105] Top shape: 128 1 1 1 (128)
I0614 16:00:00.680637  5063 layer_factory.hpp:78] Creating layer label_data_1_split
I0614 16:00:00.680647  5063 net.cpp:69] Creating Layer label_data_1_split
I0614 16:00:00.680650  5063 net.cpp:396] label_data_1_split <- label
I0614 16:00:00.680655  5063 net.cpp:358] label_data_1_split -> label_data_1_split_0
I0614 16:00:00.680661  5063 net.cpp:358] label_data_1_split -> label_data_1_split_1
I0614 16:00:00.680666  5063 net.cpp:358] label_data_1_split -> label_data_1_split_2
I0614 16:00:00.680682  5063 net.cpp:98] Setting up label_data_1_split
I0614 16:00:00.680686  5063 net.cpp:105] Top shape: 128 1 1 1 (128)
I0614 16:00:00.680694  5063 net.cpp:105] Top shape: 128 1 1 1 (128)
I0614 16:00:00.680698  5063 net.cpp:105] Top shape: 128 1 1 1 (128)
I0614 16:00:00.680701  5063 layer_factory.hpp:78] Creating layer 0_0_conv
I0614 16:00:00.680706  5063 net.cpp:69] Creating Layer 0_0_conv
I0614 16:00:00.680709  5063 net.cpp:396] 0_0_conv <- data
I0614 16:00:00.680714  5063 net.cpp:358] 0_0_conv -> 0_0_conv
I0614 16:00:00.680719  5063 net.cpp:98] Setting up 0_0_conv
I0614 16:00:00.680732  5063 net.cpp:105] Top shape: 128 16 32 32 (2097152)
I0614 16:00:00.680737  5063 layer_factory.hpp:78] Creating layer 0_0_conv_ReLU
I0614 16:00:00.680742  5063 net.cpp:69] Creating Layer 0_0_conv_ReLU
I0614 16:00:00.680744  5063 net.cpp:396] 0_0_conv_ReLU <- 0_0_conv
I0614 16:00:00.680748  5063 net.cpp:347] 0_0_conv_ReLU -> 0_0_conv (in-place)
I0614 16:00:00.680752  5063 net.cpp:98] Setting up 0_0_conv_ReLU
I0614 16:00:00.680755  5063 net.cpp:105] Top shape: 128 16 32 32 (2097152)
I0614 16:00:00.680758  5063 layer_factory.hpp:78] Creating layer 0_1_conv
I0614 16:00:00.680763  5063 net.cpp:69] Creating Layer 0_1_conv
I0614 16:00:00.680766  5063 net.cpp:396] 0_1_conv <- 0_0_conv
I0614 16:00:00.680770  5063 net.cpp:358] 0_1_conv -> 0_1_conv
I0614 16:00:00.680775  5063 net.cpp:98] Setting up 0_1_conv
I0614 16:00:00.680794  5063 net.cpp:105] Top shape: 128 16 32 32 (2097152)
I0614 16:00:00.680799  5063 layer_factory.hpp:78] Creating layer 0_1_conv_ReLU
I0614 16:00:00.680804  5063 net.cpp:69] Creating Layer 0_1_conv_ReLU
I0614 16:00:00.680806  5063 net.cpp:396] 0_1_conv_ReLU <- 0_1_conv
I0614 16:00:00.680810  5063 net.cpp:347] 0_1_conv_ReLU -> 0_1_conv (in-place)
I0614 16:00:00.680814  5063 net.cpp:98] Setting up 0_1_conv_ReLU
I0614 16:00:00.680817  5063 net.cpp:105] Top shape: 128 16 32 32 (2097152)
I0614 16:00:00.680820  5063 layer_factory.hpp:78] Creating layer 0_pool
I0614 16:00:00.680824  5063 net.cpp:69] Creating Layer 0_pool
I0614 16:00:00.680826  5063 net.cpp:396] 0_pool <- 0_1_conv
I0614 16:00:00.680830  5063 net.cpp:358] 0_pool -> 0_pool
I0614 16:00:00.680835  5063 net.cpp:98] Setting up 0_pool
I0614 16:00:00.680838  5063 net.cpp:105] Top shape: 128 16 16 16 (524288)
I0614 16:00:00.680841  5063 layer_factory.hpp:78] Creating layer 1_0_conv
I0614 16:00:00.680845  5063 net.cpp:69] Creating Layer 1_0_conv
I0614 16:00:00.680848  5063 net.cpp:396] 1_0_conv <- 0_pool
I0614 16:00:00.680852  5063 net.cpp:358] 1_0_conv -> 1_0_conv
I0614 16:00:00.680857  5063 net.cpp:98] Setting up 1_0_conv
I0614 16:00:00.680874  5063 net.cpp:105] Top shape: 128 16 16 16 (524288)
I0614 16:00:00.680881  5063 layer_factory.hpp:78] Creating layer 1_0_conv_ReLU
I0614 16:00:00.680884  5063 net.cpp:69] Creating Layer 1_0_conv_ReLU
I0614 16:00:00.680887  5063 net.cpp:396] 1_0_conv_ReLU <- 1_0_conv
I0614 16:00:00.680891  5063 net.cpp:347] 1_0_conv_ReLU -> 1_0_conv (in-place)
I0614 16:00:00.680894  5063 net.cpp:98] Setting up 1_0_conv_ReLU
I0614 16:00:00.680897  5063 net.cpp:105] Top shape: 128 16 16 16 (524288)
I0614 16:00:00.680901  5063 layer_factory.hpp:78] Creating layer 1_1_conv
I0614 16:00:00.680904  5063 net.cpp:69] Creating Layer 1_1_conv
I0614 16:00:00.680907  5063 net.cpp:396] 1_1_conv <- 1_0_conv
I0614 16:00:00.680912  5063 net.cpp:358] 1_1_conv -> 1_1_conv
I0614 16:00:00.680917  5063 net.cpp:98] Setting up 1_1_conv
I0614 16:00:00.680933  5063 net.cpp:105] Top shape: 128 16 16 16 (524288)
I0614 16:00:00.680938  5063 layer_factory.hpp:78] Creating layer 1_1_conv_ReLU
I0614 16:00:00.680943  5063 net.cpp:69] Creating Layer 1_1_conv_ReLU
I0614 16:00:00.680944  5063 net.cpp:396] 1_1_conv_ReLU <- 1_1_conv
I0614 16:00:00.680948  5063 net.cpp:347] 1_1_conv_ReLU -> 1_1_conv (in-place)
I0614 16:00:00.680953  5063 net.cpp:98] Setting up 1_1_conv_ReLU
I0614 16:00:00.680954  5063 net.cpp:105] Top shape: 128 16 16 16 (524288)
I0614 16:00:00.680958  5063 layer_factory.hpp:78] Creating layer 1_pool
I0614 16:00:00.680960  5063 net.cpp:69] Creating Layer 1_pool
I0614 16:00:00.680966  5063 net.cpp:396] 1_pool <- 1_1_conv
I0614 16:00:00.680971  5063 net.cpp:358] 1_pool -> 1_pool
I0614 16:00:00.680979  5063 net.cpp:98] Setting up 1_pool
I0614 16:00:00.680982  5063 net.cpp:105] Top shape: 128 16 8 8 (131072)
I0614 16:00:00.680985  5063 layer_factory.hpp:78] Creating layer 2_0_conv
I0614 16:00:00.680989  5063 net.cpp:69] Creating Layer 2_0_conv
I0614 16:00:00.680992  5063 net.cpp:396] 2_0_conv <- 1_pool
I0614 16:00:00.680996  5063 net.cpp:358] 2_0_conv -> 2_0_conv
I0614 16:00:00.681000  5063 net.cpp:98] Setting up 2_0_conv
I0614 16:00:00.681017  5063 net.cpp:105] Top shape: 128 16 8 8 (131072)
I0614 16:00:00.681023  5063 layer_factory.hpp:78] Creating layer 2_0_conv_ReLU
I0614 16:00:00.681026  5063 net.cpp:69] Creating Layer 2_0_conv_ReLU
I0614 16:00:00.681030  5063 net.cpp:396] 2_0_conv_ReLU <- 2_0_conv
I0614 16:00:00.681033  5063 net.cpp:347] 2_0_conv_ReLU -> 2_0_conv (in-place)
I0614 16:00:00.681037  5063 net.cpp:98] Setting up 2_0_conv_ReLU
I0614 16:00:00.681040  5063 net.cpp:105] Top shape: 128 16 8 8 (131072)
I0614 16:00:00.681042  5063 layer_factory.hpp:78] Creating layer 2_1_conv
I0614 16:00:00.681046  5063 net.cpp:69] Creating Layer 2_1_conv
I0614 16:00:00.681048  5063 net.cpp:396] 2_1_conv <- 2_0_conv
I0614 16:00:00.681054  5063 net.cpp:358] 2_1_conv -> 2_1_conv
I0614 16:00:00.681058  5063 net.cpp:98] Setting up 2_1_conv
I0614 16:00:00.681076  5063 net.cpp:105] Top shape: 128 16 8 8 (131072)
I0614 16:00:00.681079  5063 layer_factory.hpp:78] Creating layer 2_1_conv_ReLU
I0614 16:00:00.681083  5063 net.cpp:69] Creating Layer 2_1_conv_ReLU
I0614 16:00:00.681085  5063 net.cpp:396] 2_1_conv_ReLU <- 2_1_conv
I0614 16:00:00.681089  5063 net.cpp:347] 2_1_conv_ReLU -> 2_1_conv (in-place)
I0614 16:00:00.681092  5063 net.cpp:98] Setting up 2_1_conv_ReLU
I0614 16:00:00.681095  5063 net.cpp:105] Top shape: 128 16 8 8 (131072)
I0614 16:00:00.681098  5063 layer_factory.hpp:78] Creating layer 2_pool
I0614 16:00:00.681103  5063 net.cpp:69] Creating Layer 2_pool
I0614 16:00:00.681107  5063 net.cpp:396] 2_pool <- 2_1_conv
I0614 16:00:00.681109  5063 net.cpp:358] 2_pool -> 2_pool
I0614 16:00:00.681113  5063 net.cpp:98] Setting up 2_pool
I0614 16:00:00.681118  5063 net.cpp:105] Top shape: 128 16 4 4 (32768)
I0614 16:00:00.681120  5063 layer_factory.hpp:78] Creating layer middle_conv
I0614 16:00:00.681123  5063 net.cpp:69] Creating Layer middle_conv
I0614 16:00:00.681126  5063 net.cpp:396] middle_conv <- 2_pool
I0614 16:00:00.681130  5063 net.cpp:358] middle_conv -> middle_conv
I0614 16:00:00.681134  5063 net.cpp:98] Setting up middle_conv
I0614 16:00:00.681203  5063 net.cpp:105] Top shape: 128 50 1 1 (6400)
I0614 16:00:00.681207  5063 layer_factory.hpp:78] Creating layer middle_conv_ReLU
I0614 16:00:00.681211  5063 net.cpp:69] Creating Layer middle_conv_ReLU
I0614 16:00:00.681215  5063 net.cpp:396] middle_conv_ReLU <- middle_conv
I0614 16:00:00.681217  5063 net.cpp:347] middle_conv_ReLU -> middle_conv (in-place)
I0614 16:00:00.681221  5063 net.cpp:98] Setting up middle_conv_ReLU
I0614 16:00:00.681224  5063 net.cpp:105] Top shape: 128 50 1 1 (6400)
I0614 16:00:00.681227  5063 layer_factory.hpp:78] Creating layer fc1
I0614 16:00:00.681234  5063 net.cpp:69] Creating Layer fc1
I0614 16:00:00.681237  5063 net.cpp:396] fc1 <- middle_conv
I0614 16:00:00.681241  5063 net.cpp:358] fc1 -> fc1
I0614 16:00:00.681247  5063 net.cpp:98] Setting up fc1
I0614 16:00:00.681390  5063 net.cpp:105] Top shape: 128 512 1 1 (65536)
I0614 16:00:00.681396  5063 layer_factory.hpp:78] Creating layer fc1_Dropout_ReLU
I0614 16:00:00.681399  5063 net.cpp:69] Creating Layer fc1_Dropout_ReLU
I0614 16:00:00.681402  5063 net.cpp:396] fc1_Dropout_ReLU <- fc1
I0614 16:00:00.681406  5063 net.cpp:347] fc1_Dropout_ReLU -> fc1 (in-place)
I0614 16:00:00.681411  5063 net.cpp:98] Setting up fc1_Dropout_ReLU
I0614 16:00:00.681413  5063 net.cpp:105] Top shape: 128 512 1 1 (65536)
I0614 16:00:00.681416  5063 layer_factory.hpp:78] Creating layer fc2
I0614 16:00:00.681421  5063 net.cpp:69] Creating Layer fc2
I0614 16:00:00.681423  5063 net.cpp:396] fc2 <- fc1
I0614 16:00:00.681429  5063 net.cpp:358] fc2 -> fc2
I0614 16:00:00.681434  5063 net.cpp:98] Setting up fc2
I0614 16:00:00.681712  5063 net.cpp:105] Top shape: 128 100 1 1 (12800)
I0614 16:00:00.681723  5063 layer_factory.hpp:78] Creating layer fc2_fc2_0_split
I0614 16:00:00.681727  5063 net.cpp:69] Creating Layer fc2_fc2_0_split
I0614 16:00:00.681730  5063 net.cpp:396] fc2_fc2_0_split <- fc2
I0614 16:00:00.681735  5063 net.cpp:358] fc2_fc2_0_split -> fc2_fc2_0_split_0
I0614 16:00:00.681740  5063 net.cpp:358] fc2_fc2_0_split -> fc2_fc2_0_split_1
I0614 16:00:00.681746  5063 net.cpp:358] fc2_fc2_0_split -> fc2_fc2_0_split_2
I0614 16:00:00.681751  5063 net.cpp:98] Setting up fc2_fc2_0_split
I0614 16:00:00.681753  5063 net.cpp:105] Top shape: 128 100 1 1 (12800)
I0614 16:00:00.681757  5063 net.cpp:105] Top shape: 128 100 1 1 (12800)
I0614 16:00:00.681761  5063 net.cpp:105] Top shape: 128 100 1 1 (12800)
I0614 16:00:00.681763  5063 layer_factory.hpp:78] Creating layer softmax
I0614 16:00:00.681771  5063 net.cpp:69] Creating Layer softmax
I0614 16:00:00.681773  5063 net.cpp:396] softmax <- fc2_fc2_0_split_0
I0614 16:00:00.681777  5063 net.cpp:396] softmax <- label_data_1_split_0
I0614 16:00:00.681782  5063 net.cpp:358] softmax -> softmax
I0614 16:00:00.681787  5063 net.cpp:98] Setting up softmax
I0614 16:00:00.681792  5063 net.cpp:105] Top shape: 1 1 1 1 (1)
I0614 16:00:00.681795  5063 net.cpp:111]     with loss weight 1
I0614 16:00:00.681800  5063 layer_factory.hpp:78] Creating layer accuracy_top_1
I0614 16:00:00.681807  5063 net.cpp:69] Creating Layer accuracy_top_1
I0614 16:00:00.681810  5063 net.cpp:396] accuracy_top_1 <- fc2_fc2_0_split_1
I0614 16:00:00.681813  5063 net.cpp:396] accuracy_top_1 <- label_data_1_split_1
I0614 16:00:00.681819  5063 net.cpp:358] accuracy_top_1 -> accuracy_top_1
I0614 16:00:00.681824  5063 net.cpp:98] Setting up accuracy_top_1
I0614 16:00:00.681828  5063 net.cpp:105] Top shape: 1 1 1 1 (1)
I0614 16:00:00.681831  5063 layer_factory.hpp:78] Creating layer accuracy_top_5
I0614 16:00:00.681835  5063 net.cpp:69] Creating Layer accuracy_top_5
I0614 16:00:00.681838  5063 net.cpp:396] accuracy_top_5 <- fc2_fc2_0_split_2
I0614 16:00:00.681843  5063 net.cpp:396] accuracy_top_5 <- label_data_1_split_2
I0614 16:00:00.681848  5063 net.cpp:358] accuracy_top_5 -> accuracy_top_5
I0614 16:00:00.681851  5063 net.cpp:98] Setting up accuracy_top_5
I0614 16:00:00.681854  5063 net.cpp:105] Top shape: 1 1 1 1 (1)
I0614 16:00:00.681857  5063 net.cpp:174] accuracy_top_5 does not need backward computation.
I0614 16:00:00.681860  5063 net.cpp:174] accuracy_top_1 does not need backward computation.
I0614 16:00:00.681864  5063 net.cpp:172] softmax needs backward computation.
I0614 16:00:00.681866  5063 net.cpp:172] fc2_fc2_0_split needs backward computation.
I0614 16:00:00.681869  5063 net.cpp:172] fc2 needs backward computation.
I0614 16:00:00.681872  5063 net.cpp:172] fc1_Dropout_ReLU needs backward computation.
I0614 16:00:00.681875  5063 net.cpp:172] fc1 needs backward computation.
I0614 16:00:00.681879  5063 net.cpp:172] middle_conv_ReLU needs backward computation.
I0614 16:00:00.681884  5063 net.cpp:172] middle_conv needs backward computation.
I0614 16:00:00.681888  5063 net.cpp:172] 2_pool needs backward computation.
I0614 16:00:00.681891  5063 net.cpp:172] 2_1_conv_ReLU needs backward computation.
I0614 16:00:00.681895  5063 net.cpp:172] 2_1_conv needs backward computation.
I0614 16:00:00.681897  5063 net.cpp:172] 2_0_conv_ReLU needs backward computation.
I0614 16:00:00.681901  5063 net.cpp:172] 2_0_conv needs backward computation.
I0614 16:00:00.681905  5063 net.cpp:172] 1_pool needs backward computation.
I0614 16:00:00.681907  5063 net.cpp:172] 1_1_conv_ReLU needs backward computation.
I0614 16:00:00.681910  5063 net.cpp:172] 1_1_conv needs backward computation.
I0614 16:00:00.681913  5063 net.cpp:172] 1_0_conv_ReLU needs backward computation.
I0614 16:00:00.681916  5063 net.cpp:172] 1_0_conv needs backward computation.
I0614 16:00:00.681920  5063 net.cpp:172] 0_pool needs backward computation.
I0614 16:00:00.681924  5063 net.cpp:172] 0_1_conv_ReLU needs backward computation.
I0614 16:00:00.681928  5063 net.cpp:172] 0_1_conv needs backward computation.
I0614 16:00:00.681934  5063 net.cpp:172] 0_0_conv_ReLU needs backward computation.
I0614 16:00:00.681937  5063 net.cpp:172] 0_0_conv needs backward computation.
I0614 16:00:00.681941  5063 net.cpp:174] label_data_1_split does not need backward computation.
I0614 16:00:00.681944  5063 net.cpp:174] data does not need backward computation.
I0614 16:00:00.681947  5063 net.cpp:210] This network produces output accuracy_top_1
I0614 16:00:00.681951  5063 net.cpp:210] This network produces output accuracy_top_5
I0614 16:00:00.681954  5063 net.cpp:210] This network produces output softmax
I0614 16:00:00.681968  5063 net.cpp:469] Collecting Learning Rate and Weight Decay.
I0614 16:00:00.681974  5063 net.cpp:221] Network initialization done.
I0614 16:00:00.681977  5063 net.cpp:222] Memory required for data: 49147916
I0614 16:00:00.682026  5063 solver.cpp:42] Solver scaffolding done.
I0614 16:00:00.682047  5063 caffe.cpp:115] Finetuning from snapshots/16-06-14_15h51m21s_0_10_pretrainClassificationFrozen_iter_3000.caffemodel
I0614 16:00:00.682632  5063 solver.cpp:247] Solving 
I0614 16:00:00.682641  5063 solver.cpp:248] Learning Rate Policy: fixed
I0614 16:00:00.683099  5063 solver.cpp:291] Iteration 0, Testing net (#0)
I0614 16:00:00.791652  5063 solver.cpp:342]     Test net output #0: accuracy_top_1 = 0.046875
I0614 16:00:00.791671  5063 solver.cpp:342]     Test net output #1: accuracy_top_5 = 0.189063
I0614 16:00:00.791676  5063 solver.cpp:342]     Test net output #2: softmax = 4.28595 (* 1 = 4.28595 loss)
I0614 16:00:00.853970  5063 solver.cpp:213] Iteration 0, loss = 4.33259
I0614 16:00:00.853986  5063 solver.cpp:228]     Train net output #0: softmax = 4.33259 (* 1 = 4.33259 loss)
I0614 16:00:00.853994  5063 solver.cpp:473] Iteration 0, lr = 0.0001
I0614 16:00:01.736870  5063 solver.cpp:213] Iteration 10, loss = 4.22405
I0614 16:00:01.736891  5063 solver.cpp:228]     Train net output #0: softmax = 4.22405 (* 1 = 4.22405 loss)
I0614 16:00:01.736896  5063 solver.cpp:473] Iteration 10, lr = 0.0001
I0614 16:00:02.619673  5063 solver.cpp:213] Iteration 20, loss = 4.22066
I0614 16:00:02.619694  5063 solver.cpp:228]     Train net output #0: softmax = 4.22066 (* 1 = 4.22066 loss)
I0614 16:00:02.619700  5063 solver.cpp:473] Iteration 20, lr = 0.0001
I0614 16:00:03.502205  5063 solver.cpp:213] Iteration 30, loss = 4.25821
I0614 16:00:03.502226  5063 solver.cpp:228]     Train net output #0: softmax = 4.25821 (* 1 = 4.25821 loss)
I0614 16:00:03.502233  5063 solver.cpp:473] Iteration 30, lr = 0.0001
I0614 16:00:04.384861  5063 solver.cpp:213] Iteration 40, loss = 4.26077
I0614 16:00:04.384882  5063 solver.cpp:228]     Train net output #0: softmax = 4.26077 (* 1 = 4.26077 loss)
I0614 16:00:04.384887  5063 solver.cpp:473] Iteration 40, lr = 0.0001
I0614 16:00:05.267478  5063 solver.cpp:213] Iteration 50, loss = 4.34194
I0614 16:00:05.267499  5063 solver.cpp:228]     Train net output #0: softmax = 4.34194 (* 1 = 4.34194 loss)
I0614 16:00:05.267505  5063 solver.cpp:473] Iteration 50, lr = 0.0001
I0614 16:00:06.150776  5063 solver.cpp:213] Iteration 60, loss = 4.36768
I0614 16:00:06.150804  5063 solver.cpp:228]     Train net output #0: softmax = 4.36768 (* 1 = 4.36768 loss)
I0614 16:00:06.150931  5063 solver.cpp:473] Iteration 60, lr = 0.0001
I0614 16:00:07.034147  5063 solver.cpp:213] Iteration 70, loss = 4.2753
I0614 16:00:07.034169  5063 solver.cpp:228]     Train net output #0: softmax = 4.2753 (* 1 = 4.2753 loss)
I0614 16:00:07.034174  5063 solver.cpp:473] Iteration 70, lr = 0.0001
I0614 16:00:07.918697  5063 solver.cpp:213] Iteration 80, loss = 4.26533
I0614 16:00:07.918718  5063 solver.cpp:228]     Train net output #0: softmax = 4.26533 (* 1 = 4.26533 loss)
I0614 16:00:07.918723  5063 solver.cpp:473] Iteration 80, lr = 0.0001
I0614 16:00:08.801870  5063 solver.cpp:213] Iteration 90, loss = 4.17456
I0614 16:00:08.801893  5063 solver.cpp:228]     Train net output #0: softmax = 4.17456 (* 1 = 4.17456 loss)
I0614 16:00:08.801905  5063 solver.cpp:473] Iteration 90, lr = 0.0001
I0614 16:00:09.685349  5063 solver.cpp:213] Iteration 100, loss = 4.22361
I0614 16:00:09.685385  5063 solver.cpp:228]     Train net output #0: softmax = 4.22361 (* 1 = 4.22361 loss)
I0614 16:00:09.685391  5063 solver.cpp:473] Iteration 100, lr = 0.0001
I0614 16:00:10.568827  5063 solver.cpp:213] Iteration 110, loss = 4.16651
I0614 16:00:10.568850  5063 solver.cpp:228]     Train net output #0: softmax = 4.16651 (* 1 = 4.16651 loss)
I0614 16:00:10.568856  5063 solver.cpp:473] Iteration 110, lr = 0.0001
I0614 16:00:11.452651  5063 solver.cpp:213] Iteration 120, loss = 4.20051
I0614 16:00:11.452673  5063 solver.cpp:228]     Train net output #0: softmax = 4.20051 (* 1 = 4.20051 loss)
I0614 16:00:11.452678  5063 solver.cpp:473] Iteration 120, lr = 0.0001
I0614 16:00:12.336499  5063 solver.cpp:213] Iteration 130, loss = 4.18475
I0614 16:00:12.336519  5063 solver.cpp:228]     Train net output #0: softmax = 4.18475 (* 1 = 4.18475 loss)
I0614 16:00:12.336524  5063 solver.cpp:473] Iteration 130, lr = 0.0001
I0614 16:00:13.219842  5063 solver.cpp:213] Iteration 140, loss = 4.29463
I0614 16:00:13.219863  5063 solver.cpp:228]     Train net output #0: softmax = 4.29463 (* 1 = 4.29463 loss)
I0614 16:00:13.219868  5063 solver.cpp:473] Iteration 140, lr = 0.0001
I0614 16:00:14.103332  5063 solver.cpp:213] Iteration 150, loss = 4.15896
I0614 16:00:14.103350  5063 solver.cpp:228]     Train net output #0: softmax = 4.15896 (* 1 = 4.15896 loss)
I0614 16:00:14.103355  5063 solver.cpp:473] Iteration 150, lr = 0.0001
I0614 16:00:14.986594  5063 solver.cpp:213] Iteration 160, loss = 4.20294
I0614 16:00:14.986613  5063 solver.cpp:228]     Train net output #0: softmax = 4.20294 (* 1 = 4.20294 loss)
I0614 16:00:14.986618  5063 solver.cpp:473] Iteration 160, lr = 0.0001
I0614 16:00:15.869556  5063 solver.cpp:213] Iteration 170, loss = 4.18973
I0614 16:00:15.869573  5063 solver.cpp:228]     Train net output #0: softmax = 4.18973 (* 1 = 4.18973 loss)
I0614 16:00:15.869578  5063 solver.cpp:473] Iteration 170, lr = 0.0001
I0614 16:00:16.752813  5063 solver.cpp:213] Iteration 180, loss = 4.22075
I0614 16:00:16.752836  5063 solver.cpp:228]     Train net output #0: softmax = 4.22075 (* 1 = 4.22075 loss)
I0614 16:00:16.752960  5063 solver.cpp:473] Iteration 180, lr = 0.0001
I0614 16:00:17.636649  5063 solver.cpp:213] Iteration 190, loss = 4.22786
I0614 16:00:17.636667  5063 solver.cpp:228]     Train net output #0: softmax = 4.22786 (* 1 = 4.22786 loss)
I0614 16:00:17.636672  5063 solver.cpp:473] Iteration 190, lr = 0.0001
I0614 16:00:18.520627  5063 solver.cpp:213] Iteration 200, loss = 4.08261
I0614 16:00:18.520647  5063 solver.cpp:228]     Train net output #0: softmax = 4.08261 (* 1 = 4.08261 loss)
I0614 16:00:18.520651  5063 solver.cpp:473] Iteration 200, lr = 0.0001
I0614 16:00:19.404538  5063 solver.cpp:213] Iteration 210, loss = 3.96737
I0614 16:00:19.404556  5063 solver.cpp:228]     Train net output #0: softmax = 3.96737 (* 1 = 3.96737 loss)
I0614 16:00:19.404559  5063 solver.cpp:473] Iteration 210, lr = 0.0001
I0614 16:00:20.287811  5063 solver.cpp:213] Iteration 220, loss = 4.0559
I0614 16:00:20.287830  5063 solver.cpp:228]     Train net output #0: softmax = 4.0559 (* 1 = 4.0559 loss)
I0614 16:00:20.287835  5063 solver.cpp:473] Iteration 220, lr = 0.0001
I0614 16:00:21.171681  5063 solver.cpp:213] Iteration 230, loss = 4.17227
I0614 16:00:21.171700  5063 solver.cpp:228]     Train net output #0: softmax = 4.17227 (* 1 = 4.17227 loss)
I0614 16:00:21.171705  5063 solver.cpp:473] Iteration 230, lr = 0.0001
I0614 16:00:22.054988  5063 solver.cpp:213] Iteration 240, loss = 4.14992
I0614 16:00:22.055009  5063 solver.cpp:228]     Train net output #0: softmax = 4.14992 (* 1 = 4.14992 loss)
I0614 16:00:22.055135  5063 solver.cpp:473] Iteration 240, lr = 0.0001
I0614 16:00:22.938542  5063 solver.cpp:213] Iteration 250, loss = 4.11259
I0614 16:00:22.938561  5063 solver.cpp:228]     Train net output #0: softmax = 4.11259 (* 1 = 4.11259 loss)
I0614 16:00:22.938566  5063 solver.cpp:473] Iteration 250, lr = 0.0001
I0614 16:00:23.822329  5063 solver.cpp:213] Iteration 260, loss = 4.234
I0614 16:00:23.822360  5063 solver.cpp:228]     Train net output #0: softmax = 4.234 (* 1 = 4.234 loss)
I0614 16:00:23.822365  5063 solver.cpp:473] Iteration 260, lr = 0.0001
I0614 16:00:24.706027  5063 solver.cpp:213] Iteration 270, loss = 4.20694
I0614 16:00:24.706043  5063 solver.cpp:228]     Train net output #0: softmax = 4.20694 (* 1 = 4.20694 loss)
I0614 16:00:24.706048  5063 solver.cpp:473] Iteration 270, lr = 0.0001
I0614 16:00:25.590253  5063 solver.cpp:213] Iteration 280, loss = 4.11667
I0614 16:00:25.590276  5063 solver.cpp:228]     Train net output #0: softmax = 4.11667 (* 1 = 4.11667 loss)
I0614 16:00:25.590282  5063 solver.cpp:473] Iteration 280, lr = 0.0001
I0614 16:00:26.473312  5063 solver.cpp:213] Iteration 290, loss = 4.12237
I0614 16:00:26.473332  5063 solver.cpp:228]     Train net output #0: softmax = 4.12237 (* 1 = 4.12237 loss)
I0614 16:00:26.473337  5063 solver.cpp:473] Iteration 290, lr = 0.0001
I0614 16:00:27.357717  5063 solver.cpp:213] Iteration 300, loss = 4.19026
I0614 16:00:27.357739  5063 solver.cpp:228]     Train net output #0: softmax = 4.19026 (* 1 = 4.19026 loss)
I0614 16:00:27.357749  5063 solver.cpp:473] Iteration 300, lr = 0.0001
I0614 16:00:28.240859  5063 solver.cpp:213] Iteration 310, loss = 4.12111
I0614 16:00:28.240878  5063 solver.cpp:228]     Train net output #0: softmax = 4.12111 (* 1 = 4.12111 loss)
I0614 16:00:28.240883  5063 solver.cpp:473] Iteration 310, lr = 0.0001
I0614 16:00:29.124754  5063 solver.cpp:213] Iteration 320, loss = 4.01809
I0614 16:00:29.124773  5063 solver.cpp:228]     Train net output #0: softmax = 4.01809 (* 1 = 4.01809 loss)
I0614 16:00:29.124778  5063 solver.cpp:473] Iteration 320, lr = 0.0001
I0614 16:00:30.007477  5063 solver.cpp:213] Iteration 330, loss = 4.10832
I0614 16:00:30.007494  5063 solver.cpp:228]     Train net output #0: softmax = 4.10832 (* 1 = 4.10832 loss)
I0614 16:00:30.007499  5063 solver.cpp:473] Iteration 330, lr = 0.0001
I0614 16:00:30.890444  5063 solver.cpp:213] Iteration 340, loss = 4.25405
I0614 16:00:30.890508  5063 solver.cpp:228]     Train net output #0: softmax = 4.25405 (* 1 = 4.25405 loss)
I0614 16:00:30.890523  5063 solver.cpp:473] Iteration 340, lr = 0.0001
I0614 16:00:31.773926  5063 solver.cpp:213] Iteration 350, loss = 4.01559
I0614 16:00:31.773944  5063 solver.cpp:228]     Train net output #0: softmax = 4.01559 (* 1 = 4.01559 loss)
I0614 16:00:31.773948  5063 solver.cpp:473] Iteration 350, lr = 0.0001
I0614 16:00:32.656980  5063 solver.cpp:213] Iteration 360, loss = 4.11263
I0614 16:00:32.656999  5063 solver.cpp:228]     Train net output #0: softmax = 4.11263 (* 1 = 4.11263 loss)
I0614 16:00:32.657004  5063 solver.cpp:473] Iteration 360, lr = 0.0001
I0614 16:00:33.540853  5063 solver.cpp:213] Iteration 370, loss = 4.23348
I0614 16:00:33.540875  5063 solver.cpp:228]     Train net output #0: softmax = 4.23348 (* 1 = 4.23348 loss)
I0614 16:00:33.540881  5063 solver.cpp:473] Iteration 370, lr = 0.0001
I0614 16:00:34.424414  5063 solver.cpp:213] Iteration 380, loss = 4.22323
I0614 16:00:34.424435  5063 solver.cpp:228]     Train net output #0: softmax = 4.22323 (* 1 = 4.22323 loss)
I0614 16:00:34.424440  5063 solver.cpp:473] Iteration 380, lr = 0.0001
I0614 16:00:35.307842  5063 solver.cpp:213] Iteration 390, loss = 4.146
I0614 16:00:35.307862  5063 solver.cpp:228]     Train net output #0: softmax = 4.146 (* 1 = 4.146 loss)
I0614 16:00:35.307868  5063 solver.cpp:473] Iteration 390, lr = 0.0001
I0614 16:00:36.191771  5063 solver.cpp:213] Iteration 400, loss = 4.11175
I0614 16:00:36.191788  5063 solver.cpp:228]     Train net output #0: softmax = 4.11175 (* 1 = 4.11175 loss)
I0614 16:00:36.191793  5063 solver.cpp:473] Iteration 400, lr = 0.0001
I0614 16:00:37.075352  5063 solver.cpp:213] Iteration 410, loss = 4.11278
I0614 16:00:37.075378  5063 solver.cpp:228]     Train net output #0: softmax = 4.11278 (* 1 = 4.11278 loss)
I0614 16:00:37.075383  5063 solver.cpp:473] Iteration 410, lr = 0.0001
I0614 16:00:37.958340  5063 solver.cpp:213] Iteration 420, loss = 4.02485
I0614 16:00:37.958554  5063 solver.cpp:228]     Train net output #0: softmax = 4.02485 (* 1 = 4.02485 loss)
I0614 16:00:37.958561  5063 solver.cpp:473] Iteration 420, lr = 0.0001
I0614 16:00:38.842294  5063 solver.cpp:213] Iteration 430, loss = 4.2476
I0614 16:00:38.842313  5063 solver.cpp:228]     Train net output #0: softmax = 4.2476 (* 1 = 4.2476 loss)
I0614 16:00:38.842317  5063 solver.cpp:473] Iteration 430, lr = 0.0001
I0614 16:00:39.726446  5063 solver.cpp:213] Iteration 440, loss = 4.35815
I0614 16:00:39.726467  5063 solver.cpp:228]     Train net output #0: softmax = 4.35815 (* 1 = 4.35815 loss)
I0614 16:00:39.726472  5063 solver.cpp:473] Iteration 440, lr = 0.0001
I0614 16:00:40.609828  5063 solver.cpp:213] Iteration 450, loss = 4.18018
I0614 16:00:40.609851  5063 solver.cpp:228]     Train net output #0: softmax = 4.18018 (* 1 = 4.18018 loss)
I0614 16:00:40.609856  5063 solver.cpp:473] Iteration 450, lr = 0.0001
I0614 16:00:41.493896  5063 solver.cpp:213] Iteration 460, loss = 4.11062
I0614 16:00:41.493913  5063 solver.cpp:228]     Train net output #0: softmax = 4.11062 (* 1 = 4.11062 loss)
I0614 16:00:41.493918  5063 solver.cpp:473] Iteration 460, lr = 0.0001
I0614 16:00:42.377436  5063 solver.cpp:213] Iteration 470, loss = 4.02389
I0614 16:00:42.377455  5063 solver.cpp:228]     Train net output #0: softmax = 4.02389 (* 1 = 4.02389 loss)
I0614 16:00:42.377460  5063 solver.cpp:473] Iteration 470, lr = 0.0001
I0614 16:00:43.261862  5063 solver.cpp:213] Iteration 480, loss = 4.04386
I0614 16:00:43.261884  5063 solver.cpp:228]     Train net output #0: softmax = 4.04386 (* 1 = 4.04386 loss)
I0614 16:00:43.261894  5063 solver.cpp:473] Iteration 480, lr = 0.0001
I0614 16:00:44.144680  5063 solver.cpp:213] Iteration 490, loss = 4.06778
I0614 16:00:44.144698  5063 solver.cpp:228]     Train net output #0: softmax = 4.06778 (* 1 = 4.06778 loss)
I0614 16:00:44.144703  5063 solver.cpp:473] Iteration 490, lr = 0.0001
I0614 16:00:45.027817  5063 solver.cpp:213] Iteration 500, loss = 4.11612
I0614 16:00:45.027834  5063 solver.cpp:228]     Train net output #0: softmax = 4.11612 (* 1 = 4.11612 loss)
I0614 16:00:45.027856  5063 solver.cpp:473] Iteration 500, lr = 0.0001
I0614 16:00:45.911948  5063 solver.cpp:213] Iteration 510, loss = 4.14403
I0614 16:00:45.911970  5063 solver.cpp:228]     Train net output #0: softmax = 4.14403 (* 1 = 4.14403 loss)
I0614 16:00:45.911977  5063 solver.cpp:473] Iteration 510, lr = 0.0001
I0614 16:00:46.795861  5063 solver.cpp:213] Iteration 520, loss = 4.1172
I0614 16:00:46.795878  5063 solver.cpp:228]     Train net output #0: softmax = 4.1172 (* 1 = 4.1172 loss)
I0614 16:00:46.795882  5063 solver.cpp:473] Iteration 520, lr = 0.0001
I0614 16:00:47.678465  5063 solver.cpp:213] Iteration 530, loss = 4.16743
I0614 16:00:47.678483  5063 solver.cpp:228]     Train net output #0: softmax = 4.16743 (* 1 = 4.16743 loss)
I0614 16:00:47.678488  5063 solver.cpp:473] Iteration 530, lr = 0.0001
I0614 16:00:48.562144  5063 solver.cpp:213] Iteration 540, loss = 4.06569
I0614 16:00:48.562165  5063 solver.cpp:228]     Train net output #0: softmax = 4.06569 (* 1 = 4.06569 loss)
I0614 16:00:48.562170  5063 solver.cpp:473] Iteration 540, lr = 0.0001
I0614 16:00:49.445618  5063 solver.cpp:213] Iteration 550, loss = 4.2156
I0614 16:00:49.445636  5063 solver.cpp:228]     Train net output #0: softmax = 4.2156 (* 1 = 4.2156 loss)
I0614 16:00:49.445641  5063 solver.cpp:473] Iteration 550, lr = 0.0001
I0614 16:00:50.329876  5063 solver.cpp:213] Iteration 560, loss = 4.1068
I0614 16:00:50.329895  5063 solver.cpp:228]     Train net output #0: softmax = 4.1068 (* 1 = 4.1068 loss)
I0614 16:00:50.329901  5063 solver.cpp:473] Iteration 560, lr = 0.0001
I0614 16:00:51.213692  5063 solver.cpp:213] Iteration 570, loss = 4.03612
I0614 16:00:51.213712  5063 solver.cpp:228]     Train net output #0: softmax = 4.03612 (* 1 = 4.03612 loss)
I0614 16:00:51.213717  5063 solver.cpp:473] Iteration 570, lr = 0.0001
I0614 16:00:52.097342  5063 solver.cpp:213] Iteration 580, loss = 4.20372
I0614 16:00:52.097367  5063 solver.cpp:228]     Train net output #0: softmax = 4.20372 (* 1 = 4.20372 loss)
I0614 16:00:52.097373  5063 solver.cpp:473] Iteration 580, lr = 0.0001
I0614 16:00:52.980453  5063 solver.cpp:213] Iteration 590, loss = 4.04733
I0614 16:00:52.980468  5063 solver.cpp:228]     Train net output #0: softmax = 4.04733 (* 1 = 4.04733 loss)
I0614 16:00:52.980473  5063 solver.cpp:473] Iteration 590, lr = 0.0001
I0614 16:00:53.863025  5063 solver.cpp:213] Iteration 600, loss = 3.90967
I0614 16:00:53.863049  5063 solver.cpp:228]     Train net output #0: softmax = 3.90967 (* 1 = 3.90967 loss)
I0614 16:00:53.863173  5063 solver.cpp:473] Iteration 600, lr = 0.0001
I0614 16:00:54.746438  5063 solver.cpp:213] Iteration 610, loss = 4.10245
I0614 16:00:54.746459  5063 solver.cpp:228]     Train net output #0: softmax = 4.10245 (* 1 = 4.10245 loss)
I0614 16:00:54.746464  5063 solver.cpp:473] Iteration 610, lr = 0.0001
I0614 16:00:55.630636  5063 solver.cpp:213] Iteration 620, loss = 4.10625
I0614 16:00:55.630661  5063 solver.cpp:228]     Train net output #0: softmax = 4.10625 (* 1 = 4.10625 loss)
I0614 16:00:55.630666  5063 solver.cpp:473] Iteration 620, lr = 0.0001
I0614 16:00:56.513622  5063 solver.cpp:213] Iteration 630, loss = 4.11317
I0614 16:00:56.513641  5063 solver.cpp:228]     Train net output #0: softmax = 4.11317 (* 1 = 4.11317 loss)
I0614 16:00:56.513646  5063 solver.cpp:473] Iteration 630, lr = 0.0001
I0614 16:00:57.396728  5063 solver.cpp:213] Iteration 640, loss = 4.06904
I0614 16:00:57.396747  5063 solver.cpp:228]     Train net output #0: softmax = 4.06904 (* 1 = 4.06904 loss)
I0614 16:00:57.396752  5063 solver.cpp:473] Iteration 640, lr = 0.0001
I0614 16:00:58.281112  5063 solver.cpp:213] Iteration 650, loss = 4.01919
I0614 16:00:58.281129  5063 solver.cpp:228]     Train net output #0: softmax = 4.01919 (* 1 = 4.01919 loss)
I0614 16:00:58.281133  5063 solver.cpp:473] Iteration 650, lr = 0.0001
I0614 16:00:59.164695  5063 solver.cpp:213] Iteration 660, loss = 3.97757
I0614 16:00:59.164716  5063 solver.cpp:228]     Train net output #0: softmax = 3.97757 (* 1 = 3.97757 loss)
I0614 16:00:59.164849  5063 solver.cpp:473] Iteration 660, lr = 0.0001
I0614 16:01:00.048676  5063 solver.cpp:213] Iteration 670, loss = 4.05093
I0614 16:01:00.048701  5063 solver.cpp:228]     Train net output #0: softmax = 4.05093 (* 1 = 4.05093 loss)
I0614 16:01:00.048705  5063 solver.cpp:473] Iteration 670, lr = 0.0001
I0614 16:01:00.932173  5063 solver.cpp:213] Iteration 680, loss = 4.04675
I0614 16:01:00.932206  5063 solver.cpp:228]     Train net output #0: softmax = 4.04675 (* 1 = 4.04675 loss)
I0614 16:01:00.932212  5063 solver.cpp:473] Iteration 680, lr = 0.0001
I0614 16:01:01.815834  5063 solver.cpp:213] Iteration 690, loss = 4.21261
I0614 16:01:01.815847  5063 solver.cpp:228]     Train net output #0: softmax = 4.21261 (* 1 = 4.21261 loss)
I0614 16:01:01.815852  5063 solver.cpp:473] Iteration 690, lr = 0.0001
I0614 16:01:02.700229  5063 solver.cpp:213] Iteration 700, loss = 4.12488
I0614 16:01:02.700243  5063 solver.cpp:228]     Train net output #0: softmax = 4.12488 (* 1 = 4.12488 loss)
I0614 16:01:02.700248  5063 solver.cpp:473] Iteration 700, lr = 0.0001
I0614 16:01:03.584512  5063 solver.cpp:213] Iteration 710, loss = 4.13782
I0614 16:01:03.584527  5063 solver.cpp:228]     Train net output #0: softmax = 4.13782 (* 1 = 4.13782 loss)
I0614 16:01:03.584532  5063 solver.cpp:473] Iteration 710, lr = 0.0001
I0614 16:01:04.467926  5063 solver.cpp:213] Iteration 720, loss = 4.05599
I0614 16:01:04.467941  5063 solver.cpp:228]     Train net output #0: softmax = 4.05599 (* 1 = 4.05599 loss)
I0614 16:01:04.467946  5063 solver.cpp:473] Iteration 720, lr = 0.0001
I0614 16:01:05.351392  5063 solver.cpp:213] Iteration 730, loss = 4.1896
I0614 16:01:05.351407  5063 solver.cpp:228]     Train net output #0: softmax = 4.1896 (* 1 = 4.1896 loss)
I0614 16:01:05.351411  5063 solver.cpp:473] Iteration 730, lr = 0.0001
I0614 16:01:06.234406  5063 solver.cpp:213] Iteration 740, loss = 3.97378
I0614 16:01:06.234421  5063 solver.cpp:228]     Train net output #0: softmax = 3.97378 (* 1 = 3.97378 loss)
I0614 16:01:06.234431  5063 solver.cpp:473] Iteration 740, lr = 0.0001
I0614 16:01:07.117143  5063 solver.cpp:213] Iteration 750, loss = 4.16379
I0614 16:01:07.117157  5063 solver.cpp:228]     Train net output #0: softmax = 4.16379 (* 1 = 4.16379 loss)
I0614 16:01:07.117161  5063 solver.cpp:473] Iteration 750, lr = 0.0001
I0614 16:01:08.000365  5063 solver.cpp:213] Iteration 760, loss = 4.12414
I0614 16:01:08.000378  5063 solver.cpp:228]     Train net output #0: softmax = 4.12414 (* 1 = 4.12414 loss)
I0614 16:01:08.000382  5063 solver.cpp:473] Iteration 760, lr = 0.0001
I0614 16:01:08.883905  5063 solver.cpp:213] Iteration 770, loss = 4.11271
I0614 16:01:08.883920  5063 solver.cpp:228]     Train net output #0: softmax = 4.11271 (* 1 = 4.11271 loss)
I0614 16:01:08.883924  5063 solver.cpp:473] Iteration 770, lr = 0.0001
I0614 16:01:09.767911  5063 solver.cpp:213] Iteration 780, loss = 4.07552
I0614 16:01:09.767930  5063 solver.cpp:228]     Train net output #0: softmax = 4.07552 (* 1 = 4.07552 loss)
I0614 16:01:09.768070  5063 solver.cpp:473] Iteration 780, lr = 0.0001
I0614 16:01:10.651569  5063 solver.cpp:213] Iteration 790, loss = 3.9523
I0614 16:01:10.651584  5063 solver.cpp:228]     Train net output #0: softmax = 3.9523 (* 1 = 3.9523 loss)
I0614 16:01:10.651589  5063 solver.cpp:473] Iteration 790, lr = 0.0001
I0614 16:01:11.545599  5063 solver.cpp:213] Iteration 800, loss = 4.09105
I0614 16:01:11.545620  5063 solver.cpp:228]     Train net output #0: softmax = 4.09105 (* 1 = 4.09105 loss)
I0614 16:01:11.545625  5063 solver.cpp:473] Iteration 800, lr = 0.0001
I0614 16:01:12.438585  5063 solver.cpp:213] Iteration 810, loss = 4.03727
I0614 16:01:12.438608  5063 solver.cpp:228]     Train net output #0: softmax = 4.03727 (* 1 = 4.03727 loss)
I0614 16:01:12.438613  5063 solver.cpp:473] Iteration 810, lr = 0.0001
I0614 16:01:13.332041  5063 solver.cpp:213] Iteration 820, loss = 3.94533
I0614 16:01:13.332062  5063 solver.cpp:228]     Train net output #0: softmax = 3.94533 (* 1 = 3.94533 loss)
I0614 16:01:13.332067  5063 solver.cpp:473] Iteration 820, lr = 0.0001
I0614 16:01:14.215396  5063 solver.cpp:213] Iteration 830, loss = 4.07801
I0614 16:01:14.215411  5063 solver.cpp:228]     Train net output #0: softmax = 4.07801 (* 1 = 4.07801 loss)
I0614 16:01:14.215416  5063 solver.cpp:473] Iteration 830, lr = 0.0001
I0614 16:01:15.102366  5063 solver.cpp:213] Iteration 840, loss = 4.01524
I0614 16:01:15.102391  5063 solver.cpp:228]     Train net output #0: softmax = 4.01524 (* 1 = 4.01524 loss)
I0614 16:01:15.102535  5063 solver.cpp:473] Iteration 840, lr = 0.0001
I0614 16:01:15.986280  5063 solver.cpp:213] Iteration 850, loss = 4.07826
I0614 16:01:15.986304  5063 solver.cpp:228]     Train net output #0: softmax = 4.07826 (* 1 = 4.07826 loss)
I0614 16:01:15.986309  5063 solver.cpp:473] Iteration 850, lr = 0.0001
I0614 16:01:16.869853  5063 solver.cpp:213] Iteration 860, loss = 4.02214
I0614 16:01:16.869874  5063 solver.cpp:228]     Train net output #0: softmax = 4.02214 (* 1 = 4.02214 loss)
I0614 16:01:16.869879  5063 solver.cpp:473] Iteration 860, lr = 0.0001
I0614 16:01:17.754161  5063 solver.cpp:213] Iteration 870, loss = 4.01411
I0614 16:01:17.754179  5063 solver.cpp:228]     Train net output #0: softmax = 4.01411 (* 1 = 4.01411 loss)
I0614 16:01:17.754185  5063 solver.cpp:473] Iteration 870, lr = 0.0001
I0614 16:01:18.638324  5063 solver.cpp:213] Iteration 880, loss = 4.00542
I0614 16:01:18.638339  5063 solver.cpp:228]     Train net output #0: softmax = 4.00542 (* 1 = 4.00542 loss)
I0614 16:01:18.638345  5063 solver.cpp:473] Iteration 880, lr = 0.0001
I0614 16:01:19.521726  5063 solver.cpp:213] Iteration 890, loss = 4.06445
I0614 16:01:19.521740  5063 solver.cpp:228]     Train net output #0: softmax = 4.06445 (* 1 = 4.06445 loss)
I0614 16:01:19.521745  5063 solver.cpp:473] Iteration 890, lr = 0.0001
I0614 16:01:20.405279  5063 solver.cpp:213] Iteration 900, loss = 4.13131
I0614 16:01:20.405295  5063 solver.cpp:228]     Train net output #0: softmax = 4.13131 (* 1 = 4.13131 loss)
I0614 16:01:20.405306  5063 solver.cpp:473] Iteration 900, lr = 0.0001
I0614 16:01:21.289113  5063 solver.cpp:213] Iteration 910, loss = 4.0763
I0614 16:01:21.289126  5063 solver.cpp:228]     Train net output #0: softmax = 4.0763 (* 1 = 4.0763 loss)
I0614 16:01:21.289131  5063 solver.cpp:473] Iteration 910, lr = 0.0001
I0614 16:01:22.172827  5063 solver.cpp:213] Iteration 920, loss = 4.048
I0614 16:01:22.172842  5063 solver.cpp:228]     Train net output #0: softmax = 4.048 (* 1 = 4.048 loss)
I0614 16:01:22.172847  5063 solver.cpp:473] Iteration 920, lr = 0.0001
I0614 16:01:23.056560  5063 solver.cpp:213] Iteration 930, loss = 4.13062
I0614 16:01:23.056574  5063 solver.cpp:228]     Train net output #0: softmax = 4.13062 (* 1 = 4.13062 loss)
I0614 16:01:23.056578  5063 solver.cpp:473] Iteration 930, lr = 0.0001
I0614 16:01:23.940418  5063 solver.cpp:213] Iteration 940, loss = 4.16221
I0614 16:01:23.940433  5063 solver.cpp:228]     Train net output #0: softmax = 4.16221 (* 1 = 4.16221 loss)
I0614 16:01:23.940438  5063 solver.cpp:473] Iteration 940, lr = 0.0001
I0614 16:01:24.823518  5063 solver.cpp:213] Iteration 950, loss = 3.94795
I0614 16:01:24.823531  5063 solver.cpp:228]     Train net output #0: softmax = 3.94795 (* 1 = 3.94795 loss)
I0614 16:01:24.823535  5063 solver.cpp:473] Iteration 950, lr = 0.0001
I0614 16:01:25.706624  5063 solver.cpp:213] Iteration 960, loss = 4.00314
I0614 16:01:25.706640  5063 solver.cpp:228]     Train net output #0: softmax = 4.00314 (* 1 = 4.00314 loss)
I0614 16:01:25.706755  5063 solver.cpp:473] Iteration 960, lr = 0.0001
I0614 16:01:26.590247  5063 solver.cpp:213] Iteration 970, loss = 4.06429
I0614 16:01:26.590262  5063 solver.cpp:228]     Train net output #0: softmax = 4.06429 (* 1 = 4.06429 loss)
I0614 16:01:26.590266  5063 solver.cpp:473] Iteration 970, lr = 0.0001
I0614 16:01:27.473800  5063 solver.cpp:213] Iteration 980, loss = 3.95601
I0614 16:01:27.473815  5063 solver.cpp:228]     Train net output #0: softmax = 3.95601 (* 1 = 3.95601 loss)
I0614 16:01:27.473819  5063 solver.cpp:473] Iteration 980, lr = 0.0001
I0614 16:01:28.357210  5063 solver.cpp:213] Iteration 990, loss = 4.01501
I0614 16:01:28.357224  5063 solver.cpp:228]     Train net output #0: softmax = 4.01501 (* 1 = 4.01501 loss)
I0614 16:01:28.357229  5063 solver.cpp:473] Iteration 990, lr = 0.0001
I0614 16:01:29.182503  5063 solver.cpp:362] Snapshotting to snapshots/16-06-14_15h51m21s_0_11_pretrainClassification_iter_1000.caffemodel
I0614 16:01:29.183467  5063 solver.cpp:370] Snapshotting solver state to snapshots/16-06-14_15h51m21s_0_11_pretrainClassification_iter_1000.solverstate
I0614 16:01:29.183907  5063 solver.cpp:291] Iteration 1000, Testing net (#0)
I0614 16:01:29.296598  5063 solver.cpp:342]     Test net output #0: accuracy_top_1 = 0.0875
I0614 16:01:29.296613  5063 solver.cpp:342]     Test net output #1: accuracy_top_5 = 0.240625
I0614 16:01:29.296619  5063 solver.cpp:342]     Test net output #2: softmax = 4.02137 (* 1 = 4.02137 loss)
I0614 16:01:29.354743  5063 solver.cpp:213] Iteration 1000, loss = 4.13599
I0614 16:01:29.354758  5063 solver.cpp:228]     Train net output #0: softmax = 4.13599 (* 1 = 4.13599 loss)
I0614 16:01:29.354763  5063 solver.cpp:473] Iteration 1000, lr = 0.0001
I0614 16:01:30.237871  5063 solver.cpp:213] Iteration 1010, loss = 4.11116
I0614 16:01:30.237891  5063 solver.cpp:228]     Train net output #0: softmax = 4.11116 (* 1 = 4.11116 loss)
I0614 16:01:30.237896  5063 solver.cpp:473] Iteration 1010, lr = 0.0001
I0614 16:01:31.121309  5063 solver.cpp:213] Iteration 1020, loss = 4.16587
I0614 16:01:31.121525  5063 solver.cpp:228]     Train net output #0: softmax = 4.16587 (* 1 = 4.16587 loss)
I0614 16:01:31.121532  5063 solver.cpp:473] Iteration 1020, lr = 0.0001
I0614 16:01:32.004990  5063 solver.cpp:213] Iteration 1030, loss = 3.91109
I0614 16:01:32.005005  5063 solver.cpp:228]     Train net output #0: softmax = 3.91109 (* 1 = 3.91109 loss)
I0614 16:01:32.005010  5063 solver.cpp:473] Iteration 1030, lr = 0.0001
I0614 16:01:32.888653  5063 solver.cpp:213] Iteration 1040, loss = 3.98495
I0614 16:01:32.888671  5063 solver.cpp:228]     Train net output #0: softmax = 3.98495 (* 1 = 3.98495 loss)
I0614 16:01:32.888677  5063 solver.cpp:473] Iteration 1040, lr = 0.0001
I0614 16:01:33.772027  5063 solver.cpp:213] Iteration 1050, loss = 3.8434
I0614 16:01:33.772042  5063 solver.cpp:228]     Train net output #0: softmax = 3.8434 (* 1 = 3.8434 loss)
I0614 16:01:33.772047  5063 solver.cpp:473] Iteration 1050, lr = 0.0001
I0614 16:01:34.655606  5063 solver.cpp:213] Iteration 1060, loss = 4.04155
I0614 16:01:34.655621  5063 solver.cpp:228]     Train net output #0: softmax = 4.04155 (* 1 = 4.04155 loss)
I0614 16:01:34.655625  5063 solver.cpp:473] Iteration 1060, lr = 0.0001
I0614 16:01:35.538985  5063 solver.cpp:213] Iteration 1070, loss = 4.03431
I0614 16:01:35.539000  5063 solver.cpp:228]     Train net output #0: softmax = 4.03431 (* 1 = 4.03431 loss)
I0614 16:01:35.539005  5063 solver.cpp:473] Iteration 1070, lr = 0.0001
I0614 16:01:36.422854  5063 solver.cpp:213] Iteration 1080, loss = 4.01421
I0614 16:01:36.422870  5063 solver.cpp:228]     Train net output #0: softmax = 4.01421 (* 1 = 4.01421 loss)
I0614 16:01:36.422875  5063 solver.cpp:473] Iteration 1080, lr = 0.0001
I0614 16:01:37.306259  5063 solver.cpp:213] Iteration 1090, loss = 4.07238
I0614 16:01:37.306284  5063 solver.cpp:228]     Train net output #0: softmax = 4.07238 (* 1 = 4.07238 loss)
I0614 16:01:37.306289  5063 solver.cpp:473] Iteration 1090, lr = 0.0001
I0614 16:01:38.189692  5063 solver.cpp:213] Iteration 1100, loss = 4.05737
I0614 16:01:38.189709  5063 solver.cpp:228]     Train net output #0: softmax = 4.05737 (* 1 = 4.05737 loss)
I0614 16:01:38.189715  5063 solver.cpp:473] Iteration 1100, lr = 0.0001
I0614 16:01:39.073364  5063 solver.cpp:213] Iteration 1110, loss = 3.96476
I0614 16:01:39.073377  5063 solver.cpp:228]     Train net output #0: softmax = 3.96476 (* 1 = 3.96476 loss)
I0614 16:01:39.073382  5063 solver.cpp:473] Iteration 1110, lr = 0.0001
I0614 16:01:39.957309  5063 solver.cpp:213] Iteration 1120, loss = 4.13956
I0614 16:01:39.957324  5063 solver.cpp:228]     Train net output #0: softmax = 4.13956 (* 1 = 4.13956 loss)
I0614 16:01:39.957327  5063 solver.cpp:473] Iteration 1120, lr = 0.0001
I0614 16:01:40.840616  5063 solver.cpp:213] Iteration 1130, loss = 4.06201
I0614 16:01:40.840632  5063 solver.cpp:228]     Train net output #0: softmax = 4.06201 (* 1 = 4.06201 loss)
I0614 16:01:40.840637  5063 solver.cpp:473] Iteration 1130, lr = 0.0001
I0614 16:01:41.724314  5063 solver.cpp:213] Iteration 1140, loss = 4.06797
I0614 16:01:41.724331  5063 solver.cpp:228]     Train net output #0: softmax = 4.06797 (* 1 = 4.06797 loss)
I0614 16:01:41.724498  5063 solver.cpp:473] Iteration 1140, lr = 0.0001
I0614 16:01:42.607641  5063 solver.cpp:213] Iteration 1150, loss = 4.07307
I0614 16:01:42.607657  5063 solver.cpp:228]     Train net output #0: softmax = 4.07307 (* 1 = 4.07307 loss)
I0614 16:01:42.607662  5063 solver.cpp:473] Iteration 1150, lr = 0.0001
I0614 16:01:43.490687  5063 solver.cpp:213] Iteration 1160, loss = 3.89752
I0614 16:01:43.490705  5063 solver.cpp:228]     Train net output #0: softmax = 3.89752 (* 1 = 3.89752 loss)
I0614 16:01:43.490710  5063 solver.cpp:473] Iteration 1160, lr = 0.0001
I0614 16:01:44.374857  5063 solver.cpp:213] Iteration 1170, loss = 4.22805
I0614 16:01:44.374871  5063 solver.cpp:228]     Train net output #0: softmax = 4.22805 (* 1 = 4.22805 loss)
I0614 16:01:44.374877  5063 solver.cpp:473] Iteration 1170, lr = 0.0001
I0614 16:01:45.258883  5063 solver.cpp:213] Iteration 1180, loss = 3.76063
I0614 16:01:45.258898  5063 solver.cpp:228]     Train net output #0: softmax = 3.76063 (* 1 = 3.76063 loss)
I0614 16:01:45.258918  5063 solver.cpp:473] Iteration 1180, lr = 0.0001
I0614 16:01:46.142948  5063 solver.cpp:213] Iteration 1190, loss = 4.14235
I0614 16:01:46.142963  5063 solver.cpp:228]     Train net output #0: softmax = 4.14235 (* 1 = 4.14235 loss)
I0614 16:01:46.142968  5063 solver.cpp:473] Iteration 1190, lr = 0.0001
I0614 16:01:47.025110  5063 solver.cpp:213] Iteration 1200, loss = 4.15388
I0614 16:01:47.025132  5063 solver.cpp:228]     Train net output #0: softmax = 4.15388 (* 1 = 4.15388 loss)
I0614 16:01:47.025151  5063 solver.cpp:473] Iteration 1200, lr = 0.0001
I0614 16:01:47.907260  5063 solver.cpp:213] Iteration 1210, loss = 3.94383
I0614 16:01:47.907290  5063 solver.cpp:228]     Train net output #0: softmax = 3.94383 (* 1 = 3.94383 loss)
I0614 16:01:47.907295  5063 solver.cpp:473] Iteration 1210, lr = 0.0001
I0614 16:01:48.790676  5063 solver.cpp:213] Iteration 1220, loss = 3.98781
I0614 16:01:48.790693  5063 solver.cpp:228]     Train net output #0: softmax = 3.98781 (* 1 = 3.98781 loss)
I0614 16:01:48.790698  5063 solver.cpp:473] Iteration 1220, lr = 0.0001
I0614 16:01:49.673620  5063 solver.cpp:213] Iteration 1230, loss = 3.97351
I0614 16:01:49.673635  5063 solver.cpp:228]     Train net output #0: softmax = 3.97351 (* 1 = 3.97351 loss)
I0614 16:01:49.673640  5063 solver.cpp:473] Iteration 1230, lr = 0.0001
I0614 16:01:50.557365  5063 solver.cpp:213] Iteration 1240, loss = 4.11231
I0614 16:01:50.557380  5063 solver.cpp:228]     Train net output #0: softmax = 4.11231 (* 1 = 4.11231 loss)
I0614 16:01:50.557385  5063 solver.cpp:473] Iteration 1240, lr = 0.0001
I0614 16:01:51.441398  5063 solver.cpp:213] Iteration 1250, loss = 4.07765
I0614 16:01:51.441413  5063 solver.cpp:228]     Train net output #0: softmax = 4.07765 (* 1 = 4.07765 loss)
I0614 16:01:51.441418  5063 solver.cpp:473] Iteration 1250, lr = 0.0001
I0614 16:01:52.323870  5063 solver.cpp:213] Iteration 1260, loss = 3.93508
I0614 16:01:52.323890  5063 solver.cpp:228]     Train net output #0: softmax = 3.93508 (* 1 = 3.93508 loss)
I0614 16:01:52.323900  5063 solver.cpp:473] Iteration 1260, lr = 0.0001
I0614 16:01:53.206907  5063 solver.cpp:213] Iteration 1270, loss = 3.9478
I0614 16:01:53.206920  5063 solver.cpp:228]     Train net output #0: softmax = 3.9478 (* 1 = 3.9478 loss)
I0614 16:01:53.206925  5063 solver.cpp:473] Iteration 1270, lr = 0.0001
I0614 16:01:54.089771  5063 solver.cpp:213] Iteration 1280, loss = 3.989
I0614 16:01:54.089786  5063 solver.cpp:228]     Train net output #0: softmax = 3.989 (* 1 = 3.989 loss)
I0614 16:01:54.089790  5063 solver.cpp:473] Iteration 1280, lr = 0.0001
I0614 16:01:54.973466  5063 solver.cpp:213] Iteration 1290, loss = 4.07238
I0614 16:01:54.973480  5063 solver.cpp:228]     Train net output #0: softmax = 4.07238 (* 1 = 4.07238 loss)
I0614 16:01:54.973485  5063 solver.cpp:473] Iteration 1290, lr = 0.0001
I0614 16:01:55.856294  5063 solver.cpp:213] Iteration 1300, loss = 4.1347
I0614 16:01:55.856308  5063 solver.cpp:228]     Train net output #0: softmax = 4.1347 (* 1 = 4.1347 loss)
I0614 16:01:55.856314  5063 solver.cpp:473] Iteration 1300, lr = 0.0001
I0614 16:01:56.739411  5063 solver.cpp:213] Iteration 1310, loss = 3.98807
I0614 16:01:56.739425  5063 solver.cpp:228]     Train net output #0: softmax = 3.98807 (* 1 = 3.98807 loss)
I0614 16:01:56.739430  5063 solver.cpp:473] Iteration 1310, lr = 0.0001
I0614 16:01:57.622444  5063 solver.cpp:213] Iteration 1320, loss = 4.00619
I0614 16:01:57.622460  5063 solver.cpp:228]     Train net output #0: softmax = 4.00619 (* 1 = 4.00619 loss)
I0614 16:01:57.622465  5063 solver.cpp:473] Iteration 1320, lr = 0.0001
I0614 16:01:58.505857  5063 solver.cpp:213] Iteration 1330, loss = 4.10346
I0614 16:01:58.505875  5063 solver.cpp:228]     Train net output #0: softmax = 4.10346 (* 1 = 4.10346 loss)
I0614 16:01:58.505880  5063 solver.cpp:473] Iteration 1330, lr = 0.0001
I0614 16:01:59.389806  5063 solver.cpp:213] Iteration 1340, loss = 4.16092
I0614 16:01:59.389821  5063 solver.cpp:228]     Train net output #0: softmax = 4.16092 (* 1 = 4.16092 loss)
I0614 16:01:59.389840  5063 solver.cpp:473] Iteration 1340, lr = 0.0001
I0614 16:02:00.273948  5063 solver.cpp:213] Iteration 1350, loss = 3.90222
I0614 16:02:00.273962  5063 solver.cpp:228]     Train net output #0: softmax = 3.90222 (* 1 = 3.90222 loss)
I0614 16:02:00.273967  5063 solver.cpp:473] Iteration 1350, lr = 0.0001
I0614 16:02:01.157635  5063 solver.cpp:213] Iteration 1360, loss = 3.89306
I0614 16:02:01.157670  5063 solver.cpp:228]     Train net output #0: softmax = 3.89306 (* 1 = 3.89306 loss)
I0614 16:02:01.157680  5063 solver.cpp:473] Iteration 1360, lr = 0.0001
I0614 16:02:02.041317  5063 solver.cpp:213] Iteration 1370, loss = 3.8899
I0614 16:02:02.041332  5063 solver.cpp:228]     Train net output #0: softmax = 3.8899 (* 1 = 3.8899 loss)
I0614 16:02:02.041337  5063 solver.cpp:473] Iteration 1370, lr = 0.0001
I0614 16:02:02.925237  5063 solver.cpp:213] Iteration 1380, loss = 3.98989
I0614 16:02:02.925256  5063 solver.cpp:228]     Train net output #0: softmax = 3.98989 (* 1 = 3.98989 loss)
I0614 16:02:02.925266  5063 solver.cpp:473] Iteration 1380, lr = 0.0001
I0614 16:02:03.808933  5063 solver.cpp:213] Iteration 1390, loss = 4.11402
I0614 16:02:03.808950  5063 solver.cpp:228]     Train net output #0: softmax = 4.11402 (* 1 = 4.11402 loss)
I0614 16:02:03.808955  5063 solver.cpp:473] Iteration 1390, lr = 0.0001
I0614 16:02:04.691809  5063 solver.cpp:213] Iteration 1400, loss = 3.94198
I0614 16:02:04.691823  5063 solver.cpp:228]     Train net output #0: softmax = 3.94198 (* 1 = 3.94198 loss)
I0614 16:02:04.691828  5063 solver.cpp:473] Iteration 1400, lr = 0.0001
I0614 16:02:05.575866  5063 solver.cpp:213] Iteration 1410, loss = 4.1181
I0614 16:02:05.575881  5063 solver.cpp:228]     Train net output #0: softmax = 4.1181 (* 1 = 4.1181 loss)
I0614 16:02:05.575886  5063 solver.cpp:473] Iteration 1410, lr = 0.0001
I0614 16:02:06.459861  5063 solver.cpp:213] Iteration 1420, loss = 3.8015
I0614 16:02:06.459874  5063 solver.cpp:228]     Train net output #0: softmax = 3.8015 (* 1 = 3.8015 loss)
I0614 16:02:06.459879  5063 solver.cpp:473] Iteration 1420, lr = 0.0001
I0614 16:02:07.343307  5063 solver.cpp:213] Iteration 1430, loss = 3.90302
I0614 16:02:07.343322  5063 solver.cpp:228]     Train net output #0: softmax = 3.90302 (* 1 = 3.90302 loss)
I0614 16:02:07.343327  5063 solver.cpp:473] Iteration 1430, lr = 0.0001
I0614 16:02:08.226786  5063 solver.cpp:213] Iteration 1440, loss = 4.11201
I0614 16:02:08.226804  5063 solver.cpp:228]     Train net output #0: softmax = 4.11201 (* 1 = 4.11201 loss)
I0614 16:02:08.226923  5063 solver.cpp:473] Iteration 1440, lr = 0.0001
I0614 16:02:09.109479  5063 solver.cpp:213] Iteration 1450, loss = 3.94131
I0614 16:02:09.109496  5063 solver.cpp:228]     Train net output #0: softmax = 3.94131 (* 1 = 3.94131 loss)
I0614 16:02:09.109501  5063 solver.cpp:473] Iteration 1450, lr = 0.0001
I0614 16:02:09.993475  5063 solver.cpp:213] Iteration 1460, loss = 3.96942
I0614 16:02:09.993490  5063 solver.cpp:228]     Train net output #0: softmax = 3.96942 (* 1 = 3.96942 loss)
I0614 16:02:09.993495  5063 solver.cpp:473] Iteration 1460, lr = 0.0001
I0614 16:02:10.876847  5063 solver.cpp:213] Iteration 1470, loss = 3.79751
I0614 16:02:10.876863  5063 solver.cpp:228]     Train net output #0: softmax = 3.79751 (* 1 = 3.79751 loss)
I0614 16:02:10.876868  5063 solver.cpp:473] Iteration 1470, lr = 0.0001
I0614 16:02:11.759699  5063 solver.cpp:213] Iteration 1480, loss = 4.09076
I0614 16:02:11.759713  5063 solver.cpp:228]     Train net output #0: softmax = 4.09076 (* 1 = 4.09076 loss)
I0614 16:02:11.759718  5063 solver.cpp:473] Iteration 1480, lr = 0.0001
I0614 16:02:12.642956  5063 solver.cpp:213] Iteration 1490, loss = 4.06377
I0614 16:02:12.642971  5063 solver.cpp:228]     Train net output #0: softmax = 4.06377 (* 1 = 4.06377 loss)
I0614 16:02:12.642976  5063 solver.cpp:473] Iteration 1490, lr = 0.0001
I0614 16:02:13.526532  5063 solver.cpp:213] Iteration 1500, loss = 3.96278
I0614 16:02:13.526548  5063 solver.cpp:228]     Train net output #0: softmax = 3.96278 (* 1 = 3.96278 loss)
I0614 16:02:13.526553  5063 solver.cpp:473] Iteration 1500, lr = 0.0001
I0614 16:02:14.410621  5063 solver.cpp:213] Iteration 1510, loss = 3.93398
I0614 16:02:14.410639  5063 solver.cpp:228]     Train net output #0: softmax = 3.93398 (* 1 = 3.93398 loss)
I0614 16:02:14.410643  5063 solver.cpp:473] Iteration 1510, lr = 0.0001
I0614 16:02:15.294898  5063 solver.cpp:213] Iteration 1520, loss = 3.91826
I0614 16:02:15.294914  5063 solver.cpp:228]     Train net output #0: softmax = 3.91826 (* 1 = 3.91826 loss)
I0614 16:02:15.294937  5063 solver.cpp:473] Iteration 1520, lr = 0.0001
I0614 16:02:16.178737  5063 solver.cpp:213] Iteration 1530, loss = 3.92586
I0614 16:02:16.178752  5063 solver.cpp:228]     Train net output #0: softmax = 3.92586 (* 1 = 3.92586 loss)
I0614 16:02:16.178757  5063 solver.cpp:473] Iteration 1530, lr = 0.0001
I0614 16:02:17.061640  5063 solver.cpp:213] Iteration 1540, loss = 4.13982
I0614 16:02:17.061655  5063 solver.cpp:228]     Train net output #0: softmax = 4.13982 (* 1 = 4.13982 loss)
I0614 16:02:17.061660  5063 solver.cpp:473] Iteration 1540, lr = 0.0001
I0614 16:02:17.945004  5063 solver.cpp:213] Iteration 1550, loss = 3.95671
I0614 16:02:17.945019  5063 solver.cpp:228]     Train net output #0: softmax = 3.95671 (* 1 = 3.95671 loss)
I0614 16:02:17.945024  5063 solver.cpp:473] Iteration 1550, lr = 0.0001
I0614 16:02:18.828919  5063 solver.cpp:213] Iteration 1560, loss = 3.96405
I0614 16:02:18.828938  5063 solver.cpp:228]     Train net output #0: softmax = 3.96405 (* 1 = 3.96405 loss)
I0614 16:02:18.829092  5063 solver.cpp:473] Iteration 1560, lr = 0.0001
I0614 16:02:19.713060  5063 solver.cpp:213] Iteration 1570, loss = 3.88663
I0614 16:02:19.713074  5063 solver.cpp:228]     Train net output #0: softmax = 3.88663 (* 1 = 3.88663 loss)
I0614 16:02:19.713079  5063 solver.cpp:473] Iteration 1570, lr = 0.0001
I0614 16:02:20.595592  5063 solver.cpp:213] Iteration 1580, loss = 3.80802
I0614 16:02:20.595607  5063 solver.cpp:228]     Train net output #0: softmax = 3.80802 (* 1 = 3.80802 loss)
I0614 16:02:20.595612  5063 solver.cpp:473] Iteration 1580, lr = 0.0001
I0614 16:02:21.478082  5063 solver.cpp:213] Iteration 1590, loss = 4.1159
I0614 16:02:21.478096  5063 solver.cpp:228]     Train net output #0: softmax = 4.1159 (* 1 = 4.1159 loss)
I0614 16:02:21.478101  5063 solver.cpp:473] Iteration 1590, lr = 0.0001
I0614 16:02:22.361758  5063 solver.cpp:213] Iteration 1600, loss = 3.96986
I0614 16:02:22.361773  5063 solver.cpp:228]     Train net output #0: softmax = 3.96986 (* 1 = 3.96986 loss)
I0614 16:02:22.361778  5063 solver.cpp:473] Iteration 1600, lr = 0.0001
I0614 16:02:23.245432  5063 solver.cpp:213] Iteration 1610, loss = 3.902
I0614 16:02:23.245446  5063 solver.cpp:228]     Train net output #0: softmax = 3.902 (* 1 = 3.902 loss)
I0614 16:02:23.245451  5063 solver.cpp:473] Iteration 1610, lr = 0.0001
I0614 16:02:24.128454  5063 solver.cpp:213] Iteration 1620, loss = 4.06401
I0614 16:02:24.128473  5063 solver.cpp:228]     Train net output #0: softmax = 4.06401 (* 1 = 4.06401 loss)
I0614 16:02:24.128482  5063 solver.cpp:473] Iteration 1620, lr = 0.0001
I0614 16:02:25.012253  5063 solver.cpp:213] Iteration 1630, loss = 3.91564
I0614 16:02:25.012267  5063 solver.cpp:228]     Train net output #0: softmax = 3.91564 (* 1 = 3.91564 loss)
I0614 16:02:25.012272  5063 solver.cpp:473] Iteration 1630, lr = 0.0001
I0614 16:02:25.895828  5063 solver.cpp:213] Iteration 1640, loss = 4.10985
I0614 16:02:25.895845  5063 solver.cpp:228]     Train net output #0: softmax = 4.10985 (* 1 = 4.10985 loss)
I0614 16:02:25.895850  5063 solver.cpp:473] Iteration 1640, lr = 0.0001
I0614 16:02:26.778460  5063 solver.cpp:213] Iteration 1650, loss = 3.85018
I0614 16:02:26.778475  5063 solver.cpp:228]     Train net output #0: softmax = 3.85018 (* 1 = 3.85018 loss)
I0614 16:02:26.778481  5063 solver.cpp:473] Iteration 1650, lr = 0.0001
I0614 16:02:27.661561  5063 solver.cpp:213] Iteration 1660, loss = 3.94763
I0614 16:02:27.661576  5063 solver.cpp:228]     Train net output #0: softmax = 3.94763 (* 1 = 3.94763 loss)
I0614 16:02:27.661581  5063 solver.cpp:473] Iteration 1660, lr = 0.0001
I0614 16:02:28.544860  5063 solver.cpp:213] Iteration 1670, loss = 3.75932
I0614 16:02:28.544877  5063 solver.cpp:228]     Train net output #0: softmax = 3.75932 (* 1 = 3.75932 loss)
I0614 16:02:28.544881  5063 solver.cpp:473] Iteration 1670, lr = 0.0001
I0614 16:02:29.428225  5063 solver.cpp:213] Iteration 1680, loss = 4.13489
I0614 16:02:29.428241  5063 solver.cpp:228]     Train net output #0: softmax = 4.13489 (* 1 = 4.13489 loss)
I0614 16:02:29.428262  5063 solver.cpp:473] Iteration 1680, lr = 0.0001
I0614 16:02:30.312293  5063 solver.cpp:213] Iteration 1690, loss = 4.07745
I0614 16:02:30.312309  5063 solver.cpp:228]     Train net output #0: softmax = 4.07745 (* 1 = 4.07745 loss)
I0614 16:02:30.312314  5063 solver.cpp:473] Iteration 1690, lr = 0.0001
I0614 16:02:31.196084  5063 solver.cpp:213] Iteration 1700, loss = 3.87689
I0614 16:02:31.196115  5063 solver.cpp:228]     Train net output #0: softmax = 3.87689 (* 1 = 3.87689 loss)
I0614 16:02:31.196121  5063 solver.cpp:473] Iteration 1700, lr = 0.0001
I0614 16:02:32.079185  5063 solver.cpp:213] Iteration 1710, loss = 3.9621
I0614 16:02:32.079200  5063 solver.cpp:228]     Train net output #0: softmax = 3.9621 (* 1 = 3.9621 loss)
I0614 16:02:32.079205  5063 solver.cpp:473] Iteration 1710, lr = 0.0001
I0614 16:02:32.962982  5063 solver.cpp:213] Iteration 1720, loss = 3.83182
I0614 16:02:32.962997  5063 solver.cpp:228]     Train net output #0: softmax = 3.83182 (* 1 = 3.83182 loss)
I0614 16:02:32.963002  5063 solver.cpp:473] Iteration 1720, lr = 0.0001
I0614 16:02:33.846496  5063 solver.cpp:213] Iteration 1730, loss = 4.04112
I0614 16:02:33.846510  5063 solver.cpp:228]     Train net output #0: softmax = 4.04112 (* 1 = 4.04112 loss)
I0614 16:02:33.846515  5063 solver.cpp:473] Iteration 1730, lr = 0.0001
I0614 16:02:34.730142  5063 solver.cpp:213] Iteration 1740, loss = 3.89973
I0614 16:02:34.730160  5063 solver.cpp:228]     Train net output #0: softmax = 3.89973 (* 1 = 3.89973 loss)
I0614 16:02:34.730291  5063 solver.cpp:473] Iteration 1740, lr = 0.0001
I0614 16:02:35.613205  5063 solver.cpp:213] Iteration 1750, loss = 3.95568
I0614 16:02:35.613219  5063 solver.cpp:228]     Train net output #0: softmax = 3.95568 (* 1 = 3.95568 loss)
I0614 16:02:35.613224  5063 solver.cpp:473] Iteration 1750, lr = 0.0001
I0614 16:02:36.496722  5063 solver.cpp:213] Iteration 1760, loss = 3.8738
I0614 16:02:36.496737  5063 solver.cpp:228]     Train net output #0: softmax = 3.8738 (* 1 = 3.8738 loss)
I0614 16:02:36.496742  5063 solver.cpp:473] Iteration 1760, lr = 0.0001
I0614 16:02:37.378985  5063 solver.cpp:213] Iteration 1770, loss = 3.84721
I0614 16:02:37.379004  5063 solver.cpp:228]     Train net output #0: softmax = 3.84721 (* 1 = 3.84721 loss)
I0614 16:02:37.379007  5063 solver.cpp:473] Iteration 1770, lr = 0.0001
I0614 16:02:38.262660  5063 solver.cpp:213] Iteration 1780, loss = 3.82446
I0614 16:02:38.262675  5063 solver.cpp:228]     Train net output #0: softmax = 3.82446 (* 1 = 3.82446 loss)
I0614 16:02:38.262679  5063 solver.cpp:473] Iteration 1780, lr = 0.0001
I0614 16:02:39.145560  5063 solver.cpp:213] Iteration 1790, loss = 4.0127
I0614 16:02:39.145573  5063 solver.cpp:228]     Train net output #0: softmax = 4.0127 (* 1 = 4.0127 loss)
I0614 16:02:39.145578  5063 solver.cpp:473] Iteration 1790, lr = 0.0001
I0614 16:02:40.029203  5063 solver.cpp:213] Iteration 1800, loss = 3.80722
I0614 16:02:40.029222  5063 solver.cpp:228]     Train net output #0: softmax = 3.80722 (* 1 = 3.80722 loss)
I0614 16:02:40.029371  5063 solver.cpp:473] Iteration 1800, lr = 0.0001
I0614 16:02:40.912123  5063 solver.cpp:213] Iteration 1810, loss = 3.72845
I0614 16:02:40.912153  5063 solver.cpp:228]     Train net output #0: softmax = 3.72845 (* 1 = 3.72845 loss)
I0614 16:02:40.912171  5063 solver.cpp:473] Iteration 1810, lr = 0.0001
I0614 16:02:41.794965  5063 solver.cpp:213] Iteration 1820, loss = 4.04912
I0614 16:02:41.794981  5063 solver.cpp:228]     Train net output #0: softmax = 4.04912 (* 1 = 4.04912 loss)
I0614 16:02:41.794986  5063 solver.cpp:473] Iteration 1820, lr = 0.0001
I0614 16:02:42.678618  5063 solver.cpp:213] Iteration 1830, loss = 4.08109
I0614 16:02:42.678633  5063 solver.cpp:228]     Train net output #0: softmax = 4.08109 (* 1 = 4.08109 loss)
I0614 16:02:42.678638  5063 solver.cpp:473] Iteration 1830, lr = 0.0001
I0614 16:02:43.562225  5063 solver.cpp:213] Iteration 1840, loss = 3.83552
I0614 16:02:43.562239  5063 solver.cpp:228]     Train net output #0: softmax = 3.83552 (* 1 = 3.83552 loss)
I0614 16:02:43.562244  5063 solver.cpp:473] Iteration 1840, lr = 0.0001
I0614 16:02:44.445600  5063 solver.cpp:213] Iteration 1850, loss = 3.82106
I0614 16:02:44.445614  5063 solver.cpp:228]     Train net output #0: softmax = 3.82106 (* 1 = 3.82106 loss)
I0614 16:02:44.445619  5063 solver.cpp:473] Iteration 1850, lr = 0.0001
I0614 16:02:45.328852  5063 solver.cpp:213] Iteration 1860, loss = 3.86927
I0614 16:02:45.328869  5063 solver.cpp:228]     Train net output #0: softmax = 3.86927 (* 1 = 3.86927 loss)
I0614 16:02:45.328991  5063 solver.cpp:473] Iteration 1860, lr = 0.0001
I0614 16:02:46.212251  5063 solver.cpp:213] Iteration 1870, loss = 3.91518
I0614 16:02:46.212265  5063 solver.cpp:228]     Train net output #0: softmax = 3.91518 (* 1 = 3.91518 loss)
I0614 16:02:46.212270  5063 solver.cpp:473] Iteration 1870, lr = 0.0001
I0614 16:02:47.096307  5063 solver.cpp:213] Iteration 1880, loss = 3.8862
I0614 16:02:47.096323  5063 solver.cpp:228]     Train net output #0: softmax = 3.8862 (* 1 = 3.8862 loss)
I0614 16:02:47.096328  5063 solver.cpp:473] Iteration 1880, lr = 0.0001
I0614 16:02:47.979099  5063 solver.cpp:213] Iteration 1890, loss = 3.95023
I0614 16:02:47.979113  5063 solver.cpp:228]     Train net output #0: softmax = 3.95023 (* 1 = 3.95023 loss)
I0614 16:02:47.979118  5063 solver.cpp:473] Iteration 1890, lr = 0.0001
I0614 16:02:48.862785  5063 solver.cpp:213] Iteration 1900, loss = 3.89716
I0614 16:02:48.862800  5063 solver.cpp:228]     Train net output #0: softmax = 3.89716 (* 1 = 3.89716 loss)
I0614 16:02:48.862805  5063 solver.cpp:473] Iteration 1900, lr = 0.0001
I0614 16:02:49.746161  5063 solver.cpp:213] Iteration 1910, loss = 3.97932
I0614 16:02:49.746176  5063 solver.cpp:228]     Train net output #0: softmax = 3.97932 (* 1 = 3.97932 loss)
I0614 16:02:49.746181  5063 solver.cpp:473] Iteration 1910, lr = 0.0001
I0614 16:02:50.629845  5063 solver.cpp:213] Iteration 1920, loss = 3.8651
I0614 16:02:50.629860  5063 solver.cpp:228]     Train net output #0: softmax = 3.8651 (* 1 = 3.8651 loss)
I0614 16:02:50.629866  5063 solver.cpp:473] Iteration 1920, lr = 0.0001
I0614 16:02:51.513931  5063 solver.cpp:213] Iteration 1930, loss = 3.89804
I0614 16:02:51.513945  5063 solver.cpp:228]     Train net output #0: softmax = 3.89804 (* 1 = 3.89804 loss)
I0614 16:02:51.513950  5063 solver.cpp:473] Iteration 1930, lr = 0.0001
I0614 16:02:52.397306  5063 solver.cpp:213] Iteration 1940, loss = 4.12851
I0614 16:02:52.397321  5063 solver.cpp:228]     Train net output #0: softmax = 4.12851 (* 1 = 4.12851 loss)
I0614 16:02:52.397326  5063 solver.cpp:473] Iteration 1940, lr = 0.0001
I0614 16:02:53.280213  5063 solver.cpp:213] Iteration 1950, loss = 3.94037
I0614 16:02:53.280231  5063 solver.cpp:228]     Train net output #0: softmax = 3.94037 (* 1 = 3.94037 loss)
I0614 16:02:53.280236  5063 solver.cpp:473] Iteration 1950, lr = 0.0001
I0614 16:02:54.163769  5063 solver.cpp:213] Iteration 1960, loss = 3.88414
I0614 16:02:54.163784  5063 solver.cpp:228]     Train net output #0: softmax = 3.88414 (* 1 = 3.88414 loss)
I0614 16:02:54.163789  5063 solver.cpp:473] Iteration 1960, lr = 0.0001
I0614 16:02:55.046731  5063 solver.cpp:213] Iteration 1970, loss = 3.77173
I0614 16:02:55.046746  5063 solver.cpp:228]     Train net output #0: softmax = 3.77173 (* 1 = 3.77173 loss)
I0614 16:02:55.046751  5063 solver.cpp:473] Iteration 1970, lr = 0.0001
I0614 16:02:55.929473  5063 solver.cpp:213] Iteration 1980, loss = 3.98398
I0614 16:02:55.929491  5063 solver.cpp:228]     Train net output #0: softmax = 3.98398 (* 1 = 3.98398 loss)
I0614 16:02:55.929502  5063 solver.cpp:473] Iteration 1980, lr = 0.0001
I0614 16:02:56.812650  5063 solver.cpp:213] Iteration 1990, loss = 4.03402
I0614 16:02:56.812665  5063 solver.cpp:228]     Train net output #0: softmax = 4.03402 (* 1 = 4.03402 loss)
I0614 16:02:56.812671  5063 solver.cpp:473] Iteration 1990, lr = 0.0001
I0614 16:02:57.638106  5063 solver.cpp:362] Snapshotting to snapshots/16-06-14_15h51m21s_0_11_pretrainClassification_iter_2000.caffemodel
I0614 16:02:57.638888  5063 solver.cpp:370] Snapshotting solver state to snapshots/16-06-14_15h51m21s_0_11_pretrainClassification_iter_2000.solverstate
I0614 16:02:57.639318  5063 solver.cpp:291] Iteration 2000, Testing net (#0)
I0614 16:02:57.751813  5063 solver.cpp:342]     Test net output #0: accuracy_top_1 = 0.10625
I0614 16:02:57.751828  5063 solver.cpp:342]     Test net output #1: accuracy_top_5 = 0.3125
I0614 16:02:57.751834  5063 solver.cpp:342]     Test net output #2: softmax = 3.91724 (* 1 = 3.91724 loss)
I0614 16:02:57.810132  5063 solver.cpp:213] Iteration 2000, loss = 3.80059
I0614 16:02:57.810145  5063 solver.cpp:228]     Train net output #0: softmax = 3.80059 (* 1 = 3.80059 loss)
I0614 16:02:57.810150  5063 solver.cpp:473] Iteration 2000, lr = 0.0001
I0614 16:02:58.692492  5063 solver.cpp:213] Iteration 2010, loss = 4.0902
I0614 16:02:58.692510  5063 solver.cpp:228]     Train net output #0: softmax = 4.0902 (* 1 = 4.0902 loss)
I0614 16:02:58.692515  5063 solver.cpp:473] Iteration 2010, lr = 0.0001
I0614 16:02:59.574961  5063 solver.cpp:213] Iteration 2020, loss = 3.96344
I0614 16:02:59.574976  5063 solver.cpp:228]     Train net output #0: softmax = 3.96344 (* 1 = 3.96344 loss)
I0614 16:02:59.574980  5063 solver.cpp:473] Iteration 2020, lr = 0.0001
I0614 16:03:00.457643  5063 solver.cpp:213] Iteration 2030, loss = 3.84346
I0614 16:03:00.457659  5063 solver.cpp:228]     Train net output #0: softmax = 3.84346 (* 1 = 3.84346 loss)
I0614 16:03:00.457664  5063 solver.cpp:473] Iteration 2030, lr = 0.0001
I0614 16:03:01.340729  5063 solver.cpp:213] Iteration 2040, loss = 3.85019
I0614 16:03:01.340945  5063 solver.cpp:228]     Train net output #0: softmax = 3.85019 (* 1 = 3.85019 loss)
I0614 16:03:01.340953  5063 solver.cpp:473] Iteration 2040, lr = 0.0001
I0614 16:03:02.224441  5063 solver.cpp:213] Iteration 2050, loss = 4.10007
I0614 16:03:02.224454  5063 solver.cpp:228]     Train net output #0: softmax = 4.10007 (* 1 = 4.10007 loss)
I0614 16:03:02.224459  5063 solver.cpp:473] Iteration 2050, lr = 0.0001
I0614 16:03:03.108898  5063 solver.cpp:213] Iteration 2060, loss = 3.91482
I0614 16:03:03.108912  5063 solver.cpp:228]     Train net output #0: softmax = 3.91482 (* 1 = 3.91482 loss)
I0614 16:03:03.108917  5063 solver.cpp:473] Iteration 2060, lr = 0.0001
I0614 16:03:03.992090  5063 solver.cpp:213] Iteration 2070, loss = 3.9991
I0614 16:03:03.992105  5063 solver.cpp:228]     Train net output #0: softmax = 3.9991 (* 1 = 3.9991 loss)
I0614 16:03:03.992110  5063 solver.cpp:473] Iteration 2070, lr = 0.0001
I0614 16:03:04.875280  5063 solver.cpp:213] Iteration 2080, loss = 3.98878
I0614 16:03:04.875295  5063 solver.cpp:228]     Train net output #0: softmax = 3.98878 (* 1 = 3.98878 loss)
I0614 16:03:04.875300  5063 solver.cpp:473] Iteration 2080, lr = 0.0001
I0614 16:03:05.758837  5063 solver.cpp:213] Iteration 2090, loss = 3.67924
I0614 16:03:05.758852  5063 solver.cpp:228]     Train net output #0: softmax = 3.67924 (* 1 = 3.67924 loss)
I0614 16:03:05.758857  5063 solver.cpp:473] Iteration 2090, lr = 0.0001
I0614 16:03:06.643115  5063 solver.cpp:213] Iteration 2100, loss = 3.76175
I0614 16:03:06.643132  5063 solver.cpp:228]     Train net output #0: softmax = 3.76175 (* 1 = 3.76175 loss)
I0614 16:03:06.643137  5063 solver.cpp:473] Iteration 2100, lr = 0.0001
I0614 16:03:07.527276  5063 solver.cpp:213] Iteration 2110, loss = 3.73323
I0614 16:03:07.527292  5063 solver.cpp:228]     Train net output #0: softmax = 3.73323 (* 1 = 3.73323 loss)
I0614 16:03:07.527297  5063 solver.cpp:473] Iteration 2110, lr = 0.0001
I0614 16:03:08.410975  5063 solver.cpp:213] Iteration 2120, loss = 3.97168
I0614 16:03:08.410990  5063 solver.cpp:228]     Train net output #0: softmax = 3.97168 (* 1 = 3.97168 loss)
I0614 16:03:08.410995  5063 solver.cpp:473] Iteration 2120, lr = 0.0001
I0614 16:03:09.294719  5063 solver.cpp:213] Iteration 2130, loss = 3.81281
I0614 16:03:09.294736  5063 solver.cpp:228]     Train net output #0: softmax = 3.81281 (* 1 = 3.81281 loss)
I0614 16:03:09.294741  5063 solver.cpp:473] Iteration 2130, lr = 0.0001
I0614 16:03:10.178342  5063 solver.cpp:213] Iteration 2140, loss = 3.92164
I0614 16:03:10.178357  5063 solver.cpp:228]     Train net output #0: softmax = 3.92164 (* 1 = 3.92164 loss)
I0614 16:03:10.178367  5063 solver.cpp:473] Iteration 2140, lr = 0.0001
I0614 16:03:11.069628  5063 solver.cpp:213] Iteration 2150, loss = 3.80331
I0614 16:03:11.069646  5063 solver.cpp:228]     Train net output #0: softmax = 3.80331 (* 1 = 3.80331 loss)
I0614 16:03:11.069651  5063 solver.cpp:473] Iteration 2150, lr = 0.0001
I0614 16:03:11.966266  5063 solver.cpp:213] Iteration 2160, loss = 3.60198
I0614 16:03:11.966289  5063 solver.cpp:228]     Train net output #0: softmax = 3.60198 (* 1 = 3.60198 loss)
I0614 16:03:11.966416  5063 solver.cpp:473] Iteration 2160, lr = 0.0001
I0614 16:03:12.855044  5063 solver.cpp:213] Iteration 2170, loss = 3.82969
I0614 16:03:12.855064  5063 solver.cpp:228]     Train net output #0: softmax = 3.82969 (* 1 = 3.82969 loss)
I0614 16:03:12.855069  5063 solver.cpp:473] Iteration 2170, lr = 0.0001
I0614 16:03:13.738286  5063 solver.cpp:213] Iteration 2180, loss = 3.98639
I0614 16:03:13.738302  5063 solver.cpp:228]     Train net output #0: softmax = 3.98639 (* 1 = 3.98639 loss)
I0614 16:03:13.738307  5063 solver.cpp:473] Iteration 2180, lr = 0.0001
I0614 16:03:14.623869  5063 solver.cpp:213] Iteration 2190, loss = 3.78091
I0614 16:03:14.623894  5063 solver.cpp:228]     Train net output #0: softmax = 3.78091 (* 1 = 3.78091 loss)
I0614 16:03:14.623899  5063 solver.cpp:473] Iteration 2190, lr = 0.0001
I0614 16:03:15.511627  5063 solver.cpp:213] Iteration 2200, loss = 3.72319
I0614 16:03:15.511654  5063 solver.cpp:228]     Train net output #0: softmax = 3.72319 (* 1 = 3.72319 loss)
I0614 16:03:15.511677  5063 solver.cpp:473] Iteration 2200, lr = 0.0001
I0614 16:03:16.404945  5063 solver.cpp:213] Iteration 2210, loss = 3.99031
I0614 16:03:16.404971  5063 solver.cpp:228]     Train net output #0: softmax = 3.99031 (* 1 = 3.99031 loss)
I0614 16:03:16.404976  5063 solver.cpp:473] Iteration 2210, lr = 0.0001
I0614 16:03:17.288429  5063 solver.cpp:213] Iteration 2220, loss = 3.90039
I0614 16:03:17.288461  5063 solver.cpp:228]     Train net output #0: softmax = 3.90039 (* 1 = 3.90039 loss)
I0614 16:03:17.288630  5063 solver.cpp:473] Iteration 2220, lr = 0.0001
I0614 16:03:18.171859  5063 solver.cpp:213] Iteration 2230, loss = 3.7728
I0614 16:03:18.171874  5063 solver.cpp:228]     Train net output #0: softmax = 3.7728 (* 1 = 3.7728 loss)
I0614 16:03:18.171878  5063 solver.cpp:473] Iteration 2230, lr = 0.0001
I0614 16:03:19.055192  5063 solver.cpp:213] Iteration 2240, loss = 3.73372
I0614 16:03:19.055207  5063 solver.cpp:228]     Train net output #0: softmax = 3.73372 (* 1 = 3.73372 loss)
I0614 16:03:19.055212  5063 solver.cpp:473] Iteration 2240, lr = 0.0001
I0614 16:03:19.938141  5063 solver.cpp:213] Iteration 2250, loss = 3.80159
I0614 16:03:19.938156  5063 solver.cpp:228]     Train net output #0: softmax = 3.80159 (* 1 = 3.80159 loss)
I0614 16:03:19.938161  5063 solver.cpp:473] Iteration 2250, lr = 0.0001
I0614 16:03:20.821647  5063 solver.cpp:213] Iteration 2260, loss = 4.08146
I0614 16:03:20.821661  5063 solver.cpp:228]     Train net output #0: softmax = 4.08146 (* 1 = 4.08146 loss)
I0614 16:03:20.821666  5063 solver.cpp:473] Iteration 2260, lr = 0.0001
I0614 16:03:21.704797  5063 solver.cpp:213] Iteration 2270, loss = 4.00493
I0614 16:03:21.704812  5063 solver.cpp:228]     Train net output #0: softmax = 4.00493 (* 1 = 4.00493 loss)
I0614 16:03:21.704816  5063 solver.cpp:473] Iteration 2270, lr = 0.0001
I0614 16:03:22.587968  5063 solver.cpp:213] Iteration 2280, loss = 4.03576
I0614 16:03:22.587990  5063 solver.cpp:228]     Train net output #0: softmax = 4.03576 (* 1 = 4.03576 loss)
I0614 16:03:22.587996  5063 solver.cpp:473] Iteration 2280, lr = 0.0001
I0614 16:03:23.470623  5063 solver.cpp:213] Iteration 2290, loss = 3.76333
I0614 16:03:23.470638  5063 solver.cpp:228]     Train net output #0: softmax = 3.76333 (* 1 = 3.76333 loss)
I0614 16:03:23.470643  5063 solver.cpp:473] Iteration 2290, lr = 0.0001
I0614 16:03:24.353070  5063 solver.cpp:213] Iteration 2300, loss = 3.95108
I0614 16:03:24.353085  5063 solver.cpp:228]     Train net output #0: softmax = 3.95108 (* 1 = 3.95108 loss)
I0614 16:03:24.353096  5063 solver.cpp:473] Iteration 2300, lr = 0.0001
I0614 16:03:25.235625  5063 solver.cpp:213] Iteration 2310, loss = 3.89604
I0614 16:03:25.235642  5063 solver.cpp:228]     Train net output #0: softmax = 3.89604 (* 1 = 3.89604 loss)
I0614 16:03:25.235648  5063 solver.cpp:473] Iteration 2310, lr = 0.0001
I0614 16:03:26.118543  5063 solver.cpp:213] Iteration 2320, loss = 3.87483
I0614 16:03:26.118559  5063 solver.cpp:228]     Train net output #0: softmax = 3.87483 (* 1 = 3.87483 loss)
I0614 16:03:26.118564  5063 solver.cpp:473] Iteration 2320, lr = 0.0001
I0614 16:03:27.001760  5063 solver.cpp:213] Iteration 2330, loss = 4.01547
I0614 16:03:27.001788  5063 solver.cpp:228]     Train net output #0: softmax = 4.01547 (* 1 = 4.01547 loss)
I0614 16:03:27.001793  5063 solver.cpp:473] Iteration 2330, lr = 0.0001
I0614 16:03:27.884614  5063 solver.cpp:213] Iteration 2340, loss = 3.84667
I0614 16:03:27.884632  5063 solver.cpp:228]     Train net output #0: softmax = 3.84667 (* 1 = 3.84667 loss)
I0614 16:03:27.884788  5063 solver.cpp:473] Iteration 2340, lr = 0.0001
I0614 16:03:28.767793  5063 solver.cpp:213] Iteration 2350, loss = 3.83121
I0614 16:03:28.767809  5063 solver.cpp:228]     Train net output #0: softmax = 3.83121 (* 1 = 3.83121 loss)
I0614 16:03:28.767814  5063 solver.cpp:473] Iteration 2350, lr = 0.0001
I0614 16:03:29.650859  5063 solver.cpp:213] Iteration 2360, loss = 3.80545
I0614 16:03:29.650874  5063 solver.cpp:228]     Train net output #0: softmax = 3.80545 (* 1 = 3.80545 loss)
I0614 16:03:29.650895  5063 solver.cpp:473] Iteration 2360, lr = 0.0001
I0614 16:03:30.534814  5063 solver.cpp:213] Iteration 2370, loss = 3.76559
I0614 16:03:30.534832  5063 solver.cpp:228]     Train net output #0: softmax = 3.76559 (* 1 = 3.76559 loss)
I0614 16:03:30.534837  5063 solver.cpp:473] Iteration 2370, lr = 0.0001
I0614 16:03:31.416975  5063 solver.cpp:213] Iteration 2380, loss = 4.00414
I0614 16:03:31.417008  5063 solver.cpp:228]     Train net output #0: softmax = 4.00414 (* 1 = 4.00414 loss)
I0614 16:03:31.417014  5063 solver.cpp:473] Iteration 2380, lr = 0.0001
I0614 16:03:32.299248  5063 solver.cpp:213] Iteration 2390, loss = 3.77952
I0614 16:03:32.299263  5063 solver.cpp:228]     Train net output #0: softmax = 3.77952 (* 1 = 3.77952 loss)
I0614 16:03:32.299268  5063 solver.cpp:473] Iteration 2390, lr = 0.0001
I0614 16:03:33.182214  5063 solver.cpp:213] Iteration 2400, loss = 3.8383
I0614 16:03:33.182234  5063 solver.cpp:228]     Train net output #0: softmax = 3.8383 (* 1 = 3.8383 loss)
I0614 16:03:33.182387  5063 solver.cpp:473] Iteration 2400, lr = 0.0001
I0614 16:03:34.065112  5063 solver.cpp:213] Iteration 2410, loss = 3.89384
I0614 16:03:34.065129  5063 solver.cpp:228]     Train net output #0: softmax = 3.89384 (* 1 = 3.89384 loss)
I0614 16:03:34.065134  5063 solver.cpp:473] Iteration 2410, lr = 0.0001
I0614 16:03:34.948353  5063 solver.cpp:213] Iteration 2420, loss = 3.81778
I0614 16:03:34.948367  5063 solver.cpp:228]     Train net output #0: softmax = 3.81778 (* 1 = 3.81778 loss)
I0614 16:03:34.948372  5063 solver.cpp:473] Iteration 2420, lr = 0.0001
I0614 16:03:35.831617  5063 solver.cpp:213] Iteration 2430, loss = 3.61871
I0614 16:03:35.831632  5063 solver.cpp:228]     Train net output #0: softmax = 3.61871 (* 1 = 3.61871 loss)
I0614 16:03:35.831637  5063 solver.cpp:473] Iteration 2430, lr = 0.0001
I0614 16:03:36.715101  5063 solver.cpp:213] Iteration 2440, loss = 3.93665
I0614 16:03:36.715121  5063 solver.cpp:228]     Train net output #0: softmax = 3.93665 (* 1 = 3.93665 loss)
I0614 16:03:36.715126  5063 solver.cpp:473] Iteration 2440, lr = 0.0001
I0614 16:03:37.598786  5063 solver.cpp:213] Iteration 2450, loss = 3.83523
I0614 16:03:37.598814  5063 solver.cpp:228]     Train net output #0: softmax = 3.83523 (* 1 = 3.83523 loss)
I0614 16:03:37.598819  5063 solver.cpp:473] Iteration 2450, lr = 0.0001
I0614 16:03:38.482318  5063 solver.cpp:213] Iteration 2460, loss = 3.79002
I0614 16:03:38.482336  5063 solver.cpp:228]     Train net output #0: softmax = 3.79002 (* 1 = 3.79002 loss)
I0614 16:03:38.482347  5063 solver.cpp:473] Iteration 2460, lr = 0.0001
I0614 16:03:39.365571  5063 solver.cpp:213] Iteration 2470, loss = 3.98063
I0614 16:03:39.365586  5063 solver.cpp:228]     Train net output #0: softmax = 3.98063 (* 1 = 3.98063 loss)
I0614 16:03:39.365592  5063 solver.cpp:473] Iteration 2470, lr = 0.0001
I0614 16:03:40.248762  5063 solver.cpp:213] Iteration 2480, loss = 3.62881
I0614 16:03:40.248777  5063 solver.cpp:228]     Train net output #0: softmax = 3.62881 (* 1 = 3.62881 loss)
I0614 16:03:40.248782  5063 solver.cpp:473] Iteration 2480, lr = 0.0001
I0614 16:03:41.131958  5063 solver.cpp:213] Iteration 2490, loss = 3.89356
I0614 16:03:41.131973  5063 solver.cpp:228]     Train net output #0: softmax = 3.89356 (* 1 = 3.89356 loss)
I0614 16:03:41.131978  5063 solver.cpp:473] Iteration 2490, lr = 0.0001
I0614 16:03:42.015297  5063 solver.cpp:213] Iteration 2500, loss = 3.84605
I0614 16:03:42.015316  5063 solver.cpp:228]     Train net output #0: softmax = 3.84605 (* 1 = 3.84605 loss)
I0614 16:03:42.015321  5063 solver.cpp:473] Iteration 2500, lr = 0.0001
I0614 16:03:42.898288  5063 solver.cpp:213] Iteration 2510, loss = 3.84459
I0614 16:03:42.898303  5063 solver.cpp:228]     Train net output #0: softmax = 3.84459 (* 1 = 3.84459 loss)
I0614 16:03:42.898308  5063 solver.cpp:473] Iteration 2510, lr = 0.0001
I0614 16:03:43.781493  5063 solver.cpp:213] Iteration 2520, loss = 3.83983
I0614 16:03:43.781512  5063 solver.cpp:228]     Train net output #0: softmax = 3.83983 (* 1 = 3.83983 loss)
I0614 16:03:43.781520  5063 solver.cpp:473] Iteration 2520, lr = 0.0001
I0614 16:03:44.664778  5063 solver.cpp:213] Iteration 2530, loss = 3.8227
I0614 16:03:44.664793  5063 solver.cpp:228]     Train net output #0: softmax = 3.8227 (* 1 = 3.8227 loss)
I0614 16:03:44.664798  5063 solver.cpp:473] Iteration 2530, lr = 0.0001
I0614 16:03:45.548513  5063 solver.cpp:213] Iteration 2540, loss = 3.79098
I0614 16:03:45.548527  5063 solver.cpp:228]     Train net output #0: softmax = 3.79098 (* 1 = 3.79098 loss)
I0614 16:03:45.548547  5063 solver.cpp:473] Iteration 2540, lr = 0.0001
I0614 16:03:46.431409  5063 solver.cpp:213] Iteration 2550, loss = 3.79299
I0614 16:03:46.431424  5063 solver.cpp:228]     Train net output #0: softmax = 3.79299 (* 1 = 3.79299 loss)
I0614 16:03:46.431429  5063 solver.cpp:473] Iteration 2550, lr = 0.0001
I0614 16:03:47.314888  5063 solver.cpp:213] Iteration 2560, loss = 3.67507
I0614 16:03:47.314915  5063 solver.cpp:228]     Train net output #0: softmax = 3.67507 (* 1 = 3.67507 loss)
I0614 16:03:47.314919  5063 solver.cpp:473] Iteration 2560, lr = 0.0001
I0614 16:03:48.198523  5063 solver.cpp:213] Iteration 2570, loss = 4.01408
I0614 16:03:48.198539  5063 solver.cpp:228]     Train net output #0: softmax = 4.01408 (* 1 = 4.01408 loss)
I0614 16:03:48.198544  5063 solver.cpp:473] Iteration 2570, lr = 0.0001
I0614 16:03:49.080493  5063 solver.cpp:213] Iteration 2580, loss = 3.88271
I0614 16:03:49.080528  5063 solver.cpp:228]     Train net output #0: softmax = 3.88271 (* 1 = 3.88271 loss)
I0614 16:03:49.080662  5063 solver.cpp:473] Iteration 2580, lr = 0.0001
I0614 16:03:49.964087  5063 solver.cpp:213] Iteration 2590, loss = 3.69709
I0614 16:03:49.964102  5063 solver.cpp:228]     Train net output #0: softmax = 3.69709 (* 1 = 3.69709 loss)
I0614 16:03:49.964105  5063 solver.cpp:473] Iteration 2590, lr = 0.0001
I0614 16:03:50.847290  5063 solver.cpp:213] Iteration 2600, loss = 3.8435
I0614 16:03:50.847306  5063 solver.cpp:228]     Train net output #0: softmax = 3.8435 (* 1 = 3.8435 loss)
I0614 16:03:50.847311  5063 solver.cpp:473] Iteration 2600, lr = 0.0001
I0614 16:03:51.730834  5063 solver.cpp:213] Iteration 2610, loss = 3.94626
I0614 16:03:51.730854  5063 solver.cpp:228]     Train net output #0: softmax = 3.94626 (* 1 = 3.94626 loss)
I0614 16:03:51.730860  5063 solver.cpp:473] Iteration 2610, lr = 0.0001
I0614 16:03:52.614465  5063 solver.cpp:213] Iteration 2620, loss = 3.68562
I0614 16:03:52.614480  5063 solver.cpp:228]     Train net output #0: softmax = 3.68562 (* 1 = 3.68562 loss)
I0614 16:03:52.614491  5063 solver.cpp:473] Iteration 2620, lr = 0.0001
I0614 16:03:53.497038  5063 solver.cpp:213] Iteration 2630, loss = 3.86886
I0614 16:03:53.497053  5063 solver.cpp:228]     Train net output #0: softmax = 3.86886 (* 1 = 3.86886 loss)
I0614 16:03:53.497058  5063 solver.cpp:473] Iteration 2630, lr = 0.0001
I0614 16:03:54.380391  5063 solver.cpp:213] Iteration 2640, loss = 3.97012
I0614 16:03:54.380412  5063 solver.cpp:228]     Train net output #0: softmax = 3.97012 (* 1 = 3.97012 loss)
I0614 16:03:54.380568  5063 solver.cpp:473] Iteration 2640, lr = 0.0001
I0614 16:03:55.263685  5063 solver.cpp:213] Iteration 2650, loss = 4.09428
I0614 16:03:55.263702  5063 solver.cpp:228]     Train net output #0: softmax = 4.09428 (* 1 = 4.09428 loss)
I0614 16:03:55.263708  5063 solver.cpp:473] Iteration 2650, lr = 0.0001
I0614 16:03:56.146605  5063 solver.cpp:213] Iteration 2660, loss = 4.11767
I0614 16:03:56.146621  5063 solver.cpp:228]     Train net output #0: softmax = 4.11767 (* 1 = 4.11767 loss)
I0614 16:03:56.146626  5063 solver.cpp:473] Iteration 2660, lr = 0.0001
I0614 16:03:57.029772  5063 solver.cpp:213] Iteration 2670, loss = 3.88107
I0614 16:03:57.029801  5063 solver.cpp:228]     Train net output #0: softmax = 3.88107 (* 1 = 3.88107 loss)
I0614 16:03:57.029806  5063 solver.cpp:473] Iteration 2670, lr = 0.0001
I0614 16:03:57.912566  5063 solver.cpp:213] Iteration 2680, loss = 3.81814
I0614 16:03:57.912583  5063 solver.cpp:228]     Train net output #0: softmax = 3.81814 (* 1 = 3.81814 loss)
I0614 16:03:57.912588  5063 solver.cpp:473] Iteration 2680, lr = 0.0001
I0614 16:03:58.796509  5063 solver.cpp:213] Iteration 2690, loss = 3.9334
I0614 16:03:58.796528  5063 solver.cpp:228]     Train net output #0: softmax = 3.9334 (* 1 = 3.9334 loss)
I0614 16:03:58.796535  5063 solver.cpp:473] Iteration 2690, lr = 0.0001
I0614 16:03:59.679769  5063 solver.cpp:213] Iteration 2700, loss = 3.90924
I0614 16:03:59.679791  5063 solver.cpp:228]     Train net output #0: softmax = 3.90924 (* 1 = 3.90924 loss)
I0614 16:03:59.679922  5063 solver.cpp:473] Iteration 2700, lr = 0.0001
I0614 16:04:00.562937  5063 solver.cpp:213] Iteration 2710, loss = 3.86553
I0614 16:04:00.562952  5063 solver.cpp:228]     Train net output #0: softmax = 3.86553 (* 1 = 3.86553 loss)
I0614 16:04:00.562958  5063 solver.cpp:473] Iteration 2710, lr = 0.0001
I0614 16:04:01.446813  5063 solver.cpp:213] Iteration 2720, loss = 3.93718
I0614 16:04:01.446847  5063 solver.cpp:228]     Train net output #0: softmax = 3.93718 (* 1 = 3.93718 loss)
I0614 16:04:01.446853  5063 solver.cpp:473] Iteration 2720, lr = 0.0001
I0614 16:04:02.328980  5063 solver.cpp:213] Iteration 2730, loss = 3.69865
I0614 16:04:02.328999  5063 solver.cpp:228]     Train net output #0: softmax = 3.69865 (* 1 = 3.69865 loss)
I0614 16:04:02.329005  5063 solver.cpp:473] Iteration 2730, lr = 0.0001
I0614 16:04:03.204402  5063 solver.cpp:213] Iteration 2740, loss = 3.69883
I0614 16:04:03.204418  5063 solver.cpp:228]     Train net output #0: softmax = 3.69883 (* 1 = 3.69883 loss)
I0614 16:04:03.204423  5063 solver.cpp:473] Iteration 2740, lr = 0.0001
I0614 16:04:04.087455  5063 solver.cpp:213] Iteration 2750, loss = 3.75272
I0614 16:04:04.087469  5063 solver.cpp:228]     Train net output #0: softmax = 3.75272 (* 1 = 3.75272 loss)
I0614 16:04:04.087474  5063 solver.cpp:473] Iteration 2750, lr = 0.0001
I0614 16:04:04.971262  5063 solver.cpp:213] Iteration 2760, loss = 3.75545
I0614 16:04:04.971282  5063 solver.cpp:228]     Train net output #0: softmax = 3.75545 (* 1 = 3.75545 loss)
I0614 16:04:04.971441  5063 solver.cpp:473] Iteration 2760, lr = 0.0001
I0614 16:04:05.853973  5063 solver.cpp:213] Iteration 2770, loss = 3.85059
I0614 16:04:05.853989  5063 solver.cpp:228]     Train net output #0: softmax = 3.85059 (* 1 = 3.85059 loss)
I0614 16:04:05.853994  5063 solver.cpp:473] Iteration 2770, lr = 0.0001
I0614 16:04:06.730474  5063 solver.cpp:213] Iteration 2780, loss = 3.64548
I0614 16:04:06.730492  5063 solver.cpp:228]     Train net output #0: softmax = 3.64548 (* 1 = 3.64548 loss)
I0614 16:04:06.730506  5063 solver.cpp:473] Iteration 2780, lr = 0.0001
I0614 16:04:07.606740  5063 solver.cpp:213] Iteration 2790, loss = 3.68911
I0614 16:04:07.606770  5063 solver.cpp:228]     Train net output #0: softmax = 3.68911 (* 1 = 3.68911 loss)
I0614 16:04:07.606775  5063 solver.cpp:473] Iteration 2790, lr = 0.0001
I0614 16:04:08.490280  5063 solver.cpp:213] Iteration 2800, loss = 3.73216
I0614 16:04:08.490298  5063 solver.cpp:228]     Train net output #0: softmax = 3.73216 (* 1 = 3.73216 loss)
I0614 16:04:08.490304  5063 solver.cpp:473] Iteration 2800, lr = 0.0001
I0614 16:04:09.373145  5063 solver.cpp:213] Iteration 2810, loss = 3.79277
I0614 16:04:09.373162  5063 solver.cpp:228]     Train net output #0: softmax = 3.79277 (* 1 = 3.79277 loss)
I0614 16:04:09.373168  5063 solver.cpp:473] Iteration 2810, lr = 0.0001
I0614 16:04:10.256934  5063 solver.cpp:213] Iteration 2820, loss = 3.85217
I0614 16:04:10.256959  5063 solver.cpp:228]     Train net output #0: softmax = 3.85217 (* 1 = 3.85217 loss)
I0614 16:04:10.257110  5063 solver.cpp:473] Iteration 2820, lr = 0.0001
I0614 16:04:11.140799  5063 solver.cpp:213] Iteration 2830, loss = 3.91002
I0614 16:04:11.140815  5063 solver.cpp:228]     Train net output #0: softmax = 3.91002 (* 1 = 3.91002 loss)
I0614 16:04:11.140820  5063 solver.cpp:473] Iteration 2830, lr = 0.0001
I0614 16:04:12.024296  5063 solver.cpp:213] Iteration 2840, loss = 3.64917
I0614 16:04:12.024312  5063 solver.cpp:228]     Train net output #0: softmax = 3.64917 (* 1 = 3.64917 loss)
I0614 16:04:12.024317  5063 solver.cpp:473] Iteration 2840, lr = 0.0001
I0614 16:04:12.907790  5063 solver.cpp:213] Iteration 2850, loss = 3.72898
I0614 16:04:12.907806  5063 solver.cpp:228]     Train net output #0: softmax = 3.72898 (* 1 = 3.72898 loss)
I0614 16:04:12.907811  5063 solver.cpp:473] Iteration 2850, lr = 0.0001
I0614 16:04:13.789577  5063 solver.cpp:213] Iteration 2860, loss = 3.89737
I0614 16:04:13.789592  5063 solver.cpp:228]     Train net output #0: softmax = 3.89737 (* 1 = 3.89737 loss)
I0614 16:04:13.789597  5063 solver.cpp:473] Iteration 2860, lr = 0.0001
I0614 16:04:14.671993  5063 solver.cpp:213] Iteration 2870, loss = 3.81916
I0614 16:04:14.672008  5063 solver.cpp:228]     Train net output #0: softmax = 3.81916 (* 1 = 3.81916 loss)
I0614 16:04:14.672013  5063 solver.cpp:473] Iteration 2870, lr = 0.0001
I0614 16:04:15.553293  5063 solver.cpp:213] Iteration 2880, loss = 3.83729
I0614 16:04:15.553310  5063 solver.cpp:228]     Train net output #0: softmax = 3.83729 (* 1 = 3.83729 loss)
I0614 16:04:15.553333  5063 solver.cpp:473] Iteration 2880, lr = 0.0001
I0614 16:04:16.436795  5063 solver.cpp:213] Iteration 2890, loss = 3.86707
I0614 16:04:16.436810  5063 solver.cpp:228]     Train net output #0: softmax = 3.86707 (* 1 = 3.86707 loss)
I0614 16:04:16.436815  5063 solver.cpp:473] Iteration 2890, lr = 0.0001
I0614 16:04:17.319638  5063 solver.cpp:213] Iteration 2900, loss = 3.81569
I0614 16:04:17.319658  5063 solver.cpp:228]     Train net output #0: softmax = 3.81569 (* 1 = 3.81569 loss)
I0614 16:04:17.319664  5063 solver.cpp:473] Iteration 2900, lr = 0.0001
I0614 16:04:18.203160  5063 solver.cpp:213] Iteration 2910, loss = 3.87265
I0614 16:04:18.203176  5063 solver.cpp:228]     Train net output #0: softmax = 3.87265 (* 1 = 3.87265 loss)
I0614 16:04:18.203181  5063 solver.cpp:473] Iteration 2910, lr = 0.0001
I0614 16:04:19.085979  5063 solver.cpp:213] Iteration 2920, loss = 3.66214
I0614 16:04:19.085995  5063 solver.cpp:228]     Train net output #0: softmax = 3.66214 (* 1 = 3.66214 loss)
I0614 16:04:19.086000  5063 solver.cpp:473] Iteration 2920, lr = 0.0001
I0614 16:04:19.968972  5063 solver.cpp:213] Iteration 2930, loss = 3.76265
I0614 16:04:19.968987  5063 solver.cpp:228]     Train net output #0: softmax = 3.76265 (* 1 = 3.76265 loss)
I0614 16:04:19.968992  5063 solver.cpp:473] Iteration 2930, lr = 0.0001
I0614 16:04:20.850878  5063 solver.cpp:213] Iteration 2940, loss = 3.88064
I0614 16:04:20.850896  5063 solver.cpp:228]     Train net output #0: softmax = 3.88064 (* 1 = 3.88064 loss)
I0614 16:04:20.851018  5063 solver.cpp:473] Iteration 2940, lr = 0.0001
I0614 16:04:21.734169  5063 solver.cpp:213] Iteration 2950, loss = 3.56563
I0614 16:04:21.734184  5063 solver.cpp:228]     Train net output #0: softmax = 3.56563 (* 1 = 3.56563 loss)
I0614 16:04:21.734189  5063 solver.cpp:473] Iteration 2950, lr = 0.0001
I0614 16:04:22.617988  5063 solver.cpp:213] Iteration 2960, loss = 3.83931
I0614 16:04:22.618005  5063 solver.cpp:228]     Train net output #0: softmax = 3.83931 (* 1 = 3.83931 loss)
I0614 16:04:22.618010  5063 solver.cpp:473] Iteration 2960, lr = 0.0001
I0614 16:04:23.500809  5063 solver.cpp:213] Iteration 2970, loss = 3.7682
I0614 16:04:23.500824  5063 solver.cpp:228]     Train net output #0: softmax = 3.7682 (* 1 = 3.7682 loss)
I0614 16:04:23.500829  5063 solver.cpp:473] Iteration 2970, lr = 0.0001
I0614 16:04:24.384227  5063 solver.cpp:213] Iteration 2980, loss = 3.76349
I0614 16:04:24.384243  5063 solver.cpp:228]     Train net output #0: softmax = 3.76349 (* 1 = 3.76349 loss)
I0614 16:04:24.384248  5063 solver.cpp:473] Iteration 2980, lr = 0.0001
I0614 16:04:25.267437  5063 solver.cpp:213] Iteration 2990, loss = 3.64376
I0614 16:04:25.267452  5063 solver.cpp:228]     Train net output #0: softmax = 3.64376 (* 1 = 3.64376 loss)
I0614 16:04:25.267457  5063 solver.cpp:473] Iteration 2990, lr = 0.0001
I0614 16:04:26.092160  5063 solver.cpp:362] Snapshotting to snapshots/16-06-14_15h51m21s_0_11_pretrainClassification_iter_3000.caffemodel
I0614 16:04:26.092955  5063 solver.cpp:370] Snapshotting solver state to snapshots/16-06-14_15h51m21s_0_11_pretrainClassification_iter_3000.solverstate
I0614 16:04:26.093411  5063 solver.cpp:291] Iteration 3000, Testing net (#0)
I0614 16:04:26.206063  5063 solver.cpp:342]     Test net output #0: accuracy_top_1 = 0.154687
I0614 16:04:26.206078  5063 solver.cpp:342]     Test net output #1: accuracy_top_5 = 0.396875
I0614 16:04:26.206084  5063 solver.cpp:342]     Test net output #2: softmax = 3.64134 (* 1 = 3.64134 loss)
I0614 16:04:26.264107  5063 solver.cpp:213] Iteration 3000, loss = 3.87401
I0614 16:04:26.264120  5063 solver.cpp:228]     Train net output #0: softmax = 3.87401 (* 1 = 3.87401 loss)
I0614 16:04:26.264125  5063 solver.cpp:473] Iteration 3000, lr = 0.0001
I0614 16:04:27.145678  5063 solver.cpp:213] Iteration 3010, loss = 3.61003
I0614 16:04:27.145694  5063 solver.cpp:228]     Train net output #0: softmax = 3.61003 (* 1 = 3.61003 loss)
I0614 16:04:27.145699  5063 solver.cpp:473] Iteration 3010, lr = 0.0001
I0614 16:04:28.028290  5063 solver.cpp:213] Iteration 3020, loss = 3.71012
I0614 16:04:28.028308  5063 solver.cpp:228]     Train net output #0: softmax = 3.71012 (* 1 = 3.71012 loss)
I0614 16:04:28.028313  5063 solver.cpp:473] Iteration 3020, lr = 0.0001
I0614 16:04:28.910642  5063 solver.cpp:213] Iteration 3030, loss = 3.85298
I0614 16:04:28.910656  5063 solver.cpp:228]     Train net output #0: softmax = 3.85298 (* 1 = 3.85298 loss)
I0614 16:04:28.910661  5063 solver.cpp:473] Iteration 3030, lr = 0.0001
I0614 16:04:29.793263  5063 solver.cpp:213] Iteration 3040, loss = 3.74684
I0614 16:04:29.793277  5063 solver.cpp:228]     Train net output #0: softmax = 3.74684 (* 1 = 3.74684 loss)
I0614 16:04:29.793282  5063 solver.cpp:473] Iteration 3040, lr = 0.0001
I0614 16:04:30.675940  5063 solver.cpp:213] Iteration 3050, loss = 3.82019
I0614 16:04:30.675954  5063 solver.cpp:228]     Train net output #0: softmax = 3.82019 (* 1 = 3.82019 loss)
I0614 16:04:30.675958  5063 solver.cpp:473] Iteration 3050, lr = 0.0001
I0614 16:04:31.559325  5063 solver.cpp:213] Iteration 3060, loss = 3.75364
I0614 16:04:31.559375  5063 solver.cpp:228]     Train net output #0: softmax = 3.75364 (* 1 = 3.75364 loss)
I0614 16:04:31.559381  5063 solver.cpp:473] Iteration 3060, lr = 0.0001
I0614 16:04:32.441943  5063 solver.cpp:213] Iteration 3070, loss = 3.7132
I0614 16:04:32.441958  5063 solver.cpp:228]     Train net output #0: softmax = 3.7132 (* 1 = 3.7132 loss)
I0614 16:04:32.441962  5063 solver.cpp:473] Iteration 3070, lr = 0.0001
I0614 16:04:33.325196  5063 solver.cpp:213] Iteration 3080, loss = 3.78091
I0614 16:04:33.325217  5063 solver.cpp:228]     Train net output #0: softmax = 3.78091 (* 1 = 3.78091 loss)
I0614 16:04:33.325223  5063 solver.cpp:473] Iteration 3080, lr = 0.0001
I0614 16:04:34.208662  5063 solver.cpp:213] Iteration 3090, loss = 3.78931
I0614 16:04:34.208678  5063 solver.cpp:228]     Train net output #0: softmax = 3.78931 (* 1 = 3.78931 loss)
I0614 16:04:34.208683  5063 solver.cpp:473] Iteration 3090, lr = 0.0001
I0614 16:04:35.090906  5063 solver.cpp:213] Iteration 3100, loss = 3.86114
I0614 16:04:35.090921  5063 solver.cpp:228]     Train net output #0: softmax = 3.86114 (* 1 = 3.86114 loss)
I0614 16:04:35.090926  5063 solver.cpp:473] Iteration 3100, lr = 0.0001
I0614 16:04:35.973693  5063 solver.cpp:213] Iteration 3110, loss = 3.96377
I0614 16:04:35.973708  5063 solver.cpp:228]     Train net output #0: softmax = 3.96377 (* 1 = 3.96377 loss)
I0614 16:04:35.973713  5063 solver.cpp:473] Iteration 3110, lr = 0.0001
I0614 16:04:36.856323  5063 solver.cpp:213] Iteration 3120, loss = 3.86042
I0614 16:04:36.856340  5063 solver.cpp:228]     Train net output #0: softmax = 3.86042 (* 1 = 3.86042 loss)
I0614 16:04:36.856487  5063 solver.cpp:473] Iteration 3120, lr = 0.0001
I0614 16:04:37.739430  5063 solver.cpp:213] Iteration 3130, loss = 3.88557
I0614 16:04:37.739445  5063 solver.cpp:228]     Train net output #0: softmax = 3.88557 (* 1 = 3.88557 loss)
I0614 16:04:37.739450  5063 solver.cpp:473] Iteration 3130, lr = 0.0001
I0614 16:04:38.622655  5063 solver.cpp:213] Iteration 3140, loss = 3.95941
I0614 16:04:38.622670  5063 solver.cpp:228]     Train net output #0: softmax = 3.95941 (* 1 = 3.95941 loss)
I0614 16:04:38.622675  5063 solver.cpp:473] Iteration 3140, lr = 0.0001
I0614 16:04:39.506126  5063 solver.cpp:213] Iteration 3150, loss = 3.73681
I0614 16:04:39.506140  5063 solver.cpp:228]     Train net output #0: softmax = 3.73681 (* 1 = 3.73681 loss)
I0614 16:04:39.506145  5063 solver.cpp:473] Iteration 3150, lr = 0.0001
I0614 16:04:40.389698  5063 solver.cpp:213] Iteration 3160, loss = 3.84831
I0614 16:04:40.389715  5063 solver.cpp:228]     Train net output #0: softmax = 3.84831 (* 1 = 3.84831 loss)
I0614 16:04:40.389720  5063 solver.cpp:473] Iteration 3160, lr = 0.0001
I0614 16:04:41.273334  5063 solver.cpp:213] Iteration 3170, loss = 3.8223
I0614 16:04:41.273351  5063 solver.cpp:228]     Train net output #0: softmax = 3.8223 (* 1 = 3.8223 loss)
I0614 16:04:41.273356  5063 solver.cpp:473] Iteration 3170, lr = 0.0001
I0614 16:04:42.156277  5063 solver.cpp:213] Iteration 3180, loss = 3.71102
I0614 16:04:42.156296  5063 solver.cpp:228]     Train net output #0: softmax = 3.71102 (* 1 = 3.71102 loss)
I0614 16:04:42.156436  5063 solver.cpp:473] Iteration 3180, lr = 0.0001
I0614 16:04:43.039327  5063 solver.cpp:213] Iteration 3190, loss = 3.74138
I0614 16:04:43.039342  5063 solver.cpp:228]     Train net output #0: softmax = 3.74138 (* 1 = 3.74138 loss)
I0614 16:04:43.039347  5063 solver.cpp:473] Iteration 3190, lr = 0.0001
I0614 16:04:43.922618  5063 solver.cpp:213] Iteration 3200, loss = 3.69756
I0614 16:04:43.922633  5063 solver.cpp:228]     Train net output #0: softmax = 3.69756 (* 1 = 3.69756 loss)
I0614 16:04:43.922638  5063 solver.cpp:473] Iteration 3200, lr = 0.0001
I0614 16:04:44.804718  5063 solver.cpp:213] Iteration 3210, loss = 3.75747
I0614 16:04:44.804733  5063 solver.cpp:228]     Train net output #0: softmax = 3.75747 (* 1 = 3.75747 loss)
I0614 16:04:44.804738  5063 solver.cpp:473] Iteration 3210, lr = 0.0001
I0614 16:04:45.688256  5063 solver.cpp:213] Iteration 3220, loss = 3.93263
I0614 16:04:45.688271  5063 solver.cpp:228]     Train net output #0: softmax = 3.93263 (* 1 = 3.93263 loss)
I0614 16:04:45.688292  5063 solver.cpp:473] Iteration 3220, lr = 0.0001
I0614 16:04:46.571424  5063 solver.cpp:213] Iteration 3230, loss = 3.82055
I0614 16:04:46.571437  5063 solver.cpp:228]     Train net output #0: softmax = 3.82055 (* 1 = 3.82055 loss)
I0614 16:04:46.571442  5063 solver.cpp:473] Iteration 3230, lr = 0.0001
I0614 16:04:47.453687  5063 solver.cpp:213] Iteration 3240, loss = 3.73517
I0614 16:04:47.453712  5063 solver.cpp:228]     Train net output #0: softmax = 3.73517 (* 1 = 3.73517 loss)
I0614 16:04:47.453717  5063 solver.cpp:473] Iteration 3240, lr = 0.0001
I0614 16:04:48.336359  5063 solver.cpp:213] Iteration 3250, loss = 3.88656
I0614 16:04:48.336380  5063 solver.cpp:228]     Train net output #0: softmax = 3.88656 (* 1 = 3.88656 loss)
I0614 16:04:48.336385  5063 solver.cpp:473] Iteration 3250, lr = 0.0001
I0614 16:04:49.219125  5063 solver.cpp:213] Iteration 3260, loss = 3.86958
I0614 16:04:49.219142  5063 solver.cpp:228]     Train net output #0: softmax = 3.86958 (* 1 = 3.86958 loss)
I0614 16:04:49.219147  5063 solver.cpp:473] Iteration 3260, lr = 0.0001
I0614 16:04:50.101732  5063 solver.cpp:213] Iteration 3270, loss = 3.78005
I0614 16:04:50.101748  5063 solver.cpp:228]     Train net output #0: softmax = 3.78005 (* 1 = 3.78005 loss)
I0614 16:04:50.101753  5063 solver.cpp:473] Iteration 3270, lr = 0.0001
I0614 16:04:50.984985  5063 solver.cpp:213] Iteration 3280, loss = 3.77808
I0614 16:04:50.985004  5063 solver.cpp:228]     Train net output #0: softmax = 3.77808 (* 1 = 3.77808 loss)
I0614 16:04:50.985008  5063 solver.cpp:473] Iteration 3280, lr = 0.0001
I0614 16:04:51.867835  5063 solver.cpp:213] Iteration 3290, loss = 3.67997
I0614 16:04:51.867853  5063 solver.cpp:228]     Train net output #0: softmax = 3.67997 (* 1 = 3.67997 loss)
I0614 16:04:51.867858  5063 solver.cpp:473] Iteration 3290, lr = 0.0001
I0614 16:04:52.750928  5063 solver.cpp:213] Iteration 3300, loss = 3.89469
I0614 16:04:52.750951  5063 solver.cpp:228]     Train net output #0: softmax = 3.89469 (* 1 = 3.89469 loss)
I0614 16:04:52.750960  5063 solver.cpp:473] Iteration 3300, lr = 0.0001
I0614 16:04:53.633350  5063 solver.cpp:213] Iteration 3310, loss = 3.77097
I0614 16:04:53.633371  5063 solver.cpp:228]     Train net output #0: softmax = 3.77097 (* 1 = 3.77097 loss)
I0614 16:04:53.633376  5063 solver.cpp:473] Iteration 3310, lr = 0.0001
I0614 16:04:54.516651  5063 solver.cpp:213] Iteration 3320, loss = 3.73709
I0614 16:04:54.516669  5063 solver.cpp:228]     Train net output #0: softmax = 3.73709 (* 1 = 3.73709 loss)
I0614 16:04:54.516674  5063 solver.cpp:473] Iteration 3320, lr = 0.0001
I0614 16:04:55.399075  5063 solver.cpp:213] Iteration 3330, loss = 3.69347
I0614 16:04:55.399091  5063 solver.cpp:228]     Train net output #0: softmax = 3.69347 (* 1 = 3.69347 loss)
I0614 16:04:55.399096  5063 solver.cpp:473] Iteration 3330, lr = 0.0001
I0614 16:04:56.282070  5063 solver.cpp:213] Iteration 3340, loss = 3.60518
I0614 16:04:56.282085  5063 solver.cpp:228]     Train net output #0: softmax = 3.60518 (* 1 = 3.60518 loss)
I0614 16:04:56.282090  5063 solver.cpp:473] Iteration 3340, lr = 0.0001
I0614 16:04:57.164775  5063 solver.cpp:213] Iteration 3350, loss = 3.53209
I0614 16:04:57.164793  5063 solver.cpp:228]     Train net output #0: softmax = 3.53209 (* 1 = 3.53209 loss)
I0614 16:04:57.164798  5063 solver.cpp:473] Iteration 3350, lr = 0.0001
I0614 16:04:58.049049  5063 solver.cpp:213] Iteration 3360, loss = 3.51226
I0614 16:04:58.049072  5063 solver.cpp:228]     Train net output #0: softmax = 3.51226 (* 1 = 3.51226 loss)
I0614 16:04:58.049196  5063 solver.cpp:473] Iteration 3360, lr = 0.0001
I0614 16:04:58.932610  5063 solver.cpp:213] Iteration 3370, loss = 3.67117
I0614 16:04:58.932631  5063 solver.cpp:228]     Train net output #0: softmax = 3.67117 (* 1 = 3.67117 loss)
I0614 16:04:58.932636  5063 solver.cpp:473] Iteration 3370, lr = 0.0001
I0614 16:04:59.815073  5063 solver.cpp:213] Iteration 3380, loss = 3.5748
I0614 16:04:59.815090  5063 solver.cpp:228]     Train net output #0: softmax = 3.5748 (* 1 = 3.5748 loss)
I0614 16:04:59.815111  5063 solver.cpp:473] Iteration 3380, lr = 0.0001
I0614 16:05:00.697746  5063 solver.cpp:213] Iteration 3390, loss = 3.63916
I0614 16:05:00.697762  5063 solver.cpp:228]     Train net output #0: softmax = 3.63916 (* 1 = 3.63916 loss)
I0614 16:05:00.697767  5063 solver.cpp:473] Iteration 3390, lr = 0.0001
I0614 16:05:01.581338  5063 solver.cpp:213] Iteration 3400, loss = 3.77252
I0614 16:05:01.581379  5063 solver.cpp:228]     Train net output #0: softmax = 3.77252 (* 1 = 3.77252 loss)
I0614 16:05:01.581387  5063 solver.cpp:473] Iteration 3400, lr = 0.0001
I0614 16:05:02.464697  5063 solver.cpp:213] Iteration 3410, loss = 3.72701
I0614 16:05:02.464716  5063 solver.cpp:228]     Train net output #0: softmax = 3.72701 (* 1 = 3.72701 loss)
I0614 16:05:02.464721  5063 solver.cpp:473] Iteration 3410, lr = 0.0001
I0614 16:05:03.347513  5063 solver.cpp:213] Iteration 3420, loss = 3.79017
I0614 16:05:03.347534  5063 solver.cpp:228]     Train net output #0: softmax = 3.79017 (* 1 = 3.79017 loss)
I0614 16:05:03.347544  5063 solver.cpp:473] Iteration 3420, lr = 0.0001
I0614 16:05:04.230988  5063 solver.cpp:213] Iteration 3430, loss = 3.68851
I0614 16:05:04.231006  5063 solver.cpp:228]     Train net output #0: softmax = 3.68851 (* 1 = 3.68851 loss)
I0614 16:05:04.231011  5063 solver.cpp:473] Iteration 3430, lr = 0.0001
I0614 16:05:05.114622  5063 solver.cpp:213] Iteration 3440, loss = 3.71098
I0614 16:05:05.114642  5063 solver.cpp:228]     Train net output #0: softmax = 3.71098 (* 1 = 3.71098 loss)
I0614 16:05:05.114648  5063 solver.cpp:473] Iteration 3440, lr = 0.0001
I0614 16:05:05.997911  5063 solver.cpp:213] Iteration 3450, loss = 3.66605
I0614 16:05:05.997930  5063 solver.cpp:228]     Train net output #0: softmax = 3.66605 (* 1 = 3.66605 loss)
I0614 16:05:05.997933  5063 solver.cpp:473] Iteration 3450, lr = 0.0001
I0614 16:05:06.880720  5063 solver.cpp:213] Iteration 3460, loss = 3.76177
I0614 16:05:06.880740  5063 solver.cpp:228]     Train net output #0: softmax = 3.76177 (* 1 = 3.76177 loss)
I0614 16:05:06.880745  5063 solver.cpp:473] Iteration 3460, lr = 0.0001
I0614 16:05:07.763722  5063 solver.cpp:213] Iteration 3470, loss = 3.62414
I0614 16:05:07.763741  5063 solver.cpp:228]     Train net output #0: softmax = 3.62414 (* 1 = 3.62414 loss)
I0614 16:05:07.763746  5063 solver.cpp:473] Iteration 3470, lr = 0.0001
I0614 16:05:08.647147  5063 solver.cpp:213] Iteration 3480, loss = 3.6302
I0614 16:05:08.647171  5063 solver.cpp:228]     Train net output #0: softmax = 3.6302 (* 1 = 3.6302 loss)
I0614 16:05:08.647294  5063 solver.cpp:473] Iteration 3480, lr = 0.0001
I0614 16:05:09.530629  5063 solver.cpp:213] Iteration 3490, loss = 3.59758
I0614 16:05:09.530649  5063 solver.cpp:228]     Train net output #0: softmax = 3.59758 (* 1 = 3.59758 loss)
I0614 16:05:09.530654  5063 solver.cpp:473] Iteration 3490, lr = 0.0001
I0614 16:05:10.413424  5063 solver.cpp:213] Iteration 3500, loss = 3.89423
I0614 16:05:10.413444  5063 solver.cpp:228]     Train net output #0: softmax = 3.89423 (* 1 = 3.89423 loss)
I0614 16:05:10.413449  5063 solver.cpp:473] Iteration 3500, lr = 0.0001
I0614 16:05:11.296087  5063 solver.cpp:213] Iteration 3510, loss = 3.56812
I0614 16:05:11.296103  5063 solver.cpp:228]     Train net output #0: softmax = 3.56812 (* 1 = 3.56812 loss)
I0614 16:05:11.296108  5063 solver.cpp:473] Iteration 3510, lr = 0.0001
I0614 16:05:12.179152  5063 solver.cpp:213] Iteration 3520, loss = 3.87241
I0614 16:05:12.179170  5063 solver.cpp:228]     Train net output #0: softmax = 3.87241 (* 1 = 3.87241 loss)
I0614 16:05:12.179175  5063 solver.cpp:473] Iteration 3520, lr = 0.0001
I0614 16:05:13.061820  5063 solver.cpp:213] Iteration 3530, loss = 3.8456
I0614 16:05:13.061836  5063 solver.cpp:228]     Train net output #0: softmax = 3.8456 (* 1 = 3.8456 loss)
I0614 16:05:13.061841  5063 solver.cpp:473] Iteration 3530, lr = 0.0001
I0614 16:05:13.944654  5063 solver.cpp:213] Iteration 3540, loss = 3.58564
I0614 16:05:13.944672  5063 solver.cpp:228]     Train net output #0: softmax = 3.58564 (* 1 = 3.58564 loss)
I0614 16:05:13.944818  5063 solver.cpp:473] Iteration 3540, lr = 0.0001
I0614 16:05:14.828642  5063 solver.cpp:213] Iteration 3550, loss = 3.75004
I0614 16:05:14.828663  5063 solver.cpp:228]     Train net output #0: softmax = 3.75004 (* 1 = 3.75004 loss)
I0614 16:05:14.828668  5063 solver.cpp:473] Iteration 3550, lr = 0.0001
I0614 16:05:15.712330  5063 solver.cpp:213] Iteration 3560, loss = 3.7839
I0614 16:05:15.712353  5063 solver.cpp:228]     Train net output #0: softmax = 3.7839 (* 1 = 3.7839 loss)
I0614 16:05:15.712376  5063 solver.cpp:473] Iteration 3560, lr = 0.0001
I0614 16:05:16.595161  5063 solver.cpp:213] Iteration 3570, loss = 3.54242
I0614 16:05:16.595177  5063 solver.cpp:228]     Train net output #0: softmax = 3.54242 (* 1 = 3.54242 loss)
I0614 16:05:16.595182  5063 solver.cpp:473] Iteration 3570, lr = 0.0001
I0614 16:05:17.477705  5063 solver.cpp:213] Iteration 3580, loss = 3.75114
I0614 16:05:17.477720  5063 solver.cpp:228]     Train net output #0: softmax = 3.75114 (* 1 = 3.75114 loss)
I0614 16:05:17.477725  5063 solver.cpp:473] Iteration 3580, lr = 0.0001
I0614 16:05:18.361557  5063 solver.cpp:213] Iteration 3590, loss = 3.62828
I0614 16:05:18.361577  5063 solver.cpp:228]     Train net output #0: softmax = 3.62828 (* 1 = 3.62828 loss)
I0614 16:05:18.361582  5063 solver.cpp:473] Iteration 3590, lr = 0.0001
I0614 16:05:19.244740  5063 solver.cpp:213] Iteration 3600, loss = 3.69565
I0614 16:05:19.244760  5063 solver.cpp:228]     Train net output #0: softmax = 3.69565 (* 1 = 3.69565 loss)
I0614 16:05:19.244920  5063 solver.cpp:473] Iteration 3600, lr = 0.0001
I0614 16:05:20.128628  5063 solver.cpp:213] Iteration 3610, loss = 3.83677
I0614 16:05:20.128649  5063 solver.cpp:228]     Train net output #0: softmax = 3.83677 (* 1 = 3.83677 loss)
I0614 16:05:20.128654  5063 solver.cpp:473] Iteration 3610, lr = 0.0001
I0614 16:05:21.011682  5063 solver.cpp:213] Iteration 3620, loss = 3.94355
I0614 16:05:21.011703  5063 solver.cpp:228]     Train net output #0: softmax = 3.94355 (* 1 = 3.94355 loss)
I0614 16:05:21.011708  5063 solver.cpp:473] Iteration 3620, lr = 0.0001
I0614 16:05:21.889008  5063 solver.cpp:213] Iteration 3630, loss = 3.85507
I0614 16:05:21.889034  5063 solver.cpp:228]     Train net output #0: softmax = 3.85507 (* 1 = 3.85507 loss)
I0614 16:05:21.889041  5063 solver.cpp:473] Iteration 3630, lr = 0.0001
I0614 16:05:22.770292  5063 solver.cpp:213] Iteration 3640, loss = 3.82701
I0614 16:05:22.770308  5063 solver.cpp:228]     Train net output #0: softmax = 3.82701 (* 1 = 3.82701 loss)
I0614 16:05:22.770313  5063 solver.cpp:473] Iteration 3640, lr = 0.0001
I0614 16:05:23.654384  5063 solver.cpp:213] Iteration 3650, loss = 3.84462
I0614 16:05:23.654405  5063 solver.cpp:228]     Train net output #0: softmax = 3.84462 (* 1 = 3.84462 loss)
I0614 16:05:23.654410  5063 solver.cpp:473] Iteration 3650, lr = 0.0001
I0614 16:05:24.537339  5063 solver.cpp:213] Iteration 3660, loss = 3.72801
I0614 16:05:24.537358  5063 solver.cpp:228]     Train net output #0: softmax = 3.72801 (* 1 = 3.72801 loss)
I0614 16:05:24.537364  5063 solver.cpp:473] Iteration 3660, lr = 0.0001
I0614 16:05:25.420730  5063 solver.cpp:213] Iteration 3670, loss = 3.61666
I0614 16:05:25.420749  5063 solver.cpp:228]     Train net output #0: softmax = 3.61666 (* 1 = 3.61666 loss)
I0614 16:05:25.420753  5063 solver.cpp:473] Iteration 3670, lr = 0.0001
I0614 16:05:26.304167  5063 solver.cpp:213] Iteration 3680, loss = 3.77139
I0614 16:05:26.304184  5063 solver.cpp:228]     Train net output #0: softmax = 3.77139 (* 1 = 3.77139 loss)
I0614 16:05:26.304189  5063 solver.cpp:473] Iteration 3680, lr = 0.0001
I0614 16:05:27.187317  5063 solver.cpp:213] Iteration 3690, loss = 3.84866
I0614 16:05:27.187335  5063 solver.cpp:228]     Train net output #0: softmax = 3.84866 (* 1 = 3.84866 loss)
I0614 16:05:27.187340  5063 solver.cpp:473] Iteration 3690, lr = 0.0001
I0614 16:05:28.070672  5063 solver.cpp:213] Iteration 3700, loss = 3.72841
I0614 16:05:28.070690  5063 solver.cpp:228]     Train net output #0: softmax = 3.72841 (* 1 = 3.72841 loss)
I0614 16:05:28.070695  5063 solver.cpp:473] Iteration 3700, lr = 0.0001
I0614 16:05:28.953050  5063 solver.cpp:213] Iteration 3710, loss = 3.6787
I0614 16:05:28.953068  5063 solver.cpp:228]     Train net output #0: softmax = 3.6787 (* 1 = 3.6787 loss)
I0614 16:05:28.953073  5063 solver.cpp:473] Iteration 3710, lr = 0.0001
I0614 16:05:29.835527  5063 solver.cpp:213] Iteration 3720, loss = 3.56243
I0614 16:05:29.835546  5063 solver.cpp:228]     Train net output #0: softmax = 3.56243 (* 1 = 3.56243 loss)
I0614 16:05:29.835568  5063 solver.cpp:473] Iteration 3720, lr = 0.0001
I0614 16:05:30.719923  5063 solver.cpp:213] Iteration 3730, loss = 3.45713
I0614 16:05:30.719944  5063 solver.cpp:228]     Train net output #0: softmax = 3.45713 (* 1 = 3.45713 loss)
I0614 16:05:30.719949  5063 solver.cpp:473] Iteration 3730, lr = 0.0001
I0614 16:05:31.603391  5063 solver.cpp:213] Iteration 3740, loss = 3.78771
I0614 16:05:31.603433  5063 solver.cpp:228]     Train net output #0: softmax = 3.78771 (* 1 = 3.78771 loss)
I0614 16:05:31.603440  5063 solver.cpp:473] Iteration 3740, lr = 0.0001
I0614 16:05:32.486721  5063 solver.cpp:213] Iteration 3750, loss = 3.55671
I0614 16:05:32.486738  5063 solver.cpp:228]     Train net output #0: softmax = 3.55671 (* 1 = 3.55671 loss)
I0614 16:05:32.486743  5063 solver.cpp:473] Iteration 3750, lr = 0.0001
I0614 16:05:33.370615  5063 solver.cpp:213] Iteration 3760, loss = 3.55808
I0614 16:05:33.370633  5063 solver.cpp:228]     Train net output #0: softmax = 3.55808 (* 1 = 3.55808 loss)
I0614 16:05:33.370637  5063 solver.cpp:473] Iteration 3760, lr = 0.0001
I0614 16:05:34.253594  5063 solver.cpp:213] Iteration 3770, loss = 3.62335
I0614 16:05:34.253610  5063 solver.cpp:228]     Train net output #0: softmax = 3.62335 (* 1 = 3.62335 loss)
I0614 16:05:34.253615  5063 solver.cpp:473] Iteration 3770, lr = 0.0001
I0614 16:05:35.136555  5063 solver.cpp:213] Iteration 3780, loss = 3.8119
I0614 16:05:35.136579  5063 solver.cpp:228]     Train net output #0: softmax = 3.8119 (* 1 = 3.8119 loss)
I0614 16:05:35.136589  5063 solver.cpp:473] Iteration 3780, lr = 0.0001
I0614 16:05:36.019618  5063 solver.cpp:213] Iteration 3790, loss = 3.8248
I0614 16:05:36.019639  5063 solver.cpp:228]     Train net output #0: softmax = 3.8248 (* 1 = 3.8248 loss)
I0614 16:05:36.019644  5063 solver.cpp:473] Iteration 3790, lr = 0.0001
I0614 16:05:36.902683  5063 solver.cpp:213] Iteration 3800, loss = 3.77996
I0614 16:05:36.902704  5063 solver.cpp:228]     Train net output #0: softmax = 3.77996 (* 1 = 3.77996 loss)
I0614 16:05:36.902709  5063 solver.cpp:473] Iteration 3800, lr = 0.0001
I0614 16:05:37.785969  5063 solver.cpp:213] Iteration 3810, loss = 3.75562
I0614 16:05:37.785987  5063 solver.cpp:228]     Train net output #0: softmax = 3.75562 (* 1 = 3.75562 loss)
I0614 16:05:37.785992  5063 solver.cpp:473] Iteration 3810, lr = 0.0001
I0614 16:05:38.669849  5063 solver.cpp:213] Iteration 3820, loss = 3.67453
I0614 16:05:38.669868  5063 solver.cpp:228]     Train net output #0: softmax = 3.67453 (* 1 = 3.67453 loss)
I0614 16:05:38.669873  5063 solver.cpp:473] Iteration 3820, lr = 0.0001
I0614 16:05:39.553632  5063 solver.cpp:213] Iteration 3830, loss = 3.76005
I0614 16:05:39.553650  5063 solver.cpp:228]     Train net output #0: softmax = 3.76005 (* 1 = 3.76005 loss)
I0614 16:05:39.553655  5063 solver.cpp:473] Iteration 3830, lr = 0.0001
I0614 16:05:40.437371  5063 solver.cpp:213] Iteration 3840, loss = 3.76385
I0614 16:05:40.437391  5063 solver.cpp:228]     Train net output #0: softmax = 3.76385 (* 1 = 3.76385 loss)
I0614 16:05:40.437396  5063 solver.cpp:473] Iteration 3840, lr = 0.0001
I0614 16:05:41.320611  5063 solver.cpp:213] Iteration 3850, loss = 3.83985
I0614 16:05:41.320631  5063 solver.cpp:228]     Train net output #0: softmax = 3.83985 (* 1 = 3.83985 loss)
I0614 16:05:41.320636  5063 solver.cpp:473] Iteration 3850, lr = 0.0001
I0614 16:05:42.203577  5063 solver.cpp:213] Iteration 3860, loss = 3.67327
I0614 16:05:42.203596  5063 solver.cpp:228]     Train net output #0: softmax = 3.67327 (* 1 = 3.67327 loss)
I0614 16:05:42.203601  5063 solver.cpp:473] Iteration 3860, lr = 0.0001
I0614 16:05:43.086040  5063 solver.cpp:213] Iteration 3870, loss = 3.51267
I0614 16:05:43.086058  5063 solver.cpp:228]     Train net output #0: softmax = 3.51267 (* 1 = 3.51267 loss)
I0614 16:05:43.086062  5063 solver.cpp:473] Iteration 3870, lr = 0.0001
I0614 16:05:43.969524  5063 solver.cpp:213] Iteration 3880, loss = 3.58253
I0614 16:05:43.969542  5063 solver.cpp:228]     Train net output #0: softmax = 3.58253 (* 1 = 3.58253 loss)
I0614 16:05:43.969555  5063 solver.cpp:473] Iteration 3880, lr = 0.0001
I0614 16:05:44.852959  5063 solver.cpp:213] Iteration 3890, loss = 3.71694
I0614 16:05:44.852977  5063 solver.cpp:228]     Train net output #0: softmax = 3.71694 (* 1 = 3.71694 loss)
I0614 16:05:44.852982  5063 solver.cpp:473] Iteration 3890, lr = 0.0001
I0614 16:05:45.735719  5063 solver.cpp:213] Iteration 3900, loss = 3.54036
I0614 16:05:45.735740  5063 solver.cpp:228]     Train net output #0: softmax = 3.54036 (* 1 = 3.54036 loss)
I0614 16:05:45.735873  5063 solver.cpp:473] Iteration 3900, lr = 0.0001
I0614 16:05:46.618950  5063 solver.cpp:213] Iteration 3910, loss = 3.91631
I0614 16:05:46.618973  5063 solver.cpp:228]     Train net output #0: softmax = 3.91631 (* 1 = 3.91631 loss)
I0614 16:05:46.618978  5063 solver.cpp:473] Iteration 3910, lr = 0.0001
I0614 16:05:47.501634  5063 solver.cpp:213] Iteration 3920, loss = 3.65627
I0614 16:05:47.501651  5063 solver.cpp:228]     Train net output #0: softmax = 3.65627 (* 1 = 3.65627 loss)
I0614 16:05:47.501655  5063 solver.cpp:473] Iteration 3920, lr = 0.0001
I0614 16:05:48.384799  5063 solver.cpp:213] Iteration 3930, loss = 3.82744
I0614 16:05:48.384817  5063 solver.cpp:228]     Train net output #0: softmax = 3.82744 (* 1 = 3.82744 loss)
I0614 16:05:48.384822  5063 solver.cpp:473] Iteration 3930, lr = 0.0001
I0614 16:05:49.267388  5063 solver.cpp:213] Iteration 3940, loss = 3.84729
I0614 16:05:49.267406  5063 solver.cpp:228]     Train net output #0: softmax = 3.84729 (* 1 = 3.84729 loss)
I0614 16:05:49.267411  5063 solver.cpp:473] Iteration 3940, lr = 0.0001
I0614 16:05:50.150662  5063 solver.cpp:213] Iteration 3950, loss = 3.73592
I0614 16:05:50.150691  5063 solver.cpp:228]     Train net output #0: softmax = 3.73592 (* 1 = 3.73592 loss)
I0614 16:05:50.150696  5063 solver.cpp:473] Iteration 3950, lr = 0.0001
I0614 16:05:51.033733  5063 solver.cpp:213] Iteration 3960, loss = 3.69256
I0614 16:05:51.033754  5063 solver.cpp:228]     Train net output #0: softmax = 3.69256 (* 1 = 3.69256 loss)
I0614 16:05:51.033766  5063 solver.cpp:473] Iteration 3960, lr = 0.0001
I0614 16:05:51.917080  5063 solver.cpp:213] Iteration 3970, loss = 3.74859
I0614 16:05:51.917101  5063 solver.cpp:228]     Train net output #0: softmax = 3.74859 (* 1 = 3.74859 loss)
I0614 16:05:51.917107  5063 solver.cpp:473] Iteration 3970, lr = 0.0001
I0614 16:05:52.800168  5063 solver.cpp:213] Iteration 3980, loss = 3.96698
I0614 16:05:52.800185  5063 solver.cpp:228]     Train net output #0: softmax = 3.96698 (* 1 = 3.96698 loss)
I0614 16:05:52.800190  5063 solver.cpp:473] Iteration 3980, lr = 0.0001
I0614 16:05:53.683048  5063 solver.cpp:213] Iteration 3990, loss = 3.75772
I0614 16:05:53.683064  5063 solver.cpp:228]     Train net output #0: softmax = 3.75772 (* 1 = 3.75772 loss)
I0614 16:05:53.683069  5063 solver.cpp:473] Iteration 3990, lr = 0.0001
I0614 16:05:54.508780  5063 solver.cpp:362] Snapshotting to snapshots/16-06-14_15h51m21s_0_11_pretrainClassification_iter_4000.caffemodel
I0614 16:05:54.509516  5063 solver.cpp:370] Snapshotting solver state to snapshots/16-06-14_15h51m21s_0_11_pretrainClassification_iter_4000.solverstate
I0614 16:05:54.509944  5063 solver.cpp:291] Iteration 4000, Testing net (#0)
I0614 16:05:54.622485  5063 solver.cpp:342]     Test net output #0: accuracy_top_1 = 0.146875
I0614 16:05:54.622501  5063 solver.cpp:342]     Test net output #1: accuracy_top_5 = 0.364062
I0614 16:05:54.622508  5063 solver.cpp:342]     Test net output #2: softmax = 3.68039 (* 1 = 3.68039 loss)
I0614 16:05:54.680910  5063 solver.cpp:213] Iteration 4000, loss = 3.81043
I0614 16:05:54.680924  5063 solver.cpp:228]     Train net output #0: softmax = 3.81043 (* 1 = 3.81043 loss)
I0614 16:05:54.680930  5063 solver.cpp:473] Iteration 4000, lr = 0.0001
I0614 16:05:55.563719  5063 solver.cpp:213] Iteration 4010, loss = 3.88396
I0614 16:05:55.563740  5063 solver.cpp:228]     Train net output #0: softmax = 3.88396 (* 1 = 3.88396 loss)
I0614 16:05:55.563745  5063 solver.cpp:473] Iteration 4010, lr = 0.0001
I0614 16:05:56.446972  5063 solver.cpp:213] Iteration 4020, loss = 3.86593
I0614 16:05:56.446997  5063 solver.cpp:228]     Train net output #0: softmax = 3.86593 (* 1 = 3.86593 loss)
I0614 16:05:56.447003  5063 solver.cpp:473] Iteration 4020, lr = 0.0001
I0614 16:05:57.329545  5063 solver.cpp:213] Iteration 4030, loss = 3.73328
I0614 16:05:57.329571  5063 solver.cpp:228]     Train net output #0: softmax = 3.73328 (* 1 = 3.73328 loss)
I0614 16:05:57.329576  5063 solver.cpp:473] Iteration 4030, lr = 0.0001
I0614 16:05:58.212723  5063 solver.cpp:213] Iteration 4040, loss = 3.71465
I0614 16:05:58.212744  5063 solver.cpp:228]     Train net output #0: softmax = 3.71465 (* 1 = 3.71465 loss)
I0614 16:05:58.212749  5063 solver.cpp:473] Iteration 4040, lr = 0.0001
I0614 16:05:59.095954  5063 solver.cpp:213] Iteration 4050, loss = 3.65586
I0614 16:05:59.095973  5063 solver.cpp:228]     Train net output #0: softmax = 3.65586 (* 1 = 3.65586 loss)
I0614 16:05:59.095978  5063 solver.cpp:473] Iteration 4050, lr = 0.0001
I0614 16:05:59.979214  5063 solver.cpp:213] Iteration 4060, loss = 3.79696
I0614 16:05:59.979233  5063 solver.cpp:228]     Train net output #0: softmax = 3.79696 (* 1 = 3.79696 loss)
I0614 16:05:59.979236  5063 solver.cpp:473] Iteration 4060, lr = 0.0001
I0614 16:06:00.861739  5063 solver.cpp:213] Iteration 4070, loss = 3.53472
I0614 16:06:00.861757  5063 solver.cpp:228]     Train net output #0: softmax = 3.53472 (* 1 = 3.53472 loss)
I0614 16:06:00.861763  5063 solver.cpp:473] Iteration 4070, lr = 0.0001
I0614 16:06:01.744401  5063 solver.cpp:213] Iteration 4080, loss = 3.60945
I0614 16:06:01.744566  5063 solver.cpp:228]     Train net output #0: softmax = 3.60945 (* 1 = 3.60945 loss)
I0614 16:06:01.744573  5063 solver.cpp:473] Iteration 4080, lr = 0.0001
I0614 16:06:02.627893  5063 solver.cpp:213] Iteration 4090, loss = 3.82458
I0614 16:06:02.627915  5063 solver.cpp:228]     Train net output #0: softmax = 3.82458 (* 1 = 3.82458 loss)
I0614 16:06:02.627920  5063 solver.cpp:473] Iteration 4090, lr = 0.0001
I0614 16:06:03.511091  5063 solver.cpp:213] Iteration 4100, loss = 3.80288
I0614 16:06:03.511111  5063 solver.cpp:228]     Train net output #0: softmax = 3.80288 (* 1 = 3.80288 loss)
I0614 16:06:03.511116  5063 solver.cpp:473] Iteration 4100, lr = 0.0001
I0614 16:06:04.393741  5063 solver.cpp:213] Iteration 4110, loss = 3.56651
I0614 16:06:04.393760  5063 solver.cpp:228]     Train net output #0: softmax = 3.56651 (* 1 = 3.56651 loss)
I0614 16:06:04.393765  5063 solver.cpp:473] Iteration 4110, lr = 0.0001
I0614 16:06:05.276134  5063 solver.cpp:213] Iteration 4120, loss = 3.66956
I0614 16:06:05.276149  5063 solver.cpp:228]     Train net output #0: softmax = 3.66956 (* 1 = 3.66956 loss)
I0614 16:06:05.276154  5063 solver.cpp:473] Iteration 4120, lr = 0.0001
I0614 16:06:06.159235  5063 solver.cpp:213] Iteration 4130, loss = 3.77776
I0614 16:06:06.159252  5063 solver.cpp:228]     Train net output #0: softmax = 3.77776 (* 1 = 3.77776 loss)
I0614 16:06:06.159257  5063 solver.cpp:473] Iteration 4130, lr = 0.0001
I0614 16:06:07.042827  5063 solver.cpp:213] Iteration 4140, loss = 3.81323
I0614 16:06:07.042848  5063 solver.cpp:228]     Train net output #0: softmax = 3.81323 (* 1 = 3.81323 loss)
I0614 16:06:07.042858  5063 solver.cpp:473] Iteration 4140, lr = 0.0001
I0614 16:06:07.926473  5063 solver.cpp:213] Iteration 4150, loss = 3.70273
I0614 16:06:07.926497  5063 solver.cpp:228]     Train net output #0: softmax = 3.70273 (* 1 = 3.70273 loss)
I0614 16:06:07.926503  5063 solver.cpp:473] Iteration 4150, lr = 0.0001
I0614 16:06:08.808629  5063 solver.cpp:213] Iteration 4160, loss = 3.80445
I0614 16:06:08.808645  5063 solver.cpp:228]     Train net output #0: softmax = 3.80445 (* 1 = 3.80445 loss)
I0614 16:06:08.808650  5063 solver.cpp:473] Iteration 4160, lr = 0.0001
I0614 16:06:09.690918  5063 solver.cpp:213] Iteration 4170, loss = 3.96824
I0614 16:06:09.690934  5063 solver.cpp:228]     Train net output #0: softmax = 3.96824 (* 1 = 3.96824 loss)
I0614 16:06:09.690938  5063 solver.cpp:473] Iteration 4170, lr = 0.0001
I0614 16:06:10.572902  5063 solver.cpp:213] Iteration 4180, loss = 3.87199
I0614 16:06:10.572924  5063 solver.cpp:228]     Train net output #0: softmax = 3.87199 (* 1 = 3.87199 loss)
I0614 16:06:10.572929  5063 solver.cpp:473] Iteration 4180, lr = 0.0001
I0614 16:06:11.455955  5063 solver.cpp:213] Iteration 4190, loss = 3.71659
I0614 16:06:11.455972  5063 solver.cpp:228]     Train net output #0: softmax = 3.71659 (* 1 = 3.71659 loss)
I0614 16:06:11.455977  5063 solver.cpp:473] Iteration 4190, lr = 0.0001
I0614 16:06:12.338340  5063 solver.cpp:213] Iteration 4200, loss = 3.68984
I0614 16:06:12.338364  5063 solver.cpp:228]     Train net output #0: softmax = 3.68984 (* 1 = 3.68984 loss)
I0614 16:06:12.338485  5063 solver.cpp:473] Iteration 4200, lr = 0.0001
I0614 16:06:13.221747  5063 solver.cpp:213] Iteration 4210, loss = 3.55285
I0614 16:06:13.221768  5063 solver.cpp:228]     Train net output #0: softmax = 3.55285 (* 1 = 3.55285 loss)
I0614 16:06:13.221773  5063 solver.cpp:473] Iteration 4210, lr = 0.0001
I0614 16:06:14.104185  5063 solver.cpp:213] Iteration 4220, loss = 3.75953
I0614 16:06:14.104203  5063 solver.cpp:228]     Train net output #0: softmax = 3.75953 (* 1 = 3.75953 loss)
I0614 16:06:14.104208  5063 solver.cpp:473] Iteration 4220, lr = 0.0001
I0614 16:06:14.987565  5063 solver.cpp:213] Iteration 4230, loss = 3.63333
I0614 16:06:14.987583  5063 solver.cpp:228]     Train net output #0: softmax = 3.63333 (* 1 = 3.63333 loss)
I0614 16:06:14.987587  5063 solver.cpp:473] Iteration 4230, lr = 0.0001
I0614 16:06:15.871816  5063 solver.cpp:213] Iteration 4240, loss = 3.67277
I0614 16:06:15.871835  5063 solver.cpp:228]     Train net output #0: softmax = 3.67277 (* 1 = 3.67277 loss)
I0614 16:06:15.871860  5063 solver.cpp:473] Iteration 4240, lr = 0.0001
I0614 16:06:16.754762  5063 solver.cpp:213] Iteration 4250, loss = 3.5817
I0614 16:06:16.754781  5063 solver.cpp:228]     Train net output #0: softmax = 3.5817 (* 1 = 3.5817 loss)
I0614 16:06:16.754786  5063 solver.cpp:473] Iteration 4250, lr = 0.0001
I0614 16:06:17.637774  5063 solver.cpp:213] Iteration 4260, loss = 3.54549
I0614 16:06:17.637795  5063 solver.cpp:228]     Train net output #0: softmax = 3.54549 (* 1 = 3.54549 loss)
I0614 16:06:17.637801  5063 solver.cpp:473] Iteration 4260, lr = 0.0001
I0614 16:06:18.521010  5063 solver.cpp:213] Iteration 4270, loss = 3.56992
I0614 16:06:18.521031  5063 solver.cpp:228]     Train net output #0: softmax = 3.56992 (* 1 = 3.56992 loss)
I0614 16:06:18.521036  5063 solver.cpp:473] Iteration 4270, lr = 0.0001
I0614 16:06:19.403969  5063 solver.cpp:213] Iteration 4280, loss = 3.76831
I0614 16:06:19.403985  5063 solver.cpp:228]     Train net output #0: softmax = 3.76831 (* 1 = 3.76831 loss)
I0614 16:06:19.403990  5063 solver.cpp:473] Iteration 4280, lr = 0.0001
I0614 16:06:20.287565  5063 solver.cpp:213] Iteration 4290, loss = 3.7273
I0614 16:06:20.287585  5063 solver.cpp:228]     Train net output #0: softmax = 3.7273 (* 1 = 3.7273 loss)
I0614 16:06:20.287588  5063 solver.cpp:473] Iteration 4290, lr = 0.0001
I0614 16:06:21.171308  5063 solver.cpp:213] Iteration 4300, loss = 3.6853
I0614 16:06:21.171324  5063 solver.cpp:228]     Train net output #0: softmax = 3.6853 (* 1 = 3.6853 loss)
I0614 16:06:21.171327  5063 solver.cpp:473] Iteration 4300, lr = 0.0001
I0614 16:06:22.054777  5063 solver.cpp:213] Iteration 4310, loss = 3.60187
I0614 16:06:22.054796  5063 solver.cpp:228]     Train net output #0: softmax = 3.60187 (* 1 = 3.60187 loss)
I0614 16:06:22.054801  5063 solver.cpp:473] Iteration 4310, lr = 0.0001
I0614 16:06:22.938073  5063 solver.cpp:213] Iteration 4320, loss = 3.93322
I0614 16:06:22.938102  5063 solver.cpp:228]     Train net output #0: softmax = 3.93322 (* 1 = 3.93322 loss)
I0614 16:06:22.938248  5063 solver.cpp:473] Iteration 4320, lr = 0.0001
I0614 16:06:23.821496  5063 solver.cpp:213] Iteration 4330, loss = 3.65915
I0614 16:06:23.821514  5063 solver.cpp:228]     Train net output #0: softmax = 3.65915 (* 1 = 3.65915 loss)
I0614 16:06:23.821519  5063 solver.cpp:473] Iteration 4330, lr = 0.0001
I0614 16:06:24.704068  5063 solver.cpp:213] Iteration 4340, loss = 3.62542
I0614 16:06:24.704093  5063 solver.cpp:228]     Train net output #0: softmax = 3.62542 (* 1 = 3.62542 loss)
I0614 16:06:24.704098  5063 solver.cpp:473] Iteration 4340, lr = 0.0001
I0614 16:06:25.587187  5063 solver.cpp:213] Iteration 4350, loss = 3.73462
I0614 16:06:25.587204  5063 solver.cpp:228]     Train net output #0: softmax = 3.73462 (* 1 = 3.73462 loss)
I0614 16:06:25.587209  5063 solver.cpp:473] Iteration 4350, lr = 0.0001
I0614 16:06:26.470329  5063 solver.cpp:213] Iteration 4360, loss = 3.6548
I0614 16:06:26.470345  5063 solver.cpp:228]     Train net output #0: softmax = 3.6548 (* 1 = 3.6548 loss)
I0614 16:06:26.470350  5063 solver.cpp:473] Iteration 4360, lr = 0.0001
I0614 16:06:27.350816  5063 solver.cpp:213] Iteration 4370, loss = 3.49793
I0614 16:06:27.350846  5063 solver.cpp:228]     Train net output #0: softmax = 3.49793 (* 1 = 3.49793 loss)
I0614 16:06:27.350858  5063 solver.cpp:473] Iteration 4370, lr = 0.0001
I0614 16:06:28.234082  5063 solver.cpp:213] Iteration 4380, loss = 3.74429
I0614 16:06:28.234108  5063 solver.cpp:228]     Train net output #0: softmax = 3.74429 (* 1 = 3.74429 loss)
I0614 16:06:28.234261  5063 solver.cpp:473] Iteration 4380, lr = 0.0001
I0614 16:06:29.116878  5063 solver.cpp:213] Iteration 4390, loss = 3.62623
I0614 16:06:29.116894  5063 solver.cpp:228]     Train net output #0: softmax = 3.62623 (* 1 = 3.62623 loss)
I0614 16:06:29.116899  5063 solver.cpp:473] Iteration 4390, lr = 0.0001
I0614 16:06:30.000155  5063 solver.cpp:213] Iteration 4400, loss = 3.59939
I0614 16:06:30.000174  5063 solver.cpp:228]     Train net output #0: softmax = 3.59939 (* 1 = 3.59939 loss)
I0614 16:06:30.000193  5063 solver.cpp:473] Iteration 4400, lr = 0.0001
I0614 16:06:30.883493  5063 solver.cpp:213] Iteration 4410, loss = 3.626
I0614 16:06:30.883512  5063 solver.cpp:228]     Train net output #0: softmax = 3.626 (* 1 = 3.626 loss)
I0614 16:06:30.883517  5063 solver.cpp:473] Iteration 4410, lr = 0.0001
I0614 16:06:31.767240  5063 solver.cpp:213] Iteration 4420, loss = 3.65063
I0614 16:06:31.767277  5063 solver.cpp:228]     Train net output #0: softmax = 3.65063 (* 1 = 3.65063 loss)
I0614 16:06:31.767282  5063 solver.cpp:473] Iteration 4420, lr = 0.0001
I0614 16:06:32.650421  5063 solver.cpp:213] Iteration 4430, loss = 3.66977
I0614 16:06:32.650441  5063 solver.cpp:228]     Train net output #0: softmax = 3.66977 (* 1 = 3.66977 loss)
I0614 16:06:32.650446  5063 solver.cpp:473] Iteration 4430, lr = 0.0001
I0614 16:06:33.533921  5063 solver.cpp:213] Iteration 4440, loss = 3.75502
I0614 16:06:33.533941  5063 solver.cpp:228]     Train net output #0: softmax = 3.75502 (* 1 = 3.75502 loss)
I0614 16:06:33.533946  5063 solver.cpp:473] Iteration 4440, lr = 0.0001
I0614 16:06:34.417186  5063 solver.cpp:213] Iteration 4450, loss = 3.76391
I0614 16:06:34.417203  5063 solver.cpp:228]     Train net output #0: softmax = 3.76391 (* 1 = 3.76391 loss)
I0614 16:06:34.417208  5063 solver.cpp:473] Iteration 4450, lr = 0.0001
I0614 16:06:35.300272  5063 solver.cpp:213] Iteration 4460, loss = 3.41355
I0614 16:06:35.300290  5063 solver.cpp:228]     Train net output #0: softmax = 3.41355 (* 1 = 3.41355 loss)
I0614 16:06:35.300295  5063 solver.cpp:473] Iteration 4460, lr = 0.0001
I0614 16:06:36.183378  5063 solver.cpp:213] Iteration 4470, loss = 3.67148
I0614 16:06:36.183398  5063 solver.cpp:228]     Train net output #0: softmax = 3.67148 (* 1 = 3.67148 loss)
I0614 16:06:36.183403  5063 solver.cpp:473] Iteration 4470, lr = 0.0001
I0614 16:06:37.065896  5063 solver.cpp:213] Iteration 4480, loss = 3.55672
I0614 16:06:37.065913  5063 solver.cpp:228]     Train net output #0: softmax = 3.55672 (* 1 = 3.55672 loss)
I0614 16:06:37.065918  5063 solver.cpp:473] Iteration 4480, lr = 0.0001
I0614 16:06:37.948876  5063 solver.cpp:213] Iteration 4490, loss = 3.624
I0614 16:06:37.948899  5063 solver.cpp:228]     Train net output #0: softmax = 3.624 (* 1 = 3.624 loss)
I0614 16:06:37.948902  5063 solver.cpp:473] Iteration 4490, lr = 0.0001
I0614 16:06:38.832551  5063 solver.cpp:213] Iteration 4500, loss = 3.61314
I0614 16:06:38.832576  5063 solver.cpp:228]     Train net output #0: softmax = 3.61314 (* 1 = 3.61314 loss)
I0614 16:06:38.832736  5063 solver.cpp:473] Iteration 4500, lr = 0.0001
I0614 16:06:39.716624  5063 solver.cpp:213] Iteration 4510, loss = 3.64006
I0614 16:06:39.716642  5063 solver.cpp:228]     Train net output #0: softmax = 3.64006 (* 1 = 3.64006 loss)
I0614 16:06:39.716647  5063 solver.cpp:473] Iteration 4510, lr = 0.0001
I0614 16:06:40.599810  5063 solver.cpp:213] Iteration 4520, loss = 3.65081
I0614 16:06:40.599828  5063 solver.cpp:228]     Train net output #0: softmax = 3.65081 (* 1 = 3.65081 loss)
I0614 16:06:40.599833  5063 solver.cpp:473] Iteration 4520, lr = 0.0001
I0614 16:06:41.483535  5063 solver.cpp:213] Iteration 4530, loss = 3.59997
I0614 16:06:41.483552  5063 solver.cpp:228]     Train net output #0: softmax = 3.59997 (* 1 = 3.59997 loss)
I0614 16:06:41.483556  5063 solver.cpp:473] Iteration 4530, lr = 0.0001
I0614 16:06:42.366930  5063 solver.cpp:213] Iteration 4540, loss = 3.73069
I0614 16:06:42.366950  5063 solver.cpp:228]     Train net output #0: softmax = 3.73069 (* 1 = 3.73069 loss)
I0614 16:06:42.366953  5063 solver.cpp:473] Iteration 4540, lr = 0.0001
I0614 16:06:43.250334  5063 solver.cpp:213] Iteration 4550, loss = 3.57967
I0614 16:06:43.250355  5063 solver.cpp:228]     Train net output #0: softmax = 3.57967 (* 1 = 3.57967 loss)
I0614 16:06:43.250360  5063 solver.cpp:473] Iteration 4550, lr = 0.0001
I0614 16:06:44.133839  5063 solver.cpp:213] Iteration 4560, loss = 3.74554
I0614 16:06:44.133862  5063 solver.cpp:228]     Train net output #0: softmax = 3.74554 (* 1 = 3.74554 loss)
I0614 16:06:44.133987  5063 solver.cpp:473] Iteration 4560, lr = 0.0001
I0614 16:06:45.016825  5063 solver.cpp:213] Iteration 4570, loss = 3.91307
I0614 16:06:45.016842  5063 solver.cpp:228]     Train net output #0: softmax = 3.91307 (* 1 = 3.91307 loss)
I0614 16:06:45.016847  5063 solver.cpp:473] Iteration 4570, lr = 0.0001
I0614 16:06:45.899755  5063 solver.cpp:213] Iteration 4580, loss = 3.58611
I0614 16:06:45.899772  5063 solver.cpp:228]     Train net output #0: softmax = 3.58611 (* 1 = 3.58611 loss)
I0614 16:06:45.899793  5063 solver.cpp:473] Iteration 4580, lr = 0.0001
I0614 16:06:46.780585  5063 solver.cpp:213] Iteration 4590, loss = 3.76688
I0614 16:06:46.780606  5063 solver.cpp:228]     Train net output #0: softmax = 3.76688 (* 1 = 3.76688 loss)
I0614 16:06:46.780611  5063 solver.cpp:473] Iteration 4590, lr = 0.0001
I0614 16:06:47.654451  5063 solver.cpp:213] Iteration 4600, loss = 3.82173
I0614 16:06:47.654474  5063 solver.cpp:228]     Train net output #0: softmax = 3.82173 (* 1 = 3.82173 loss)
I0614 16:06:47.654481  5063 solver.cpp:473] Iteration 4600, lr = 0.0001
I0614 16:06:48.538275  5063 solver.cpp:213] Iteration 4610, loss = 3.45831
I0614 16:06:48.538297  5063 solver.cpp:228]     Train net output #0: softmax = 3.45831 (* 1 = 3.45831 loss)
I0614 16:06:48.538302  5063 solver.cpp:473] Iteration 4610, lr = 0.0001
I0614 16:06:49.421875  5063 solver.cpp:213] Iteration 4620, loss = 3.59658
I0614 16:06:49.421896  5063 solver.cpp:228]     Train net output #0: softmax = 3.59658 (* 1 = 3.59658 loss)
I0614 16:06:49.421901  5063 solver.cpp:473] Iteration 4620, lr = 0.0001
I0614 16:06:50.305425  5063 solver.cpp:213] Iteration 4630, loss = 3.4775
I0614 16:06:50.305441  5063 solver.cpp:228]     Train net output #0: softmax = 3.4775 (* 1 = 3.4775 loss)
I0614 16:06:50.305446  5063 solver.cpp:473] Iteration 4630, lr = 0.0001
I0614 16:06:51.188277  5063 solver.cpp:213] Iteration 4640, loss = 3.76782
I0614 16:06:51.188293  5063 solver.cpp:228]     Train net output #0: softmax = 3.76782 (* 1 = 3.76782 loss)
I0614 16:06:51.188298  5063 solver.cpp:473] Iteration 4640, lr = 0.0001
I0614 16:06:52.071565  5063 solver.cpp:213] Iteration 4650, loss = 3.74869
I0614 16:06:52.071583  5063 solver.cpp:228]     Train net output #0: softmax = 3.74869 (* 1 = 3.74869 loss)
I0614 16:06:52.071588  5063 solver.cpp:473] Iteration 4650, lr = 0.0001
I0614 16:06:52.954835  5063 solver.cpp:213] Iteration 4660, loss = 3.57151
I0614 16:06:52.954855  5063 solver.cpp:228]     Train net output #0: softmax = 3.57151 (* 1 = 3.57151 loss)
I0614 16:06:52.954865  5063 solver.cpp:473] Iteration 4660, lr = 0.0001
I0614 16:06:53.838294  5063 solver.cpp:213] Iteration 4670, loss = 3.50666
I0614 16:06:53.838316  5063 solver.cpp:228]     Train net output #0: softmax = 3.50666 (* 1 = 3.50666 loss)
I0614 16:06:53.838321  5063 solver.cpp:473] Iteration 4670, lr = 0.0001
I0614 16:06:54.721165  5063 solver.cpp:213] Iteration 4680, loss = 3.80529
I0614 16:06:54.721185  5063 solver.cpp:228]     Train net output #0: softmax = 3.80529 (* 1 = 3.80529 loss)
I0614 16:06:54.721196  5063 solver.cpp:473] Iteration 4680, lr = 0.0001
I0614 16:06:55.605263  5063 solver.cpp:213] Iteration 4690, loss = 3.53366
I0614 16:06:55.605283  5063 solver.cpp:228]     Train net output #0: softmax = 3.53366 (* 1 = 3.53366 loss)
I0614 16:06:55.605288  5063 solver.cpp:473] Iteration 4690, lr = 0.0001
I0614 16:06:56.488466  5063 solver.cpp:213] Iteration 4700, loss = 3.48327
I0614 16:06:56.488483  5063 solver.cpp:228]     Train net output #0: softmax = 3.48327 (* 1 = 3.48327 loss)
I0614 16:06:56.488488  5063 solver.cpp:473] Iteration 4700, lr = 0.0001
I0614 16:06:57.371455  5063 solver.cpp:213] Iteration 4710, loss = 3.76879
I0614 16:06:57.371474  5063 solver.cpp:228]     Train net output #0: softmax = 3.76879 (* 1 = 3.76879 loss)
I0614 16:06:57.371479  5063 solver.cpp:473] Iteration 4710, lr = 0.0001
I0614 16:06:58.255203  5063 solver.cpp:213] Iteration 4720, loss = 3.65516
I0614 16:06:58.255225  5063 solver.cpp:228]     Train net output #0: softmax = 3.65516 (* 1 = 3.65516 loss)
I0614 16:06:58.255230  5063 solver.cpp:473] Iteration 4720, lr = 0.0001
I0614 16:06:59.138522  5063 solver.cpp:213] Iteration 4730, loss = 3.61783
I0614 16:06:59.138540  5063 solver.cpp:228]     Train net output #0: softmax = 3.61783 (* 1 = 3.61783 loss)
I0614 16:06:59.138545  5063 solver.cpp:473] Iteration 4730, lr = 0.0001
I0614 16:07:00.021590  5063 solver.cpp:213] Iteration 4740, loss = 3.60697
I0614 16:07:00.021613  5063 solver.cpp:228]     Train net output #0: softmax = 3.60697 (* 1 = 3.60697 loss)
I0614 16:07:00.021770  5063 solver.cpp:473] Iteration 4740, lr = 0.0001
I0614 16:07:00.904418  5063 solver.cpp:213] Iteration 4750, loss = 3.608
I0614 16:07:00.904436  5063 solver.cpp:228]     Train net output #0: softmax = 3.608 (* 1 = 3.608 loss)
I0614 16:07:00.904441  5063 solver.cpp:473] Iteration 4750, lr = 0.0001
I0614 16:07:01.786382  5063 solver.cpp:213] Iteration 4760, loss = 3.58756
I0614 16:07:01.786414  5063 solver.cpp:228]     Train net output #0: softmax = 3.58756 (* 1 = 3.58756 loss)
I0614 16:07:01.786419  5063 solver.cpp:473] Iteration 4760, lr = 0.0001
I0614 16:07:02.670307  5063 solver.cpp:213] Iteration 4770, loss = 3.69179
I0614 16:07:02.670328  5063 solver.cpp:228]     Train net output #0: softmax = 3.69179 (* 1 = 3.69179 loss)
I0614 16:07:02.670333  5063 solver.cpp:473] Iteration 4770, lr = 0.0001
I0614 16:07:03.554244  5063 solver.cpp:213] Iteration 4780, loss = 3.49818
I0614 16:07:03.554266  5063 solver.cpp:228]     Train net output #0: softmax = 3.49818 (* 1 = 3.49818 loss)
I0614 16:07:03.554271  5063 solver.cpp:473] Iteration 4780, lr = 0.0001
I0614 16:07:04.437364  5063 solver.cpp:213] Iteration 4790, loss = 3.52559
I0614 16:07:04.437382  5063 solver.cpp:228]     Train net output #0: softmax = 3.52559 (* 1 = 3.52559 loss)
I0614 16:07:04.437387  5063 solver.cpp:473] Iteration 4790, lr = 0.0001
I0614 16:07:05.319330  5063 solver.cpp:213] Iteration 4800, loss = 3.70171
I0614 16:07:05.319350  5063 solver.cpp:228]     Train net output #0: softmax = 3.70171 (* 1 = 3.70171 loss)
I0614 16:07:05.319361  5063 solver.cpp:473] Iteration 4800, lr = 0.0001
I0614 16:07:06.203132  5063 solver.cpp:213] Iteration 4810, loss = 3.82066
I0614 16:07:06.203150  5063 solver.cpp:228]     Train net output #0: softmax = 3.82066 (* 1 = 3.82066 loss)
I0614 16:07:06.203155  5063 solver.cpp:473] Iteration 4810, lr = 0.0001
I0614 16:07:07.080139  5063 solver.cpp:213] Iteration 4820, loss = 3.66541
I0614 16:07:07.080162  5063 solver.cpp:228]     Train net output #0: softmax = 3.66541 (* 1 = 3.66541 loss)
I0614 16:07:07.080175  5063 solver.cpp:473] Iteration 4820, lr = 0.0001
I0614 16:07:07.956796  5063 solver.cpp:213] Iteration 4830, loss = 3.71683
I0614 16:07:07.956817  5063 solver.cpp:228]     Train net output #0: softmax = 3.71683 (* 1 = 3.71683 loss)
I0614 16:07:07.956821  5063 solver.cpp:473] Iteration 4830, lr = 0.0001
I0614 16:07:08.840210  5063 solver.cpp:213] Iteration 4840, loss = 3.62667
I0614 16:07:08.840234  5063 solver.cpp:228]     Train net output #0: softmax = 3.62667 (* 1 = 3.62667 loss)
I0614 16:07:08.840239  5063 solver.cpp:473] Iteration 4840, lr = 0.0001
I0614 16:07:09.723443  5063 solver.cpp:213] Iteration 4850, loss = 3.39731
I0614 16:07:09.723461  5063 solver.cpp:228]     Train net output #0: softmax = 3.39731 (* 1 = 3.39731 loss)
I0614 16:07:09.723466  5063 solver.cpp:473] Iteration 4850, lr = 0.0001
I0614 16:07:10.606375  5063 solver.cpp:213] Iteration 4860, loss = 3.70294
I0614 16:07:10.606392  5063 solver.cpp:228]     Train net output #0: softmax = 3.70294 (* 1 = 3.70294 loss)
I0614 16:07:10.606397  5063 solver.cpp:473] Iteration 4860, lr = 0.0001
I0614 16:07:11.488982  5063 solver.cpp:213] Iteration 4870, loss = 3.47332
I0614 16:07:11.488997  5063 solver.cpp:228]     Train net output #0: softmax = 3.47332 (* 1 = 3.47332 loss)
I0614 16:07:11.489002  5063 solver.cpp:473] Iteration 4870, lr = 0.0001
I0614 16:07:12.371692  5063 solver.cpp:213] Iteration 4880, loss = 3.49312
I0614 16:07:12.371709  5063 solver.cpp:228]     Train net output #0: softmax = 3.49312 (* 1 = 3.49312 loss)
I0614 16:07:12.371714  5063 solver.cpp:473] Iteration 4880, lr = 0.0001
I0614 16:07:13.255256  5063 solver.cpp:213] Iteration 4890, loss = 3.54842
I0614 16:07:13.255275  5063 solver.cpp:228]     Train net output #0: softmax = 3.54842 (* 1 = 3.54842 loss)
I0614 16:07:13.255280  5063 solver.cpp:473] Iteration 4890, lr = 0.0001
I0614 16:07:14.138191  5063 solver.cpp:213] Iteration 4900, loss = 3.55987
I0614 16:07:14.138214  5063 solver.cpp:228]     Train net output #0: softmax = 3.55987 (* 1 = 3.55987 loss)
I0614 16:07:14.138219  5063 solver.cpp:473] Iteration 4900, lr = 0.0001
I0614 16:07:15.020876  5063 solver.cpp:213] Iteration 4910, loss = 3.68144
I0614 16:07:15.020895  5063 solver.cpp:228]     Train net output #0: softmax = 3.68144 (* 1 = 3.68144 loss)
I0614 16:07:15.020900  5063 solver.cpp:473] Iteration 4910, lr = 0.0001
I0614 16:07:15.903723  5063 solver.cpp:213] Iteration 4920, loss = 3.28509
I0614 16:07:15.903745  5063 solver.cpp:228]     Train net output #0: softmax = 3.28509 (* 1 = 3.28509 loss)
I0614 16:07:15.903882  5063 solver.cpp:473] Iteration 4920, lr = 0.0001
I0614 16:07:16.787214  5063 solver.cpp:213] Iteration 4930, loss = 3.65536
I0614 16:07:16.787230  5063 solver.cpp:228]     Train net output #0: softmax = 3.65536 (* 1 = 3.65536 loss)
I0614 16:07:16.787235  5063 solver.cpp:473] Iteration 4930, lr = 0.0001
I0614 16:07:17.670799  5063 solver.cpp:213] Iteration 4940, loss = 3.79416
I0614 16:07:17.670819  5063 solver.cpp:228]     Train net output #0: softmax = 3.79416 (* 1 = 3.79416 loss)
I0614 16:07:17.670824  5063 solver.cpp:473] Iteration 4940, lr = 0.0001
I0614 16:07:18.553380  5063 solver.cpp:213] Iteration 4950, loss = 3.70992
I0614 16:07:18.553397  5063 solver.cpp:228]     Train net output #0: softmax = 3.70992 (* 1 = 3.70992 loss)
I0614 16:07:18.553401  5063 solver.cpp:473] Iteration 4950, lr = 0.0001
I0614 16:07:19.436839  5063 solver.cpp:213] Iteration 4960, loss = 3.78052
I0614 16:07:19.436861  5063 solver.cpp:228]     Train net output #0: softmax = 3.78052 (* 1 = 3.78052 loss)
I0614 16:07:19.436866  5063 solver.cpp:473] Iteration 4960, lr = 0.0001
I0614 16:07:20.319460  5063 solver.cpp:213] Iteration 4970, loss = 3.546
I0614 16:07:20.319476  5063 solver.cpp:228]     Train net output #0: softmax = 3.546 (* 1 = 3.546 loss)
I0614 16:07:20.319481  5063 solver.cpp:473] Iteration 4970, lr = 0.0001
I0614 16:07:21.203369  5063 solver.cpp:213] Iteration 4980, loss = 3.76136
I0614 16:07:21.203392  5063 solver.cpp:228]     Train net output #0: softmax = 3.76136 (* 1 = 3.76136 loss)
I0614 16:07:21.203409  5063 solver.cpp:473] Iteration 4980, lr = 0.0001
I0614 16:07:22.086851  5063 solver.cpp:213] Iteration 4990, loss = 3.75179
I0614 16:07:22.086868  5063 solver.cpp:228]     Train net output #0: softmax = 3.75179 (* 1 = 3.75179 loss)
I0614 16:07:22.086872  5063 solver.cpp:473] Iteration 4990, lr = 0.0001
I0614 16:07:22.912621  5063 solver.cpp:362] Snapshotting to snapshots/16-06-14_15h51m21s_0_11_pretrainClassification_iter_5000.caffemodel
I0614 16:07:22.913401  5063 solver.cpp:370] Snapshotting solver state to snapshots/16-06-14_15h51m21s_0_11_pretrainClassification_iter_5000.solverstate
I0614 16:07:22.913827  5063 solver.cpp:291] Iteration 5000, Testing net (#0)
I0614 16:07:23.026037  5063 solver.cpp:342]     Test net output #0: accuracy_top_1 = 0.1625
I0614 16:07:23.026053  5063 solver.cpp:342]     Test net output #1: accuracy_top_5 = 0.4125
I0614 16:07:23.026059  5063 solver.cpp:342]     Test net output #2: softmax = 3.58095 (* 1 = 3.58095 loss)
I0614 16:07:23.084100  5063 solver.cpp:213] Iteration 5000, loss = 3.70774
I0614 16:07:23.084113  5063 solver.cpp:228]     Train net output #0: softmax = 3.70774 (* 1 = 3.70774 loss)
I0614 16:07:23.084117  5063 solver.cpp:473] Iteration 5000, lr = 0.0001
I0614 16:07:23.966747  5063 solver.cpp:213] Iteration 5010, loss = 3.45752
I0614 16:07:23.966769  5063 solver.cpp:228]     Train net output #0: softmax = 3.45752 (* 1 = 3.45752 loss)
I0614 16:07:23.966774  5063 solver.cpp:473] Iteration 5010, lr = 0.0001
I0614 16:07:24.849334  5063 solver.cpp:213] Iteration 5020, loss = 3.60115
I0614 16:07:24.849350  5063 solver.cpp:228]     Train net output #0: softmax = 3.60115 (* 1 = 3.60115 loss)
I0614 16:07:24.849355  5063 solver.cpp:473] Iteration 5020, lr = 0.0001
I0614 16:07:25.733043  5063 solver.cpp:213] Iteration 5030, loss = 3.7587
I0614 16:07:25.733060  5063 solver.cpp:228]     Train net output #0: softmax = 3.7587 (* 1 = 3.7587 loss)
I0614 16:07:25.733065  5063 solver.cpp:473] Iteration 5030, lr = 0.0001
I0614 16:07:26.616114  5063 solver.cpp:213] Iteration 5040, loss = 3.72221
I0614 16:07:26.616137  5063 solver.cpp:228]     Train net output #0: softmax = 3.72221 (* 1 = 3.72221 loss)
I0614 16:07:26.616142  5063 solver.cpp:473] Iteration 5040, lr = 0.0001
I0614 16:07:27.499061  5063 solver.cpp:213] Iteration 5050, loss = 3.6968
I0614 16:07:27.499078  5063 solver.cpp:228]     Train net output #0: softmax = 3.6968 (* 1 = 3.6968 loss)
I0614 16:07:27.499083  5063 solver.cpp:473] Iteration 5050, lr = 0.0001
I0614 16:07:28.381682  5063 solver.cpp:213] Iteration 5060, loss = 3.53479
I0614 16:07:28.381700  5063 solver.cpp:228]     Train net output #0: softmax = 3.53479 (* 1 = 3.53479 loss)
I0614 16:07:28.381705  5063 solver.cpp:473] Iteration 5060, lr = 0.0001
I0614 16:07:29.264741  5063 solver.cpp:213] Iteration 5070, loss = 3.44195
I0614 16:07:29.264763  5063 solver.cpp:228]     Train net output #0: softmax = 3.44195 (* 1 = 3.44195 loss)
I0614 16:07:29.264768  5063 solver.cpp:473] Iteration 5070, lr = 0.0001
I0614 16:07:30.147932  5063 solver.cpp:213] Iteration 5080, loss = 3.58574
I0614 16:07:30.147949  5063 solver.cpp:228]     Train net output #0: softmax = 3.58574 (* 1 = 3.58574 loss)
I0614 16:07:30.147954  5063 solver.cpp:473] Iteration 5080, lr = 0.0001
I0614 16:07:31.029640  5063 solver.cpp:213] Iteration 5090, loss = 3.47209
I0614 16:07:31.029656  5063 solver.cpp:228]     Train net output #0: softmax = 3.47209 (* 1 = 3.47209 loss)
I0614 16:07:31.029661  5063 solver.cpp:473] Iteration 5090, lr = 0.0001
I0614 16:07:31.912914  5063 solver.cpp:213] Iteration 5100, loss = 3.52726
I0614 16:07:31.913089  5063 solver.cpp:228]     Train net output #0: softmax = 3.52726 (* 1 = 3.52726 loss)
I0614 16:07:31.913096  5063 solver.cpp:473] Iteration 5100, lr = 0.0001
I0614 16:07:32.795830  5063 solver.cpp:213] Iteration 5110, loss = 3.58042
I0614 16:07:32.795846  5063 solver.cpp:228]     Train net output #0: softmax = 3.58042 (* 1 = 3.58042 loss)
I0614 16:07:32.795851  5063 solver.cpp:473] Iteration 5110, lr = 0.0001
I0614 16:07:33.678295  5063 solver.cpp:213] Iteration 5120, loss = 3.54429
I0614 16:07:33.678320  5063 solver.cpp:228]     Train net output #0: softmax = 3.54429 (* 1 = 3.54429 loss)
I0614 16:07:33.678325  5063 solver.cpp:473] Iteration 5120, lr = 0.0001
I0614 16:07:34.561817  5063 solver.cpp:213] Iteration 5130, loss = 3.66485
I0614 16:07:34.561841  5063 solver.cpp:228]     Train net output #0: softmax = 3.66485 (* 1 = 3.66485 loss)
I0614 16:07:34.561846  5063 solver.cpp:473] Iteration 5130, lr = 0.0001
I0614 16:07:35.444484  5063 solver.cpp:213] Iteration 5140, loss = 3.40793
I0614 16:07:35.444500  5063 solver.cpp:228]     Train net output #0: softmax = 3.40793 (* 1 = 3.40793 loss)
I0614 16:07:35.444505  5063 solver.cpp:473] Iteration 5140, lr = 0.0001
I0614 16:07:36.327731  5063 solver.cpp:213] Iteration 5150, loss = 3.72561
I0614 16:07:36.327747  5063 solver.cpp:228]     Train net output #0: softmax = 3.72561 (* 1 = 3.72561 loss)
I0614 16:07:36.327752  5063 solver.cpp:473] Iteration 5150, lr = 0.0001
I0614 16:07:37.210111  5063 solver.cpp:213] Iteration 5160, loss = 3.53708
I0614 16:07:37.210130  5063 solver.cpp:228]     Train net output #0: softmax = 3.53708 (* 1 = 3.53708 loss)
I0614 16:07:37.210274  5063 solver.cpp:473] Iteration 5160, lr = 0.0001
I0614 16:07:38.092808  5063 solver.cpp:213] Iteration 5170, loss = 3.49525
I0614 16:07:38.092828  5063 solver.cpp:228]     Train net output #0: softmax = 3.49525 (* 1 = 3.49525 loss)
I0614 16:07:38.092833  5063 solver.cpp:473] Iteration 5170, lr = 0.0001
I0614 16:07:38.975836  5063 solver.cpp:213] Iteration 5180, loss = 3.4175
I0614 16:07:38.975853  5063 solver.cpp:228]     Train net output #0: softmax = 3.4175 (* 1 = 3.4175 loss)
I0614 16:07:38.975858  5063 solver.cpp:473] Iteration 5180, lr = 0.0001
I0614 16:07:39.858995  5063 solver.cpp:213] Iteration 5190, loss = 3.77743
I0614 16:07:39.859016  5063 solver.cpp:228]     Train net output #0: softmax = 3.77743 (* 1 = 3.77743 loss)
I0614 16:07:39.859021  5063 solver.cpp:473] Iteration 5190, lr = 0.0001
I0614 16:07:40.741180  5063 solver.cpp:213] Iteration 5200, loss = 3.68887
I0614 16:07:40.741195  5063 solver.cpp:228]     Train net output #0: softmax = 3.68887 (* 1 = 3.68887 loss)
I0614 16:07:40.741200  5063 solver.cpp:473] Iteration 5200, lr = 0.0001
I0614 16:07:41.624863  5063 solver.cpp:213] Iteration 5210, loss = 3.5344
I0614 16:07:41.624882  5063 solver.cpp:228]     Train net output #0: softmax = 3.5344 (* 1 = 3.5344 loss)
I0614 16:07:41.624887  5063 solver.cpp:473] Iteration 5210, lr = 0.0001
I0614 16:07:42.507853  5063 solver.cpp:213] Iteration 5220, loss = 3.47844
I0614 16:07:42.507874  5063 solver.cpp:228]     Train net output #0: softmax = 3.47844 (* 1 = 3.47844 loss)
I0614 16:07:42.507879  5063 solver.cpp:473] Iteration 5220, lr = 0.0001
I0614 16:07:43.390626  5063 solver.cpp:213] Iteration 5230, loss = 3.67864
I0614 16:07:43.390645  5063 solver.cpp:228]     Train net output #0: softmax = 3.67864 (* 1 = 3.67864 loss)
I0614 16:07:43.390650  5063 solver.cpp:473] Iteration 5230, lr = 0.0001
I0614 16:07:44.273291  5063 solver.cpp:213] Iteration 5240, loss = 3.39811
I0614 16:07:44.273308  5063 solver.cpp:228]     Train net output #0: softmax = 3.39811 (* 1 = 3.39811 loss)
I0614 16:07:44.273313  5063 solver.cpp:473] Iteration 5240, lr = 0.0001
I0614 16:07:45.157095  5063 solver.cpp:213] Iteration 5250, loss = 3.63159
I0614 16:07:45.157119  5063 solver.cpp:228]     Train net output #0: softmax = 3.63159 (* 1 = 3.63159 loss)
I0614 16:07:45.157124  5063 solver.cpp:473] Iteration 5250, lr = 0.0001
I0614 16:07:46.040513  5063 solver.cpp:213] Iteration 5260, loss = 3.6544
I0614 16:07:46.040529  5063 solver.cpp:228]     Train net output #0: softmax = 3.6544 (* 1 = 3.6544 loss)
I0614 16:07:46.040550  5063 solver.cpp:473] Iteration 5260, lr = 0.0001
I0614 16:07:46.922947  5063 solver.cpp:213] Iteration 5270, loss = 3.74622
I0614 16:07:46.922966  5063 solver.cpp:228]     Train net output #0: softmax = 3.74622 (* 1 = 3.74622 loss)
I0614 16:07:46.922971  5063 solver.cpp:473] Iteration 5270, lr = 0.0001
I0614 16:07:47.805529  5063 solver.cpp:213] Iteration 5280, loss = 3.54986
I0614 16:07:47.805559  5063 solver.cpp:228]     Train net output #0: softmax = 3.54986 (* 1 = 3.54986 loss)
I0614 16:07:47.805568  5063 solver.cpp:473] Iteration 5280, lr = 0.0001
I0614 16:07:48.687989  5063 solver.cpp:213] Iteration 5290, loss = 3.54328
I0614 16:07:48.688009  5063 solver.cpp:228]     Train net output #0: softmax = 3.54328 (* 1 = 3.54328 loss)
I0614 16:07:48.688014  5063 solver.cpp:473] Iteration 5290, lr = 0.0001
I0614 16:07:49.570860  5063 solver.cpp:213] Iteration 5300, loss = 3.75392
I0614 16:07:49.570879  5063 solver.cpp:228]     Train net output #0: softmax = 3.75392 (* 1 = 3.75392 loss)
I0614 16:07:49.570884  5063 solver.cpp:473] Iteration 5300, lr = 0.0001
I0614 16:07:50.453619  5063 solver.cpp:213] Iteration 5310, loss = 3.32067
I0614 16:07:50.453649  5063 solver.cpp:228]     Train net output #0: softmax = 3.32067 (* 1 = 3.32067 loss)
I0614 16:07:50.453655  5063 solver.cpp:473] Iteration 5310, lr = 0.0001
I0614 16:07:51.335577  5063 solver.cpp:213] Iteration 5320, loss = 3.6694
I0614 16:07:51.335595  5063 solver.cpp:228]     Train net output #0: softmax = 3.6694 (* 1 = 3.6694 loss)
I0614 16:07:51.335600  5063 solver.cpp:473] Iteration 5320, lr = 0.0001
I0614 16:07:52.219575  5063 solver.cpp:213] Iteration 5330, loss = 3.69839
I0614 16:07:52.219594  5063 solver.cpp:228]     Train net output #0: softmax = 3.69839 (* 1 = 3.69839 loss)
I0614 16:07:52.219599  5063 solver.cpp:473] Iteration 5330, lr = 0.0001
I0614 16:07:53.102810  5063 solver.cpp:213] Iteration 5340, loss = 3.57935
I0614 16:07:53.102831  5063 solver.cpp:228]     Train net output #0: softmax = 3.57935 (* 1 = 3.57935 loss)
I0614 16:07:53.102967  5063 solver.cpp:473] Iteration 5340, lr = 0.0001
I0614 16:07:53.985837  5063 solver.cpp:213] Iteration 5350, loss = 3.78852
I0614 16:07:53.985858  5063 solver.cpp:228]     Train net output #0: softmax = 3.78852 (* 1 = 3.78852 loss)
I0614 16:07:53.985863  5063 solver.cpp:473] Iteration 5350, lr = 0.0001
I0614 16:07:54.869659  5063 solver.cpp:213] Iteration 5360, loss = 3.42094
I0614 16:07:54.869684  5063 solver.cpp:228]     Train net output #0: softmax = 3.42094 (* 1 = 3.42094 loss)
I0614 16:07:54.869689  5063 solver.cpp:473] Iteration 5360, lr = 0.0001
I0614 16:07:55.752338  5063 solver.cpp:213] Iteration 5370, loss = 3.41439
I0614 16:07:55.752356  5063 solver.cpp:228]     Train net output #0: softmax = 3.41439 (* 1 = 3.41439 loss)
I0614 16:07:55.752360  5063 solver.cpp:473] Iteration 5370, lr = 0.0001
I0614 16:07:56.635442  5063 solver.cpp:213] Iteration 5380, loss = 3.51406
I0614 16:07:56.635462  5063 solver.cpp:228]     Train net output #0: softmax = 3.51406 (* 1 = 3.51406 loss)
I0614 16:07:56.635467  5063 solver.cpp:473] Iteration 5380, lr = 0.0001
I0614 16:07:57.518232  5063 solver.cpp:213] Iteration 5390, loss = 3.44192
I0614 16:07:57.518250  5063 solver.cpp:228]     Train net output #0: softmax = 3.44192 (* 1 = 3.44192 loss)
I0614 16:07:57.518255  5063 solver.cpp:473] Iteration 5390, lr = 0.0001
I0614 16:07:58.402017  5063 solver.cpp:213] Iteration 5400, loss = 3.45831
I0614 16:07:58.402040  5063 solver.cpp:228]     Train net output #0: softmax = 3.45831 (* 1 = 3.45831 loss)
I0614 16:07:58.402045  5063 solver.cpp:473] Iteration 5400, lr = 0.0001
I0614 16:07:59.285785  5063 solver.cpp:213] Iteration 5410, loss = 3.51011
I0614 16:07:59.285806  5063 solver.cpp:228]     Train net output #0: softmax = 3.51011 (* 1 = 3.51011 loss)
I0614 16:07:59.285811  5063 solver.cpp:473] Iteration 5410, lr = 0.0001
I0614 16:08:00.169837  5063 solver.cpp:213] Iteration 5420, loss = 3.67619
I0614 16:08:00.169859  5063 solver.cpp:228]     Train net output #0: softmax = 3.67619 (* 1 = 3.67619 loss)
I0614 16:08:00.169879  5063 solver.cpp:473] Iteration 5420, lr = 0.0001
I0614 16:08:01.052955  5063 solver.cpp:213] Iteration 5430, loss = 3.58704
I0614 16:08:01.052974  5063 solver.cpp:228]     Train net output #0: softmax = 3.58704 (* 1 = 3.58704 loss)
I0614 16:08:01.052979  5063 solver.cpp:473] Iteration 5430, lr = 0.0001
I0614 16:08:01.936738  5063 solver.cpp:213] Iteration 5440, loss = 3.60371
I0614 16:08:01.936785  5063 solver.cpp:228]     Train net output #0: softmax = 3.60371 (* 1 = 3.60371 loss)
I0614 16:08:01.936791  5063 solver.cpp:473] Iteration 5440, lr = 0.0001
I0614 16:08:02.819195  5063 solver.cpp:213] Iteration 5450, loss = 3.50531
I0614 16:08:02.819210  5063 solver.cpp:228]     Train net output #0: softmax = 3.50531 (* 1 = 3.50531 loss)
I0614 16:08:02.819214  5063 solver.cpp:473] Iteration 5450, lr = 0.0001
I0614 16:08:03.703199  5063 solver.cpp:213] Iteration 5460, loss = 3.6222
I0614 16:08:03.703224  5063 solver.cpp:228]     Train net output #0: softmax = 3.6222 (* 1 = 3.6222 loss)
I0614 16:08:03.709576  5063 solver.cpp:473] Iteration 5460, lr = 0.0001
I0614 16:08:04.585678  5063 solver.cpp:213] Iteration 5470, loss = 3.79672
I0614 16:08:04.585693  5063 solver.cpp:228]     Train net output #0: softmax = 3.79672 (* 1 = 3.79672 loss)
I0614 16:08:04.585698  5063 solver.cpp:473] Iteration 5470, lr = 0.0001
I0614 16:08:05.468919  5063 solver.cpp:213] Iteration 5480, loss = 3.55137
I0614 16:08:05.468940  5063 solver.cpp:228]     Train net output #0: softmax = 3.55137 (* 1 = 3.55137 loss)
I0614 16:08:05.468945  5063 solver.cpp:473] Iteration 5480, lr = 0.0001
I0614 16:08:06.351990  5063 solver.cpp:213] Iteration 5490, loss = 3.19984
I0614 16:08:06.352011  5063 solver.cpp:228]     Train net output #0: softmax = 3.19984 (* 1 = 3.19984 loss)
I0614 16:08:06.352016  5063 solver.cpp:473] Iteration 5490, lr = 0.0001
I0614 16:08:07.234884  5063 solver.cpp:213] Iteration 5500, loss = 3.57969
I0614 16:08:07.234901  5063 solver.cpp:228]     Train net output #0: softmax = 3.57969 (* 1 = 3.57969 loss)
I0614 16:08:07.234907  5063 solver.cpp:473] Iteration 5500, lr = 0.0001
I0614 16:08:08.117597  5063 solver.cpp:213] Iteration 5510, loss = 3.5224
I0614 16:08:08.117615  5063 solver.cpp:228]     Train net output #0: softmax = 3.5224 (* 1 = 3.5224 loss)
I0614 16:08:08.117620  5063 solver.cpp:473] Iteration 5510, lr = 0.0001
I0614 16:08:09.001466  5063 solver.cpp:213] Iteration 5520, loss = 3.59008
I0614 16:08:09.001488  5063 solver.cpp:228]     Train net output #0: softmax = 3.59008 (* 1 = 3.59008 loss)
I0614 16:08:09.001642  5063 solver.cpp:473] Iteration 5520, lr = 0.0001
I0614 16:08:09.884495  5063 solver.cpp:213] Iteration 5530, loss = 3.59487
I0614 16:08:09.884516  5063 solver.cpp:228]     Train net output #0: softmax = 3.59487 (* 1 = 3.59487 loss)
I0614 16:08:09.884521  5063 solver.cpp:473] Iteration 5530, lr = 0.0001
I0614 16:08:10.767608  5063 solver.cpp:213] Iteration 5540, loss = 3.55378
I0614 16:08:10.767632  5063 solver.cpp:228]     Train net output #0: softmax = 3.55378 (* 1 = 3.55378 loss)
I0614 16:08:10.767637  5063 solver.cpp:473] Iteration 5540, lr = 0.0001
I0614 16:08:11.651072  5063 solver.cpp:213] Iteration 5550, loss = 3.34234
I0614 16:08:11.651092  5063 solver.cpp:228]     Train net output #0: softmax = 3.34234 (* 1 = 3.34234 loss)
I0614 16:08:11.651096  5063 solver.cpp:473] Iteration 5550, lr = 0.0001
I0614 16:08:12.535096  5063 solver.cpp:213] Iteration 5560, loss = 3.56241
I0614 16:08:12.535114  5063 solver.cpp:228]     Train net output #0: softmax = 3.56241 (* 1 = 3.56241 loss)
I0614 16:08:12.535118  5063 solver.cpp:473] Iteration 5560, lr = 0.0001
I0614 16:08:13.417726  5063 solver.cpp:213] Iteration 5570, loss = 3.42019
I0614 16:08:13.417743  5063 solver.cpp:228]     Train net output #0: softmax = 3.42019 (* 1 = 3.42019 loss)
I0614 16:08:13.417749  5063 solver.cpp:473] Iteration 5570, lr = 0.0001
I0614 16:08:14.300299  5063 solver.cpp:213] Iteration 5580, loss = 3.59884
I0614 16:08:14.300320  5063 solver.cpp:228]     Train net output #0: softmax = 3.59884 (* 1 = 3.59884 loss)
I0614 16:08:14.300331  5063 solver.cpp:473] Iteration 5580, lr = 0.0001
I0614 16:08:15.183292  5063 solver.cpp:213] Iteration 5590, loss = 3.44064
I0614 16:08:15.183308  5063 solver.cpp:228]     Train net output #0: softmax = 3.44064 (* 1 = 3.44064 loss)
I0614 16:08:15.183313  5063 solver.cpp:473] Iteration 5590, lr = 0.0001
I0614 16:08:16.066485  5063 solver.cpp:213] Iteration 5600, loss = 3.4229
I0614 16:08:16.066506  5063 solver.cpp:228]     Train net output #0: softmax = 3.4229 (* 1 = 3.4229 loss)
I0614 16:08:16.066529  5063 solver.cpp:473] Iteration 5600, lr = 0.0001
I0614 16:08:16.949318  5063 solver.cpp:213] Iteration 5610, loss = 3.53451
I0614 16:08:16.949337  5063 solver.cpp:228]     Train net output #0: softmax = 3.53451 (* 1 = 3.53451 loss)
I0614 16:08:16.949342  5063 solver.cpp:473] Iteration 5610, lr = 0.0001
I0614 16:08:17.832314  5063 solver.cpp:213] Iteration 5620, loss = 3.54889
I0614 16:08:17.832329  5063 solver.cpp:228]     Train net output #0: softmax = 3.54889 (* 1 = 3.54889 loss)
I0614 16:08:17.832334  5063 solver.cpp:473] Iteration 5620, lr = 0.0001
I0614 16:08:18.715421  5063 solver.cpp:213] Iteration 5630, loss = 3.27238
I0614 16:08:18.715441  5063 solver.cpp:228]     Train net output #0: softmax = 3.27238 (* 1 = 3.27238 loss)
I0614 16:08:18.715446  5063 solver.cpp:473] Iteration 5630, lr = 0.0001
I0614 16:08:19.598395  5063 solver.cpp:213] Iteration 5640, loss = 3.60201
I0614 16:08:19.598414  5063 solver.cpp:228]     Train net output #0: softmax = 3.60201 (* 1 = 3.60201 loss)
I0614 16:08:19.598419  5063 solver.cpp:473] Iteration 5640, lr = 0.0001
I0614 16:08:20.481606  5063 solver.cpp:213] Iteration 5650, loss = 3.59362
I0614 16:08:20.481626  5063 solver.cpp:228]     Train net output #0: softmax = 3.59362 (* 1 = 3.59362 loss)
I0614 16:08:20.481631  5063 solver.cpp:473] Iteration 5650, lr = 0.0001
I0614 16:08:21.363939  5063 solver.cpp:213] Iteration 5660, loss = 3.75782
I0614 16:08:21.363960  5063 solver.cpp:228]     Train net output #0: softmax = 3.75782 (* 1 = 3.75782 loss)
I0614 16:08:21.363965  5063 solver.cpp:473] Iteration 5660, lr = 0.0001
I0614 16:08:22.246757  5063 solver.cpp:213] Iteration 5670, loss = 3.66065
I0614 16:08:22.246775  5063 solver.cpp:228]     Train net output #0: softmax = 3.66065 (* 1 = 3.66065 loss)
I0614 16:08:22.246780  5063 solver.cpp:473] Iteration 5670, lr = 0.0001
I0614 16:08:23.129814  5063 solver.cpp:213] Iteration 5680, loss = 3.55408
I0614 16:08:23.129832  5063 solver.cpp:228]     Train net output #0: softmax = 3.55408 (* 1 = 3.55408 loss)
I0614 16:08:23.129837  5063 solver.cpp:473] Iteration 5680, lr = 0.0001
I0614 16:08:24.012553  5063 solver.cpp:213] Iteration 5690, loss = 3.65524
I0614 16:08:24.012572  5063 solver.cpp:228]     Train net output #0: softmax = 3.65524 (* 1 = 3.65524 loss)
I0614 16:08:24.012576  5063 solver.cpp:473] Iteration 5690, lr = 0.0001
I0614 16:08:24.895875  5063 solver.cpp:213] Iteration 5700, loss = 3.43084
I0614 16:08:24.895896  5063 solver.cpp:228]     Train net output #0: softmax = 3.43084 (* 1 = 3.43084 loss)
I0614 16:08:24.895907  5063 solver.cpp:473] Iteration 5700, lr = 0.0001
I0614 16:08:25.778313  5063 solver.cpp:213] Iteration 5710, loss = 3.67293
I0614 16:08:25.778332  5063 solver.cpp:228]     Train net output #0: softmax = 3.67293 (* 1 = 3.67293 loss)
I0614 16:08:25.778337  5063 solver.cpp:473] Iteration 5710, lr = 0.0001
I0614 16:08:26.661267  5063 solver.cpp:213] Iteration 5720, loss = 3.33815
I0614 16:08:26.661290  5063 solver.cpp:228]     Train net output #0: softmax = 3.33815 (* 1 = 3.33815 loss)
I0614 16:08:26.661295  5063 solver.cpp:473] Iteration 5720, lr = 0.0001
I0614 16:08:27.543576  5063 solver.cpp:213] Iteration 5730, loss = 3.45352
I0614 16:08:27.543591  5063 solver.cpp:228]     Train net output #0: softmax = 3.45352 (* 1 = 3.45352 loss)
I0614 16:08:27.543596  5063 solver.cpp:473] Iteration 5730, lr = 0.0001
I0614 16:08:28.426146  5063 solver.cpp:213] Iteration 5740, loss = 3.53134
I0614 16:08:28.426163  5063 solver.cpp:228]     Train net output #0: softmax = 3.53134 (* 1 = 3.53134 loss)
I0614 16:08:28.426167  5063 solver.cpp:473] Iteration 5740, lr = 0.0001
I0614 16:08:29.308881  5063 solver.cpp:213] Iteration 5750, loss = 3.28438
I0614 16:08:29.308898  5063 solver.cpp:228]     Train net output #0: softmax = 3.28438 (* 1 = 3.28438 loss)
I0614 16:08:29.308903  5063 solver.cpp:473] Iteration 5750, lr = 0.0001
I0614 16:08:30.192292  5063 solver.cpp:213] Iteration 5760, loss = 3.34184
I0614 16:08:30.192313  5063 solver.cpp:228]     Train net output #0: softmax = 3.34184 (* 1 = 3.34184 loss)
I0614 16:08:30.192445  5063 solver.cpp:473] Iteration 5760, lr = 0.0001
I0614 16:08:31.075134  5063 solver.cpp:213] Iteration 5770, loss = 3.78526
I0614 16:08:31.075152  5063 solver.cpp:228]     Train net output #0: softmax = 3.78526 (* 1 = 3.78526 loss)
I0614 16:08:31.075156  5063 solver.cpp:473] Iteration 5770, lr = 0.0001
I0614 16:08:31.958513  5063 solver.cpp:213] Iteration 5780, loss = 3.39409
I0614 16:08:31.958554  5063 solver.cpp:228]     Train net output #0: softmax = 3.39409 (* 1 = 3.39409 loss)
I0614 16:08:31.958560  5063 solver.cpp:473] Iteration 5780, lr = 0.0001
I0614 16:08:32.840783  5063 solver.cpp:213] Iteration 5790, loss = 3.63525
I0614 16:08:32.840800  5063 solver.cpp:228]     Train net output #0: softmax = 3.63525 (* 1 = 3.63525 loss)
I0614 16:08:32.840804  5063 solver.cpp:473] Iteration 5790, lr = 0.0001
I0614 16:08:33.722909  5063 solver.cpp:213] Iteration 5800, loss = 3.6289
I0614 16:08:33.722925  5063 solver.cpp:228]     Train net output #0: softmax = 3.6289 (* 1 = 3.6289 loss)
I0614 16:08:33.722930  5063 solver.cpp:473] Iteration 5800, lr = 0.0001
I0614 16:08:34.606515  5063 solver.cpp:213] Iteration 5810, loss = 3.68879
I0614 16:08:34.606534  5063 solver.cpp:228]     Train net output #0: softmax = 3.68879 (* 1 = 3.68879 loss)
I0614 16:08:34.606539  5063 solver.cpp:473] Iteration 5810, lr = 0.0001
I0614 16:08:35.489017  5063 solver.cpp:213] Iteration 5820, loss = 3.4371
I0614 16:08:35.489037  5063 solver.cpp:228]     Train net output #0: softmax = 3.4371 (* 1 = 3.4371 loss)
I0614 16:08:35.489042  5063 solver.cpp:473] Iteration 5820, lr = 0.0001
I0614 16:08:36.372434  5063 solver.cpp:213] Iteration 5830, loss = 3.48661
I0614 16:08:36.372453  5063 solver.cpp:228]     Train net output #0: softmax = 3.48661 (* 1 = 3.48661 loss)
I0614 16:08:36.372458  5063 solver.cpp:473] Iteration 5830, lr = 0.0001
I0614 16:08:37.255538  5063 solver.cpp:213] Iteration 5840, loss = 3.55997
I0614 16:08:37.255554  5063 solver.cpp:228]     Train net output #0: softmax = 3.55997 (* 1 = 3.55997 loss)
I0614 16:08:37.255559  5063 solver.cpp:473] Iteration 5840, lr = 0.0001
I0614 16:08:38.138278  5063 solver.cpp:213] Iteration 5850, loss = 3.48579
I0614 16:08:38.138301  5063 solver.cpp:228]     Train net output #0: softmax = 3.48579 (* 1 = 3.48579 loss)
I0614 16:08:38.138306  5063 solver.cpp:473] Iteration 5850, lr = 0.0001
I0614 16:08:39.021489  5063 solver.cpp:213] Iteration 5860, loss = 3.67789
I0614 16:08:39.021509  5063 solver.cpp:228]     Train net output #0: softmax = 3.67789 (* 1 = 3.67789 loss)
I0614 16:08:39.021513  5063 solver.cpp:473] Iteration 5860, lr = 0.0001
I0614 16:08:39.904855  5063 solver.cpp:213] Iteration 5870, loss = 3.43434
I0614 16:08:39.904875  5063 solver.cpp:228]     Train net output #0: softmax = 3.43434 (* 1 = 3.43434 loss)
I0614 16:08:39.904878  5063 solver.cpp:473] Iteration 5870, lr = 0.0001
I0614 16:08:40.787878  5063 solver.cpp:213] Iteration 5880, loss = 3.34561
I0614 16:08:40.787899  5063 solver.cpp:228]     Train net output #0: softmax = 3.34561 (* 1 = 3.34561 loss)
I0614 16:08:40.788051  5063 solver.cpp:473] Iteration 5880, lr = 0.0001
I0614 16:08:41.671146  5063 solver.cpp:213] Iteration 5890, loss = 3.79276
I0614 16:08:41.671164  5063 solver.cpp:228]     Train net output #0: softmax = 3.79276 (* 1 = 3.79276 loss)
I0614 16:08:41.671169  5063 solver.cpp:473] Iteration 5890, lr = 0.0001
I0614 16:08:42.554114  5063 solver.cpp:213] Iteration 5900, loss = 3.54696
I0614 16:08:42.554134  5063 solver.cpp:228]     Train net output #0: softmax = 3.54696 (* 1 = 3.54696 loss)
I0614 16:08:42.554139  5063 solver.cpp:473] Iteration 5900, lr = 0.0001
I0614 16:08:43.437785  5063 solver.cpp:213] Iteration 5910, loss = 3.72506
I0614 16:08:43.437808  5063 solver.cpp:228]     Train net output #0: softmax = 3.72506 (* 1 = 3.72506 loss)
I0614 16:08:43.437813  5063 solver.cpp:473] Iteration 5910, lr = 0.0001
I0614 16:08:44.320441  5063 solver.cpp:213] Iteration 5920, loss = 3.70652
I0614 16:08:44.320461  5063 solver.cpp:228]     Train net output #0: softmax = 3.70652 (* 1 = 3.70652 loss)
I0614 16:08:44.320472  5063 solver.cpp:473] Iteration 5920, lr = 0.0001
I0614 16:08:45.203619  5063 solver.cpp:213] Iteration 5930, loss = 3.61401
I0614 16:08:45.203637  5063 solver.cpp:228]     Train net output #0: softmax = 3.61401 (* 1 = 3.61401 loss)
I0614 16:08:45.203642  5063 solver.cpp:473] Iteration 5930, lr = 0.0001
I0614 16:08:46.086660  5063 solver.cpp:213] Iteration 5940, loss = 3.66838
I0614 16:08:46.086681  5063 solver.cpp:228]     Train net output #0: softmax = 3.66838 (* 1 = 3.66838 loss)
I0614 16:08:46.086836  5063 solver.cpp:473] Iteration 5940, lr = 0.0001
I0614 16:08:46.970881  5063 solver.cpp:213] Iteration 5950, loss = 3.41508
I0614 16:08:46.970901  5063 solver.cpp:228]     Train net output #0: softmax = 3.41508 (* 1 = 3.41508 loss)
I0614 16:08:46.970906  5063 solver.cpp:473] Iteration 5950, lr = 0.0001
I0614 16:08:47.854583  5063 solver.cpp:213] Iteration 5960, loss = 3.6143
I0614 16:08:47.854600  5063 solver.cpp:228]     Train net output #0: softmax = 3.6143 (* 1 = 3.6143 loss)
I0614 16:08:47.854605  5063 solver.cpp:473] Iteration 5960, lr = 0.0001
I0614 16:08:48.737540  5063 solver.cpp:213] Iteration 5970, loss = 3.43941
I0614 16:08:48.737560  5063 solver.cpp:228]     Train net output #0: softmax = 3.43941 (* 1 = 3.43941 loss)
I0614 16:08:48.737565  5063 solver.cpp:473] Iteration 5970, lr = 0.0001
I0614 16:08:49.620733  5063 solver.cpp:213] Iteration 5980, loss = 3.29211
I0614 16:08:49.620759  5063 solver.cpp:228]     Train net output #0: softmax = 3.29211 (* 1 = 3.29211 loss)
I0614 16:08:49.620764  5063 solver.cpp:473] Iteration 5980, lr = 0.0001
I0614 16:08:50.503824  5063 solver.cpp:213] Iteration 5990, loss = 3.30162
I0614 16:08:50.503844  5063 solver.cpp:228]     Train net output #0: softmax = 3.30162 (* 1 = 3.30162 loss)
I0614 16:08:50.503849  5063 solver.cpp:473] Iteration 5990, lr = 0.0001
I0614 16:08:51.329120  5063 solver.cpp:362] Snapshotting to snapshots/16-06-14_15h51m21s_0_11_pretrainClassification_iter_6000.caffemodel
I0614 16:08:51.329893  5063 solver.cpp:370] Snapshotting solver state to snapshots/16-06-14_15h51m21s_0_11_pretrainClassification_iter_6000.solverstate
I0614 16:08:51.330323  5063 solver.cpp:291] Iteration 6000, Testing net (#0)
I0614 16:08:51.443156  5063 solver.cpp:342]     Test net output #0: accuracy_top_1 = 0.173438
I0614 16:08:51.443172  5063 solver.cpp:342]     Test net output #1: accuracy_top_5 = 0.440625
I0614 16:08:51.443178  5063 solver.cpp:342]     Test net output #2: softmax = 3.47666 (* 1 = 3.47666 loss)
I0614 16:08:51.501401  5063 solver.cpp:213] Iteration 6000, loss = 3.54466
I0614 16:08:51.501415  5063 solver.cpp:228]     Train net output #0: softmax = 3.54466 (* 1 = 3.54466 loss)
I0614 16:08:51.501420  5063 solver.cpp:473] Iteration 6000, lr = 0.0001
I0614 16:08:52.384980  5063 solver.cpp:213] Iteration 6010, loss = 3.51739
I0614 16:08:52.384999  5063 solver.cpp:228]     Train net output #0: softmax = 3.51739 (* 1 = 3.51739 loss)
I0614 16:08:52.385004  5063 solver.cpp:473] Iteration 6010, lr = 0.0001
I0614 16:08:53.267621  5063 solver.cpp:213] Iteration 6020, loss = 3.48204
I0614 16:08:53.267637  5063 solver.cpp:228]     Train net output #0: softmax = 3.48204 (* 1 = 3.48204 loss)
I0614 16:08:53.267640  5063 solver.cpp:473] Iteration 6020, lr = 0.0001
I0614 16:08:54.150094  5063 solver.cpp:213] Iteration 6030, loss = 3.68058
I0614 16:08:54.150116  5063 solver.cpp:228]     Train net output #0: softmax = 3.68058 (* 1 = 3.68058 loss)
I0614 16:08:54.150121  5063 solver.cpp:473] Iteration 6030, lr = 0.0001
I0614 16:08:55.033159  5063 solver.cpp:213] Iteration 6040, loss = 3.55759
I0614 16:08:55.033179  5063 solver.cpp:228]     Train net output #0: softmax = 3.55759 (* 1 = 3.55759 loss)
I0614 16:08:55.033184  5063 solver.cpp:473] Iteration 6040, lr = 0.0001
I0614 16:08:55.917071  5063 solver.cpp:213] Iteration 6050, loss = 3.75408
I0614 16:08:55.917090  5063 solver.cpp:228]     Train net output #0: softmax = 3.75408 (* 1 = 3.75408 loss)
I0614 16:08:55.917095  5063 solver.cpp:473] Iteration 6050, lr = 0.0001
I0614 16:08:56.800345  5063 solver.cpp:213] Iteration 6060, loss = 3.53987
I0614 16:08:56.800370  5063 solver.cpp:228]     Train net output #0: softmax = 3.53987 (* 1 = 3.53987 loss)
I0614 16:08:56.800375  5063 solver.cpp:473] Iteration 6060, lr = 0.0001
I0614 16:08:57.683392  5063 solver.cpp:213] Iteration 6070, loss = 3.42307
I0614 16:08:57.683409  5063 solver.cpp:228]     Train net output #0: softmax = 3.42307 (* 1 = 3.42307 loss)
I0614 16:08:57.683413  5063 solver.cpp:473] Iteration 6070, lr = 0.0001
I0614 16:08:58.565851  5063 solver.cpp:213] Iteration 6080, loss = 3.41783
I0614 16:08:58.565870  5063 solver.cpp:228]     Train net output #0: softmax = 3.41783 (* 1 = 3.41783 loss)
I0614 16:08:58.565874  5063 solver.cpp:473] Iteration 6080, lr = 0.0001
I0614 16:08:59.448920  5063 solver.cpp:213] Iteration 6090, loss = 3.46218
I0614 16:08:59.448941  5063 solver.cpp:228]     Train net output #0: softmax = 3.46218 (* 1 = 3.46218 loss)
I0614 16:08:59.448946  5063 solver.cpp:473] Iteration 6090, lr = 0.0001
I0614 16:09:00.331220  5063 solver.cpp:213] Iteration 6100, loss = 3.63511
I0614 16:09:00.331239  5063 solver.cpp:228]     Train net output #0: softmax = 3.63511 (* 1 = 3.63511 loss)
I0614 16:09:00.331244  5063 solver.cpp:473] Iteration 6100, lr = 0.0001
I0614 16:09:01.213933  5063 solver.cpp:213] Iteration 6110, loss = 3.45863
I0614 16:09:01.213950  5063 solver.cpp:228]     Train net output #0: softmax = 3.45863 (* 1 = 3.45863 loss)
I0614 16:09:01.213955  5063 solver.cpp:473] Iteration 6110, lr = 0.0001
I0614 16:09:02.097198  5063 solver.cpp:213] Iteration 6120, loss = 3.60381
I0614 16:09:02.097360  5063 solver.cpp:228]     Train net output #0: softmax = 3.60381 (* 1 = 3.60381 loss)
I0614 16:09:02.097368  5063 solver.cpp:473] Iteration 6120, lr = 0.0001
I0614 16:09:02.980139  5063 solver.cpp:213] Iteration 6130, loss = 3.44992
I0614 16:09:02.980154  5063 solver.cpp:228]     Train net output #0: softmax = 3.44992 (* 1 = 3.44992 loss)
I0614 16:09:02.980159  5063 solver.cpp:473] Iteration 6130, lr = 0.0001
I0614 16:09:03.863129  5063 solver.cpp:213] Iteration 6140, loss = 3.33372
I0614 16:09:03.863145  5063 solver.cpp:228]     Train net output #0: softmax = 3.33372 (* 1 = 3.33372 loss)
I0614 16:09:03.863150  5063 solver.cpp:473] Iteration 6140, lr = 0.0001
I0614 16:09:04.747273  5063 solver.cpp:213] Iteration 6150, loss = 3.42945
I0614 16:09:04.747298  5063 solver.cpp:228]     Train net output #0: softmax = 3.42945 (* 1 = 3.42945 loss)
I0614 16:09:04.747303  5063 solver.cpp:473] Iteration 6150, lr = 0.0001
I0614 16:09:05.630188  5063 solver.cpp:213] Iteration 6160, loss = 3.66499
I0614 16:09:05.630208  5063 solver.cpp:228]     Train net output #0: softmax = 3.66499 (* 1 = 3.66499 loss)
I0614 16:09:05.630213  5063 solver.cpp:473] Iteration 6160, lr = 0.0001
I0614 16:09:06.513368  5063 solver.cpp:213] Iteration 6170, loss = 3.41975
I0614 16:09:06.513384  5063 solver.cpp:228]     Train net output #0: softmax = 3.41975 (* 1 = 3.41975 loss)
I0614 16:09:06.513389  5063 solver.cpp:473] Iteration 6170, lr = 0.0001
I0614 16:09:07.396123  5063 solver.cpp:213] Iteration 6180, loss = 3.55493
I0614 16:09:07.396144  5063 solver.cpp:228]     Train net output #0: softmax = 3.55493 (* 1 = 3.55493 loss)
I0614 16:09:07.396149  5063 solver.cpp:473] Iteration 6180, lr = 0.0001
I0614 16:09:08.278144  5063 solver.cpp:213] Iteration 6190, loss = 3.70917
I0614 16:09:08.278162  5063 solver.cpp:228]     Train net output #0: softmax = 3.70917 (* 1 = 3.70917 loss)
I0614 16:09:08.278167  5063 solver.cpp:473] Iteration 6190, lr = 0.0001
I0614 16:09:09.160765  5063 solver.cpp:213] Iteration 6200, loss = 3.51988
I0614 16:09:09.160781  5063 solver.cpp:228]     Train net output #0: softmax = 3.51988 (* 1 = 3.51988 loss)
I0614 16:09:09.160786  5063 solver.cpp:473] Iteration 6200, lr = 0.0001
I0614 16:09:10.043571  5063 solver.cpp:213] Iteration 6210, loss = 3.35694
I0614 16:09:10.043592  5063 solver.cpp:228]     Train net output #0: softmax = 3.35694 (* 1 = 3.35694 loss)
I0614 16:09:10.043597  5063 solver.cpp:473] Iteration 6210, lr = 0.0001
I0614 16:09:10.926406  5063 solver.cpp:213] Iteration 6220, loss = 3.451
I0614 16:09:10.926426  5063 solver.cpp:228]     Train net output #0: softmax = 3.451 (* 1 = 3.451 loss)
I0614 16:09:10.926432  5063 solver.cpp:473] Iteration 6220, lr = 0.0001
I0614 16:09:11.810083  5063 solver.cpp:213] Iteration 6230, loss = 3.45595
I0614 16:09:11.810103  5063 solver.cpp:228]     Train net output #0: softmax = 3.45595 (* 1 = 3.45595 loss)
I0614 16:09:11.810108  5063 solver.cpp:473] Iteration 6230, lr = 0.0001
I0614 16:09:12.693943  5063 solver.cpp:213] Iteration 6240, loss = 3.42254
I0614 16:09:12.693969  5063 solver.cpp:228]     Train net output #0: softmax = 3.42254 (* 1 = 3.42254 loss)
I0614 16:09:12.694113  5063 solver.cpp:473] Iteration 6240, lr = 0.0001
I0614 16:09:13.576855  5063 solver.cpp:213] Iteration 6250, loss = 3.5755
I0614 16:09:13.576874  5063 solver.cpp:228]     Train net output #0: softmax = 3.5755 (* 1 = 3.5755 loss)
I0614 16:09:13.576879  5063 solver.cpp:473] Iteration 6250, lr = 0.0001
I0614 16:09:14.459420  5063 solver.cpp:213] Iteration 6260, loss = 3.52298
I0614 16:09:14.459437  5063 solver.cpp:228]     Train net output #0: softmax = 3.52298 (* 1 = 3.52298 loss)
I0614 16:09:14.459442  5063 solver.cpp:473] Iteration 6260, lr = 0.0001
I0614 16:09:15.342089  5063 solver.cpp:213] Iteration 6270, loss = 3.58122
I0614 16:09:15.342110  5063 solver.cpp:228]     Train net output #0: softmax = 3.58122 (* 1 = 3.58122 loss)
I0614 16:09:15.342115  5063 solver.cpp:473] Iteration 6270, lr = 0.0001
I0614 16:09:16.224558  5063 solver.cpp:213] Iteration 6280, loss = 3.60678
I0614 16:09:16.224576  5063 solver.cpp:228]     Train net output #0: softmax = 3.60678 (* 1 = 3.60678 loss)
I0614 16:09:16.224596  5063 solver.cpp:473] Iteration 6280, lr = 0.0001
I0614 16:09:17.106894  5063 solver.cpp:213] Iteration 6290, loss = 3.6019
I0614 16:09:17.106914  5063 solver.cpp:228]     Train net output #0: softmax = 3.6019 (* 1 = 3.6019 loss)
I0614 16:09:17.106919  5063 solver.cpp:473] Iteration 6290, lr = 0.0001
I0614 16:09:17.989639  5063 solver.cpp:213] Iteration 6300, loss = 3.69978
I0614 16:09:17.989658  5063 solver.cpp:228]     Train net output #0: softmax = 3.69978 (* 1 = 3.69978 loss)
I0614 16:09:17.989804  5063 solver.cpp:473] Iteration 6300, lr = 0.0001
I0614 16:09:18.873344  5063 solver.cpp:213] Iteration 6310, loss = 3.76456
I0614 16:09:18.873363  5063 solver.cpp:228]     Train net output #0: softmax = 3.76456 (* 1 = 3.76456 loss)
I0614 16:09:18.873368  5063 solver.cpp:473] Iteration 6310, lr = 0.0001
I0614 16:09:19.755185  5063 solver.cpp:213] Iteration 6320, loss = 3.73833
I0614 16:09:19.755210  5063 solver.cpp:228]     Train net output #0: softmax = 3.73833 (* 1 = 3.73833 loss)
I0614 16:09:19.755216  5063 solver.cpp:473] Iteration 6320, lr = 0.0001
I0614 16:09:20.637606  5063 solver.cpp:213] Iteration 6330, loss = 3.42852
I0614 16:09:20.637627  5063 solver.cpp:228]     Train net output #0: softmax = 3.42852 (* 1 = 3.42852 loss)
I0614 16:09:20.637632  5063 solver.cpp:473] Iteration 6330, lr = 0.0001
I0614 16:09:21.520576  5063 solver.cpp:213] Iteration 6340, loss = 3.42288
I0614 16:09:21.520593  5063 solver.cpp:228]     Train net output #0: softmax = 3.42288 (* 1 = 3.42288 loss)
I0614 16:09:21.520598  5063 solver.cpp:473] Iteration 6340, lr = 0.0001
I0614 16:09:22.403993  5063 solver.cpp:213] Iteration 6350, loss = 3.32927
I0614 16:09:22.404011  5063 solver.cpp:228]     Train net output #0: softmax = 3.32927 (* 1 = 3.32927 loss)
I0614 16:09:22.404016  5063 solver.cpp:473] Iteration 6350, lr = 0.0001
I0614 16:09:23.286746  5063 solver.cpp:213] Iteration 6360, loss = 3.46682
I0614 16:09:23.286767  5063 solver.cpp:228]     Train net output #0: softmax = 3.46682 (* 1 = 3.46682 loss)
I0614 16:09:23.286923  5063 solver.cpp:473] Iteration 6360, lr = 0.0001
I0614 16:09:24.166831  5063 solver.cpp:213] Iteration 6370, loss = 3.43278
I0614 16:09:24.166851  5063 solver.cpp:228]     Train net output #0: softmax = 3.43278 (* 1 = 3.43278 loss)
I0614 16:09:24.166856  5063 solver.cpp:473] Iteration 6370, lr = 0.0001
I0614 16:09:25.049489  5063 solver.cpp:213] Iteration 6380, loss = 3.52825
I0614 16:09:25.049516  5063 solver.cpp:228]     Train net output #0: softmax = 3.52825 (* 1 = 3.52825 loss)
I0614 16:09:25.049522  5063 solver.cpp:473] Iteration 6380, lr = 0.0001
I0614 16:09:25.932426  5063 solver.cpp:213] Iteration 6390, loss = 3.72127
I0614 16:09:25.932446  5063 solver.cpp:228]     Train net output #0: softmax = 3.72127 (* 1 = 3.72127 loss)
I0614 16:09:25.932451  5063 solver.cpp:473] Iteration 6390, lr = 0.0001
I0614 16:09:26.814937  5063 solver.cpp:213] Iteration 6400, loss = 3.51604
I0614 16:09:26.814954  5063 solver.cpp:228]     Train net output #0: softmax = 3.51604 (* 1 = 3.51604 loss)
I0614 16:09:26.814959  5063 solver.cpp:473] Iteration 6400, lr = 0.0001
I0614 16:09:27.697801  5063 solver.cpp:213] Iteration 6410, loss = 3.62029
I0614 16:09:27.697818  5063 solver.cpp:228]     Train net output #0: softmax = 3.62029 (* 1 = 3.62029 loss)
I0614 16:09:27.697821  5063 solver.cpp:473] Iteration 6410, lr = 0.0001
I0614 16:09:28.581167  5063 solver.cpp:213] Iteration 6420, loss = 3.68479
I0614 16:09:28.581188  5063 solver.cpp:228]     Train net output #0: softmax = 3.68479 (* 1 = 3.68479 loss)
I0614 16:09:28.581193  5063 solver.cpp:473] Iteration 6420, lr = 0.0001
I0614 16:09:29.464186  5063 solver.cpp:213] Iteration 6430, loss = 3.71242
I0614 16:09:29.464205  5063 solver.cpp:228]     Train net output #0: softmax = 3.71242 (* 1 = 3.71242 loss)
I0614 16:09:29.464210  5063 solver.cpp:473] Iteration 6430, lr = 0.0001
I0614 16:09:30.347143  5063 solver.cpp:213] Iteration 6440, loss = 3.61084
I0614 16:09:30.347167  5063 solver.cpp:228]     Train net output #0: softmax = 3.61084 (* 1 = 3.61084 loss)
I0614 16:09:30.347189  5063 solver.cpp:473] Iteration 6440, lr = 0.0001
I0614 16:09:31.229871  5063 solver.cpp:213] Iteration 6450, loss = 3.47433
I0614 16:09:31.229888  5063 solver.cpp:228]     Train net output #0: softmax = 3.47433 (* 1 = 3.47433 loss)
I0614 16:09:31.229893  5063 solver.cpp:473] Iteration 6450, lr = 0.0001
I0614 16:09:32.112651  5063 solver.cpp:213] Iteration 6460, loss = 3.14351
I0614 16:09:32.112689  5063 solver.cpp:228]     Train net output #0: softmax = 3.14351 (* 1 = 3.14351 loss)
I0614 16:09:32.112694  5063 solver.cpp:473] Iteration 6460, lr = 0.0001
I0614 16:09:32.995277  5063 solver.cpp:213] Iteration 6470, loss = 3.23378
I0614 16:09:32.995293  5063 solver.cpp:228]     Train net output #0: softmax = 3.23378 (* 1 = 3.23378 loss)
I0614 16:09:32.995298  5063 solver.cpp:473] Iteration 6470, lr = 0.0001
I0614 16:09:33.878340  5063 solver.cpp:213] Iteration 6480, loss = 3.4817
I0614 16:09:33.878358  5063 solver.cpp:228]     Train net output #0: softmax = 3.4817 (* 1 = 3.4817 loss)
I0614 16:09:33.878368  5063 solver.cpp:473] Iteration 6480, lr = 0.0001
I0614 16:09:34.761879  5063 solver.cpp:213] Iteration 6490, loss = 3.43342
I0614 16:09:34.761899  5063 solver.cpp:228]     Train net output #0: softmax = 3.43342 (* 1 = 3.43342 loss)
I0614 16:09:34.761904  5063 solver.cpp:473] Iteration 6490, lr = 0.0001
I0614 16:09:35.644290  5063 solver.cpp:213] Iteration 6500, loss = 3.44685
I0614 16:09:35.644315  5063 solver.cpp:228]     Train net output #0: softmax = 3.44685 (* 1 = 3.44685 loss)
I0614 16:09:35.644320  5063 solver.cpp:473] Iteration 6500, lr = 0.0001
I0614 16:09:36.528306  5063 solver.cpp:213] Iteration 6510, loss = 3.34037
I0614 16:09:36.528323  5063 solver.cpp:228]     Train net output #0: softmax = 3.34037 (* 1 = 3.34037 loss)
I0614 16:09:36.528328  5063 solver.cpp:473] Iteration 6510, lr = 0.0001
I0614 16:09:37.411104  5063 solver.cpp:213] Iteration 6520, loss = 3.28943
I0614 16:09:37.411123  5063 solver.cpp:228]     Train net output #0: softmax = 3.28943 (* 1 = 3.28943 loss)
I0614 16:09:37.411126  5063 solver.cpp:473] Iteration 6520, lr = 0.0001
I0614 16:09:38.294466  5063 solver.cpp:213] Iteration 6530, loss = 3.42267
I0614 16:09:38.294486  5063 solver.cpp:228]     Train net output #0: softmax = 3.42267 (* 1 = 3.42267 loss)
I0614 16:09:38.294492  5063 solver.cpp:473] Iteration 6530, lr = 0.0001
I0614 16:09:39.177564  5063 solver.cpp:213] Iteration 6540, loss = 3.48734
I0614 16:09:39.177714  5063 solver.cpp:228]     Train net output #0: softmax = 3.48734 (* 1 = 3.48734 loss)
I0614 16:09:39.177722  5063 solver.cpp:473] Iteration 6540, lr = 0.0001
I0614 16:09:40.061394  5063 solver.cpp:213] Iteration 6550, loss = 3.55558
I0614 16:09:40.061417  5063 solver.cpp:228]     Train net output #0: softmax = 3.55558 (* 1 = 3.55558 loss)
I0614 16:09:40.061424  5063 solver.cpp:473] Iteration 6550, lr = 0.0001
I0614 16:09:40.943837  5063 solver.cpp:213] Iteration 6560, loss = 3.23526
I0614 16:09:40.943855  5063 solver.cpp:228]     Train net output #0: softmax = 3.23526 (* 1 = 3.23526 loss)
I0614 16:09:40.943859  5063 solver.cpp:473] Iteration 6560, lr = 0.0001
I0614 16:09:41.826508  5063 solver.cpp:213] Iteration 6570, loss = 3.49447
I0614 16:09:41.826521  5063 solver.cpp:228]     Train net output #0: softmax = 3.49447 (* 1 = 3.49447 loss)
I0614 16:09:41.826526  5063 solver.cpp:473] Iteration 6570, lr = 0.0001
I0614 16:09:42.709724  5063 solver.cpp:213] Iteration 6580, loss = 3.45343
I0614 16:09:42.709744  5063 solver.cpp:228]     Train net output #0: softmax = 3.45343 (* 1 = 3.45343 loss)
I0614 16:09:42.709749  5063 solver.cpp:473] Iteration 6580, lr = 0.0001
I0614 16:09:43.593797  5063 solver.cpp:213] Iteration 6590, loss = 3.79434
I0614 16:09:43.593817  5063 solver.cpp:228]     Train net output #0: softmax = 3.79434 (* 1 = 3.79434 loss)
I0614 16:09:43.593822  5063 solver.cpp:473] Iteration 6590, lr = 0.0001
I0614 16:09:44.476933  5063 solver.cpp:213] Iteration 6600, loss = 3.30273
I0614 16:09:44.476954  5063 solver.cpp:228]     Train net output #0: softmax = 3.30273 (* 1 = 3.30273 loss)
I0614 16:09:44.476959  5063 solver.cpp:473] Iteration 6600, lr = 0.0001
I0614 16:09:45.359828  5063 solver.cpp:213] Iteration 6610, loss = 3.54158
I0614 16:09:45.359850  5063 solver.cpp:228]     Train net output #0: softmax = 3.54158 (* 1 = 3.54158 loss)
I0614 16:09:45.359855  5063 solver.cpp:473] Iteration 6610, lr = 0.0001
I0614 16:09:46.242185  5063 solver.cpp:213] Iteration 6620, loss = 3.52115
I0614 16:09:46.242200  5063 solver.cpp:228]     Train net output #0: softmax = 3.52115 (* 1 = 3.52115 loss)
I0614 16:09:46.242220  5063 solver.cpp:473] Iteration 6620, lr = 0.0001
I0614 16:09:47.124477  5063 solver.cpp:213] Iteration 6630, loss = 3.67403
I0614 16:09:47.124495  5063 solver.cpp:228]     Train net output #0: softmax = 3.67403 (* 1 = 3.67403 loss)
I0614 16:09:47.124500  5063 solver.cpp:473] Iteration 6630, lr = 0.0001
I0614 16:09:48.006750  5063 solver.cpp:213] Iteration 6640, loss = 3.62421
I0614 16:09:48.006769  5063 solver.cpp:228]     Train net output #0: softmax = 3.62421 (* 1 = 3.62421 loss)
I0614 16:09:48.006774  5063 solver.cpp:473] Iteration 6640, lr = 0.0001
I0614 16:09:48.889802  5063 solver.cpp:213] Iteration 6650, loss = 3.56208
I0614 16:09:48.889819  5063 solver.cpp:228]     Train net output #0: softmax = 3.56208 (* 1 = 3.56208 loss)
I0614 16:09:48.889824  5063 solver.cpp:473] Iteration 6650, lr = 0.0001
I0614 16:09:49.772819  5063 solver.cpp:213] Iteration 6660, loss = 3.4741
I0614 16:09:49.772840  5063 solver.cpp:228]     Train net output #0: softmax = 3.4741 (* 1 = 3.4741 loss)
I0614 16:09:49.773000  5063 solver.cpp:473] Iteration 6660, lr = 0.0001
I0614 16:09:50.655943  5063 solver.cpp:213] Iteration 6670, loss = 3.3385
I0614 16:09:50.655967  5063 solver.cpp:228]     Train net output #0: softmax = 3.3385 (* 1 = 3.3385 loss)
I0614 16:09:50.655972  5063 solver.cpp:473] Iteration 6670, lr = 0.0001
I0614 16:09:51.538918  5063 solver.cpp:213] Iteration 6680, loss = 3.45913
I0614 16:09:51.538949  5063 solver.cpp:228]     Train net output #0: softmax = 3.45913 (* 1 = 3.45913 loss)
I0614 16:09:51.538954  5063 solver.cpp:473] Iteration 6680, lr = 0.0001
I0614 16:09:52.422164  5063 solver.cpp:213] Iteration 6690, loss = 3.74569
I0614 16:09:52.422183  5063 solver.cpp:228]     Train net output #0: softmax = 3.74569 (* 1 = 3.74569 loss)
I0614 16:09:52.422188  5063 solver.cpp:473] Iteration 6690, lr = 0.0001
I0614 16:09:53.305071  5063 solver.cpp:213] Iteration 6700, loss = 3.76528
I0614 16:09:53.305086  5063 solver.cpp:228]     Train net output #0: softmax = 3.76528 (* 1 = 3.76528 loss)
I0614 16:09:53.305097  5063 solver.cpp:473] Iteration 6700, lr = 0.0001
I0614 16:09:54.187546  5063 solver.cpp:213] Iteration 6710, loss = 3.56612
I0614 16:09:54.187563  5063 solver.cpp:228]     Train net output #0: softmax = 3.56612 (* 1 = 3.56612 loss)
I0614 16:09:54.187568  5063 solver.cpp:473] Iteration 6710, lr = 0.0001
I0614 16:09:55.070274  5063 solver.cpp:213] Iteration 6720, loss = 3.32238
I0614 16:09:55.070302  5063 solver.cpp:228]     Train net output #0: softmax = 3.32238 (* 1 = 3.32238 loss)
I0614 16:09:55.070464  5063 solver.cpp:473] Iteration 6720, lr = 0.0001
I0614 16:09:55.953833  5063 solver.cpp:213] Iteration 6730, loss = 3.30744
I0614 16:09:55.953852  5063 solver.cpp:228]     Train net output #0: softmax = 3.30744 (* 1 = 3.30744 loss)
I0614 16:09:55.953857  5063 solver.cpp:473] Iteration 6730, lr = 0.0001
I0614 16:09:56.836319  5063 solver.cpp:213] Iteration 6740, loss = 3.1306
I0614 16:09:56.836339  5063 solver.cpp:228]     Train net output #0: softmax = 3.1306 (* 1 = 3.1306 loss)
I0614 16:09:56.836344  5063 solver.cpp:473] Iteration 6740, lr = 0.0001
I0614 16:09:57.719585  5063 solver.cpp:213] Iteration 6750, loss = 3.37953
I0614 16:09:57.719604  5063 solver.cpp:228]     Train net output #0: softmax = 3.37953 (* 1 = 3.37953 loss)
I0614 16:09:57.719607  5063 solver.cpp:473] Iteration 6750, lr = 0.0001
I0614 16:09:58.602109  5063 solver.cpp:213] Iteration 6760, loss = 3.56652
I0614 16:09:58.602130  5063 solver.cpp:228]     Train net output #0: softmax = 3.56652 (* 1 = 3.56652 loss)
I0614 16:09:58.602135  5063 solver.cpp:473] Iteration 6760, lr = 0.0001
I0614 16:09:59.484905  5063 solver.cpp:213] Iteration 6770, loss = 3.3762
I0614 16:09:59.484923  5063 solver.cpp:228]     Train net output #0: softmax = 3.3762 (* 1 = 3.3762 loss)
I0614 16:09:59.484928  5063 solver.cpp:473] Iteration 6770, lr = 0.0001
I0614 16:10:00.367966  5063 solver.cpp:213] Iteration 6780, loss = 3.45797
I0614 16:10:00.367991  5063 solver.cpp:228]     Train net output #0: softmax = 3.45797 (* 1 = 3.45797 loss)
I0614 16:10:00.368010  5063 solver.cpp:473] Iteration 6780, lr = 0.0001
I0614 16:10:01.250468  5063 solver.cpp:213] Iteration 6790, loss = 3.37855
I0614 16:10:01.250488  5063 solver.cpp:228]     Train net output #0: softmax = 3.37855 (* 1 = 3.37855 loss)
I0614 16:10:01.250493  5063 solver.cpp:473] Iteration 6790, lr = 0.0001
I0614 16:10:02.134297  5063 solver.cpp:213] Iteration 6800, loss = 3.57463
I0614 16:10:02.134331  5063 solver.cpp:228]     Train net output #0: softmax = 3.57463 (* 1 = 3.57463 loss)
I0614 16:10:02.134336  5063 solver.cpp:473] Iteration 6800, lr = 0.0001
I0614 16:10:03.017364  5063 solver.cpp:213] Iteration 6810, loss = 3.46789
I0614 16:10:03.017380  5063 solver.cpp:228]     Train net output #0: softmax = 3.46789 (* 1 = 3.46789 loss)
I0614 16:10:03.017385  5063 solver.cpp:473] Iteration 6810, lr = 0.0001
I0614 16:10:03.900043  5063 solver.cpp:213] Iteration 6820, loss = 3.48459
I0614 16:10:03.900061  5063 solver.cpp:228]     Train net output #0: softmax = 3.48459 (* 1 = 3.48459 loss)
I0614 16:10:03.900066  5063 solver.cpp:473] Iteration 6820, lr = 0.0001
I0614 16:10:04.783138  5063 solver.cpp:213] Iteration 6830, loss = 3.63539
I0614 16:10:04.783157  5063 solver.cpp:228]     Train net output #0: softmax = 3.63539 (* 1 = 3.63539 loss)
I0614 16:10:04.783162  5063 solver.cpp:473] Iteration 6830, lr = 0.0001
I0614 16:10:05.666173  5063 solver.cpp:213] Iteration 6840, loss = 3.52859
I0614 16:10:05.666201  5063 solver.cpp:228]     Train net output #0: softmax = 3.52859 (* 1 = 3.52859 loss)
I0614 16:10:05.666352  5063 solver.cpp:473] Iteration 6840, lr = 0.0001
I0614 16:10:06.549777  5063 solver.cpp:213] Iteration 6850, loss = 3.39447
I0614 16:10:06.549794  5063 solver.cpp:228]     Train net output #0: softmax = 3.39447 (* 1 = 3.39447 loss)
I0614 16:10:06.549799  5063 solver.cpp:473] Iteration 6850, lr = 0.0001
I0614 16:10:07.432507  5063 solver.cpp:213] Iteration 6860, loss = 3.35895
I0614 16:10:07.432528  5063 solver.cpp:228]     Train net output #0: softmax = 3.35895 (* 1 = 3.35895 loss)
I0614 16:10:07.432539  5063 solver.cpp:473] Iteration 6860, lr = 0.0001
I0614 16:10:08.316578  5063 solver.cpp:213] Iteration 6870, loss = 3.38385
I0614 16:10:08.316601  5063 solver.cpp:228]     Train net output #0: softmax = 3.38385 (* 1 = 3.38385 loss)
I0614 16:10:08.316604  5063 solver.cpp:473] Iteration 6870, lr = 0.0001
I0614 16:10:09.199194  5063 solver.cpp:213] Iteration 6880, loss = 3.65865
I0614 16:10:09.199211  5063 solver.cpp:228]     Train net output #0: softmax = 3.65865 (* 1 = 3.65865 loss)
I0614 16:10:09.199216  5063 solver.cpp:473] Iteration 6880, lr = 0.0001
I0614 16:10:10.081661  5063 solver.cpp:213] Iteration 6890, loss = 3.5552
I0614 16:10:10.081684  5063 solver.cpp:228]     Train net output #0: softmax = 3.5552 (* 1 = 3.5552 loss)
I0614 16:10:10.081689  5063 solver.cpp:473] Iteration 6890, lr = 0.0001
I0614 16:10:10.964380  5063 solver.cpp:213] Iteration 6900, loss = 3.20964
I0614 16:10:10.964398  5063 solver.cpp:228]     Train net output #0: softmax = 3.20964 (* 1 = 3.20964 loss)
I0614 16:10:10.964408  5063 solver.cpp:473] Iteration 6900, lr = 0.0001
I0614 16:10:11.847237  5063 solver.cpp:213] Iteration 6910, loss = 3.2672
I0614 16:10:11.847255  5063 solver.cpp:228]     Train net output #0: softmax = 3.2672 (* 1 = 3.2672 loss)
I0614 16:10:11.847259  5063 solver.cpp:473] Iteration 6910, lr = 0.0001
I0614 16:10:12.730219  5063 solver.cpp:213] Iteration 6920, loss = 3.38941
I0614 16:10:12.730234  5063 solver.cpp:228]     Train net output #0: softmax = 3.38941 (* 1 = 3.38941 loss)
I0614 16:10:12.730239  5063 solver.cpp:473] Iteration 6920, lr = 0.0001
I0614 16:10:13.612915  5063 solver.cpp:213] Iteration 6930, loss = 3.49
I0614 16:10:13.612931  5063 solver.cpp:228]     Train net output #0: softmax = 3.49 (* 1 = 3.49 loss)
I0614 16:10:13.612936  5063 solver.cpp:473] Iteration 6930, lr = 0.0001
I0614 16:10:14.496371  5063 solver.cpp:213] Iteration 6940, loss = 3.54716
I0614 16:10:14.496390  5063 solver.cpp:228]     Train net output #0: softmax = 3.54716 (* 1 = 3.54716 loss)
I0614 16:10:14.496395  5063 solver.cpp:473] Iteration 6940, lr = 0.0001
I0614 16:10:15.378893  5063 solver.cpp:213] Iteration 6950, loss = 3.47685
I0614 16:10:15.378916  5063 solver.cpp:228]     Train net output #0: softmax = 3.47685 (* 1 = 3.47685 loss)
I0614 16:10:15.378921  5063 solver.cpp:473] Iteration 6950, lr = 0.0001
I0614 16:10:16.262140  5063 solver.cpp:213] Iteration 6960, loss = 3.52751
I0614 16:10:16.262161  5063 solver.cpp:228]     Train net output #0: softmax = 3.52751 (* 1 = 3.52751 loss)
I0614 16:10:16.262290  5063 solver.cpp:473] Iteration 6960, lr = 0.0001
I0614 16:10:17.145145  5063 solver.cpp:213] Iteration 6970, loss = 3.31289
I0614 16:10:17.145160  5063 solver.cpp:228]     Train net output #0: softmax = 3.31289 (* 1 = 3.31289 loss)
I0614 16:10:17.145165  5063 solver.cpp:473] Iteration 6970, lr = 0.0001
I0614 16:10:18.028115  5063 solver.cpp:213] Iteration 6980, loss = 3.5225
I0614 16:10:18.028137  5063 solver.cpp:228]     Train net output #0: softmax = 3.5225 (* 1 = 3.5225 loss)
I0614 16:10:18.028143  5063 solver.cpp:473] Iteration 6980, lr = 0.0001
I0614 16:10:18.910411  5063 solver.cpp:213] Iteration 6990, loss = 3.35046
I0614 16:10:18.910429  5063 solver.cpp:228]     Train net output #0: softmax = 3.35046 (* 1 = 3.35046 loss)
I0614 16:10:18.910434  5063 solver.cpp:473] Iteration 6990, lr = 0.0001
I0614 16:10:19.735630  5063 solver.cpp:362] Snapshotting to snapshots/16-06-14_15h51m21s_0_11_pretrainClassification_iter_7000.caffemodel
I0614 16:10:19.736388  5063 solver.cpp:370] Snapshotting solver state to snapshots/16-06-14_15h51m21s_0_11_pretrainClassification_iter_7000.solverstate
I0614 16:10:19.736819  5063 solver.cpp:291] Iteration 7000, Testing net (#0)
I0614 16:10:19.849522  5063 solver.cpp:342]     Test net output #0: accuracy_top_1 = 0.185937
I0614 16:10:19.849539  5063 solver.cpp:342]     Test net output #1: accuracy_top_5 = 0.426562
I0614 16:10:19.849545  5063 solver.cpp:342]     Test net output #2: softmax = 3.47348 (* 1 = 3.47348 loss)
I0614 16:10:19.907745  5063 solver.cpp:213] Iteration 7000, loss = 3.39816
I0614 16:10:19.907759  5063 solver.cpp:228]     Train net output #0: softmax = 3.39816 (* 1 = 3.39816 loss)
I0614 16:10:19.907764  5063 solver.cpp:473] Iteration 7000, lr = 0.0001
I0614 16:10:20.790271  5063 solver.cpp:213] Iteration 7010, loss = 3.51324
I0614 16:10:20.790293  5063 solver.cpp:228]     Train net output #0: softmax = 3.51324 (* 1 = 3.51324 loss)
I0614 16:10:20.790298  5063 solver.cpp:473] Iteration 7010, lr = 0.0001
I0614 16:10:21.672858  5063 solver.cpp:213] Iteration 7020, loss = 3.65683
I0614 16:10:21.672876  5063 solver.cpp:228]     Train net output #0: softmax = 3.65683 (* 1 = 3.65683 loss)
I0614 16:10:21.672881  5063 solver.cpp:473] Iteration 7020, lr = 0.0001
I0614 16:10:22.556628  5063 solver.cpp:213] Iteration 7030, loss = 3.47082
I0614 16:10:22.556646  5063 solver.cpp:228]     Train net output #0: softmax = 3.47082 (* 1 = 3.47082 loss)
I0614 16:10:22.556651  5063 solver.cpp:473] Iteration 7030, lr = 0.0001
I0614 16:10:23.439476  5063 solver.cpp:213] Iteration 7040, loss = 3.26287
I0614 16:10:23.439493  5063 solver.cpp:228]     Train net output #0: softmax = 3.26287 (* 1 = 3.26287 loss)
I0614 16:10:23.439498  5063 solver.cpp:473] Iteration 7040, lr = 0.0001
I0614 16:10:24.322624  5063 solver.cpp:213] Iteration 7050, loss = 3.38035
I0614 16:10:24.322643  5063 solver.cpp:228]     Train net output #0: softmax = 3.38035 (* 1 = 3.38035 loss)
I0614 16:10:24.322648  5063 solver.cpp:473] Iteration 7050, lr = 0.0001
I0614 16:10:25.205552  5063 solver.cpp:213] Iteration 7060, loss = 3.46495
I0614 16:10:25.205575  5063 solver.cpp:228]     Train net output #0: softmax = 3.46495 (* 1 = 3.46495 loss)
I0614 16:10:25.205580  5063 solver.cpp:473] Iteration 7060, lr = 0.0001
I0614 16:10:26.088557  5063 solver.cpp:213] Iteration 7070, loss = 3.33205
I0614 16:10:26.088577  5063 solver.cpp:228]     Train net output #0: softmax = 3.33205 (* 1 = 3.33205 loss)
I0614 16:10:26.088582  5063 solver.cpp:473] Iteration 7070, lr = 0.0001
I0614 16:10:26.971200  5063 solver.cpp:213] Iteration 7080, loss = 3.40782
I0614 16:10:26.971221  5063 solver.cpp:228]     Train net output #0: softmax = 3.40782 (* 1 = 3.40782 loss)
I0614 16:10:26.971374  5063 solver.cpp:473] Iteration 7080, lr = 0.0001
I0614 16:10:27.854409  5063 solver.cpp:213] Iteration 7090, loss = 3.5908
I0614 16:10:27.854427  5063 solver.cpp:228]     Train net output #0: softmax = 3.5908 (* 1 = 3.5908 loss)
I0614 16:10:27.854432  5063 solver.cpp:473] Iteration 7090, lr = 0.0001
I0614 16:10:28.738056  5063 solver.cpp:213] Iteration 7100, loss = 3.58708
I0614 16:10:28.738075  5063 solver.cpp:228]     Train net output #0: softmax = 3.58708 (* 1 = 3.58708 loss)
I0614 16:10:28.738080  5063 solver.cpp:473] Iteration 7100, lr = 0.0001
I0614 16:10:29.621320  5063 solver.cpp:213] Iteration 7110, loss = 3.50165
I0614 16:10:29.621340  5063 solver.cpp:228]     Train net output #0: softmax = 3.50165 (* 1 = 3.50165 loss)
I0614 16:10:29.621343  5063 solver.cpp:473] Iteration 7110, lr = 0.0001
I0614 16:10:30.504699  5063 solver.cpp:213] Iteration 7120, loss = 3.49362
I0614 16:10:30.504724  5063 solver.cpp:228]     Train net output #0: softmax = 3.49362 (* 1 = 3.49362 loss)
I0614 16:10:30.504729  5063 solver.cpp:473] Iteration 7120, lr = 0.0001
I0614 16:10:31.387305  5063 solver.cpp:213] Iteration 7130, loss = 3.38381
I0614 16:10:31.387320  5063 solver.cpp:228]     Train net output #0: softmax = 3.38381 (* 1 = 3.38381 loss)
I0614 16:10:31.387326  5063 solver.cpp:473] Iteration 7130, lr = 0.0001
I0614 16:10:32.270738  5063 solver.cpp:213] Iteration 7140, loss = 3.37013
I0614 16:10:32.270912  5063 solver.cpp:228]     Train net output #0: softmax = 3.37013 (* 1 = 3.37013 loss)
I0614 16:10:32.270920  5063 solver.cpp:473] Iteration 7140, lr = 0.0001
I0614 16:10:33.153645  5063 solver.cpp:213] Iteration 7150, loss = 3.68456
I0614 16:10:33.153662  5063 solver.cpp:228]     Train net output #0: softmax = 3.68456 (* 1 = 3.68456 loss)
I0614 16:10:33.153667  5063 solver.cpp:473] Iteration 7150, lr = 0.0001
I0614 16:10:34.036994  5063 solver.cpp:213] Iteration 7160, loss = 3.35771
I0614 16:10:34.037019  5063 solver.cpp:228]     Train net output #0: softmax = 3.35771 (* 1 = 3.35771 loss)
I0614 16:10:34.037024  5063 solver.cpp:473] Iteration 7160, lr = 0.0001
I0614 16:10:34.920611  5063 solver.cpp:213] Iteration 7170, loss = 3.37743
I0614 16:10:34.920631  5063 solver.cpp:228]     Train net output #0: softmax = 3.37743 (* 1 = 3.37743 loss)
I0614 16:10:34.920636  5063 solver.cpp:473] Iteration 7170, lr = 0.0001
I0614 16:10:35.803313  5063 solver.cpp:213] Iteration 7180, loss = 3.62658
I0614 16:10:35.803334  5063 solver.cpp:228]     Train net output #0: softmax = 3.62658 (* 1 = 3.62658 loss)
I0614 16:10:35.803339  5063 solver.cpp:473] Iteration 7180, lr = 0.0001
I0614 16:10:36.685012  5063 solver.cpp:213] Iteration 7190, loss = 3.50796
I0614 16:10:36.685029  5063 solver.cpp:228]     Train net output #0: softmax = 3.50796 (* 1 = 3.50796 loss)
I0614 16:10:36.685034  5063 solver.cpp:473] Iteration 7190, lr = 0.0001
I0614 16:10:37.567394  5063 solver.cpp:213] Iteration 7200, loss = 3.39152
I0614 16:10:37.567410  5063 solver.cpp:228]     Train net output #0: softmax = 3.39152 (* 1 = 3.39152 loss)
I0614 16:10:37.567415  5063 solver.cpp:473] Iteration 7200, lr = 0.0001
I0614 16:10:38.450433  5063 solver.cpp:213] Iteration 7210, loss = 3.26707
I0614 16:10:38.450453  5063 solver.cpp:228]     Train net output #0: softmax = 3.26707 (* 1 = 3.26707 loss)
I0614 16:10:38.450458  5063 solver.cpp:473] Iteration 7210, lr = 0.0001
I0614 16:10:39.332993  5063 solver.cpp:213] Iteration 7220, loss = 3.592
I0614 16:10:39.333011  5063 solver.cpp:228]     Train net output #0: softmax = 3.592 (* 1 = 3.592 loss)
I0614 16:10:39.333016  5063 solver.cpp:473] Iteration 7220, lr = 0.0001
I0614 16:10:40.215834  5063 solver.cpp:213] Iteration 7230, loss = 3.26934
I0614 16:10:40.215850  5063 solver.cpp:228]     Train net output #0: softmax = 3.26934 (* 1 = 3.26934 loss)
I0614 16:10:40.215855  5063 solver.cpp:473] Iteration 7230, lr = 0.0001
I0614 16:10:41.098703  5063 solver.cpp:213] Iteration 7240, loss = 3.46433
I0614 16:10:41.098726  5063 solver.cpp:228]     Train net output #0: softmax = 3.46433 (* 1 = 3.46433 loss)
I0614 16:10:41.098731  5063 solver.cpp:473] Iteration 7240, lr = 0.0001
I0614 16:10:41.981276  5063 solver.cpp:213] Iteration 7250, loss = 3.30954
I0614 16:10:41.981294  5063 solver.cpp:228]     Train net output #0: softmax = 3.30954 (* 1 = 3.30954 loss)
I0614 16:10:41.981299  5063 solver.cpp:473] Iteration 7250, lr = 0.0001
I0614 16:10:42.864277  5063 solver.cpp:213] Iteration 7260, loss = 3.61652
I0614 16:10:42.864298  5063 solver.cpp:228]     Train net output #0: softmax = 3.61652 (* 1 = 3.61652 loss)
I0614 16:10:42.864425  5063 solver.cpp:473] Iteration 7260, lr = 0.0001
I0614 16:10:43.748004  5063 solver.cpp:213] Iteration 7270, loss = 3.64732
I0614 16:10:43.748023  5063 solver.cpp:228]     Train net output #0: softmax = 3.64732 (* 1 = 3.64732 loss)
I0614 16:10:43.748028  5063 solver.cpp:473] Iteration 7270, lr = 0.0001
I0614 16:10:44.630851  5063 solver.cpp:213] Iteration 7280, loss = 3.30696
I0614 16:10:44.630868  5063 solver.cpp:228]     Train net output #0: softmax = 3.30696 (* 1 = 3.30696 loss)
I0614 16:10:44.630873  5063 solver.cpp:473] Iteration 7280, lr = 0.0001
I0614 16:10:45.513717  5063 solver.cpp:213] Iteration 7290, loss = 3.10411
I0614 16:10:45.513738  5063 solver.cpp:228]     Train net output #0: softmax = 3.10411 (* 1 = 3.10411 loss)
I0614 16:10:45.513743  5063 solver.cpp:473] Iteration 7290, lr = 0.0001
I0614 16:10:46.397989  5063 solver.cpp:213] Iteration 7300, loss = 3.21343
I0614 16:10:46.398010  5063 solver.cpp:228]     Train net output #0: softmax = 3.21343 (* 1 = 3.21343 loss)
I0614 16:10:46.398033  5063 solver.cpp:473] Iteration 7300, lr = 0.0001
I0614 16:10:47.281342  5063 solver.cpp:213] Iteration 7310, loss = 3.23163
I0614 16:10:47.281360  5063 solver.cpp:228]     Train net output #0: softmax = 3.23163 (* 1 = 3.23163 loss)
I0614 16:10:47.281364  5063 solver.cpp:473] Iteration 7310, lr = 0.0001
I0614 16:10:48.164468  5063 solver.cpp:213] Iteration 7320, loss = 3.45912
I0614 16:10:48.164499  5063 solver.cpp:228]     Train net output #0: softmax = 3.45912 (* 1 = 3.45912 loss)
I0614 16:10:48.164507  5063 solver.cpp:473] Iteration 7320, lr = 0.0001
I0614 16:10:49.047868  5063 solver.cpp:213] Iteration 7330, loss = 3.40201
I0614 16:10:49.047886  5063 solver.cpp:228]     Train net output #0: softmax = 3.40201 (* 1 = 3.40201 loss)
I0614 16:10:49.047891  5063 solver.cpp:473] Iteration 7330, lr = 0.0001
I0614 16:10:49.930974  5063 solver.cpp:213] Iteration 7340, loss = 3.64637
I0614 16:10:49.930994  5063 solver.cpp:228]     Train net output #0: softmax = 3.64637 (* 1 = 3.64637 loss)
I0614 16:10:49.930999  5063 solver.cpp:473] Iteration 7340, lr = 0.0001
I0614 16:10:50.814218  5063 solver.cpp:213] Iteration 7350, loss = 3.3759
I0614 16:10:50.814242  5063 solver.cpp:228]     Train net output #0: softmax = 3.3759 (* 1 = 3.3759 loss)
I0614 16:10:50.814247  5063 solver.cpp:473] Iteration 7350, lr = 0.0001
I0614 16:10:51.696825  5063 solver.cpp:213] Iteration 7360, loss = 3.24245
I0614 16:10:51.696842  5063 solver.cpp:228]     Train net output #0: softmax = 3.24245 (* 1 = 3.24245 loss)
I0614 16:10:51.696847  5063 solver.cpp:473] Iteration 7360, lr = 0.0001
I0614 16:10:52.579782  5063 solver.cpp:213] Iteration 7370, loss = 3.53285
I0614 16:10:52.579797  5063 solver.cpp:228]     Train net output #0: softmax = 3.53285 (* 1 = 3.53285 loss)
I0614 16:10:52.579802  5063 solver.cpp:473] Iteration 7370, lr = 0.0001
I0614 16:10:53.462134  5063 solver.cpp:213] Iteration 7380, loss = 3.36245
I0614 16:10:53.462152  5063 solver.cpp:228]     Train net output #0: softmax = 3.36245 (* 1 = 3.36245 loss)
I0614 16:10:53.462157  5063 solver.cpp:473] Iteration 7380, lr = 0.0001
I0614 16:10:54.344856  5063 solver.cpp:213] Iteration 7390, loss = 3.40656
I0614 16:10:54.344873  5063 solver.cpp:228]     Train net output #0: softmax = 3.40656 (* 1 = 3.40656 loss)
I0614 16:10:54.344878  5063 solver.cpp:473] Iteration 7390, lr = 0.0001
I0614 16:10:55.227506  5063 solver.cpp:213] Iteration 7400, loss = 3.39389
I0614 16:10:55.227525  5063 solver.cpp:228]     Train net output #0: softmax = 3.39389 (* 1 = 3.39389 loss)
I0614 16:10:55.227530  5063 solver.cpp:473] Iteration 7400, lr = 0.0001
I0614 16:10:56.110896  5063 solver.cpp:213] Iteration 7410, loss = 3.50628
I0614 16:10:56.110918  5063 solver.cpp:228]     Train net output #0: softmax = 3.50628 (* 1 = 3.50628 loss)
I0614 16:10:56.110924  5063 solver.cpp:473] Iteration 7410, lr = 0.0001
I0614 16:10:56.993643  5063 solver.cpp:213] Iteration 7420, loss = 3.56182
I0614 16:10:56.993660  5063 solver.cpp:228]     Train net output #0: softmax = 3.56182 (* 1 = 3.56182 loss)
I0614 16:10:56.993665  5063 solver.cpp:473] Iteration 7420, lr = 0.0001
I0614 16:10:57.876615  5063 solver.cpp:213] Iteration 7430, loss = 3.2839
I0614 16:10:57.876629  5063 solver.cpp:228]     Train net output #0: softmax = 3.2839 (* 1 = 3.2839 loss)
I0614 16:10:57.876636  5063 solver.cpp:473] Iteration 7430, lr = 0.0001
I0614 16:10:58.760025  5063 solver.cpp:213] Iteration 7440, loss = 3.41872
I0614 16:10:58.760047  5063 solver.cpp:228]     Train net output #0: softmax = 3.41872 (* 1 = 3.41872 loss)
I0614 16:10:58.760192  5063 solver.cpp:473] Iteration 7440, lr = 0.0001
I0614 16:10:59.643245  5063 solver.cpp:213] Iteration 7450, loss = 3.60691
I0614 16:10:59.643263  5063 solver.cpp:228]     Train net output #0: softmax = 3.60691 (* 1 = 3.60691 loss)
I0614 16:10:59.643268  5063 solver.cpp:473] Iteration 7450, lr = 0.0001
I0614 16:11:00.525666  5063 solver.cpp:213] Iteration 7460, loss = 3.35621
I0614 16:11:00.525686  5063 solver.cpp:228]     Train net output #0: softmax = 3.35621 (* 1 = 3.35621 loss)
I0614 16:11:00.525707  5063 solver.cpp:473] Iteration 7460, lr = 0.0001
I0614 16:11:01.408423  5063 solver.cpp:213] Iteration 7470, loss = 3.28742
I0614 16:11:01.408442  5063 solver.cpp:228]     Train net output #0: softmax = 3.28742 (* 1 = 3.28742 loss)
I0614 16:11:01.408447  5063 solver.cpp:473] Iteration 7470, lr = 0.0001
I0614 16:11:02.292057  5063 solver.cpp:213] Iteration 7480, loss = 3.51343
I0614 16:11:02.292101  5063 solver.cpp:228]     Train net output #0: softmax = 3.51343 (* 1 = 3.51343 loss)
I0614 16:11:02.292107  5063 solver.cpp:473] Iteration 7480, lr = 0.0001
I0614 16:11:03.174960  5063 solver.cpp:213] Iteration 7490, loss = 3.66406
I0614 16:11:03.174974  5063 solver.cpp:228]     Train net output #0: softmax = 3.66406 (* 1 = 3.66406 loss)
I0614 16:11:03.174979  5063 solver.cpp:473] Iteration 7490, lr = 0.0001
I0614 16:11:04.057961  5063 solver.cpp:213] Iteration 7500, loss = 3.53599
I0614 16:11:04.057983  5063 solver.cpp:228]     Train net output #0: softmax = 3.53599 (* 1 = 3.53599 loss)
I0614 16:11:04.058118  5063 solver.cpp:473] Iteration 7500, lr = 0.0001
I0614 16:11:04.940978  5063 solver.cpp:213] Iteration 7510, loss = 3.19708
I0614 16:11:04.940996  5063 solver.cpp:228]     Train net output #0: softmax = 3.19708 (* 1 = 3.19708 loss)
I0614 16:11:04.941000  5063 solver.cpp:473] Iteration 7510, lr = 0.0001
I0614 16:11:05.823882  5063 solver.cpp:213] Iteration 7520, loss = 3.42979
I0614 16:11:05.823905  5063 solver.cpp:228]     Train net output #0: softmax = 3.42979 (* 1 = 3.42979 loss)
I0614 16:11:05.823910  5063 solver.cpp:473] Iteration 7520, lr = 0.0001
I0614 16:11:06.706594  5063 solver.cpp:213] Iteration 7530, loss = 3.35128
I0614 16:11:06.706611  5063 solver.cpp:228]     Train net output #0: softmax = 3.35128 (* 1 = 3.35128 loss)
I0614 16:11:06.706616  5063 solver.cpp:473] Iteration 7530, lr = 0.0001
I0614 16:11:07.589078  5063 solver.cpp:213] Iteration 7540, loss = 3.49369
I0614 16:11:07.589099  5063 solver.cpp:228]     Train net output #0: softmax = 3.49369 (* 1 = 3.49369 loss)
I0614 16:11:07.589104  5063 solver.cpp:473] Iteration 7540, lr = 0.0001
I0614 16:11:08.471699  5063 solver.cpp:213] Iteration 7550, loss = 3.3825
I0614 16:11:08.471719  5063 solver.cpp:228]     Train net output #0: softmax = 3.3825 (* 1 = 3.3825 loss)
I0614 16:11:08.471724  5063 solver.cpp:473] Iteration 7550, lr = 0.0001
I0614 16:11:09.354908  5063 solver.cpp:213] Iteration 7560, loss = 3.47349
I0614 16:11:09.354930  5063 solver.cpp:228]     Train net output #0: softmax = 3.47349 (* 1 = 3.47349 loss)
I0614 16:11:09.355056  5063 solver.cpp:473] Iteration 7560, lr = 0.0001
I0614 16:11:10.238307  5063 solver.cpp:213] Iteration 7570, loss = 3.66921
I0614 16:11:10.238327  5063 solver.cpp:228]     Train net output #0: softmax = 3.66921 (* 1 = 3.66921 loss)
I0614 16:11:10.238332  5063 solver.cpp:473] Iteration 7570, lr = 0.0001
I0614 16:11:11.121745  5063 solver.cpp:213] Iteration 7580, loss = 3.47124
I0614 16:11:11.121768  5063 solver.cpp:228]     Train net output #0: softmax = 3.47124 (* 1 = 3.47124 loss)
I0614 16:11:11.121773  5063 solver.cpp:473] Iteration 7580, lr = 0.0001
I0614 16:11:12.004010  5063 solver.cpp:213] Iteration 7590, loss = 3.42597
I0614 16:11:12.004030  5063 solver.cpp:228]     Train net output #0: softmax = 3.42597 (* 1 = 3.42597 loss)
I0614 16:11:12.004035  5063 solver.cpp:473] Iteration 7590, lr = 0.0001
I0614 16:11:12.887665  5063 solver.cpp:213] Iteration 7600, loss = 3.31767
I0614 16:11:12.887683  5063 solver.cpp:228]     Train net output #0: softmax = 3.31767 (* 1 = 3.31767 loss)
I0614 16:11:12.887688  5063 solver.cpp:473] Iteration 7600, lr = 0.0001
I0614 16:11:13.770818  5063 solver.cpp:213] Iteration 7610, loss = 3.36243
I0614 16:11:13.770836  5063 solver.cpp:228]     Train net output #0: softmax = 3.36243 (* 1 = 3.36243 loss)
I0614 16:11:13.770841  5063 solver.cpp:473] Iteration 7610, lr = 0.0001
I0614 16:11:14.652675  5063 solver.cpp:213] Iteration 7620, loss = 3.20551
I0614 16:11:14.652699  5063 solver.cpp:228]     Train net output #0: softmax = 3.20551 (* 1 = 3.20551 loss)
I0614 16:11:14.652709  5063 solver.cpp:473] Iteration 7620, lr = 0.0001
I0614 16:11:15.535508  5063 solver.cpp:213] Iteration 7630, loss = 3.41297
I0614 16:11:15.535527  5063 solver.cpp:228]     Train net output #0: softmax = 3.41297 (* 1 = 3.41297 loss)
I0614 16:11:15.535532  5063 solver.cpp:473] Iteration 7630, lr = 0.0001
I0614 16:11:16.418659  5063 solver.cpp:213] Iteration 7640, loss = 3.45751
I0614 16:11:16.418689  5063 solver.cpp:228]     Train net output #0: softmax = 3.45751 (* 1 = 3.45751 loss)
I0614 16:11:16.418707  5063 solver.cpp:473] Iteration 7640, lr = 0.0001
I0614 16:11:17.302429  5063 solver.cpp:213] Iteration 7650, loss = 3.4852
I0614 16:11:17.302448  5063 solver.cpp:228]     Train net output #0: softmax = 3.4852 (* 1 = 3.4852 loss)
I0614 16:11:17.302453  5063 solver.cpp:473] Iteration 7650, lr = 0.0001
I0614 16:11:18.185564  5063 solver.cpp:213] Iteration 7660, loss = 3.52122
I0614 16:11:18.185583  5063 solver.cpp:228]     Train net output #0: softmax = 3.52122 (* 1 = 3.52122 loss)
I0614 16:11:18.185588  5063 solver.cpp:473] Iteration 7660, lr = 0.0001
I0614 16:11:19.069514  5063 solver.cpp:213] Iteration 7670, loss = 3.30501
I0614 16:11:19.069530  5063 solver.cpp:228]     Train net output #0: softmax = 3.30501 (* 1 = 3.30501 loss)
I0614 16:11:19.069535  5063 solver.cpp:473] Iteration 7670, lr = 0.0001
I0614 16:11:19.953080  5063 solver.cpp:213] Iteration 7680, loss = 3.31863
I0614 16:11:19.953102  5063 solver.cpp:228]     Train net output #0: softmax = 3.31863 (* 1 = 3.31863 loss)
I0614 16:11:19.953260  5063 solver.cpp:473] Iteration 7680, lr = 0.0001
I0614 16:11:20.835969  5063 solver.cpp:213] Iteration 7690, loss = 3.57848
I0614 16:11:20.835988  5063 solver.cpp:228]     Train net output #0: softmax = 3.57848 (* 1 = 3.57848 loss)
I0614 16:11:20.835994  5063 solver.cpp:473] Iteration 7690, lr = 0.0001
I0614 16:11:21.718394  5063 solver.cpp:213] Iteration 7700, loss = 3.35937
I0614 16:11:21.718415  5063 solver.cpp:228]     Train net output #0: softmax = 3.35937 (* 1 = 3.35937 loss)
I0614 16:11:21.718420  5063 solver.cpp:473] Iteration 7700, lr = 0.0001
I0614 16:11:22.602246  5063 solver.cpp:213] Iteration 7710, loss = 3.50245
I0614 16:11:22.602267  5063 solver.cpp:228]     Train net output #0: softmax = 3.50245 (* 1 = 3.50245 loss)
I0614 16:11:22.602273  5063 solver.cpp:473] Iteration 7710, lr = 0.0001
I0614 16:11:23.485400  5063 solver.cpp:213] Iteration 7720, loss = 3.15796
I0614 16:11:23.485419  5063 solver.cpp:228]     Train net output #0: softmax = 3.15796 (* 1 = 3.15796 loss)
I0614 16:11:23.485424  5063 solver.cpp:473] Iteration 7720, lr = 0.0001
I0614 16:11:24.368594  5063 solver.cpp:213] Iteration 7730, loss = 3.63455
I0614 16:11:24.368610  5063 solver.cpp:228]     Train net output #0: softmax = 3.63455 (* 1 = 3.63455 loss)
I0614 16:11:24.368615  5063 solver.cpp:473] Iteration 7730, lr = 0.0001
I0614 16:11:25.250838  5063 solver.cpp:213] Iteration 7740, loss = 3.54857
I0614 16:11:25.250857  5063 solver.cpp:228]     Train net output #0: softmax = 3.54857 (* 1 = 3.54857 loss)
I0614 16:11:25.250982  5063 solver.cpp:473] Iteration 7740, lr = 0.0001
I0614 16:11:26.134093  5063 solver.cpp:213] Iteration 7750, loss = 3.38603
I0614 16:11:26.134114  5063 solver.cpp:228]     Train net output #0: softmax = 3.38603 (* 1 = 3.38603 loss)
I0614 16:11:26.134119  5063 solver.cpp:473] Iteration 7750, lr = 0.0001
I0614 16:11:27.016523  5063 solver.cpp:213] Iteration 7760, loss = 3.50905
I0614 16:11:27.016540  5063 solver.cpp:228]     Train net output #0: softmax = 3.50905 (* 1 = 3.50905 loss)
I0614 16:11:27.016543  5063 solver.cpp:473] Iteration 7760, lr = 0.0001
I0614 16:11:27.899374  5063 solver.cpp:213] Iteration 7770, loss = 3.29745
I0614 16:11:27.899391  5063 solver.cpp:228]     Train net output #0: softmax = 3.29745 (* 1 = 3.29745 loss)
I0614 16:11:27.899396  5063 solver.cpp:473] Iteration 7770, lr = 0.0001
I0614 16:11:28.781955  5063 solver.cpp:213] Iteration 7780, loss = 3.45872
I0614 16:11:28.781970  5063 solver.cpp:228]     Train net output #0: softmax = 3.45872 (* 1 = 3.45872 loss)
I0614 16:11:28.781975  5063 solver.cpp:473] Iteration 7780, lr = 0.0001
I0614 16:11:29.664765  5063 solver.cpp:213] Iteration 7790, loss = 3.52865
I0614 16:11:29.664779  5063 solver.cpp:228]     Train net output #0: softmax = 3.52865 (* 1 = 3.52865 loss)
I0614 16:11:29.664784  5063 solver.cpp:473] Iteration 7790, lr = 0.0001
I0614 16:11:30.547945  5063 solver.cpp:213] Iteration 7800, loss = 3.47772
I0614 16:11:30.547965  5063 solver.cpp:228]     Train net output #0: softmax = 3.47772 (* 1 = 3.47772 loss)
I0614 16:11:30.547986  5063 solver.cpp:473] Iteration 7800, lr = 0.0001
I0614 16:11:31.431401  5063 solver.cpp:213] Iteration 7810, loss = 3.53141
I0614 16:11:31.431424  5063 solver.cpp:228]     Train net output #0: softmax = 3.53141 (* 1 = 3.53141 loss)
I0614 16:11:31.431429  5063 solver.cpp:473] Iteration 7810, lr = 0.0001
I0614 16:11:32.315105  5063 solver.cpp:213] Iteration 7820, loss = 3.42997
I0614 16:11:32.315140  5063 solver.cpp:228]     Train net output #0: softmax = 3.42997 (* 1 = 3.42997 loss)
I0614 16:11:32.315146  5063 solver.cpp:473] Iteration 7820, lr = 0.0001
I0614 16:11:33.198107  5063 solver.cpp:213] Iteration 7830, loss = 3.21976
I0614 16:11:33.198125  5063 solver.cpp:228]     Train net output #0: softmax = 3.21976 (* 1 = 3.21976 loss)
I0614 16:11:33.198130  5063 solver.cpp:473] Iteration 7830, lr = 0.0001
I0614 16:11:34.080847  5063 solver.cpp:213] Iteration 7840, loss = 3.55169
I0614 16:11:34.080864  5063 solver.cpp:228]     Train net output #0: softmax = 3.55169 (* 1 = 3.55169 loss)
I0614 16:11:34.080869  5063 solver.cpp:473] Iteration 7840, lr = 0.0001
I0614 16:11:34.964592  5063 solver.cpp:213] Iteration 7850, loss = 3.41894
I0614 16:11:34.964612  5063 solver.cpp:228]     Train net output #0: softmax = 3.41894 (* 1 = 3.41894 loss)
I0614 16:11:34.964617  5063 solver.cpp:473] Iteration 7850, lr = 0.0001
I0614 16:11:35.847702  5063 solver.cpp:213] Iteration 7860, loss = 3.31535
I0614 16:11:35.847725  5063 solver.cpp:228]     Train net output #0: softmax = 3.31535 (* 1 = 3.31535 loss)
I0614 16:11:35.847887  5063 solver.cpp:473] Iteration 7860, lr = 0.0001
I0614 16:11:36.730980  5063 solver.cpp:213] Iteration 7870, loss = 3.53134
I0614 16:11:36.731006  5063 solver.cpp:228]     Train net output #0: softmax = 3.53134 (* 1 = 3.53134 loss)
I0614 16:11:36.731011  5063 solver.cpp:473] Iteration 7870, lr = 0.0001
I0614 16:11:37.613610  5063 solver.cpp:213] Iteration 7880, loss = 3.45555
I0614 16:11:37.613626  5063 solver.cpp:228]     Train net output #0: softmax = 3.45555 (* 1 = 3.45555 loss)
I0614 16:11:37.613631  5063 solver.cpp:473] Iteration 7880, lr = 0.0001
I0614 16:11:38.496778  5063 solver.cpp:213] Iteration 7890, loss = 3.55728
I0614 16:11:38.496799  5063 solver.cpp:228]     Train net output #0: softmax = 3.55728 (* 1 = 3.55728 loss)
I0614 16:11:38.496804  5063 solver.cpp:473] Iteration 7890, lr = 0.0001
I0614 16:11:39.379415  5063 solver.cpp:213] Iteration 7900, loss = 3.2231
I0614 16:11:39.379433  5063 solver.cpp:228]     Train net output #0: softmax = 3.2231 (* 1 = 3.2231 loss)
I0614 16:11:39.379438  5063 solver.cpp:473] Iteration 7900, lr = 0.0001
I0614 16:11:40.262028  5063 solver.cpp:213] Iteration 7910, loss = 3.44032
I0614 16:11:40.262044  5063 solver.cpp:228]     Train net output #0: softmax = 3.44032 (* 1 = 3.44032 loss)
I0614 16:11:40.262049  5063 solver.cpp:473] Iteration 7910, lr = 0.0001
I0614 16:11:41.145112  5063 solver.cpp:213] Iteration 7920, loss = 3.26495
I0614 16:11:41.145133  5063 solver.cpp:228]     Train net output #0: softmax = 3.26495 (* 1 = 3.26495 loss)
I0614 16:11:41.145143  5063 solver.cpp:473] Iteration 7920, lr = 0.0001
I0614 16:11:42.027356  5063 solver.cpp:213] Iteration 7930, loss = 3.70761
I0614 16:11:42.027379  5063 solver.cpp:228]     Train net output #0: softmax = 3.70761 (* 1 = 3.70761 loss)
I0614 16:11:42.027384  5063 solver.cpp:473] Iteration 7930, lr = 0.0001
I0614 16:11:42.911298  5063 solver.cpp:213] Iteration 7940, loss = 3.31814
I0614 16:11:42.911320  5063 solver.cpp:228]     Train net output #0: softmax = 3.31814 (* 1 = 3.31814 loss)
I0614 16:11:42.911325  5063 solver.cpp:473] Iteration 7940, lr = 0.0001
I0614 16:11:43.794095  5063 solver.cpp:213] Iteration 7950, loss = 3.36545
I0614 16:11:43.794113  5063 solver.cpp:228]     Train net output #0: softmax = 3.36545 (* 1 = 3.36545 loss)
I0614 16:11:43.794118  5063 solver.cpp:473] Iteration 7950, lr = 0.0001
I0614 16:11:44.676883  5063 solver.cpp:213] Iteration 7960, loss = 3.45572
I0614 16:11:44.676903  5063 solver.cpp:228]     Train net output #0: softmax = 3.45572 (* 1 = 3.45572 loss)
I0614 16:11:44.676914  5063 solver.cpp:473] Iteration 7960, lr = 0.0001
I0614 16:11:45.559346  5063 solver.cpp:213] Iteration 7970, loss = 3.33002
I0614 16:11:45.559363  5063 solver.cpp:228]     Train net output #0: softmax = 3.33002 (* 1 = 3.33002 loss)
I0614 16:11:45.559368  5063 solver.cpp:473] Iteration 7970, lr = 0.0001
I0614 16:11:46.442798  5063 solver.cpp:213] Iteration 7980, loss = 3.39502
I0614 16:11:46.442817  5063 solver.cpp:228]     Train net output #0: softmax = 3.39502 (* 1 = 3.39502 loss)
I0614 16:11:46.442836  5063 solver.cpp:473] Iteration 7980, lr = 0.0001
I0614 16:11:47.325932  5063 solver.cpp:213] Iteration 7990, loss = 3.45278
I0614 16:11:47.325955  5063 solver.cpp:228]     Train net output #0: softmax = 3.45278 (* 1 = 3.45278 loss)
I0614 16:11:47.325960  5063 solver.cpp:473] Iteration 7990, lr = 0.0001
I0614 16:11:48.151394  5063 solver.cpp:362] Snapshotting to snapshots/16-06-14_15h51m21s_0_11_pretrainClassification_iter_8000.caffemodel
I0614 16:11:48.152179  5063 solver.cpp:370] Snapshotting solver state to snapshots/16-06-14_15h51m21s_0_11_pretrainClassification_iter_8000.solverstate
I0614 16:11:48.152627  5063 solver.cpp:291] Iteration 8000, Testing net (#0)
I0614 16:11:48.265394  5063 solver.cpp:342]     Test net output #0: accuracy_top_1 = 0.221875
I0614 16:11:48.265410  5063 solver.cpp:342]     Test net output #1: accuracy_top_5 = 0.479688
I0614 16:11:48.265416  5063 solver.cpp:342]     Test net output #2: softmax = 3.34568 (* 1 = 3.34568 loss)
I0614 16:11:48.323622  5063 solver.cpp:213] Iteration 8000, loss = 3.35704
I0614 16:11:48.323635  5063 solver.cpp:228]     Train net output #0: softmax = 3.35704 (* 1 = 3.35704 loss)
I0614 16:11:48.323640  5063 solver.cpp:473] Iteration 8000, lr = 0.0001
I0614 16:11:49.205977  5063 solver.cpp:213] Iteration 8010, loss = 3.18502
I0614 16:11:49.205993  5063 solver.cpp:228]     Train net output #0: softmax = 3.18502 (* 1 = 3.18502 loss)
I0614 16:11:49.205998  5063 solver.cpp:473] Iteration 8010, lr = 0.0001
I0614 16:11:50.089030  5063 solver.cpp:213] Iteration 8020, loss = 3.21415
I0614 16:11:50.089051  5063 solver.cpp:228]     Train net output #0: softmax = 3.21415 (* 1 = 3.21415 loss)
I0614 16:11:50.089056  5063 solver.cpp:473] Iteration 8020, lr = 0.0001
I0614 16:11:50.971202  5063 solver.cpp:213] Iteration 8030, loss = 3.31629
I0614 16:11:50.971221  5063 solver.cpp:228]     Train net output #0: softmax = 3.31629 (* 1 = 3.31629 loss)
I0614 16:11:50.971226  5063 solver.cpp:473] Iteration 8030, lr = 0.0001
I0614 16:11:51.854145  5063 solver.cpp:213] Iteration 8040, loss = 3.64874
I0614 16:11:51.854167  5063 solver.cpp:228]     Train net output #0: softmax = 3.64874 (* 1 = 3.64874 loss)
I0614 16:11:51.854370  5063 solver.cpp:473] Iteration 8040, lr = 0.0001
I0614 16:11:52.737179  5063 solver.cpp:213] Iteration 8050, loss = 3.24066
I0614 16:11:52.737210  5063 solver.cpp:228]     Train net output #0: softmax = 3.24066 (* 1 = 3.24066 loss)
I0614 16:11:52.737215  5063 solver.cpp:473] Iteration 8050, lr = 0.0001
I0614 16:11:53.620487  5063 solver.cpp:213] Iteration 8060, loss = 3.11008
I0614 16:11:53.620506  5063 solver.cpp:228]     Train net output #0: softmax = 3.11008 (* 1 = 3.11008 loss)
I0614 16:11:53.620510  5063 solver.cpp:473] Iteration 8060, lr = 0.0001
I0614 16:11:54.504618  5063 solver.cpp:213] Iteration 8070, loss = 3.50228
I0614 16:11:54.504633  5063 solver.cpp:228]     Train net output #0: softmax = 3.50228 (* 1 = 3.50228 loss)
I0614 16:11:54.504638  5063 solver.cpp:473] Iteration 8070, lr = 0.0001
I0614 16:11:55.388509  5063 solver.cpp:213] Iteration 8080, loss = 3.5746
I0614 16:11:55.388523  5063 solver.cpp:228]     Train net output #0: softmax = 3.5746 (* 1 = 3.5746 loss)
I0614 16:11:55.388527  5063 solver.cpp:473] Iteration 8080, lr = 0.0001
I0614 16:11:56.272351  5063 solver.cpp:213] Iteration 8090, loss = 3.35323
I0614 16:11:56.272367  5063 solver.cpp:228]     Train net output #0: softmax = 3.35323 (* 1 = 3.35323 loss)
I0614 16:11:56.272372  5063 solver.cpp:473] Iteration 8090, lr = 0.0001
I0614 16:11:57.154650  5063 solver.cpp:213] Iteration 8100, loss = 3.40535
I0614 16:11:57.154670  5063 solver.cpp:228]     Train net output #0: softmax = 3.40535 (* 1 = 3.40535 loss)
I0614 16:11:57.154680  5063 solver.cpp:473] Iteration 8100, lr = 0.0001
I0614 16:11:58.038095  5063 solver.cpp:213] Iteration 8110, loss = 3.47207
I0614 16:11:58.038120  5063 solver.cpp:228]     Train net output #0: softmax = 3.47207 (* 1 = 3.47207 loss)
I0614 16:11:58.038125  5063 solver.cpp:473] Iteration 8110, lr = 0.0001
I0614 16:11:58.921314  5063 solver.cpp:213] Iteration 8120, loss = 3.45159
I0614 16:11:58.921344  5063 solver.cpp:228]     Train net output #0: softmax = 3.45159 (* 1 = 3.45159 loss)
I0614 16:11:58.921350  5063 solver.cpp:473] Iteration 8120, lr = 0.0001
I0614 16:11:59.803833  5063 solver.cpp:213] Iteration 8130, loss = 3.23723
I0614 16:11:59.803849  5063 solver.cpp:228]     Train net output #0: softmax = 3.23723 (* 1 = 3.23723 loss)
I0614 16:11:59.803854  5063 solver.cpp:473] Iteration 8130, lr = 0.0001
I0614 16:12:00.688181  5063 solver.cpp:213] Iteration 8140, loss = 3.50233
I0614 16:12:00.688201  5063 solver.cpp:228]     Train net output #0: softmax = 3.50233 (* 1 = 3.50233 loss)
I0614 16:12:00.688206  5063 solver.cpp:473] Iteration 8140, lr = 0.0001
I0614 16:12:01.571254  5063 solver.cpp:213] Iteration 8150, loss = 3.3989
I0614 16:12:01.571271  5063 solver.cpp:228]     Train net output #0: softmax = 3.3989 (* 1 = 3.3989 loss)
I0614 16:12:01.571276  5063 solver.cpp:473] Iteration 8150, lr = 0.0001
I0614 16:12:02.454077  5063 solver.cpp:213] Iteration 8160, loss = 3.46026
I0614 16:12:02.454118  5063 solver.cpp:228]     Train net output #0: softmax = 3.46026 (* 1 = 3.46026 loss)
I0614 16:12:02.454123  5063 solver.cpp:473] Iteration 8160, lr = 0.0001
I0614 16:12:03.336858  5063 solver.cpp:213] Iteration 8170, loss = 3.50654
I0614 16:12:03.336874  5063 solver.cpp:228]     Train net output #0: softmax = 3.50654 (* 1 = 3.50654 loss)
I0614 16:12:03.336879  5063 solver.cpp:473] Iteration 8170, lr = 0.0001
I0614 16:12:04.219766  5063 solver.cpp:213] Iteration 8180, loss = 3.45437
I0614 16:12:04.219784  5063 solver.cpp:228]     Train net output #0: softmax = 3.45437 (* 1 = 3.45437 loss)
I0614 16:12:04.219789  5063 solver.cpp:473] Iteration 8180, lr = 0.0001
I0614 16:12:05.103987  5063 solver.cpp:213] Iteration 8190, loss = 3.62639
I0614 16:12:05.104007  5063 solver.cpp:228]     Train net output #0: softmax = 3.62639 (* 1 = 3.62639 loss)
I0614 16:12:05.104012  5063 solver.cpp:473] Iteration 8190, lr = 0.0001
I0614 16:12:05.987337  5063 solver.cpp:213] Iteration 8200, loss = 3.46125
I0614 16:12:05.987356  5063 solver.cpp:228]     Train net output #0: softmax = 3.46125 (* 1 = 3.46125 loss)
I0614 16:12:05.987361  5063 solver.cpp:473] Iteration 8200, lr = 0.0001
I0614 16:12:06.871084  5063 solver.cpp:213] Iteration 8210, loss = 3.35524
I0614 16:12:06.871104  5063 solver.cpp:228]     Train net output #0: softmax = 3.35524 (* 1 = 3.35524 loss)
I0614 16:12:06.871109  5063 solver.cpp:473] Iteration 8210, lr = 0.0001
I0614 16:12:07.753506  5063 solver.cpp:213] Iteration 8220, loss = 3.32767
I0614 16:12:07.753535  5063 solver.cpp:228]     Train net output #0: softmax = 3.32767 (* 1 = 3.32767 loss)
I0614 16:12:07.753546  5063 solver.cpp:473] Iteration 8220, lr = 0.0001
I0614 16:12:08.636538  5063 solver.cpp:213] Iteration 8230, loss = 3.43318
I0614 16:12:08.636557  5063 solver.cpp:228]     Train net output #0: softmax = 3.43318 (* 1 = 3.43318 loss)
I0614 16:12:08.636562  5063 solver.cpp:473] Iteration 8230, lr = 0.0001
I0614 16:12:09.519942  5063 solver.cpp:213] Iteration 8240, loss = 3.41678
I0614 16:12:09.519958  5063 solver.cpp:228]     Train net output #0: softmax = 3.41678 (* 1 = 3.41678 loss)
I0614 16:12:09.519963  5063 solver.cpp:473] Iteration 8240, lr = 0.0001
I0614 16:12:10.403655  5063 solver.cpp:213] Iteration 8250, loss = 3.319
I0614 16:12:10.403671  5063 solver.cpp:228]     Train net output #0: softmax = 3.319 (* 1 = 3.319 loss)
I0614 16:12:10.403677  5063 solver.cpp:473] Iteration 8250, lr = 0.0001
I0614 16:12:11.286643  5063 solver.cpp:213] Iteration 8260, loss = 3.65057
I0614 16:12:11.286669  5063 solver.cpp:228]     Train net output #0: softmax = 3.65057 (* 1 = 3.65057 loss)
I0614 16:12:11.286674  5063 solver.cpp:473] Iteration 8260, lr = 0.0001
I0614 16:12:12.170368  5063 solver.cpp:213] Iteration 8270, loss = 3.52599
I0614 16:12:12.170392  5063 solver.cpp:228]     Train net output #0: softmax = 3.52599 (* 1 = 3.52599 loss)
I0614 16:12:12.170397  5063 solver.cpp:473] Iteration 8270, lr = 0.0001
I0614 16:12:13.053365  5063 solver.cpp:213] Iteration 8280, loss = 3.41004
I0614 16:12:13.053393  5063 solver.cpp:228]     Train net output #0: softmax = 3.41004 (* 1 = 3.41004 loss)
I0614 16:12:13.053530  5063 solver.cpp:473] Iteration 8280, lr = 0.0001
I0614 16:12:13.936380  5063 solver.cpp:213] Iteration 8290, loss = 3.25619
I0614 16:12:13.936396  5063 solver.cpp:228]     Train net output #0: softmax = 3.25619 (* 1 = 3.25619 loss)
I0614 16:12:13.936401  5063 solver.cpp:473] Iteration 8290, lr = 0.0001
I0614 16:12:14.819700  5063 solver.cpp:213] Iteration 8300, loss = 3.74007
I0614 16:12:14.819720  5063 solver.cpp:228]     Train net output #0: softmax = 3.74007 (* 1 = 3.74007 loss)
I0614 16:12:14.819725  5063 solver.cpp:473] Iteration 8300, lr = 0.0001
I0614 16:12:15.702546  5063 solver.cpp:213] Iteration 8310, loss = 3.55449
I0614 16:12:15.702564  5063 solver.cpp:228]     Train net output #0: softmax = 3.55449 (* 1 = 3.55449 loss)
I0614 16:12:15.702567  5063 solver.cpp:473] Iteration 8310, lr = 0.0001
I0614 16:12:16.585366  5063 solver.cpp:213] Iteration 8320, loss = 3.60737
I0614 16:12:16.585383  5063 solver.cpp:228]     Train net output #0: softmax = 3.60737 (* 1 = 3.60737 loss)
I0614 16:12:16.585404  5063 solver.cpp:473] Iteration 8320, lr = 0.0001
I0614 16:12:17.468566  5063 solver.cpp:213] Iteration 8330, loss = 3.46939
I0614 16:12:17.468588  5063 solver.cpp:228]     Train net output #0: softmax = 3.46939 (* 1 = 3.46939 loss)
I0614 16:12:17.468593  5063 solver.cpp:473] Iteration 8330, lr = 0.0001
I0614 16:12:18.351267  5063 solver.cpp:213] Iteration 8340, loss = 3.12077
I0614 16:12:18.351289  5063 solver.cpp:228]     Train net output #0: softmax = 3.12077 (* 1 = 3.12077 loss)
I0614 16:12:18.351424  5063 solver.cpp:473] Iteration 8340, lr = 0.0001
I0614 16:12:19.235388  5063 solver.cpp:213] Iteration 8350, loss = 3.34491
I0614 16:12:19.235406  5063 solver.cpp:228]     Train net output #0: softmax = 3.34491 (* 1 = 3.34491 loss)
I0614 16:12:19.235410  5063 solver.cpp:473] Iteration 8350, lr = 0.0001
I0614 16:12:20.117576  5063 solver.cpp:213] Iteration 8360, loss = 3.2198
I0614 16:12:20.117594  5063 solver.cpp:228]     Train net output #0: softmax = 3.2198 (* 1 = 3.2198 loss)
I0614 16:12:20.117597  5063 solver.cpp:473] Iteration 8360, lr = 0.0001
I0614 16:12:21.001103  5063 solver.cpp:213] Iteration 8370, loss = 3.38705
I0614 16:12:21.001119  5063 solver.cpp:228]     Train net output #0: softmax = 3.38705 (* 1 = 3.38705 loss)
I0614 16:12:21.001124  5063 solver.cpp:473] Iteration 8370, lr = 0.0001
I0614 16:12:21.883949  5063 solver.cpp:213] Iteration 8380, loss = 3.47603
I0614 16:12:21.883968  5063 solver.cpp:228]     Train net output #0: softmax = 3.47603 (* 1 = 3.47603 loss)
I0614 16:12:21.883972  5063 solver.cpp:473] Iteration 8380, lr = 0.0001
I0614 16:12:22.766796  5063 solver.cpp:213] Iteration 8390, loss = 3.38791
I0614 16:12:22.766819  5063 solver.cpp:228]     Train net output #0: softmax = 3.38791 (* 1 = 3.38791 loss)
I0614 16:12:22.766824  5063 solver.cpp:473] Iteration 8390, lr = 0.0001
I0614 16:12:23.650540  5063 solver.cpp:213] Iteration 8400, loss = 3.13906
I0614 16:12:23.650563  5063 solver.cpp:228]     Train net output #0: softmax = 3.13906 (* 1 = 3.13906 loss)
I0614 16:12:23.650723  5063 solver.cpp:473] Iteration 8400, lr = 0.0001
I0614 16:12:24.533820  5063 solver.cpp:213] Iteration 8410, loss = 3.04171
I0614 16:12:24.533839  5063 solver.cpp:228]     Train net output #0: softmax = 3.04171 (* 1 = 3.04171 loss)
I0614 16:12:24.533844  5063 solver.cpp:473] Iteration 8410, lr = 0.0001
I0614 16:12:25.417292  5063 solver.cpp:213] Iteration 8420, loss = 3.25239
I0614 16:12:25.417317  5063 solver.cpp:228]     Train net output #0: softmax = 3.25239 (* 1 = 3.25239 loss)
I0614 16:12:25.417322  5063 solver.cpp:473] Iteration 8420, lr = 0.0001
I0614 16:12:26.300457  5063 solver.cpp:213] Iteration 8430, loss = 3.56833
I0614 16:12:26.300477  5063 solver.cpp:228]     Train net output #0: softmax = 3.56833 (* 1 = 3.56833 loss)
I0614 16:12:26.300480  5063 solver.cpp:473] Iteration 8430, lr = 0.0001
I0614 16:12:27.183768  5063 solver.cpp:213] Iteration 8440, loss = 3.2888
I0614 16:12:27.183786  5063 solver.cpp:228]     Train net output #0: softmax = 3.2888 (* 1 = 3.2888 loss)
I0614 16:12:27.183791  5063 solver.cpp:473] Iteration 8440, lr = 0.0001
I0614 16:12:28.067078  5063 solver.cpp:213] Iteration 8450, loss = 3.18558
I0614 16:12:28.067100  5063 solver.cpp:228]     Train net output #0: softmax = 3.18558 (* 1 = 3.18558 loss)
I0614 16:12:28.067104  5063 solver.cpp:473] Iteration 8450, lr = 0.0001
I0614 16:12:28.950598  5063 solver.cpp:213] Iteration 8460, loss = 3.5379
I0614 16:12:28.950619  5063 solver.cpp:228]     Train net output #0: softmax = 3.5379 (* 1 = 3.5379 loss)
I0614 16:12:28.950742  5063 solver.cpp:473] Iteration 8460, lr = 0.0001
I0614 16:12:29.834331  5063 solver.cpp:213] Iteration 8470, loss = 3.41446
I0614 16:12:29.834347  5063 solver.cpp:228]     Train net output #0: softmax = 3.41446 (* 1 = 3.41446 loss)
I0614 16:12:29.834352  5063 solver.cpp:473] Iteration 8470, lr = 0.0001
I0614 16:12:30.717403  5063 solver.cpp:213] Iteration 8480, loss = 3.27153
I0614 16:12:30.717422  5063 solver.cpp:228]     Train net output #0: softmax = 3.27153 (* 1 = 3.27153 loss)
I0614 16:12:30.717443  5063 solver.cpp:473] Iteration 8480, lr = 0.0001
I0614 16:12:31.600570  5063 solver.cpp:213] Iteration 8490, loss = 3.28506
I0614 16:12:31.600589  5063 solver.cpp:228]     Train net output #0: softmax = 3.28506 (* 1 = 3.28506 loss)
I0614 16:12:31.600594  5063 solver.cpp:473] Iteration 8490, lr = 0.0001
I0614 16:12:32.483912  5063 solver.cpp:213] Iteration 8500, loss = 3.37594
I0614 16:12:32.483953  5063 solver.cpp:228]     Train net output #0: softmax = 3.37594 (* 1 = 3.37594 loss)
I0614 16:12:32.483959  5063 solver.cpp:473] Iteration 8500, lr = 0.0001
I0614 16:12:33.367158  5063 solver.cpp:213] Iteration 8510, loss = 3.60348
I0614 16:12:33.367175  5063 solver.cpp:228]     Train net output #0: softmax = 3.60348 (* 1 = 3.60348 loss)
I0614 16:12:33.367180  5063 solver.cpp:473] Iteration 8510, lr = 0.0001
I0614 16:12:34.250351  5063 solver.cpp:213] Iteration 8520, loss = 3.49455
I0614 16:12:34.250370  5063 solver.cpp:228]     Train net output #0: softmax = 3.49455 (* 1 = 3.49455 loss)
I0614 16:12:34.250380  5063 solver.cpp:473] Iteration 8520, lr = 0.0001
I0614 16:12:35.133600  5063 solver.cpp:213] Iteration 8530, loss = 3.37268
I0614 16:12:35.133620  5063 solver.cpp:228]     Train net output #0: softmax = 3.37268 (* 1 = 3.37268 loss)
I0614 16:12:35.133625  5063 solver.cpp:473] Iteration 8530, lr = 0.0001
I0614 16:12:36.016244  5063 solver.cpp:213] Iteration 8540, loss = 3.31927
I0614 16:12:36.016263  5063 solver.cpp:228]     Train net output #0: softmax = 3.31927 (* 1 = 3.31927 loss)
I0614 16:12:36.016268  5063 solver.cpp:473] Iteration 8540, lr = 0.0001
I0614 16:12:36.899853  5063 solver.cpp:213] Iteration 8550, loss = 3.34408
I0614 16:12:36.899873  5063 solver.cpp:228]     Train net output #0: softmax = 3.34408 (* 1 = 3.34408 loss)
I0614 16:12:36.899878  5063 solver.cpp:473] Iteration 8550, lr = 0.0001
I0614 16:12:37.783255  5063 solver.cpp:213] Iteration 8560, loss = 3.40802
I0614 16:12:37.783277  5063 solver.cpp:228]     Train net output #0: softmax = 3.40802 (* 1 = 3.40802 loss)
I0614 16:12:37.783282  5063 solver.cpp:473] Iteration 8560, lr = 0.0001
I0614 16:12:38.666159  5063 solver.cpp:213] Iteration 8570, loss = 3.24081
I0614 16:12:38.666175  5063 solver.cpp:228]     Train net output #0: softmax = 3.24081 (* 1 = 3.24081 loss)
I0614 16:12:38.666180  5063 solver.cpp:473] Iteration 8570, lr = 0.0001
I0614 16:12:39.549026  5063 solver.cpp:213] Iteration 8580, loss = 3.56335
I0614 16:12:39.549052  5063 solver.cpp:228]     Train net output #0: softmax = 3.56335 (* 1 = 3.56335 loss)
I0614 16:12:39.549057  5063 solver.cpp:473] Iteration 8580, lr = 0.0001
I0614 16:12:40.430989  5063 solver.cpp:213] Iteration 8590, loss = 3.34281
I0614 16:12:40.431008  5063 solver.cpp:228]     Train net output #0: softmax = 3.34281 (* 1 = 3.34281 loss)
I0614 16:12:40.431013  5063 solver.cpp:473] Iteration 8590, lr = 0.0001
I0614 16:12:41.314198  5063 solver.cpp:213] Iteration 8600, loss = 3.36625
I0614 16:12:41.314213  5063 solver.cpp:228]     Train net output #0: softmax = 3.36625 (* 1 = 3.36625 loss)
I0614 16:12:41.314218  5063 solver.cpp:473] Iteration 8600, lr = 0.0001
I0614 16:12:42.197150  5063 solver.cpp:213] Iteration 8610, loss = 3.31433
I0614 16:12:42.197168  5063 solver.cpp:228]     Train net output #0: softmax = 3.31433 (* 1 = 3.31433 loss)
I0614 16:12:42.197173  5063 solver.cpp:473] Iteration 8610, lr = 0.0001
I0614 16:12:43.080498  5063 solver.cpp:213] Iteration 8620, loss = 3.32324
I0614 16:12:43.080521  5063 solver.cpp:228]     Train net output #0: softmax = 3.32324 (* 1 = 3.32324 loss)
I0614 16:12:43.080526  5063 solver.cpp:473] Iteration 8620, lr = 0.0001
I0614 16:12:43.963140  5063 solver.cpp:213] Iteration 8630, loss = 3.49767
I0614 16:12:43.963157  5063 solver.cpp:228]     Train net output #0: softmax = 3.49767 (* 1 = 3.49767 loss)
I0614 16:12:43.963162  5063 solver.cpp:473] Iteration 8630, lr = 0.0001
I0614 16:12:44.846194  5063 solver.cpp:213] Iteration 8640, loss = 3.3875
I0614 16:12:44.846215  5063 solver.cpp:228]     Train net output #0: softmax = 3.3875 (* 1 = 3.3875 loss)
I0614 16:12:44.846374  5063 solver.cpp:473] Iteration 8640, lr = 0.0001
I0614 16:12:45.729363  5063 solver.cpp:213] Iteration 8650, loss = 3.43956
I0614 16:12:45.729382  5063 solver.cpp:228]     Train net output #0: softmax = 3.43956 (* 1 = 3.43956 loss)
I0614 16:12:45.729387  5063 solver.cpp:473] Iteration 8650, lr = 0.0001
I0614 16:12:46.613195  5063 solver.cpp:213] Iteration 8660, loss = 3.41978
I0614 16:12:46.613214  5063 solver.cpp:228]     Train net output #0: softmax = 3.41978 (* 1 = 3.41978 loss)
I0614 16:12:46.613234  5063 solver.cpp:473] Iteration 8660, lr = 0.0001
I0614 16:12:47.496961  5063 solver.cpp:213] Iteration 8670, loss = 3.51257
I0614 16:12:47.496979  5063 solver.cpp:228]     Train net output #0: softmax = 3.51257 (* 1 = 3.51257 loss)
I0614 16:12:47.496984  5063 solver.cpp:473] Iteration 8670, lr = 0.0001
I0614 16:12:48.379493  5063 solver.cpp:213] Iteration 8680, loss = 3.14575
I0614 16:12:48.379515  5063 solver.cpp:228]     Train net output #0: softmax = 3.14575 (* 1 = 3.14575 loss)
I0614 16:12:48.379520  5063 solver.cpp:473] Iteration 8680, lr = 0.0001
I0614 16:12:49.262135  5063 solver.cpp:213] Iteration 8690, loss = 3.48989
I0614 16:12:49.262151  5063 solver.cpp:228]     Train net output #0: softmax = 3.48989 (* 1 = 3.48989 loss)
I0614 16:12:49.262156  5063 solver.cpp:473] Iteration 8690, lr = 0.0001
I0614 16:12:50.145524  5063 solver.cpp:213] Iteration 8700, loss = 3.54527
I0614 16:12:50.145546  5063 solver.cpp:228]     Train net output #0: softmax = 3.54527 (* 1 = 3.54527 loss)
I0614 16:12:50.145557  5063 solver.cpp:473] Iteration 8700, lr = 0.0001
I0614 16:12:51.029219  5063 solver.cpp:213] Iteration 8710, loss = 3.45458
I0614 16:12:51.029240  5063 solver.cpp:228]     Train net output #0: softmax = 3.45458 (* 1 = 3.45458 loss)
I0614 16:12:51.029245  5063 solver.cpp:473] Iteration 8710, lr = 0.0001
I0614 16:12:51.912091  5063 solver.cpp:213] Iteration 8720, loss = 3.52445
I0614 16:12:51.912109  5063 solver.cpp:228]     Train net output #0: softmax = 3.52445 (* 1 = 3.52445 loss)
I0614 16:12:51.912113  5063 solver.cpp:473] Iteration 8720, lr = 0.0001
I0614 16:12:52.795343  5063 solver.cpp:213] Iteration 8730, loss = 3.20637
I0614 16:12:52.795366  5063 solver.cpp:228]     Train net output #0: softmax = 3.20637 (* 1 = 3.20637 loss)
I0614 16:12:52.795370  5063 solver.cpp:473] Iteration 8730, lr = 0.0001
I0614 16:12:53.678884  5063 solver.cpp:213] Iteration 8740, loss = 3.41889
I0614 16:12:53.678911  5063 solver.cpp:228]     Train net output #0: softmax = 3.41889 (* 1 = 3.41889 loss)
I0614 16:12:53.678916  5063 solver.cpp:473] Iteration 8740, lr = 0.0001
I0614 16:12:54.562443  5063 solver.cpp:213] Iteration 8750, loss = 3.37352
I0614 16:12:54.562460  5063 solver.cpp:228]     Train net output #0: softmax = 3.37352 (* 1 = 3.37352 loss)
I0614 16:12:54.562465  5063 solver.cpp:473] Iteration 8750, lr = 0.0001
I0614 16:12:55.445485  5063 solver.cpp:213] Iteration 8760, loss = 3.44266
I0614 16:12:55.445504  5063 solver.cpp:228]     Train net output #0: softmax = 3.44266 (* 1 = 3.44266 loss)
I0614 16:12:55.445508  5063 solver.cpp:473] Iteration 8760, lr = 0.0001
I0614 16:12:56.328524  5063 solver.cpp:213] Iteration 8770, loss = 3.38551
I0614 16:12:56.328542  5063 solver.cpp:228]     Train net output #0: softmax = 3.38551 (* 1 = 3.38551 loss)
I0614 16:12:56.328547  5063 solver.cpp:473] Iteration 8770, lr = 0.0001
I0614 16:12:57.211432  5063 solver.cpp:213] Iteration 8780, loss = 3.32763
I0614 16:12:57.211449  5063 solver.cpp:228]     Train net output #0: softmax = 3.32763 (* 1 = 3.32763 loss)
I0614 16:12:57.211454  5063 solver.cpp:473] Iteration 8780, lr = 0.0001
I0614 16:12:58.094230  5063 solver.cpp:213] Iteration 8790, loss = 3.30418
I0614 16:12:58.094254  5063 solver.cpp:228]     Train net output #0: softmax = 3.30418 (* 1 = 3.30418 loss)
I0614 16:12:58.094259  5063 solver.cpp:473] Iteration 8790, lr = 0.0001
I0614 16:12:58.978170  5063 solver.cpp:213] Iteration 8800, loss = 3.44615
I0614 16:12:58.978191  5063 solver.cpp:228]     Train net output #0: softmax = 3.44615 (* 1 = 3.44615 loss)
I0614 16:12:58.978196  5063 solver.cpp:473] Iteration 8800, lr = 0.0001
I0614 16:12:59.861846  5063 solver.cpp:213] Iteration 8810, loss = 3.24907
I0614 16:12:59.861865  5063 solver.cpp:228]     Train net output #0: softmax = 3.24907 (* 1 = 3.24907 loss)
I0614 16:12:59.861871  5063 solver.cpp:473] Iteration 8810, lr = 0.0001
I0614 16:13:00.744498  5063 solver.cpp:213] Iteration 8820, loss = 3.5733
I0614 16:13:00.744515  5063 solver.cpp:228]     Train net output #0: softmax = 3.5733 (* 1 = 3.5733 loss)
I0614 16:13:00.744647  5063 solver.cpp:473] Iteration 8820, lr = 0.0001
I0614 16:13:01.628885  5063 solver.cpp:213] Iteration 8830, loss = 3.42678
I0614 16:13:01.628904  5063 solver.cpp:228]     Train net output #0: softmax = 3.42678 (* 1 = 3.42678 loss)
I0614 16:13:01.628909  5063 solver.cpp:473] Iteration 8830, lr = 0.0001
I0614 16:13:02.512975  5063 solver.cpp:213] Iteration 8840, loss = 3.30382
I0614 16:13:02.513015  5063 solver.cpp:228]     Train net output #0: softmax = 3.30382 (* 1 = 3.30382 loss)
I0614 16:13:02.513021  5063 solver.cpp:473] Iteration 8840, lr = 0.0001
I0614 16:13:03.387828  5063 solver.cpp:213] Iteration 8850, loss = 3.35039
I0614 16:13:03.387854  5063 solver.cpp:228]     Train net output #0: softmax = 3.35039 (* 1 = 3.35039 loss)
I0614 16:13:03.387861  5063 solver.cpp:473] Iteration 8850, lr = 0.0001
I0614 16:13:04.269397  5063 solver.cpp:213] Iteration 8860, loss = 3.4746
I0614 16:13:04.269417  5063 solver.cpp:228]     Train net output #0: softmax = 3.4746 (* 1 = 3.4746 loss)
I0614 16:13:04.269421  5063 solver.cpp:473] Iteration 8860, lr = 0.0001
I0614 16:13:05.152307  5063 solver.cpp:213] Iteration 8870, loss = 3.24274
I0614 16:13:05.152323  5063 solver.cpp:228]     Train net output #0: softmax = 3.24274 (* 1 = 3.24274 loss)
I0614 16:13:05.152328  5063 solver.cpp:473] Iteration 8870, lr = 0.0001
I0614 16:13:06.036075  5063 solver.cpp:213] Iteration 8880, loss = 3.27282
I0614 16:13:06.036100  5063 solver.cpp:228]     Train net output #0: softmax = 3.27282 (* 1 = 3.27282 loss)
I0614 16:13:06.036111  5063 solver.cpp:473] Iteration 8880, lr = 0.0001
I0614 16:13:06.915402  5063 solver.cpp:213] Iteration 8890, loss = 3.57743
I0614 16:13:06.915426  5063 solver.cpp:228]     Train net output #0: softmax = 3.57743 (* 1 = 3.57743 loss)
I0614 16:13:06.915431  5063 solver.cpp:473] Iteration 8890, lr = 0.0001
I0614 16:13:07.796125  5063 solver.cpp:213] Iteration 8900, loss = 3.57292
I0614 16:13:07.796144  5063 solver.cpp:228]     Train net output #0: softmax = 3.57292 (* 1 = 3.57292 loss)
I0614 16:13:07.796154  5063 solver.cpp:473] Iteration 8900, lr = 0.0001
I0614 16:13:08.679391  5063 solver.cpp:213] Iteration 8910, loss = 3.73411
I0614 16:13:08.679411  5063 solver.cpp:228]     Train net output #0: softmax = 3.73411 (* 1 = 3.73411 loss)
I0614 16:13:08.679416  5063 solver.cpp:473] Iteration 8910, lr = 0.0001
I0614 16:13:09.562845  5063 solver.cpp:213] Iteration 8920, loss = 3.33289
I0614 16:13:09.562860  5063 solver.cpp:228]     Train net output #0: softmax = 3.33289 (* 1 = 3.33289 loss)
I0614 16:13:09.562865  5063 solver.cpp:473] Iteration 8920, lr = 0.0001
I0614 16:13:10.446451  5063 solver.cpp:213] Iteration 8930, loss = 3.36791
I0614 16:13:10.446475  5063 solver.cpp:228]     Train net output #0: softmax = 3.36791 (* 1 = 3.36791 loss)
I0614 16:13:10.446480  5063 solver.cpp:473] Iteration 8930, lr = 0.0001
I0614 16:13:11.330250  5063 solver.cpp:213] Iteration 8940, loss = 3.37904
I0614 16:13:11.330270  5063 solver.cpp:228]     Train net output #0: softmax = 3.37904 (* 1 = 3.37904 loss)
I0614 16:13:11.330395  5063 solver.cpp:473] Iteration 8940, lr = 0.0001
I0614 16:13:12.213585  5063 solver.cpp:213] Iteration 8950, loss = 3.55984
I0614 16:13:12.213603  5063 solver.cpp:228]     Train net output #0: softmax = 3.55984 (* 1 = 3.55984 loss)
I0614 16:13:12.213608  5063 solver.cpp:473] Iteration 8950, lr = 0.0001
I0614 16:13:13.096876  5063 solver.cpp:213] Iteration 8960, loss = 3.32603
I0614 16:13:13.096892  5063 solver.cpp:228]     Train net output #0: softmax = 3.32603 (* 1 = 3.32603 loss)
I0614 16:13:13.096897  5063 solver.cpp:473] Iteration 8960, lr = 0.0001
I0614 16:13:13.980300  5063 solver.cpp:213] Iteration 8970, loss = 3.46678
I0614 16:13:13.980316  5063 solver.cpp:228]     Train net output #0: softmax = 3.46678 (* 1 = 3.46678 loss)
I0614 16:13:13.980321  5063 solver.cpp:473] Iteration 8970, lr = 0.0001
I0614 16:13:14.863548  5063 solver.cpp:213] Iteration 8980, loss = 3.41893
I0614 16:13:14.863571  5063 solver.cpp:228]     Train net output #0: softmax = 3.41893 (* 1 = 3.41893 loss)
I0614 16:13:14.863576  5063 solver.cpp:473] Iteration 8980, lr = 0.0001
I0614 16:13:15.747440  5063 solver.cpp:213] Iteration 8990, loss = 3.47495
I0614 16:13:15.747462  5063 solver.cpp:228]     Train net output #0: softmax = 3.47495 (* 1 = 3.47495 loss)
I0614 16:13:15.747467  5063 solver.cpp:473] Iteration 8990, lr = 0.0001
I0614 16:13:16.573855  5063 solver.cpp:362] Snapshotting to snapshots/16-06-14_15h51m21s_0_11_pretrainClassification_iter_9000.caffemodel
I0614 16:13:16.574611  5063 solver.cpp:370] Snapshotting solver state to snapshots/16-06-14_15h51m21s_0_11_pretrainClassification_iter_9000.solverstate
I0614 16:13:16.575045  5063 solver.cpp:291] Iteration 9000, Testing net (#0)
I0614 16:13:16.687741  5063 solver.cpp:342]     Test net output #0: accuracy_top_1 = 0.210938
I0614 16:13:16.687758  5063 solver.cpp:342]     Test net output #1: accuracy_top_5 = 0.459375
I0614 16:13:16.687764  5063 solver.cpp:342]     Test net output #2: softmax = 3.36676 (* 1 = 3.36676 loss)
I0614 16:13:16.745916  5063 solver.cpp:213] Iteration 9000, loss = 3.34228
I0614 16:13:16.745930  5063 solver.cpp:228]     Train net output #0: softmax = 3.34228 (* 1 = 3.34228 loss)
I0614 16:13:16.745934  5063 solver.cpp:473] Iteration 9000, lr = 0.0001
I0614 16:13:17.629859  5063 solver.cpp:213] Iteration 9010, loss = 3.45389
I0614 16:13:17.629879  5063 solver.cpp:228]     Train net output #0: softmax = 3.45389 (* 1 = 3.45389 loss)
I0614 16:13:17.629885  5063 solver.cpp:473] Iteration 9010, lr = 0.0001
I0614 16:13:18.513864  5063 solver.cpp:213] Iteration 9020, loss = 3.32472
I0614 16:13:18.513885  5063 solver.cpp:228]     Train net output #0: softmax = 3.32472 (* 1 = 3.32472 loss)
I0614 16:13:18.513891  5063 solver.cpp:473] Iteration 9020, lr = 0.0001
I0614 16:13:19.398048  5063 solver.cpp:213] Iteration 9030, loss = 3.21679
I0614 16:13:19.398069  5063 solver.cpp:228]     Train net output #0: softmax = 3.21679 (* 1 = 3.21679 loss)
I0614 16:13:19.398074  5063 solver.cpp:473] Iteration 9030, lr = 0.0001
I0614 16:13:20.282238  5063 solver.cpp:213] Iteration 9040, loss = 3.2692
I0614 16:13:20.282259  5063 solver.cpp:228]     Train net output #0: softmax = 3.2692 (* 1 = 3.2692 loss)
I0614 16:13:20.282264  5063 solver.cpp:473] Iteration 9040, lr = 0.0001
I0614 16:13:21.165947  5063 solver.cpp:213] Iteration 9050, loss = 3.32957
I0614 16:13:21.165972  5063 solver.cpp:228]     Train net output #0: softmax = 3.32957 (* 1 = 3.32957 loss)
I0614 16:13:21.165977  5063 solver.cpp:473] Iteration 9050, lr = 0.0001
I0614 16:13:22.048781  5063 solver.cpp:213] Iteration 9060, loss = 3.40801
I0614 16:13:22.048799  5063 solver.cpp:228]     Train net output #0: softmax = 3.40801 (* 1 = 3.40801 loss)
I0614 16:13:22.048810  5063 solver.cpp:473] Iteration 9060, lr = 0.0001
I0614 16:13:22.931548  5063 solver.cpp:213] Iteration 9070, loss = 3.34268
I0614 16:13:22.931563  5063 solver.cpp:228]     Train net output #0: softmax = 3.34268 (* 1 = 3.34268 loss)
I0614 16:13:22.931568  5063 solver.cpp:473] Iteration 9070, lr = 0.0001
I0614 16:13:23.815667  5063 solver.cpp:213] Iteration 9080, loss = 3.49555
I0614 16:13:23.815687  5063 solver.cpp:228]     Train net output #0: softmax = 3.49555 (* 1 = 3.49555 loss)
I0614 16:13:23.815692  5063 solver.cpp:473] Iteration 9080, lr = 0.0001
I0614 16:13:24.698367  5063 solver.cpp:213] Iteration 9090, loss = 3.1797
I0614 16:13:24.698384  5063 solver.cpp:228]     Train net output #0: softmax = 3.1797 (* 1 = 3.1797 loss)
I0614 16:13:24.698388  5063 solver.cpp:473] Iteration 9090, lr = 0.0001
I0614 16:13:25.581295  5063 solver.cpp:213] Iteration 9100, loss = 3.24302
I0614 16:13:25.581313  5063 solver.cpp:228]     Train net output #0: softmax = 3.24302 (* 1 = 3.24302 loss)
I0614 16:13:25.581318  5063 solver.cpp:473] Iteration 9100, lr = 0.0001
I0614 16:13:26.464287  5063 solver.cpp:213] Iteration 9110, loss = 3.4809
I0614 16:13:26.464303  5063 solver.cpp:228]     Train net output #0: softmax = 3.4809 (* 1 = 3.4809 loss)
I0614 16:13:26.464308  5063 solver.cpp:473] Iteration 9110, lr = 0.0001
I0614 16:13:27.347841  5063 solver.cpp:213] Iteration 9120, loss = 3.4456
I0614 16:13:27.347868  5063 solver.cpp:228]     Train net output #0: softmax = 3.4456 (* 1 = 3.4456 loss)
I0614 16:13:27.347991  5063 solver.cpp:473] Iteration 9120, lr = 0.0001
I0614 16:13:28.230824  5063 solver.cpp:213] Iteration 9130, loss = 3.32704
I0614 16:13:28.230845  5063 solver.cpp:228]     Train net output #0: softmax = 3.32704 (* 1 = 3.32704 loss)
I0614 16:13:28.230850  5063 solver.cpp:473] Iteration 9130, lr = 0.0001
I0614 16:13:29.113859  5063 solver.cpp:213] Iteration 9140, loss = 3.38784
I0614 16:13:29.113878  5063 solver.cpp:228]     Train net output #0: softmax = 3.38784 (* 1 = 3.38784 loss)
I0614 16:13:29.113883  5063 solver.cpp:473] Iteration 9140, lr = 0.0001
I0614 16:13:29.997148  5063 solver.cpp:213] Iteration 9150, loss = 3.45907
I0614 16:13:29.997164  5063 solver.cpp:228]     Train net output #0: softmax = 3.45907 (* 1 = 3.45907 loss)
I0614 16:13:29.997169  5063 solver.cpp:473] Iteration 9150, lr = 0.0001
I0614 16:13:30.879627  5063 solver.cpp:213] Iteration 9160, loss = 3.3759
I0614 16:13:30.879645  5063 solver.cpp:228]     Train net output #0: softmax = 3.3759 (* 1 = 3.3759 loss)
I0614 16:13:30.879650  5063 solver.cpp:473] Iteration 9160, lr = 0.0001
I0614 16:13:31.762768  5063 solver.cpp:213] Iteration 9170, loss = 3.26076
I0614 16:13:31.762792  5063 solver.cpp:228]     Train net output #0: softmax = 3.26076 (* 1 = 3.26076 loss)
I0614 16:13:31.762797  5063 solver.cpp:473] Iteration 9170, lr = 0.0001
I0614 16:13:32.645143  5063 solver.cpp:213] Iteration 9180, loss = 3.48298
I0614 16:13:32.645185  5063 solver.cpp:228]     Train net output #0: softmax = 3.48298 (* 1 = 3.48298 loss)
I0614 16:13:32.645193  5063 solver.cpp:473] Iteration 9180, lr = 0.0001
I0614 16:13:33.527990  5063 solver.cpp:213] Iteration 9190, loss = 3.54319
I0614 16:13:33.528008  5063 solver.cpp:228]     Train net output #0: softmax = 3.54319 (* 1 = 3.54319 loss)
I0614 16:13:33.528013  5063 solver.cpp:473] Iteration 9190, lr = 0.0001
I0614 16:13:34.411684  5063 solver.cpp:213] Iteration 9200, loss = 3.17862
I0614 16:13:34.411700  5063 solver.cpp:228]     Train net output #0: softmax = 3.17862 (* 1 = 3.17862 loss)
I0614 16:13:34.411705  5063 solver.cpp:473] Iteration 9200, lr = 0.0001
I0614 16:13:35.296123  5063 solver.cpp:213] Iteration 9210, loss = 3.49934
I0614 16:13:35.296144  5063 solver.cpp:228]     Train net output #0: softmax = 3.49934 (* 1 = 3.49934 loss)
I0614 16:13:35.296149  5063 solver.cpp:473] Iteration 9210, lr = 0.0001
I0614 16:13:36.178982  5063 solver.cpp:213] Iteration 9220, loss = 3.36625
I0614 16:13:36.179000  5063 solver.cpp:228]     Train net output #0: softmax = 3.36625 (* 1 = 3.36625 loss)
I0614 16:13:36.179005  5063 solver.cpp:473] Iteration 9220, lr = 0.0001
I0614 16:13:37.062188  5063 solver.cpp:213] Iteration 9230, loss = 3.31892
I0614 16:13:37.062211  5063 solver.cpp:228]     Train net output #0: softmax = 3.31892 (* 1 = 3.31892 loss)
I0614 16:13:37.062216  5063 solver.cpp:473] Iteration 9230, lr = 0.0001
I0614 16:13:37.945068  5063 solver.cpp:213] Iteration 9240, loss = 3.17902
I0614 16:13:37.945089  5063 solver.cpp:228]     Train net output #0: softmax = 3.17902 (* 1 = 3.17902 loss)
I0614 16:13:37.945214  5063 solver.cpp:473] Iteration 9240, lr = 0.0001
I0614 16:13:38.828037  5063 solver.cpp:213] Iteration 9250, loss = 3.34268
I0614 16:13:38.828057  5063 solver.cpp:228]     Train net output #0: softmax = 3.34268 (* 1 = 3.34268 loss)
I0614 16:13:38.828061  5063 solver.cpp:473] Iteration 9250, lr = 0.0001
I0614 16:13:39.711550  5063 solver.cpp:213] Iteration 9260, loss = 3.12559
I0614 16:13:39.711567  5063 solver.cpp:228]     Train net output #0: softmax = 3.12559 (* 1 = 3.12559 loss)
I0614 16:13:39.711572  5063 solver.cpp:473] Iteration 9260, lr = 0.0001
I0614 16:13:40.595242  5063 solver.cpp:213] Iteration 9270, loss = 3.29121
I0614 16:13:40.595257  5063 solver.cpp:228]     Train net output #0: softmax = 3.29121 (* 1 = 3.29121 loss)
I0614 16:13:40.595262  5063 solver.cpp:473] Iteration 9270, lr = 0.0001
I0614 16:13:41.478672  5063 solver.cpp:213] Iteration 9280, loss = 3.21875
I0614 16:13:41.478688  5063 solver.cpp:228]     Train net output #0: softmax = 3.21875 (* 1 = 3.21875 loss)
I0614 16:13:41.478693  5063 solver.cpp:473] Iteration 9280, lr = 0.0001
I0614 16:13:42.362773  5063 solver.cpp:213] Iteration 9290, loss = 3.31448
I0614 16:13:42.362798  5063 solver.cpp:228]     Train net output #0: softmax = 3.31448 (* 1 = 3.31448 loss)
I0614 16:13:42.362803  5063 solver.cpp:473] Iteration 9290, lr = 0.0001
I0614 16:13:43.246242  5063 solver.cpp:213] Iteration 9300, loss = 3.51356
I0614 16:13:43.246263  5063 solver.cpp:228]     Train net output #0: softmax = 3.51356 (* 1 = 3.51356 loss)
I0614 16:13:43.246397  5063 solver.cpp:473] Iteration 9300, lr = 0.0001
I0614 16:13:44.130054  5063 solver.cpp:213] Iteration 9310, loss = 3.39001
I0614 16:13:44.130071  5063 solver.cpp:228]     Train net output #0: softmax = 3.39001 (* 1 = 3.39001 loss)
I0614 16:13:44.130076  5063 solver.cpp:473] Iteration 9310, lr = 0.0001
I0614 16:13:45.013962  5063 solver.cpp:213] Iteration 9320, loss = 3.191
I0614 16:13:45.013981  5063 solver.cpp:228]     Train net output #0: softmax = 3.191 (* 1 = 3.191 loss)
I0614 16:13:45.013986  5063 solver.cpp:473] Iteration 9320, lr = 0.0001
I0614 16:13:45.897753  5063 solver.cpp:213] Iteration 9330, loss = 3.33422
I0614 16:13:45.897770  5063 solver.cpp:228]     Train net output #0: softmax = 3.33422 (* 1 = 3.33422 loss)
I0614 16:13:45.897775  5063 solver.cpp:473] Iteration 9330, lr = 0.0001
I0614 16:13:46.781188  5063 solver.cpp:213] Iteration 9340, loss = 3.43826
I0614 16:13:46.781206  5063 solver.cpp:228]     Train net output #0: softmax = 3.43826 (* 1 = 3.43826 loss)
I0614 16:13:46.781227  5063 solver.cpp:473] Iteration 9340, lr = 0.0001
I0614 16:13:47.664376  5063 solver.cpp:213] Iteration 9350, loss = 3.38971
I0614 16:13:47.664397  5063 solver.cpp:228]     Train net output #0: softmax = 3.38971 (* 1 = 3.38971 loss)
I0614 16:13:47.664402  5063 solver.cpp:473] Iteration 9350, lr = 0.0001
I0614 16:13:48.547456  5063 solver.cpp:213] Iteration 9360, loss = 3.262
I0614 16:13:48.547480  5063 solver.cpp:228]     Train net output #0: softmax = 3.262 (* 1 = 3.262 loss)
I0614 16:13:48.547485  5063 solver.cpp:473] Iteration 9360, lr = 0.0001
I0614 16:13:49.431361  5063 solver.cpp:213] Iteration 9370, loss = 3.49153
I0614 16:13:49.431380  5063 solver.cpp:228]     Train net output #0: softmax = 3.49153 (* 1 = 3.49153 loss)
I0614 16:13:49.431385  5063 solver.cpp:473] Iteration 9370, lr = 0.0001
I0614 16:13:50.314124  5063 solver.cpp:213] Iteration 9380, loss = 3.54219
I0614 16:13:50.314139  5063 solver.cpp:228]     Train net output #0: softmax = 3.54219 (* 1 = 3.54219 loss)
I0614 16:13:50.314144  5063 solver.cpp:473] Iteration 9380, lr = 0.0001
I0614 16:13:51.197830  5063 solver.cpp:213] Iteration 9390, loss = 3.55498
I0614 16:13:51.197849  5063 solver.cpp:228]     Train net output #0: softmax = 3.55498 (* 1 = 3.55498 loss)
I0614 16:13:51.197854  5063 solver.cpp:473] Iteration 9390, lr = 0.0001
I0614 16:13:52.081264  5063 solver.cpp:213] Iteration 9400, loss = 3.34829
I0614 16:13:52.081282  5063 solver.cpp:228]     Train net output #0: softmax = 3.34829 (* 1 = 3.34829 loss)
I0614 16:13:52.081287  5063 solver.cpp:473] Iteration 9400, lr = 0.0001
I0614 16:13:52.964332  5063 solver.cpp:213] Iteration 9410, loss = 3.37944
I0614 16:13:52.964356  5063 solver.cpp:228]     Train net output #0: softmax = 3.37944 (* 1 = 3.37944 loss)
I0614 16:13:52.964362  5063 solver.cpp:473] Iteration 9410, lr = 0.0001
I0614 16:13:53.847247  5063 solver.cpp:213] Iteration 9420, loss = 3.4785
I0614 16:13:53.847281  5063 solver.cpp:228]     Train net output #0: softmax = 3.4785 (* 1 = 3.4785 loss)
I0614 16:13:53.847436  5063 solver.cpp:473] Iteration 9420, lr = 0.0001
I0614 16:13:54.731098  5063 solver.cpp:213] Iteration 9430, loss = 3.18306
I0614 16:13:54.731117  5063 solver.cpp:228]     Train net output #0: softmax = 3.18306 (* 1 = 3.18306 loss)
I0614 16:13:54.731122  5063 solver.cpp:473] Iteration 9430, lr = 0.0001
I0614 16:13:55.614508  5063 solver.cpp:213] Iteration 9440, loss = 3.10075
I0614 16:13:55.614527  5063 solver.cpp:228]     Train net output #0: softmax = 3.10075 (* 1 = 3.10075 loss)
I0614 16:13:55.614533  5063 solver.cpp:473] Iteration 9440, lr = 0.0001
I0614 16:13:56.497611  5063 solver.cpp:213] Iteration 9450, loss = 3.18694
I0614 16:13:56.497629  5063 solver.cpp:228]     Train net output #0: softmax = 3.18694 (* 1 = 3.18694 loss)
I0614 16:13:56.497634  5063 solver.cpp:473] Iteration 9450, lr = 0.0001
I0614 16:13:57.381111  5063 solver.cpp:213] Iteration 9460, loss = 3.34388
I0614 16:13:57.381130  5063 solver.cpp:228]     Train net output #0: softmax = 3.34388 (* 1 = 3.34388 loss)
I0614 16:13:57.381135  5063 solver.cpp:473] Iteration 9460, lr = 0.0001
I0614 16:13:58.265528  5063 solver.cpp:213] Iteration 9470, loss = 3.35902
I0614 16:13:58.265554  5063 solver.cpp:228]     Train net output #0: softmax = 3.35902 (* 1 = 3.35902 loss)
I0614 16:13:58.265559  5063 solver.cpp:473] Iteration 9470, lr = 0.0001
I0614 16:13:59.149075  5063 solver.cpp:213] Iteration 9480, loss = 3.22886
I0614 16:13:59.149094  5063 solver.cpp:228]     Train net output #0: softmax = 3.22886 (* 1 = 3.22886 loss)
I0614 16:13:59.149104  5063 solver.cpp:473] Iteration 9480, lr = 0.0001
I0614 16:14:00.032245  5063 solver.cpp:213] Iteration 9490, loss = 3.32231
I0614 16:14:00.032261  5063 solver.cpp:228]     Train net output #0: softmax = 3.32231 (* 1 = 3.32231 loss)
I0614 16:14:00.032268  5063 solver.cpp:473] Iteration 9490, lr = 0.0001
I0614 16:14:00.915477  5063 solver.cpp:213] Iteration 9500, loss = 3.4947
I0614 16:14:00.915495  5063 solver.cpp:228]     Train net output #0: softmax = 3.4947 (* 1 = 3.4947 loss)
I0614 16:14:00.915516  5063 solver.cpp:473] Iteration 9500, lr = 0.0001
I0614 16:14:01.798506  5063 solver.cpp:213] Iteration 9510, loss = 3.37746
I0614 16:14:01.798522  5063 solver.cpp:228]     Train net output #0: softmax = 3.37746 (* 1 = 3.37746 loss)
I0614 16:14:01.798527  5063 solver.cpp:473] Iteration 9510, lr = 0.0001
I0614 16:14:02.682350  5063 solver.cpp:213] Iteration 9520, loss = 3.39364
I0614 16:14:02.682395  5063 solver.cpp:228]     Train net output #0: softmax = 3.39364 (* 1 = 3.39364 loss)
I0614 16:14:02.682401  5063 solver.cpp:473] Iteration 9520, lr = 0.0001
I0614 16:14:03.565974  5063 solver.cpp:213] Iteration 9530, loss = 3.39464
I0614 16:14:03.565991  5063 solver.cpp:228]     Train net output #0: softmax = 3.39464 (* 1 = 3.39464 loss)
I0614 16:14:03.565996  5063 solver.cpp:473] Iteration 9530, lr = 0.0001
I0614 16:14:04.448808  5063 solver.cpp:213] Iteration 9540, loss = 3.35221
I0614 16:14:04.448825  5063 solver.cpp:228]     Train net output #0: softmax = 3.35221 (* 1 = 3.35221 loss)
I0614 16:14:04.448830  5063 solver.cpp:473] Iteration 9540, lr = 0.0001
I0614 16:14:05.331889  5063 solver.cpp:213] Iteration 9550, loss = 3.34499
I0614 16:14:05.331907  5063 solver.cpp:228]     Train net output #0: softmax = 3.34499 (* 1 = 3.34499 loss)
I0614 16:14:05.331912  5063 solver.cpp:473] Iteration 9550, lr = 0.0001
I0614 16:14:06.215700  5063 solver.cpp:213] Iteration 9560, loss = 3.41923
I0614 16:14:06.215718  5063 solver.cpp:228]     Train net output #0: softmax = 3.41923 (* 1 = 3.41923 loss)
I0614 16:14:06.215723  5063 solver.cpp:473] Iteration 9560, lr = 0.0001
I0614 16:14:07.098656  5063 solver.cpp:213] Iteration 9570, loss = 3.25725
I0614 16:14:07.098673  5063 solver.cpp:228]     Train net output #0: softmax = 3.25725 (* 1 = 3.25725 loss)
I0614 16:14:07.098678  5063 solver.cpp:473] Iteration 9570, lr = 0.0001
I0614 16:14:07.981751  5063 solver.cpp:213] Iteration 9580, loss = 3.33559
I0614 16:14:07.981773  5063 solver.cpp:228]     Train net output #0: softmax = 3.33559 (* 1 = 3.33559 loss)
I0614 16:14:07.981778  5063 solver.cpp:473] Iteration 9580, lr = 0.0001
I0614 16:14:08.864742  5063 solver.cpp:213] Iteration 9590, loss = 3.15108
I0614 16:14:08.864761  5063 solver.cpp:228]     Train net output #0: softmax = 3.15108 (* 1 = 3.15108 loss)
I0614 16:14:08.864766  5063 solver.cpp:473] Iteration 9590, lr = 0.0001
I0614 16:14:09.748245  5063 solver.cpp:213] Iteration 9600, loss = 3.15965
I0614 16:14:09.748266  5063 solver.cpp:228]     Train net output #0: softmax = 3.15965 (* 1 = 3.15965 loss)
I0614 16:14:09.748276  5063 solver.cpp:473] Iteration 9600, lr = 0.0001
I0614 16:14:10.631739  5063 solver.cpp:213] Iteration 9610, loss = 3.23024
I0614 16:14:10.631757  5063 solver.cpp:228]     Train net output #0: softmax = 3.23024 (* 1 = 3.23024 loss)
I0614 16:14:10.631762  5063 solver.cpp:473] Iteration 9610, lr = 0.0001
I0614 16:14:11.515353  5063 solver.cpp:213] Iteration 9620, loss = 3.21168
I0614 16:14:11.515370  5063 solver.cpp:228]     Train net output #0: softmax = 3.21168 (* 1 = 3.21168 loss)
I0614 16:14:11.515375  5063 solver.cpp:473] Iteration 9620, lr = 0.0001
I0614 16:14:12.398290  5063 solver.cpp:213] Iteration 9630, loss = 3.31023
I0614 16:14:12.398308  5063 solver.cpp:228]     Train net output #0: softmax = 3.31023 (* 1 = 3.31023 loss)
I0614 16:14:12.398313  5063 solver.cpp:473] Iteration 9630, lr = 0.0001
I0614 16:14:13.281498  5063 solver.cpp:213] Iteration 9640, loss = 3.20569
I0614 16:14:13.281520  5063 solver.cpp:228]     Train net output #0: softmax = 3.20569 (* 1 = 3.20569 loss)
I0614 16:14:13.281525  5063 solver.cpp:473] Iteration 9640, lr = 0.0001
I0614 16:14:14.164772  5063 solver.cpp:213] Iteration 9650, loss = 3.35007
I0614 16:14:14.164790  5063 solver.cpp:228]     Train net output #0: softmax = 3.35007 (* 1 = 3.35007 loss)
I0614 16:14:14.164794  5063 solver.cpp:473] Iteration 9650, lr = 0.0001
I0614 16:14:15.047899  5063 solver.cpp:213] Iteration 9660, loss = 3.34394
I0614 16:14:15.047920  5063 solver.cpp:228]     Train net output #0: softmax = 3.34394 (* 1 = 3.34394 loss)
I0614 16:14:15.047930  5063 solver.cpp:473] Iteration 9660, lr = 0.0001
I0614 16:14:15.931141  5063 solver.cpp:213] Iteration 9670, loss = 3.29872
I0614 16:14:15.931159  5063 solver.cpp:228]     Train net output #0: softmax = 3.29872 (* 1 = 3.29872 loss)
I0614 16:14:15.931164  5063 solver.cpp:473] Iteration 9670, lr = 0.0001
I0614 16:14:16.814807  5063 solver.cpp:213] Iteration 9680, loss = 3.25665
I0614 16:14:16.814838  5063 solver.cpp:228]     Train net output #0: softmax = 3.25665 (* 1 = 3.25665 loss)
I0614 16:14:16.814858  5063 solver.cpp:473] Iteration 9680, lr = 0.0001
I0614 16:14:17.698976  5063 solver.cpp:213] Iteration 9690, loss = 3.29159
I0614 16:14:17.698997  5063 solver.cpp:228]     Train net output #0: softmax = 3.29159 (* 1 = 3.29159 loss)
I0614 16:14:17.699002  5063 solver.cpp:473] Iteration 9690, lr = 0.0001
I0614 16:14:18.580843  5063 solver.cpp:213] Iteration 9700, loss = 3.3986
I0614 16:14:18.580860  5063 solver.cpp:228]     Train net output #0: softmax = 3.3986 (* 1 = 3.3986 loss)
I0614 16:14:18.580865  5063 solver.cpp:473] Iteration 9700, lr = 0.0001
I0614 16:14:19.463508  5063 solver.cpp:213] Iteration 9710, loss = 3.40241
I0614 16:14:19.463526  5063 solver.cpp:228]     Train net output #0: softmax = 3.40241 (* 1 = 3.40241 loss)
I0614 16:14:19.463529  5063 solver.cpp:473] Iteration 9710, lr = 0.0001
I0614 16:14:20.346418  5063 solver.cpp:213] Iteration 9720, loss = 3.29149
I0614 16:14:20.346439  5063 solver.cpp:228]     Train net output #0: softmax = 3.29149 (* 1 = 3.29149 loss)
I0614 16:14:20.346449  5063 solver.cpp:473] Iteration 9720, lr = 0.0001
I0614 16:14:21.228894  5063 solver.cpp:213] Iteration 9730, loss = 3.32152
I0614 16:14:21.228910  5063 solver.cpp:228]     Train net output #0: softmax = 3.32152 (* 1 = 3.32152 loss)
I0614 16:14:21.228915  5063 solver.cpp:473] Iteration 9730, lr = 0.0001
I0614 16:14:22.112190  5063 solver.cpp:213] Iteration 9740, loss = 3.31764
I0614 16:14:22.112206  5063 solver.cpp:228]     Train net output #0: softmax = 3.31764 (* 1 = 3.31764 loss)
I0614 16:14:22.112211  5063 solver.cpp:473] Iteration 9740, lr = 0.0001
I0614 16:14:22.995457  5063 solver.cpp:213] Iteration 9750, loss = 3.41272
I0614 16:14:22.995477  5063 solver.cpp:228]     Train net output #0: softmax = 3.41272 (* 1 = 3.41272 loss)
I0614 16:14:22.995482  5063 solver.cpp:473] Iteration 9750, lr = 0.0001
I0614 16:14:23.878980  5063 solver.cpp:213] Iteration 9760, loss = 3.18856
I0614 16:14:23.878998  5063 solver.cpp:228]     Train net output #0: softmax = 3.18856 (* 1 = 3.18856 loss)
I0614 16:14:23.879004  5063 solver.cpp:473] Iteration 9760, lr = 0.0001
I0614 16:14:24.761867  5063 solver.cpp:213] Iteration 9770, loss = 3.46417
I0614 16:14:24.761883  5063 solver.cpp:228]     Train net output #0: softmax = 3.46417 (* 1 = 3.46417 loss)
I0614 16:14:24.761888  5063 solver.cpp:473] Iteration 9770, lr = 0.0001
I0614 16:14:25.645467  5063 solver.cpp:213] Iteration 9780, loss = 3.3601
I0614 16:14:25.645486  5063 solver.cpp:228]     Train net output #0: softmax = 3.3601 (* 1 = 3.3601 loss)
I0614 16:14:25.645496  5063 solver.cpp:473] Iteration 9780, lr = 0.0001
I0614 16:14:26.529690  5063 solver.cpp:213] Iteration 9790, loss = 3.15775
I0614 16:14:26.529708  5063 solver.cpp:228]     Train net output #0: softmax = 3.15775 (* 1 = 3.15775 loss)
I0614 16:14:26.529713  5063 solver.cpp:473] Iteration 9790, lr = 0.0001
I0614 16:14:27.412781  5063 solver.cpp:213] Iteration 9800, loss = 3.23357
I0614 16:14:27.412798  5063 solver.cpp:228]     Train net output #0: softmax = 3.23357 (* 1 = 3.23357 loss)
I0614 16:14:27.412802  5063 solver.cpp:473] Iteration 9800, lr = 0.0001
I0614 16:14:28.295788  5063 solver.cpp:213] Iteration 9810, loss = 3.38643
I0614 16:14:28.295810  5063 solver.cpp:228]     Train net output #0: softmax = 3.38643 (* 1 = 3.38643 loss)
I0614 16:14:28.295816  5063 solver.cpp:473] Iteration 9810, lr = 0.0001
I0614 16:14:29.178700  5063 solver.cpp:213] Iteration 9820, loss = 3.01782
I0614 16:14:29.178719  5063 solver.cpp:228]     Train net output #0: softmax = 3.01782 (* 1 = 3.01782 loss)
I0614 16:14:29.178724  5063 solver.cpp:473] Iteration 9820, lr = 0.0001
I0614 16:14:30.061808  5063 solver.cpp:213] Iteration 9830, loss = 3.23314
I0614 16:14:30.061826  5063 solver.cpp:228]     Train net output #0: softmax = 3.23314 (* 1 = 3.23314 loss)
I0614 16:14:30.061830  5063 solver.cpp:473] Iteration 9830, lr = 0.0001
I0614 16:14:30.945163  5063 solver.cpp:213] Iteration 9840, loss = 3.1754
I0614 16:14:30.945334  5063 solver.cpp:228]     Train net output #0: softmax = 3.1754 (* 1 = 3.1754 loss)
I0614 16:14:30.945355  5063 solver.cpp:473] Iteration 9840, lr = 0.0001
I0614 16:14:31.828526  5063 solver.cpp:213] Iteration 9850, loss = 3.36365
I0614 16:14:31.828544  5063 solver.cpp:228]     Train net output #0: softmax = 3.36365 (* 1 = 3.36365 loss)
I0614 16:14:31.828549  5063 solver.cpp:473] Iteration 9850, lr = 0.0001
I0614 16:14:32.712056  5063 solver.cpp:213] Iteration 9860, loss = 3.36089
I0614 16:14:32.712090  5063 solver.cpp:228]     Train net output #0: softmax = 3.36089 (* 1 = 3.36089 loss)
I0614 16:14:32.712095  5063 solver.cpp:473] Iteration 9860, lr = 0.0001
I0614 16:14:33.595226  5063 solver.cpp:213] Iteration 9870, loss = 3.50085
I0614 16:14:33.595250  5063 solver.cpp:228]     Train net output #0: softmax = 3.50085 (* 1 = 3.50085 loss)
I0614 16:14:33.595255  5063 solver.cpp:473] Iteration 9870, lr = 0.0001
I0614 16:14:34.478626  5063 solver.cpp:213] Iteration 9880, loss = 3.31866
I0614 16:14:34.478642  5063 solver.cpp:228]     Train net output #0: softmax = 3.31866 (* 1 = 3.31866 loss)
I0614 16:14:34.478647  5063 solver.cpp:473] Iteration 9880, lr = 0.0001
I0614 16:14:35.361976  5063 solver.cpp:213] Iteration 9890, loss = 3.24516
I0614 16:14:35.361991  5063 solver.cpp:228]     Train net output #0: softmax = 3.24516 (* 1 = 3.24516 loss)
I0614 16:14:35.361996  5063 solver.cpp:473] Iteration 9890, lr = 0.0001
I0614 16:14:36.244843  5063 solver.cpp:213] Iteration 9900, loss = 3.3327
I0614 16:14:36.244861  5063 solver.cpp:228]     Train net output #0: softmax = 3.3327 (* 1 = 3.3327 loss)
I0614 16:14:36.244871  5063 solver.cpp:473] Iteration 9900, lr = 0.0001
I0614 16:14:37.127688  5063 solver.cpp:213] Iteration 9910, loss = 3.3828
I0614 16:14:37.127707  5063 solver.cpp:228]     Train net output #0: softmax = 3.3828 (* 1 = 3.3828 loss)
I0614 16:14:37.127712  5063 solver.cpp:473] Iteration 9910, lr = 0.0001
I0614 16:14:38.010154  5063 solver.cpp:213] Iteration 9920, loss = 3.15884
I0614 16:14:38.010169  5063 solver.cpp:228]     Train net output #0: softmax = 3.15884 (* 1 = 3.15884 loss)
I0614 16:14:38.010174  5063 solver.cpp:473] Iteration 9920, lr = 0.0001
I0614 16:14:38.893054  5063 solver.cpp:213] Iteration 9930, loss = 3.27957
I0614 16:14:38.893076  5063 solver.cpp:228]     Train net output #0: softmax = 3.27957 (* 1 = 3.27957 loss)
I0614 16:14:38.893081  5063 solver.cpp:473] Iteration 9930, lr = 0.0001
I0614 16:14:39.776401  5063 solver.cpp:213] Iteration 9940, loss = 3.42273
I0614 16:14:39.776419  5063 solver.cpp:228]     Train net output #0: softmax = 3.42273 (* 1 = 3.42273 loss)
I0614 16:14:39.776424  5063 solver.cpp:473] Iteration 9940, lr = 0.0001
I0614 16:14:40.660192  5063 solver.cpp:213] Iteration 9950, loss = 3.38237
I0614 16:14:40.660209  5063 solver.cpp:228]     Train net output #0: softmax = 3.38237 (* 1 = 3.38237 loss)
I0614 16:14:40.660214  5063 solver.cpp:473] Iteration 9950, lr = 0.0001
I0614 16:14:41.544742  5063 solver.cpp:213] Iteration 9960, loss = 3.25725
I0614 16:14:41.544761  5063 solver.cpp:228]     Train net output #0: softmax = 3.25725 (* 1 = 3.25725 loss)
I0614 16:14:41.544766  5063 solver.cpp:473] Iteration 9960, lr = 0.0001
I0614 16:14:42.427990  5063 solver.cpp:213] Iteration 9970, loss = 3.19783
I0614 16:14:42.428007  5063 solver.cpp:228]     Train net output #0: softmax = 3.19783 (* 1 = 3.19783 loss)
I0614 16:14:42.428012  5063 solver.cpp:473] Iteration 9970, lr = 0.0001
I0614 16:14:43.310909  5063 solver.cpp:213] Iteration 9980, loss = 3.10634
I0614 16:14:43.310923  5063 solver.cpp:228]     Train net output #0: softmax = 3.10634 (* 1 = 3.10634 loss)
I0614 16:14:43.310928  5063 solver.cpp:473] Iteration 9980, lr = 0.0001
I0614 16:14:44.194090  5063 solver.cpp:213] Iteration 9990, loss = 3.38706
I0614 16:14:44.194113  5063 solver.cpp:228]     Train net output #0: softmax = 3.38706 (* 1 = 3.38706 loss)
I0614 16:14:44.194118  5063 solver.cpp:473] Iteration 9990, lr = 0.0001
I0614 16:14:45.019582  5063 solver.cpp:362] Snapshotting to snapshots/16-06-14_15h51m21s_0_11_pretrainClassification_iter_10000.caffemodel
I0614 16:14:45.020334  5063 solver.cpp:370] Snapshotting solver state to snapshots/16-06-14_15h51m21s_0_11_pretrainClassification_iter_10000.solverstate
I0614 16:14:45.020761  5063 solver.cpp:291] Iteration 10000, Testing net (#0)
I0614 16:14:45.133278  5063 solver.cpp:342]     Test net output #0: accuracy_top_1 = 0.229687
I0614 16:14:45.133306  5063 solver.cpp:342]     Test net output #1: accuracy_top_5 = 0.46875
I0614 16:14:45.133311  5063 solver.cpp:342]     Test net output #2: softmax = 3.29038 (* 1 = 3.29038 loss)
I0614 16:14:45.191536  5063 solver.cpp:213] Iteration 10000, loss = 3.13641
I0614 16:14:45.191550  5063 solver.cpp:228]     Train net output #0: softmax = 3.13641 (* 1 = 3.13641 loss)
I0614 16:14:45.191555  5063 solver.cpp:473] Iteration 10000, lr = 0.0001
I0614 16:14:46.074194  5063 solver.cpp:213] Iteration 10010, loss = 3.26774
I0614 16:14:46.074211  5063 solver.cpp:228]     Train net output #0: softmax = 3.26774 (* 1 = 3.26774 loss)
I0614 16:14:46.074216  5063 solver.cpp:473] Iteration 10010, lr = 0.0001
I0614 16:14:46.958256  5063 solver.cpp:213] Iteration 10020, loss = 3.24952
I0614 16:14:46.958274  5063 solver.cpp:228]     Train net output #0: softmax = 3.24952 (* 1 = 3.24952 loss)
I0614 16:14:46.958464  5063 solver.cpp:473] Iteration 10020, lr = 0.0001
I0614 16:14:47.841606  5063 solver.cpp:213] Iteration 10030, loss = 3.30133
I0614 16:14:47.841625  5063 solver.cpp:228]     Train net output #0: softmax = 3.30133 (* 1 = 3.30133 loss)
I0614 16:14:47.841630  5063 solver.cpp:473] Iteration 10030, lr = 0.0001
I0614 16:14:48.725196  5063 solver.cpp:213] Iteration 10040, loss = 3.49048
I0614 16:14:48.725217  5063 solver.cpp:228]     Train net output #0: softmax = 3.49048 (* 1 = 3.49048 loss)
I0614 16:14:48.725222  5063 solver.cpp:473] Iteration 10040, lr = 0.0001
I0614 16:14:49.608620  5063 solver.cpp:213] Iteration 10050, loss = 3.50418
I0614 16:14:49.608640  5063 solver.cpp:228]     Train net output #0: softmax = 3.50418 (* 1 = 3.50418 loss)
I0614 16:14:49.608645  5063 solver.cpp:473] Iteration 10050, lr = 0.0001
I0614 16:14:50.491983  5063 solver.cpp:213] Iteration 10060, loss = 3.28728
I0614 16:14:50.492000  5063 solver.cpp:228]     Train net output #0: softmax = 3.28728 (* 1 = 3.28728 loss)
I0614 16:14:50.492005  5063 solver.cpp:473] Iteration 10060, lr = 0.0001
I0614 16:14:51.375044  5063 solver.cpp:213] Iteration 10070, loss = 3.29226
I0614 16:14:51.375061  5063 solver.cpp:228]     Train net output #0: softmax = 3.29226 (* 1 = 3.29226 loss)
I0614 16:14:51.375066  5063 solver.cpp:473] Iteration 10070, lr = 0.0001
I0614 16:14:52.257786  5063 solver.cpp:213] Iteration 10080, loss = 3.35026
I0614 16:14:52.257805  5063 solver.cpp:228]     Train net output #0: softmax = 3.35026 (* 1 = 3.35026 loss)
I0614 16:14:52.257815  5063 solver.cpp:473] Iteration 10080, lr = 0.0001
I0614 16:14:53.132485  5063 solver.cpp:213] Iteration 10090, loss = 3.36721
I0614 16:14:53.132508  5063 solver.cpp:228]     Train net output #0: softmax = 3.36721 (* 1 = 3.36721 loss)
I0614 16:14:53.132514  5063 solver.cpp:473] Iteration 10090, lr = 0.0001
I0614 16:14:54.005600  5063 solver.cpp:213] Iteration 10100, loss = 3.4138
I0614 16:14:54.005628  5063 solver.cpp:228]     Train net output #0: softmax = 3.4138 (* 1 = 3.4138 loss)
I0614 16:14:54.005635  5063 solver.cpp:473] Iteration 10100, lr = 0.0001
I0614 16:14:54.887794  5063 solver.cpp:213] Iteration 10110, loss = 3.18855
I0614 16:14:54.887811  5063 solver.cpp:228]     Train net output #0: softmax = 3.18855 (* 1 = 3.18855 loss)
I0614 16:14:54.887816  5063 solver.cpp:473] Iteration 10110, lr = 0.0001
I0614 16:14:55.770973  5063 solver.cpp:213] Iteration 10120, loss = 3.16189
I0614 16:14:55.771003  5063 solver.cpp:228]     Train net output #0: softmax = 3.16189 (* 1 = 3.16189 loss)
I0614 16:14:55.771020  5063 solver.cpp:473] Iteration 10120, lr = 0.0001
I0614 16:14:56.653561  5063 solver.cpp:213] Iteration 10130, loss = 3.36581
I0614 16:14:56.653578  5063 solver.cpp:228]     Train net output #0: softmax = 3.36581 (* 1 = 3.36581 loss)
I0614 16:14:56.653584  5063 solver.cpp:473] Iteration 10130, lr = 0.0001
I0614 16:14:57.536332  5063 solver.cpp:213] Iteration 10140, loss = 3.40666
I0614 16:14:57.536351  5063 solver.cpp:228]     Train net output #0: softmax = 3.40666 (* 1 = 3.40666 loss)
I0614 16:14:57.536356  5063 solver.cpp:473] Iteration 10140, lr = 0.0001
I0614 16:14:58.419493  5063 solver.cpp:213] Iteration 10150, loss = 3.26781
I0614 16:14:58.419512  5063 solver.cpp:228]     Train net output #0: softmax = 3.26781 (* 1 = 3.26781 loss)
I0614 16:14:58.419517  5063 solver.cpp:473] Iteration 10150, lr = 0.0001
I0614 16:14:59.302417  5063 solver.cpp:213] Iteration 10160, loss = 3.63392
I0614 16:14:59.302435  5063 solver.cpp:228]     Train net output #0: softmax = 3.63392 (* 1 = 3.63392 loss)
I0614 16:14:59.302440  5063 solver.cpp:473] Iteration 10160, lr = 0.0001
I0614 16:15:00.185628  5063 solver.cpp:213] Iteration 10170, loss = 3.23577
I0614 16:15:00.185652  5063 solver.cpp:228]     Train net output #0: softmax = 3.23577 (* 1 = 3.23577 loss)
I0614 16:15:00.185657  5063 solver.cpp:473] Iteration 10170, lr = 0.0001
I0614 16:15:01.068457  5063 solver.cpp:213] Iteration 10180, loss = 3.42315
I0614 16:15:01.068475  5063 solver.cpp:228]     Train net output #0: softmax = 3.42315 (* 1 = 3.42315 loss)
I0614 16:15:01.068480  5063 solver.cpp:473] Iteration 10180, lr = 0.0001
I0614 16:15:01.951977  5063 solver.cpp:213] Iteration 10190, loss = 3.44979
I0614 16:15:01.951993  5063 solver.cpp:228]     Train net output #0: softmax = 3.44979 (* 1 = 3.44979 loss)
I0614 16:15:01.951998  5063 solver.cpp:473] Iteration 10190, lr = 0.0001
I0614 16:15:02.835762  5063 solver.cpp:213] Iteration 10200, loss = 3.4033
I0614 16:15:02.835952  5063 solver.cpp:228]     Train net output #0: softmax = 3.4033 (* 1 = 3.4033 loss)
I0614 16:15:02.835958  5063 solver.cpp:473] Iteration 10200, lr = 0.0001
I0614 16:15:03.719276  5063 solver.cpp:213] Iteration 10210, loss = 3.3189
I0614 16:15:03.719295  5063 solver.cpp:228]     Train net output #0: softmax = 3.3189 (* 1 = 3.3189 loss)
I0614 16:15:03.719300  5063 solver.cpp:473] Iteration 10210, lr = 0.0001
I0614 16:15:04.602452  5063 solver.cpp:213] Iteration 10220, loss = 3.24609
I0614 16:15:04.602473  5063 solver.cpp:228]     Train net output #0: softmax = 3.24609 (* 1 = 3.24609 loss)
I0614 16:15:04.602478  5063 solver.cpp:473] Iteration 10220, lr = 0.0001
I0614 16:15:05.486047  5063 solver.cpp:213] Iteration 10230, loss = 3.51317
I0614 16:15:05.486065  5063 solver.cpp:228]     Train net output #0: softmax = 3.51317 (* 1 = 3.51317 loss)
I0614 16:15:05.486070  5063 solver.cpp:473] Iteration 10230, lr = 0.0001
I0614 16:15:06.369675  5063 solver.cpp:213] Iteration 10240, loss = 3.3728
I0614 16:15:06.369693  5063 solver.cpp:228]     Train net output #0: softmax = 3.3728 (* 1 = 3.3728 loss)
I0614 16:15:06.369699  5063 solver.cpp:473] Iteration 10240, lr = 0.0001
I0614 16:15:07.252447  5063 solver.cpp:213] Iteration 10250, loss = 3.36038
I0614 16:15:07.252465  5063 solver.cpp:228]     Train net output #0: softmax = 3.36038 (* 1 = 3.36038 loss)
I0614 16:15:07.252470  5063 solver.cpp:473] Iteration 10250, lr = 0.0001
I0614 16:15:08.135448  5063 solver.cpp:213] Iteration 10260, loss = 3.30848
I0614 16:15:08.135469  5063 solver.cpp:228]     Train net output #0: softmax = 3.30848 (* 1 = 3.30848 loss)
I0614 16:15:08.135480  5063 solver.cpp:473] Iteration 10260, lr = 0.0001
I0614 16:15:09.018360  5063 solver.cpp:213] Iteration 10270, loss = 3.55391
I0614 16:15:09.018378  5063 solver.cpp:228]     Train net output #0: softmax = 3.55391 (* 1 = 3.55391 loss)
I0614 16:15:09.018383  5063 solver.cpp:473] Iteration 10270, lr = 0.0001
I0614 16:15:09.901958  5063 solver.cpp:213] Iteration 10280, loss = 3.26611
I0614 16:15:09.901979  5063 solver.cpp:228]     Train net output #0: softmax = 3.26611 (* 1 = 3.26611 loss)
I0614 16:15:09.901984  5063 solver.cpp:473] Iteration 10280, lr = 0.0001
I0614 16:15:10.785869  5063 solver.cpp:213] Iteration 10290, loss = 3.33783
I0614 16:15:10.785887  5063 solver.cpp:228]     Train net output #0: softmax = 3.33783 (* 1 = 3.33783 loss)
I0614 16:15:10.785902  5063 solver.cpp:473] Iteration 10290, lr = 0.0001
I0614 16:15:11.668786  5063 solver.cpp:213] Iteration 10300, loss = 3.29009
I0614 16:15:11.668802  5063 solver.cpp:228]     Train net output #0: softmax = 3.29009 (* 1 = 3.29009 loss)
I0614 16:15:11.668805  5063 solver.cpp:473] Iteration 10300, lr = 0.0001
I0614 16:15:12.552222  5063 solver.cpp:213] Iteration 10310, loss = 3.30335
I0614 16:15:12.552238  5063 solver.cpp:228]     Train net output #0: softmax = 3.30335 (* 1 = 3.30335 loss)
I0614 16:15:12.552243  5063 solver.cpp:473] Iteration 10310, lr = 0.0001
I0614 16:15:13.435418  5063 solver.cpp:213] Iteration 10320, loss = 3.16684
I0614 16:15:13.435436  5063 solver.cpp:228]     Train net output #0: softmax = 3.16684 (* 1 = 3.16684 loss)
I0614 16:15:13.435441  5063 solver.cpp:473] Iteration 10320, lr = 0.0001
I0614 16:15:14.319203  5063 solver.cpp:213] Iteration 10330, loss = 3.30998
I0614 16:15:14.319224  5063 solver.cpp:228]     Train net output #0: softmax = 3.30998 (* 1 = 3.30998 loss)
I0614 16:15:14.319229  5063 solver.cpp:473] Iteration 10330, lr = 0.0001
I0614 16:15:15.201655  5063 solver.cpp:213] Iteration 10340, loss = 3.58546
I0614 16:15:15.201678  5063 solver.cpp:228]     Train net output #0: softmax = 3.58546 (* 1 = 3.58546 loss)
I0614 16:15:15.201683  5063 solver.cpp:473] Iteration 10340, lr = 0.0001
I0614 16:15:16.085481  5063 solver.cpp:213] Iteration 10350, loss = 3.39083
I0614 16:15:16.085500  5063 solver.cpp:228]     Train net output #0: softmax = 3.39083 (* 1 = 3.39083 loss)
I0614 16:15:16.085505  5063 solver.cpp:473] Iteration 10350, lr = 0.0001
I0614 16:15:16.968864  5063 solver.cpp:213] Iteration 10360, loss = 3.22702
I0614 16:15:16.968894  5063 solver.cpp:228]     Train net output #0: softmax = 3.22702 (* 1 = 3.22702 loss)
I0614 16:15:16.968900  5063 solver.cpp:473] Iteration 10360, lr = 0.0001
I0614 16:15:17.852118  5063 solver.cpp:213] Iteration 10370, loss = 3.18325
I0614 16:15:17.852135  5063 solver.cpp:228]     Train net output #0: softmax = 3.18325 (* 1 = 3.18325 loss)
I0614 16:15:17.852141  5063 solver.cpp:473] Iteration 10370, lr = 0.0001
I0614 16:15:18.735919  5063 solver.cpp:213] Iteration 10380, loss = 3.51292
I0614 16:15:18.735941  5063 solver.cpp:228]     Train net output #0: softmax = 3.51292 (* 1 = 3.51292 loss)
I0614 16:15:18.735952  5063 solver.cpp:473] Iteration 10380, lr = 0.0001
I0614 16:15:19.619118  5063 solver.cpp:213] Iteration 10390, loss = 3.51621
I0614 16:15:19.619140  5063 solver.cpp:228]     Train net output #0: softmax = 3.51621 (* 1 = 3.51621 loss)
I0614 16:15:19.619145  5063 solver.cpp:473] Iteration 10390, lr = 0.0001
I0614 16:15:20.501921  5063 solver.cpp:213] Iteration 10400, loss = 3.35791
I0614 16:15:20.501940  5063 solver.cpp:228]     Train net output #0: softmax = 3.35791 (* 1 = 3.35791 loss)
I0614 16:15:20.501945  5063 solver.cpp:473] Iteration 10400, lr = 0.0001
I0614 16:15:21.385841  5063 solver.cpp:213] Iteration 10410, loss = 3.39578
I0614 16:15:21.385861  5063 solver.cpp:228]     Train net output #0: softmax = 3.39578 (* 1 = 3.39578 loss)
I0614 16:15:21.385866  5063 solver.cpp:473] Iteration 10410, lr = 0.0001
I0614 16:15:22.269098  5063 solver.cpp:213] Iteration 10420, loss = 3.47421
I0614 16:15:22.269117  5063 solver.cpp:228]     Train net output #0: softmax = 3.47421 (* 1 = 3.47421 loss)
I0614 16:15:22.269122  5063 solver.cpp:473] Iteration 10420, lr = 0.0001
I0614 16:15:23.153306  5063 solver.cpp:213] Iteration 10430, loss = 3.55083
I0614 16:15:23.153324  5063 solver.cpp:228]     Train net output #0: softmax = 3.55083 (* 1 = 3.55083 loss)
I0614 16:15:23.153328  5063 solver.cpp:473] Iteration 10430, lr = 0.0001
I0614 16:15:24.036262  5063 solver.cpp:213] Iteration 10440, loss = 3.4919
I0614 16:15:24.036280  5063 solver.cpp:228]     Train net output #0: softmax = 3.4919 (* 1 = 3.4919 loss)
I0614 16:15:24.036406  5063 solver.cpp:473] Iteration 10440, lr = 0.0001
I0614 16:15:24.919214  5063 solver.cpp:213] Iteration 10450, loss = 3.38867
I0614 16:15:24.919236  5063 solver.cpp:228]     Train net output #0: softmax = 3.38867 (* 1 = 3.38867 loss)
I0614 16:15:24.919247  5063 solver.cpp:473] Iteration 10450, lr = 0.0001
I0614 16:15:25.801843  5063 solver.cpp:213] Iteration 10460, loss = 3.23476
I0614 16:15:25.801859  5063 solver.cpp:228]     Train net output #0: softmax = 3.23476 (* 1 = 3.23476 loss)
I0614 16:15:25.801864  5063 solver.cpp:473] Iteration 10460, lr = 0.0001
I0614 16:15:26.684965  5063 solver.cpp:213] Iteration 10470, loss = 3.23588
I0614 16:15:26.684983  5063 solver.cpp:228]     Train net output #0: softmax = 3.23588 (* 1 = 3.23588 loss)
I0614 16:15:26.684988  5063 solver.cpp:473] Iteration 10470, lr = 0.0001
I0614 16:15:27.568611  5063 solver.cpp:213] Iteration 10480, loss = 3.21486
I0614 16:15:27.568629  5063 solver.cpp:228]     Train net output #0: softmax = 3.21486 (* 1 = 3.21486 loss)
I0614 16:15:27.568634  5063 solver.cpp:473] Iteration 10480, lr = 0.0001
I0614 16:15:28.452503  5063 solver.cpp:213] Iteration 10490, loss = 3.33786
I0614 16:15:28.452522  5063 solver.cpp:228]     Train net output #0: softmax = 3.33786 (* 1 = 3.33786 loss)
I0614 16:15:28.452527  5063 solver.cpp:473] Iteration 10490, lr = 0.0001
I0614 16:15:29.336258  5063 solver.cpp:213] Iteration 10500, loss = 3.21484
I0614 16:15:29.336280  5063 solver.cpp:228]     Train net output #0: softmax = 3.21484 (* 1 = 3.21484 loss)
I0614 16:15:29.336289  5063 solver.cpp:473] Iteration 10500, lr = 0.0001
I0614 16:15:30.219336  5063 solver.cpp:213] Iteration 10510, loss = 3.24281
I0614 16:15:30.219358  5063 solver.cpp:228]     Train net output #0: softmax = 3.24281 (* 1 = 3.24281 loss)
I0614 16:15:30.219364  5063 solver.cpp:473] Iteration 10510, lr = 0.0001
I0614 16:15:31.103130  5063 solver.cpp:213] Iteration 10520, loss = 3.33391
I0614 16:15:31.103165  5063 solver.cpp:228]     Train net output #0: softmax = 3.33391 (* 1 = 3.33391 loss)
I0614 16:15:31.103171  5063 solver.cpp:473] Iteration 10520, lr = 0.0001
I0614 16:15:31.986023  5063 solver.cpp:213] Iteration 10530, loss = 3.39297
I0614 16:15:31.986039  5063 solver.cpp:228]     Train net output #0: softmax = 3.39297 (* 1 = 3.39297 loss)
I0614 16:15:31.986044  5063 solver.cpp:473] Iteration 10530, lr = 0.0001
I0614 16:15:32.869771  5063 solver.cpp:213] Iteration 10540, loss = 3.2998
I0614 16:15:32.869801  5063 solver.cpp:228]     Train net output #0: softmax = 3.2998 (* 1 = 3.2998 loss)
I0614 16:15:32.869807  5063 solver.cpp:473] Iteration 10540, lr = 0.0001
I0614 16:15:33.753120  5063 solver.cpp:213] Iteration 10550, loss = 3.31275
I0614 16:15:33.753136  5063 solver.cpp:228]     Train net output #0: softmax = 3.31275 (* 1 = 3.31275 loss)
I0614 16:15:33.753141  5063 solver.cpp:473] Iteration 10550, lr = 0.0001
I0614 16:15:34.636693  5063 solver.cpp:213] Iteration 10560, loss = 3.21164
I0614 16:15:34.636713  5063 solver.cpp:228]     Train net output #0: softmax = 3.21164 (* 1 = 3.21164 loss)
I0614 16:15:34.636718  5063 solver.cpp:473] Iteration 10560, lr = 0.0001
I0614 16:15:35.519829  5063 solver.cpp:213] Iteration 10570, loss = 3.58232
I0614 16:15:35.519851  5063 solver.cpp:228]     Train net output #0: softmax = 3.58232 (* 1 = 3.58232 loss)
I0614 16:15:35.519856  5063 solver.cpp:473] Iteration 10570, lr = 0.0001
I0614 16:15:36.403304  5063 solver.cpp:213] Iteration 10580, loss = 3.25082
I0614 16:15:36.403323  5063 solver.cpp:228]     Train net output #0: softmax = 3.25082 (* 1 = 3.25082 loss)
I0614 16:15:36.403328  5063 solver.cpp:473] Iteration 10580, lr = 0.0001
I0614 16:15:37.286953  5063 solver.cpp:213] Iteration 10590, loss = 3.22555
I0614 16:15:37.286972  5063 solver.cpp:228]     Train net output #0: softmax = 3.22555 (* 1 = 3.22555 loss)
I0614 16:15:37.286977  5063 solver.cpp:473] Iteration 10590, lr = 0.0001
I0614 16:15:38.170277  5063 solver.cpp:213] Iteration 10600, loss = 3.28049
I0614 16:15:38.170297  5063 solver.cpp:228]     Train net output #0: softmax = 3.28049 (* 1 = 3.28049 loss)
I0614 16:15:38.170302  5063 solver.cpp:473] Iteration 10600, lr = 0.0001
I0614 16:15:39.053364  5063 solver.cpp:213] Iteration 10610, loss = 3.34491
I0614 16:15:39.053380  5063 solver.cpp:228]     Train net output #0: softmax = 3.34491 (* 1 = 3.34491 loss)
I0614 16:15:39.053391  5063 solver.cpp:473] Iteration 10610, lr = 0.0001
I0614 16:15:39.936868  5063 solver.cpp:213] Iteration 10620, loss = 3.20257
I0614 16:15:39.936887  5063 solver.cpp:228]     Train net output #0: softmax = 3.20257 (* 1 = 3.20257 loss)
I0614 16:15:39.937037  5063 solver.cpp:473] Iteration 10620, lr = 0.0001
I0614 16:15:40.820349  5063 solver.cpp:213] Iteration 10630, loss = 3.40919
I0614 16:15:40.820371  5063 solver.cpp:228]     Train net output #0: softmax = 3.40919 (* 1 = 3.40919 loss)
I0614 16:15:40.820376  5063 solver.cpp:473] Iteration 10630, lr = 0.0001
I0614 16:15:41.704025  5063 solver.cpp:213] Iteration 10640, loss = 3.31339
I0614 16:15:41.704044  5063 solver.cpp:228]     Train net output #0: softmax = 3.31339 (* 1 = 3.31339 loss)
I0614 16:15:41.704049  5063 solver.cpp:473] Iteration 10640, lr = 0.0001
I0614 16:15:42.587970  5063 solver.cpp:213] Iteration 10650, loss = 3.30302
I0614 16:15:42.587988  5063 solver.cpp:228]     Train net output #0: softmax = 3.30302 (* 1 = 3.30302 loss)
I0614 16:15:42.587993  5063 solver.cpp:473] Iteration 10650, lr = 0.0001
I0614 16:15:43.471668  5063 solver.cpp:213] Iteration 10660, loss = 3.28184
I0614 16:15:43.471688  5063 solver.cpp:228]     Train net output #0: softmax = 3.28184 (* 1 = 3.28184 loss)
I0614 16:15:43.471693  5063 solver.cpp:473] Iteration 10660, lr = 0.0001
I0614 16:15:44.354442  5063 solver.cpp:213] Iteration 10670, loss = 3.29112
I0614 16:15:44.354460  5063 solver.cpp:228]     Train net output #0: softmax = 3.29112 (* 1 = 3.29112 loss)
I0614 16:15:44.354463  5063 solver.cpp:473] Iteration 10670, lr = 0.0001
I0614 16:15:45.238086  5063 solver.cpp:213] Iteration 10680, loss = 3.43997
I0614 16:15:45.238111  5063 solver.cpp:228]     Train net output #0: softmax = 3.43997 (* 1 = 3.43997 loss)
I0614 16:15:45.238239  5063 solver.cpp:473] Iteration 10680, lr = 0.0001
I0614 16:15:46.121089  5063 solver.cpp:213] Iteration 10690, loss = 3.36294
I0614 16:15:46.121109  5063 solver.cpp:228]     Train net output #0: softmax = 3.36294 (* 1 = 3.36294 loss)
I0614 16:15:46.121114  5063 solver.cpp:473] Iteration 10690, lr = 0.0001
I0614 16:15:47.004850  5063 solver.cpp:213] Iteration 10700, loss = 3.39134
I0614 16:15:47.004894  5063 solver.cpp:228]     Train net output #0: softmax = 3.39134 (* 1 = 3.39134 loss)
I0614 16:15:47.004900  5063 solver.cpp:473] Iteration 10700, lr = 0.0001
I0614 16:15:47.887758  5063 solver.cpp:213] Iteration 10710, loss = 3.09981
I0614 16:15:47.887774  5063 solver.cpp:228]     Train net output #0: softmax = 3.09981 (* 1 = 3.09981 loss)
I0614 16:15:47.887779  5063 solver.cpp:473] Iteration 10710, lr = 0.0001
I0614 16:15:48.771387  5063 solver.cpp:213] Iteration 10720, loss = 3.37072
I0614 16:15:48.771404  5063 solver.cpp:228]     Train net output #0: softmax = 3.37072 (* 1 = 3.37072 loss)
I0614 16:15:48.771409  5063 solver.cpp:473] Iteration 10720, lr = 0.0001
I0614 16:15:49.654808  5063 solver.cpp:213] Iteration 10730, loss = 3.41024
I0614 16:15:49.654824  5063 solver.cpp:228]     Train net output #0: softmax = 3.41024 (* 1 = 3.41024 loss)
I0614 16:15:49.654829  5063 solver.cpp:473] Iteration 10730, lr = 0.0001
I0614 16:15:50.538753  5063 solver.cpp:213] Iteration 10740, loss = 3.34587
I0614 16:15:50.538776  5063 solver.cpp:228]     Train net output #0: softmax = 3.34587 (* 1 = 3.34587 loss)
I0614 16:15:50.538782  5063 solver.cpp:473] Iteration 10740, lr = 0.0001
I0614 16:15:51.422355  5063 solver.cpp:213] Iteration 10750, loss = 3.18981
I0614 16:15:51.422375  5063 solver.cpp:228]     Train net output #0: softmax = 3.18981 (* 1 = 3.18981 loss)
I0614 16:15:51.422380  5063 solver.cpp:473] Iteration 10750, lr = 0.0001
I0614 16:15:52.305097  5063 solver.cpp:213] Iteration 10760, loss = 3.24509
I0614 16:15:52.305114  5063 solver.cpp:228]     Train net output #0: softmax = 3.24509 (* 1 = 3.24509 loss)
I0614 16:15:52.305119  5063 solver.cpp:473] Iteration 10760, lr = 0.0001
I0614 16:15:53.187880  5063 solver.cpp:213] Iteration 10770, loss = 3.29958
I0614 16:15:53.187894  5063 solver.cpp:228]     Train net output #0: softmax = 3.29958 (* 1 = 3.29958 loss)
I0614 16:15:53.187904  5063 solver.cpp:473] Iteration 10770, lr = 0.0001
I0614 16:15:54.070621  5063 solver.cpp:213] Iteration 10780, loss = 3.22017
I0614 16:15:54.070636  5063 solver.cpp:228]     Train net output #0: softmax = 3.22017 (* 1 = 3.22017 loss)
I0614 16:15:54.070641  5063 solver.cpp:473] Iteration 10780, lr = 0.0001
I0614 16:15:54.954879  5063 solver.cpp:213] Iteration 10790, loss = 3.39806
I0614 16:15:54.954908  5063 solver.cpp:228]     Train net output #0: softmax = 3.39806 (* 1 = 3.39806 loss)
I0614 16:15:54.954913  5063 solver.cpp:473] Iteration 10790, lr = 0.0001
I0614 16:15:55.837404  5063 solver.cpp:213] Iteration 10800, loss = 3.2477
I0614 16:15:55.837429  5063 solver.cpp:228]     Train net output #0: softmax = 3.2477 (* 1 = 3.2477 loss)
I0614 16:15:55.837555  5063 solver.cpp:473] Iteration 10800, lr = 0.0001
I0614 16:15:56.720736  5063 solver.cpp:213] Iteration 10810, loss = 3.4304
I0614 16:15:56.720757  5063 solver.cpp:228]     Train net output #0: softmax = 3.4304 (* 1 = 3.4304 loss)
I0614 16:15:56.720762  5063 solver.cpp:473] Iteration 10810, lr = 0.0001
I0614 16:15:57.604097  5063 solver.cpp:213] Iteration 10820, loss = 3.73579
I0614 16:15:57.604115  5063 solver.cpp:228]     Train net output #0: softmax = 3.73579 (* 1 = 3.73579 loss)
I0614 16:15:57.604120  5063 solver.cpp:473] Iteration 10820, lr = 0.0001
I0614 16:15:58.487685  5063 solver.cpp:213] Iteration 10830, loss = 3.27418
I0614 16:15:58.487707  5063 solver.cpp:228]     Train net output #0: softmax = 3.27418 (* 1 = 3.27418 loss)
I0614 16:15:58.487712  5063 solver.cpp:473] Iteration 10830, lr = 0.0001
I0614 16:15:59.370698  5063 solver.cpp:213] Iteration 10840, loss = 3.44675
I0614 16:15:59.370717  5063 solver.cpp:228]     Train net output #0: softmax = 3.44675 (* 1 = 3.44675 loss)
I0614 16:15:59.370721  5063 solver.cpp:473] Iteration 10840, lr = 0.0001
I0614 16:16:00.254252  5063 solver.cpp:213] Iteration 10850, loss = 3.51053
I0614 16:16:00.254271  5063 solver.cpp:228]     Train net output #0: softmax = 3.51053 (* 1 = 3.51053 loss)
I0614 16:16:00.254276  5063 solver.cpp:473] Iteration 10850, lr = 0.0001
I0614 16:16:01.137205  5063 solver.cpp:213] Iteration 10860, loss = 3.04722
I0614 16:16:01.137373  5063 solver.cpp:228]     Train net output #0: softmax = 3.04722 (* 1 = 3.04722 loss)
I0614 16:16:01.137380  5063 solver.cpp:473] Iteration 10860, lr = 0.0001
I0614 16:16:02.020021  5063 solver.cpp:213] Iteration 10870, loss = 3.16272
I0614 16:16:02.020038  5063 solver.cpp:228]     Train net output #0: softmax = 3.16272 (* 1 = 3.16272 loss)
I0614 16:16:02.020043  5063 solver.cpp:473] Iteration 10870, lr = 0.0001
I0614 16:16:02.902680  5063 solver.cpp:213] Iteration 10880, loss = 3.29721
I0614 16:16:02.902714  5063 solver.cpp:228]     Train net output #0: softmax = 3.29721 (* 1 = 3.29721 loss)
I0614 16:16:02.902719  5063 solver.cpp:473] Iteration 10880, lr = 0.0001
I0614 16:16:03.785100  5063 solver.cpp:213] Iteration 10890, loss = 3.43142
I0614 16:16:03.785117  5063 solver.cpp:228]     Train net output #0: softmax = 3.43142 (* 1 = 3.43142 loss)
I0614 16:16:03.785122  5063 solver.cpp:473] Iteration 10890, lr = 0.0001
I0614 16:16:04.667735  5063 solver.cpp:213] Iteration 10900, loss = 3.36578
I0614 16:16:04.667754  5063 solver.cpp:228]     Train net output #0: softmax = 3.36578 (* 1 = 3.36578 loss)
I0614 16:16:04.667758  5063 solver.cpp:473] Iteration 10900, lr = 0.0001
I0614 16:16:05.550487  5063 solver.cpp:213] Iteration 10910, loss = 3.17155
I0614 16:16:05.550508  5063 solver.cpp:228]     Train net output #0: softmax = 3.17155 (* 1 = 3.17155 loss)
I0614 16:16:05.550513  5063 solver.cpp:473] Iteration 10910, lr = 0.0001
I0614 16:16:06.433198  5063 solver.cpp:213] Iteration 10920, loss = 3.04406
I0614 16:16:06.433220  5063 solver.cpp:228]     Train net output #0: softmax = 3.04406 (* 1 = 3.04406 loss)
I0614 16:16:06.433225  5063 solver.cpp:473] Iteration 10920, lr = 0.0001
I0614 16:16:07.316431  5063 solver.cpp:213] Iteration 10930, loss = 3.41902
I0614 16:16:07.316450  5063 solver.cpp:228]     Train net output #0: softmax = 3.41902 (* 1 = 3.41902 loss)
I0614 16:16:07.316460  5063 solver.cpp:473] Iteration 10930, lr = 0.0001
I0614 16:16:08.199379  5063 solver.cpp:213] Iteration 10940, loss = 3.26708
I0614 16:16:08.199398  5063 solver.cpp:228]     Train net output #0: softmax = 3.26708 (* 1 = 3.26708 loss)
I0614 16:16:08.199403  5063 solver.cpp:473] Iteration 10940, lr = 0.0001
I0614 16:16:09.082412  5063 solver.cpp:213] Iteration 10950, loss = 3.30205
I0614 16:16:09.082428  5063 solver.cpp:228]     Train net output #0: softmax = 3.30205 (* 1 = 3.30205 loss)
I0614 16:16:09.082432  5063 solver.cpp:473] Iteration 10950, lr = 0.0001
I0614 16:16:09.965034  5063 solver.cpp:213] Iteration 10960, loss = 3.49808
I0614 16:16:09.965050  5063 solver.cpp:228]     Train net output #0: softmax = 3.49808 (* 1 = 3.49808 loss)
I0614 16:16:09.965055  5063 solver.cpp:473] Iteration 10960, lr = 0.0001
I0614 16:16:10.848419  5063 solver.cpp:213] Iteration 10970, loss = 3.32275
I0614 16:16:10.848443  5063 solver.cpp:228]     Train net output #0: softmax = 3.32275 (* 1 = 3.32275 loss)
I0614 16:16:10.848448  5063 solver.cpp:473] Iteration 10970, lr = 0.0001
I0614 16:16:11.731253  5063 solver.cpp:213] Iteration 10980, loss = 3.31903
I0614 16:16:11.731273  5063 solver.cpp:228]     Train net output #0: softmax = 3.31903 (* 1 = 3.31903 loss)
I0614 16:16:11.731283  5063 solver.cpp:473] Iteration 10980, lr = 0.0001
I0614 16:16:12.614626  5063 solver.cpp:213] Iteration 10990, loss = 3.19551
I0614 16:16:12.614645  5063 solver.cpp:228]     Train net output #0: softmax = 3.19551 (* 1 = 3.19551 loss)
I0614 16:16:12.614650  5063 solver.cpp:473] Iteration 10990, lr = 0.0001
I0614 16:16:13.439658  5063 solver.cpp:362] Snapshotting to snapshots/16-06-14_15h51m21s_0_11_pretrainClassification_iter_11000.caffemodel
I0614 16:16:13.440426  5063 solver.cpp:370] Snapshotting solver state to snapshots/16-06-14_15h51m21s_0_11_pretrainClassification_iter_11000.solverstate
I0614 16:16:13.440858  5063 solver.cpp:291] Iteration 11000, Testing net (#0)
I0614 16:16:13.553694  5063 solver.cpp:342]     Test net output #0: accuracy_top_1 = 0.209375
I0614 16:16:13.553710  5063 solver.cpp:342]     Test net output #1: accuracy_top_5 = 0.503125
I0614 16:16:13.553717  5063 solver.cpp:342]     Test net output #2: softmax = 3.27688 (* 1 = 3.27688 loss)
I0614 16:16:13.611935  5063 solver.cpp:213] Iteration 11000, loss = 3.25291
I0614 16:16:13.611949  5063 solver.cpp:228]     Train net output #0: softmax = 3.25291 (* 1 = 3.25291 loss)
I0614 16:16:13.611954  5063 solver.cpp:473] Iteration 11000, lr = 0.0001
I0614 16:16:14.494845  5063 solver.cpp:213] Iteration 11010, loss = 3.25255
I0614 16:16:14.494865  5063 solver.cpp:228]     Train net output #0: softmax = 3.25255 (* 1 = 3.25255 loss)
I0614 16:16:14.494889  5063 solver.cpp:473] Iteration 11010, lr = 0.0001
I0614 16:16:15.377770  5063 solver.cpp:213] Iteration 11020, loss = 3.27885
I0614 16:16:15.377787  5063 solver.cpp:228]     Train net output #0: softmax = 3.27885 (* 1 = 3.27885 loss)
I0614 16:16:15.377792  5063 solver.cpp:473] Iteration 11020, lr = 0.0001
I0614 16:16:16.260912  5063 solver.cpp:213] Iteration 11030, loss = 3.27523
I0614 16:16:16.260936  5063 solver.cpp:228]     Train net output #0: softmax = 3.27523 (* 1 = 3.27523 loss)
I0614 16:16:16.260941  5063 solver.cpp:473] Iteration 11030, lr = 0.0001
I0614 16:16:17.144697  5063 solver.cpp:213] Iteration 11040, loss = 3.25735
I0614 16:16:17.144717  5063 solver.cpp:228]     Train net output #0: softmax = 3.25735 (* 1 = 3.25735 loss)
I0614 16:16:17.144728  5063 solver.cpp:473] Iteration 11040, lr = 0.0001
I0614 16:16:18.027739  5063 solver.cpp:213] Iteration 11050, loss = 3.20619
I0614 16:16:18.027757  5063 solver.cpp:228]     Train net output #0: softmax = 3.20619 (* 1 = 3.20619 loss)
I0614 16:16:18.027762  5063 solver.cpp:473] Iteration 11050, lr = 0.0001
I0614 16:16:18.911206  5063 solver.cpp:213] Iteration 11060, loss = 3.52422
I0614 16:16:18.911223  5063 solver.cpp:228]     Train net output #0: softmax = 3.52422 (* 1 = 3.52422 loss)
I0614 16:16:18.911228  5063 solver.cpp:473] Iteration 11060, lr = 0.0001
I0614 16:16:19.794694  5063 solver.cpp:213] Iteration 11070, loss = 3.47627
I0614 16:16:19.794713  5063 solver.cpp:228]     Train net output #0: softmax = 3.47627 (* 1 = 3.47627 loss)
I0614 16:16:19.794718  5063 solver.cpp:473] Iteration 11070, lr = 0.0001
I0614 16:16:20.677978  5063 solver.cpp:213] Iteration 11080, loss = 3.28311
I0614 16:16:20.677999  5063 solver.cpp:228]     Train net output #0: softmax = 3.28311 (* 1 = 3.28311 loss)
I0614 16:16:20.678004  5063 solver.cpp:473] Iteration 11080, lr = 0.0001
I0614 16:16:21.561007  5063 solver.cpp:213] Iteration 11090, loss = 3.40513
I0614 16:16:21.561034  5063 solver.cpp:228]     Train net output #0: softmax = 3.40513 (* 1 = 3.40513 loss)
I0614 16:16:21.561040  5063 solver.cpp:473] Iteration 11090, lr = 0.0001
I0614 16:16:22.444037  5063 solver.cpp:213] Iteration 11100, loss = 3.15545
I0614 16:16:22.444058  5063 solver.cpp:228]     Train net output #0: softmax = 3.15545 (* 1 = 3.15545 loss)
I0614 16:16:22.444063  5063 solver.cpp:473] Iteration 11100, lr = 0.0001
I0614 16:16:23.317461  5063 solver.cpp:213] Iteration 11110, loss = 3.39649
I0614 16:16:23.317481  5063 solver.cpp:228]     Train net output #0: softmax = 3.39649 (* 1 = 3.39649 loss)
I0614 16:16:23.317487  5063 solver.cpp:473] Iteration 11110, lr = 0.0001
I0614 16:16:24.195947  5063 solver.cpp:213] Iteration 11120, loss = 3.25511
I0614 16:16:24.195963  5063 solver.cpp:228]     Train net output #0: softmax = 3.25511 (* 1 = 3.25511 loss)
I0614 16:16:24.195967  5063 solver.cpp:473] Iteration 11120, lr = 0.0001
I0614 16:16:25.079113  5063 solver.cpp:213] Iteration 11130, loss = 3.13278
I0614 16:16:25.079133  5063 solver.cpp:228]     Train net output #0: softmax = 3.13278 (* 1 = 3.13278 loss)
I0614 16:16:25.079138  5063 solver.cpp:473] Iteration 11130, lr = 0.0001
I0614 16:16:25.962452  5063 solver.cpp:213] Iteration 11140, loss = 3.2769
I0614 16:16:25.962477  5063 solver.cpp:228]     Train net output #0: softmax = 3.2769 (* 1 = 3.2769 loss)
I0614 16:16:25.962482  5063 solver.cpp:473] Iteration 11140, lr = 0.0001
I0614 16:16:26.844733  5063 solver.cpp:213] Iteration 11150, loss = 3.09753
I0614 16:16:26.844753  5063 solver.cpp:228]     Train net output #0: softmax = 3.09753 (* 1 = 3.09753 loss)
I0614 16:16:26.844758  5063 solver.cpp:473] Iteration 11150, lr = 0.0001
I0614 16:16:27.726730  5063 solver.cpp:213] Iteration 11160, loss = 3.33
I0614 16:16:27.726749  5063 solver.cpp:228]     Train net output #0: softmax = 3.33 (* 1 = 3.33 loss)
I0614 16:16:27.726872  5063 solver.cpp:473] Iteration 11160, lr = 0.0001
I0614 16:16:28.609623  5063 solver.cpp:213] Iteration 11170, loss = 3.04032
I0614 16:16:28.609642  5063 solver.cpp:228]     Train net output #0: softmax = 3.04032 (* 1 = 3.04032 loss)
I0614 16:16:28.609661  5063 solver.cpp:473] Iteration 11170, lr = 0.0001
I0614 16:16:29.492578  5063 solver.cpp:213] Iteration 11180, loss = 3.47436
I0614 16:16:29.492593  5063 solver.cpp:228]     Train net output #0: softmax = 3.47436 (* 1 = 3.47436 loss)
I0614 16:16:29.492597  5063 solver.cpp:473] Iteration 11180, lr = 0.0001
I0614 16:16:30.376103  5063 solver.cpp:213] Iteration 11190, loss = 3.31295
I0614 16:16:30.376121  5063 solver.cpp:228]     Train net output #0: softmax = 3.31295 (* 1 = 3.31295 loss)
I0614 16:16:30.376126  5063 solver.cpp:473] Iteration 11190, lr = 0.0001
I0614 16:16:31.258342  5063 solver.cpp:213] Iteration 11200, loss = 3.33443
I0614 16:16:31.258363  5063 solver.cpp:228]     Train net output #0: softmax = 3.33443 (* 1 = 3.33443 loss)
I0614 16:16:31.258368  5063 solver.cpp:473] Iteration 11200, lr = 0.0001
I0614 16:16:32.141506  5063 solver.cpp:213] Iteration 11210, loss = 3.47018
I0614 16:16:32.141523  5063 solver.cpp:228]     Train net output #0: softmax = 3.47018 (* 1 = 3.47018 loss)
I0614 16:16:32.141528  5063 solver.cpp:473] Iteration 11210, lr = 0.0001
I0614 16:16:33.024859  5063 solver.cpp:213] Iteration 11220, loss = 3.16624
I0614 16:16:33.025028  5063 solver.cpp:228]     Train net output #0: softmax = 3.16624 (* 1 = 3.16624 loss)
I0614 16:16:33.025035  5063 solver.cpp:473] Iteration 11220, lr = 0.0001
I0614 16:16:33.908915  5063 solver.cpp:213] Iteration 11230, loss = 3.44252
I0614 16:16:33.908932  5063 solver.cpp:228]     Train net output #0: softmax = 3.44252 (* 1 = 3.44252 loss)
I0614 16:16:33.908937  5063 solver.cpp:473] Iteration 11230, lr = 0.0001
I0614 16:16:34.792189  5063 solver.cpp:213] Iteration 11240, loss = 3.3862
I0614 16:16:34.792207  5063 solver.cpp:228]     Train net output #0: softmax = 3.3862 (* 1 = 3.3862 loss)
I0614 16:16:34.792212  5063 solver.cpp:473] Iteration 11240, lr = 0.0001
I0614 16:16:35.674739  5063 solver.cpp:213] Iteration 11250, loss = 3.34134
I0614 16:16:35.674759  5063 solver.cpp:228]     Train net output #0: softmax = 3.34134 (* 1 = 3.34134 loss)
I0614 16:16:35.674764  5063 solver.cpp:473] Iteration 11250, lr = 0.0001
I0614 16:16:36.557920  5063 solver.cpp:213] Iteration 11260, loss = 3.12999
I0614 16:16:36.557940  5063 solver.cpp:228]     Train net output #0: softmax = 3.12999 (* 1 = 3.12999 loss)
I0614 16:16:36.557945  5063 solver.cpp:473] Iteration 11260, lr = 0.0001
I0614 16:16:37.440891  5063 solver.cpp:213] Iteration 11270, loss = 3.27465
I0614 16:16:37.440907  5063 solver.cpp:228]     Train net output #0: softmax = 3.27465 (* 1 = 3.27465 loss)
I0614 16:16:37.440912  5063 solver.cpp:473] Iteration 11270, lr = 0.0001
I0614 16:16:38.322964  5063 solver.cpp:213] Iteration 11280, loss = 3.30782
I0614 16:16:38.322984  5063 solver.cpp:228]     Train net output #0: softmax = 3.30782 (* 1 = 3.30782 loss)
I0614 16:16:38.323123  5063 solver.cpp:473] Iteration 11280, lr = 0.0001
I0614 16:16:39.206056  5063 solver.cpp:213] Iteration 11290, loss = 3.3016
I0614 16:16:39.206073  5063 solver.cpp:228]     Train net output #0: softmax = 3.3016 (* 1 = 3.3016 loss)
I0614 16:16:39.206079  5063 solver.cpp:473] Iteration 11290, lr = 0.0001
I0614 16:16:40.089388  5063 solver.cpp:213] Iteration 11300, loss = 3.50012
I0614 16:16:40.089408  5063 solver.cpp:228]     Train net output #0: softmax = 3.50012 (* 1 = 3.50012 loss)
I0614 16:16:40.089413  5063 solver.cpp:473] Iteration 11300, lr = 0.0001
I0614 16:16:40.972851  5063 solver.cpp:213] Iteration 11310, loss = 3.19378
I0614 16:16:40.972873  5063 solver.cpp:228]     Train net output #0: softmax = 3.19378 (* 1 = 3.19378 loss)
I0614 16:16:40.972878  5063 solver.cpp:473] Iteration 11310, lr = 0.0001
I0614 16:16:41.856340  5063 solver.cpp:213] Iteration 11320, loss = 3.0581
I0614 16:16:41.856358  5063 solver.cpp:228]     Train net output #0: softmax = 3.0581 (* 1 = 3.0581 loss)
I0614 16:16:41.856362  5063 solver.cpp:473] Iteration 11320, lr = 0.0001
I0614 16:16:42.738085  5063 solver.cpp:213] Iteration 11330, loss = 3.18761
I0614 16:16:42.738106  5063 solver.cpp:228]     Train net output #0: softmax = 3.18761 (* 1 = 3.18761 loss)
I0614 16:16:42.738112  5063 solver.cpp:473] Iteration 11330, lr = 0.0001
I0614 16:16:43.621079  5063 solver.cpp:213] Iteration 11340, loss = 3.19481
I0614 16:16:43.621096  5063 solver.cpp:228]     Train net output #0: softmax = 3.19481 (* 1 = 3.19481 loss)
I0614 16:16:43.621100  5063 solver.cpp:473] Iteration 11340, lr = 0.0001
I0614 16:16:44.503340  5063 solver.cpp:213] Iteration 11350, loss = 3.29104
I0614 16:16:44.503355  5063 solver.cpp:228]     Train net output #0: softmax = 3.29104 (* 1 = 3.29104 loss)
I0614 16:16:44.503360  5063 solver.cpp:473] Iteration 11350, lr = 0.0001
I0614 16:16:45.386924  5063 solver.cpp:213] Iteration 11360, loss = 3.25345
I0614 16:16:45.386943  5063 solver.cpp:228]     Train net output #0: softmax = 3.25345 (* 1 = 3.25345 loss)
I0614 16:16:45.386948  5063 solver.cpp:473] Iteration 11360, lr = 0.0001
I0614 16:16:46.270444  5063 solver.cpp:213] Iteration 11370, loss = 3.36485
I0614 16:16:46.270468  5063 solver.cpp:228]     Train net output #0: softmax = 3.36485 (* 1 = 3.36485 loss)
I0614 16:16:46.270473  5063 solver.cpp:473] Iteration 11370, lr = 0.0001
I0614 16:16:47.153003  5063 solver.cpp:213] Iteration 11380, loss = 3.31135
I0614 16:16:47.153038  5063 solver.cpp:228]     Train net output #0: softmax = 3.31135 (* 1 = 3.31135 loss)
I0614 16:16:47.153043  5063 solver.cpp:473] Iteration 11380, lr = 0.0001
I0614 16:16:48.035733  5063 solver.cpp:213] Iteration 11390, loss = 3.14004
I0614 16:16:48.035751  5063 solver.cpp:228]     Train net output #0: softmax = 3.14004 (* 1 = 3.14004 loss)
I0614 16:16:48.035756  5063 solver.cpp:473] Iteration 11390, lr = 0.0001
I0614 16:16:48.919008  5063 solver.cpp:213] Iteration 11400, loss = 3.3457
I0614 16:16:48.919029  5063 solver.cpp:228]     Train net output #0: softmax = 3.3457 (* 1 = 3.3457 loss)
I0614 16:16:48.919217  5063 solver.cpp:473] Iteration 11400, lr = 0.0001
I0614 16:16:49.802131  5063 solver.cpp:213] Iteration 11410, loss = 3.2149
I0614 16:16:49.802147  5063 solver.cpp:228]     Train net output #0: softmax = 3.2149 (* 1 = 3.2149 loss)
I0614 16:16:49.802152  5063 solver.cpp:473] Iteration 11410, lr = 0.0001
I0614 16:16:50.684936  5063 solver.cpp:213] Iteration 11420, loss = 3.19167
I0614 16:16:50.684952  5063 solver.cpp:228]     Train net output #0: softmax = 3.19167 (* 1 = 3.19167 loss)
I0614 16:16:50.684957  5063 solver.cpp:473] Iteration 11420, lr = 0.0001
I0614 16:16:51.567885  5063 solver.cpp:213] Iteration 11430, loss = 2.98076
I0614 16:16:51.567905  5063 solver.cpp:228]     Train net output #0: softmax = 2.98076 (* 1 = 2.98076 loss)
I0614 16:16:51.567910  5063 solver.cpp:473] Iteration 11430, lr = 0.0001
I0614 16:16:52.450458  5063 solver.cpp:213] Iteration 11440, loss = 3.3577
I0614 16:16:52.450472  5063 solver.cpp:228]     Train net output #0: softmax = 3.3577 (* 1 = 3.3577 loss)
I0614 16:16:52.450477  5063 solver.cpp:473] Iteration 11440, lr = 0.0001
I0614 16:16:53.332962  5063 solver.cpp:213] Iteration 11450, loss = 3.33702
I0614 16:16:53.332978  5063 solver.cpp:228]     Train net output #0: softmax = 3.33702 (* 1 = 3.33702 loss)
I0614 16:16:53.332983  5063 solver.cpp:473] Iteration 11450, lr = 0.0001
I0614 16:16:54.216074  5063 solver.cpp:213] Iteration 11460, loss = 3.30454
I0614 16:16:54.216095  5063 solver.cpp:228]     Train net output #0: softmax = 3.30454 (* 1 = 3.30454 loss)
I0614 16:16:54.216223  5063 solver.cpp:473] Iteration 11460, lr = 0.0001
I0614 16:16:55.099655  5063 solver.cpp:213] Iteration 11470, loss = 3.13654
I0614 16:16:55.099673  5063 solver.cpp:228]     Train net output #0: softmax = 3.13654 (* 1 = 3.13654 loss)
I0614 16:16:55.099678  5063 solver.cpp:473] Iteration 11470, lr = 0.0001
I0614 16:16:55.982725  5063 solver.cpp:213] Iteration 11480, loss = 3.51151
I0614 16:16:55.982746  5063 solver.cpp:228]     Train net output #0: softmax = 3.51151 (* 1 = 3.51151 loss)
I0614 16:16:55.982751  5063 solver.cpp:473] Iteration 11480, lr = 0.0001
I0614 16:16:56.865532  5063 solver.cpp:213] Iteration 11490, loss = 3.06772
I0614 16:16:56.865553  5063 solver.cpp:228]     Train net output #0: softmax = 3.06772 (* 1 = 3.06772 loss)
I0614 16:16:56.865559  5063 solver.cpp:473] Iteration 11490, lr = 0.0001
I0614 16:16:57.748946  5063 solver.cpp:213] Iteration 11500, loss = 3.37392
I0614 16:16:57.748963  5063 solver.cpp:228]     Train net output #0: softmax = 3.37392 (* 1 = 3.37392 loss)
I0614 16:16:57.748968  5063 solver.cpp:473] Iteration 11500, lr = 0.0001
I0614 16:16:58.632257  5063 solver.cpp:213] Iteration 11510, loss = 3.35523
I0614 16:16:58.632277  5063 solver.cpp:228]     Train net output #0: softmax = 3.35523 (* 1 = 3.35523 loss)
I0614 16:16:58.632282  5063 solver.cpp:473] Iteration 11510, lr = 0.0001
I0614 16:16:59.514710  5063 solver.cpp:213] Iteration 11520, loss = 3.36677
I0614 16:16:59.514729  5063 solver.cpp:228]     Train net output #0: softmax = 3.36677 (* 1 = 3.36677 loss)
I0614 16:16:59.514734  5063 solver.cpp:473] Iteration 11520, lr = 0.0001
I0614 16:17:00.397688  5063 solver.cpp:213] Iteration 11530, loss = 3.24947
I0614 16:17:00.397706  5063 solver.cpp:228]     Train net output #0: softmax = 3.24947 (* 1 = 3.24947 loss)
I0614 16:17:00.397712  5063 solver.cpp:473] Iteration 11530, lr = 0.0001
I0614 16:17:01.281957  5063 solver.cpp:213] Iteration 11540, loss = 3.2341
I0614 16:17:01.281992  5063 solver.cpp:228]     Train net output #0: softmax = 3.2341 (* 1 = 3.2341 loss)
I0614 16:17:01.281997  5063 solver.cpp:473] Iteration 11540, lr = 0.0001
I0614 16:17:02.165246  5063 solver.cpp:213] Iteration 11550, loss = 3.31992
I0614 16:17:02.165271  5063 solver.cpp:228]     Train net output #0: softmax = 3.31992 (* 1 = 3.31992 loss)
I0614 16:17:02.165277  5063 solver.cpp:473] Iteration 11550, lr = 0.0001
I0614 16:17:03.043376  5063 solver.cpp:213] Iteration 11560, loss = 2.95858
I0614 16:17:03.043423  5063 solver.cpp:228]     Train net output #0: softmax = 2.95858 (* 1 = 2.95858 loss)
I0614 16:17:03.043429  5063 solver.cpp:473] Iteration 11560, lr = 0.0001
I0614 16:17:03.926723  5063 solver.cpp:213] Iteration 11570, loss = 3.40132
I0614 16:17:03.926743  5063 solver.cpp:228]     Train net output #0: softmax = 3.40132 (* 1 = 3.40132 loss)
I0614 16:17:03.926748  5063 solver.cpp:473] Iteration 11570, lr = 0.0001
I0614 16:17:04.809367  5063 solver.cpp:213] Iteration 11580, loss = 3.33853
I0614 16:17:04.809384  5063 solver.cpp:228]     Train net output #0: softmax = 3.33853 (* 1 = 3.33853 loss)
I0614 16:17:04.809396  5063 solver.cpp:473] Iteration 11580, lr = 0.0001
I0614 16:17:05.693089  5063 solver.cpp:213] Iteration 11590, loss = 3.36589
I0614 16:17:05.693106  5063 solver.cpp:228]     Train net output #0: softmax = 3.36589 (* 1 = 3.36589 loss)
I0614 16:17:05.693111  5063 solver.cpp:473] Iteration 11590, lr = 0.0001
I0614 16:17:06.576575  5063 solver.cpp:213] Iteration 11600, loss = 3.39733
I0614 16:17:06.576592  5063 solver.cpp:228]     Train net output #0: softmax = 3.39733 (* 1 = 3.39733 loss)
I0614 16:17:06.576597  5063 solver.cpp:473] Iteration 11600, lr = 0.0001
I0614 16:17:07.460702  5063 solver.cpp:213] Iteration 11610, loss = 3.08171
I0614 16:17:07.460727  5063 solver.cpp:228]     Train net output #0: softmax = 3.08171 (* 1 = 3.08171 loss)
I0614 16:17:07.460732  5063 solver.cpp:473] Iteration 11610, lr = 0.0001
I0614 16:17:08.344261  5063 solver.cpp:213] Iteration 11620, loss = 2.99555
I0614 16:17:08.344280  5063 solver.cpp:228]     Train net output #0: softmax = 2.99555 (* 1 = 2.99555 loss)
I0614 16:17:08.344285  5063 solver.cpp:473] Iteration 11620, lr = 0.0001
I0614 16:17:09.227002  5063 solver.cpp:213] Iteration 11630, loss = 3.16738
I0614 16:17:09.227020  5063 solver.cpp:228]     Train net output #0: softmax = 3.16738 (* 1 = 3.16738 loss)
I0614 16:17:09.227023  5063 solver.cpp:473] Iteration 11630, lr = 0.0001
I0614 16:17:10.110229  5063 solver.cpp:213] Iteration 11640, loss = 3.25068
I0614 16:17:10.110249  5063 solver.cpp:228]     Train net output #0: softmax = 3.25068 (* 1 = 3.25068 loss)
I0614 16:17:10.110258  5063 solver.cpp:473] Iteration 11640, lr = 0.0001
I0614 16:17:10.993767  5063 solver.cpp:213] Iteration 11650, loss = 3.18098
I0614 16:17:10.993783  5063 solver.cpp:228]     Train net output #0: softmax = 3.18098 (* 1 = 3.18098 loss)
I0614 16:17:10.993788  5063 solver.cpp:473] Iteration 11650, lr = 0.0001
I0614 16:17:11.877200  5063 solver.cpp:213] Iteration 11660, loss = 3.17034
I0614 16:17:11.877223  5063 solver.cpp:228]     Train net output #0: softmax = 3.17034 (* 1 = 3.17034 loss)
I0614 16:17:11.877228  5063 solver.cpp:473] Iteration 11660, lr = 0.0001
I0614 16:17:12.760651  5063 solver.cpp:213] Iteration 11670, loss = 3.29636
I0614 16:17:12.760671  5063 solver.cpp:228]     Train net output #0: softmax = 3.29636 (* 1 = 3.29636 loss)
I0614 16:17:12.760676  5063 solver.cpp:473] Iteration 11670, lr = 0.0001
I0614 16:17:13.643676  5063 solver.cpp:213] Iteration 11680, loss = 3.44954
I0614 16:17:13.643693  5063 solver.cpp:228]     Train net output #0: softmax = 3.44954 (* 1 = 3.44954 loss)
I0614 16:17:13.643698  5063 solver.cpp:473] Iteration 11680, lr = 0.0001
I0614 16:17:14.527052  5063 solver.cpp:213] Iteration 11690, loss = 3.35943
I0614 16:17:14.527072  5063 solver.cpp:228]     Train net output #0: softmax = 3.35943 (* 1 = 3.35943 loss)
I0614 16:17:14.527077  5063 solver.cpp:473] Iteration 11690, lr = 0.0001
I0614 16:17:15.409878  5063 solver.cpp:213] Iteration 11700, loss = 3.17423
I0614 16:17:15.409895  5063 solver.cpp:228]     Train net output #0: softmax = 3.17423 (* 1 = 3.17423 loss)
I0614 16:17:15.409900  5063 solver.cpp:473] Iteration 11700, lr = 0.0001
I0614 16:17:16.293179  5063 solver.cpp:213] Iteration 11710, loss = 3.26676
I0614 16:17:16.293197  5063 solver.cpp:228]     Train net output #0: softmax = 3.26676 (* 1 = 3.26676 loss)
I0614 16:17:16.293202  5063 solver.cpp:473] Iteration 11710, lr = 0.0001
I0614 16:17:17.175833  5063 solver.cpp:213] Iteration 11720, loss = 3.36336
I0614 16:17:17.175869  5063 solver.cpp:228]     Train net output #0: softmax = 3.36336 (* 1 = 3.36336 loss)
I0614 16:17:17.175875  5063 solver.cpp:473] Iteration 11720, lr = 0.0001
I0614 16:17:18.058173  5063 solver.cpp:213] Iteration 11730, loss = 3.05944
I0614 16:17:18.058190  5063 solver.cpp:228]     Train net output #0: softmax = 3.05944 (* 1 = 3.05944 loss)
I0614 16:17:18.058195  5063 solver.cpp:473] Iteration 11730, lr = 0.0001
I0614 16:17:18.941409  5063 solver.cpp:213] Iteration 11740, loss = 2.97943
I0614 16:17:18.941427  5063 solver.cpp:228]     Train net output #0: softmax = 2.97943 (* 1 = 2.97943 loss)
I0614 16:17:18.941432  5063 solver.cpp:473] Iteration 11740, lr = 0.0001
I0614 16:17:19.824240  5063 solver.cpp:213] Iteration 11750, loss = 3.34828
I0614 16:17:19.824256  5063 solver.cpp:228]     Train net output #0: softmax = 3.34828 (* 1 = 3.34828 loss)
I0614 16:17:19.824261  5063 solver.cpp:473] Iteration 11750, lr = 0.0001
I0614 16:17:20.708302  5063 solver.cpp:213] Iteration 11760, loss = 3.33658
I0614 16:17:20.708323  5063 solver.cpp:228]     Train net output #0: softmax = 3.33658 (* 1 = 3.33658 loss)
I0614 16:17:20.708333  5063 solver.cpp:473] Iteration 11760, lr = 0.0001
I0614 16:17:21.592310  5063 solver.cpp:213] Iteration 11770, loss = 3.41946
I0614 16:17:21.592330  5063 solver.cpp:228]     Train net output #0: softmax = 3.41946 (* 1 = 3.41946 loss)
I0614 16:17:21.592335  5063 solver.cpp:473] Iteration 11770, lr = 0.0001
I0614 16:17:22.475913  5063 solver.cpp:213] Iteration 11780, loss = 3.34489
I0614 16:17:22.475934  5063 solver.cpp:228]     Train net output #0: softmax = 3.34489 (* 1 = 3.34489 loss)
I0614 16:17:22.475939  5063 solver.cpp:473] Iteration 11780, lr = 0.0001
I0614 16:17:23.358935  5063 solver.cpp:213] Iteration 11790, loss = 3.24149
I0614 16:17:23.358955  5063 solver.cpp:228]     Train net output #0: softmax = 3.24149 (* 1 = 3.24149 loss)
I0614 16:17:23.358960  5063 solver.cpp:473] Iteration 11790, lr = 0.0001
I0614 16:17:24.242794  5063 solver.cpp:213] Iteration 11800, loss = 3.00602
I0614 16:17:24.242810  5063 solver.cpp:228]     Train net output #0: softmax = 3.00602 (* 1 = 3.00602 loss)
I0614 16:17:24.242815  5063 solver.cpp:473] Iteration 11800, lr = 0.0001
I0614 16:17:25.125735  5063 solver.cpp:213] Iteration 11810, loss = 3.22889
I0614 16:17:25.125752  5063 solver.cpp:228]     Train net output #0: softmax = 3.22889 (* 1 = 3.22889 loss)
I0614 16:17:25.125757  5063 solver.cpp:473] Iteration 11810, lr = 0.0001
I0614 16:17:26.008648  5063 solver.cpp:213] Iteration 11820, loss = 3.16237
I0614 16:17:26.008667  5063 solver.cpp:228]     Train net output #0: softmax = 3.16237 (* 1 = 3.16237 loss)
I0614 16:17:26.008677  5063 solver.cpp:473] Iteration 11820, lr = 0.0001
I0614 16:17:26.892055  5063 solver.cpp:213] Iteration 11830, loss = 3.32544
I0614 16:17:26.892072  5063 solver.cpp:228]     Train net output #0: softmax = 3.32544 (* 1 = 3.32544 loss)
I0614 16:17:26.892076  5063 solver.cpp:473] Iteration 11830, lr = 0.0001
I0614 16:17:27.774835  5063 solver.cpp:213] Iteration 11840, loss = 3.26196
I0614 16:17:27.774850  5063 solver.cpp:228]     Train net output #0: softmax = 3.26196 (* 1 = 3.26196 loss)
I0614 16:17:27.774855  5063 solver.cpp:473] Iteration 11840, lr = 0.0001
I0614 16:17:28.658211  5063 solver.cpp:213] Iteration 11850, loss = 3.09766
I0614 16:17:28.658234  5063 solver.cpp:228]     Train net output #0: softmax = 3.09766 (* 1 = 3.09766 loss)
I0614 16:17:28.658239  5063 solver.cpp:473] Iteration 11850, lr = 0.0001
I0614 16:17:29.541867  5063 solver.cpp:213] Iteration 11860, loss = 3.31108
I0614 16:17:29.541888  5063 solver.cpp:228]     Train net output #0: softmax = 3.31108 (* 1 = 3.31108 loss)
I0614 16:17:29.541899  5063 solver.cpp:473] Iteration 11860, lr = 0.0001
I0614 16:17:30.424834  5063 solver.cpp:213] Iteration 11870, loss = 3.30999
I0614 16:17:30.424849  5063 solver.cpp:228]     Train net output #0: softmax = 3.30999 (* 1 = 3.30999 loss)
I0614 16:17:30.424854  5063 solver.cpp:473] Iteration 11870, lr = 0.0001
I0614 16:17:31.308806  5063 solver.cpp:213] Iteration 11880, loss = 2.90766
I0614 16:17:31.308979  5063 solver.cpp:228]     Train net output #0: softmax = 2.90766 (* 1 = 2.90766 loss)
I0614 16:17:31.308987  5063 solver.cpp:473] Iteration 11880, lr = 0.0001
I0614 16:17:32.191642  5063 solver.cpp:213] Iteration 11890, loss = 3.2857
I0614 16:17:32.191658  5063 solver.cpp:228]     Train net output #0: softmax = 3.2857 (* 1 = 3.2857 loss)
I0614 16:17:32.191663  5063 solver.cpp:473] Iteration 11890, lr = 0.0001
I0614 16:17:33.074445  5063 solver.cpp:213] Iteration 11900, loss = 3.31316
I0614 16:17:33.074476  5063 solver.cpp:228]     Train net output #0: softmax = 3.31316 (* 1 = 3.31316 loss)
I0614 16:17:33.074482  5063 solver.cpp:473] Iteration 11900, lr = 0.0001
I0614 16:17:33.957332  5063 solver.cpp:213] Iteration 11910, loss = 3.58737
I0614 16:17:33.957353  5063 solver.cpp:228]     Train net output #0: softmax = 3.58737 (* 1 = 3.58737 loss)
I0614 16:17:33.957358  5063 solver.cpp:473] Iteration 11910, lr = 0.0001
I0614 16:17:34.840113  5063 solver.cpp:213] Iteration 11920, loss = 3.42161
I0614 16:17:34.840133  5063 solver.cpp:228]     Train net output #0: softmax = 3.42161 (* 1 = 3.42161 loss)
I0614 16:17:34.840139  5063 solver.cpp:473] Iteration 11920, lr = 0.0001
I0614 16:17:35.723273  5063 solver.cpp:213] Iteration 11930, loss = 3.24257
I0614 16:17:35.723291  5063 solver.cpp:228]     Train net output #0: softmax = 3.24257 (* 1 = 3.24257 loss)
I0614 16:17:35.723296  5063 solver.cpp:473] Iteration 11930, lr = 0.0001
I0614 16:17:36.607316  5063 solver.cpp:213] Iteration 11940, loss = 3.25529
I0614 16:17:36.607336  5063 solver.cpp:228]     Train net output #0: softmax = 3.25529 (* 1 = 3.25529 loss)
I0614 16:17:36.607342  5063 solver.cpp:473] Iteration 11940, lr = 0.0001
I0614 16:17:37.490396  5063 solver.cpp:213] Iteration 11950, loss = 3.06719
I0614 16:17:37.490412  5063 solver.cpp:228]     Train net output #0: softmax = 3.06719 (* 1 = 3.06719 loss)
I0614 16:17:37.490417  5063 solver.cpp:473] Iteration 11950, lr = 0.0001
I0614 16:17:38.373091  5063 solver.cpp:213] Iteration 11960, loss = 3.28306
I0614 16:17:38.373109  5063 solver.cpp:228]     Train net output #0: softmax = 3.28306 (* 1 = 3.28306 loss)
I0614 16:17:38.373114  5063 solver.cpp:473] Iteration 11960, lr = 0.0001
I0614 16:17:39.256575  5063 solver.cpp:213] Iteration 11970, loss = 3.1086
I0614 16:17:39.256595  5063 solver.cpp:228]     Train net output #0: softmax = 3.1086 (* 1 = 3.1086 loss)
I0614 16:17:39.256600  5063 solver.cpp:473] Iteration 11970, lr = 0.0001
I0614 16:17:40.139224  5063 solver.cpp:213] Iteration 11980, loss = 3.17396
I0614 16:17:40.139242  5063 solver.cpp:228]     Train net output #0: softmax = 3.17396 (* 1 = 3.17396 loss)
I0614 16:17:40.139247  5063 solver.cpp:473] Iteration 11980, lr = 0.0001
I0614 16:17:41.022778  5063 solver.cpp:213] Iteration 11990, loss = 3.35656
I0614 16:17:41.022796  5063 solver.cpp:228]     Train net output #0: softmax = 3.35656 (* 1 = 3.35656 loss)
I0614 16:17:41.022800  5063 solver.cpp:473] Iteration 11990, lr = 0.0001
I0614 16:17:41.847596  5063 solver.cpp:362] Snapshotting to snapshots/16-06-14_15h51m21s_0_11_pretrainClassification_iter_12000.caffemodel
I0614 16:17:41.848387  5063 solver.cpp:370] Snapshotting solver state to snapshots/16-06-14_15h51m21s_0_11_pretrainClassification_iter_12000.solverstate
I0614 16:17:41.848816  5063 solver.cpp:291] Iteration 12000, Testing net (#0)
I0614 16:17:41.961685  5063 solver.cpp:342]     Test net output #0: accuracy_top_1 = 0.198437
I0614 16:17:41.961701  5063 solver.cpp:342]     Test net output #1: accuracy_top_5 = 0.484375
I0614 16:17:41.961707  5063 solver.cpp:342]     Test net output #2: softmax = 3.31881 (* 1 = 3.31881 loss)
I0614 16:17:42.019893  5063 solver.cpp:213] Iteration 12000, loss = 2.86543
I0614 16:17:42.019912  5063 solver.cpp:228]     Train net output #0: softmax = 2.86543 (* 1 = 2.86543 loss)
I0614 16:17:42.019918  5063 solver.cpp:473] Iteration 12000, lr = 0.0001
I0614 16:17:42.899487  5063 solver.cpp:213] Iteration 12010, loss = 3.09303
I0614 16:17:42.899510  5063 solver.cpp:228]     Train net output #0: softmax = 3.09303 (* 1 = 3.09303 loss)
I0614 16:17:42.899516  5063 solver.cpp:473] Iteration 12010, lr = 0.0001
I0614 16:17:43.773211  5063 solver.cpp:213] Iteration 12020, loss = 3.39761
I0614 16:17:43.773232  5063 solver.cpp:228]     Train net output #0: softmax = 3.39761 (* 1 = 3.39761 loss)
I0614 16:17:43.773237  5063 solver.cpp:473] Iteration 12020, lr = 0.0001
I0614 16:17:44.655683  5063 solver.cpp:213] Iteration 12030, loss = 3.31114
I0614 16:17:44.655700  5063 solver.cpp:228]     Train net output #0: softmax = 3.31114 (* 1 = 3.31114 loss)
I0614 16:17:44.655721  5063 solver.cpp:473] Iteration 12030, lr = 0.0001
I0614 16:17:45.538949  5063 solver.cpp:213] Iteration 12040, loss = 3.35338
I0614 16:17:45.538965  5063 solver.cpp:228]     Train net output #0: softmax = 3.35338 (* 1 = 3.35338 loss)
I0614 16:17:45.538969  5063 solver.cpp:473] Iteration 12040, lr = 0.0001
I0614 16:17:46.422402  5063 solver.cpp:213] Iteration 12050, loss = 3.41125
I0614 16:17:46.422420  5063 solver.cpp:228]     Train net output #0: softmax = 3.41125 (* 1 = 3.41125 loss)
I0614 16:17:46.422425  5063 solver.cpp:473] Iteration 12050, lr = 0.0001
I0614 16:17:47.305408  5063 solver.cpp:213] Iteration 12060, loss = 3.31041
I0614 16:17:47.305429  5063 solver.cpp:228]     Train net output #0: softmax = 3.31041 (* 1 = 3.31041 loss)
I0614 16:17:47.305439  5063 solver.cpp:473] Iteration 12060, lr = 0.0001
I0614 16:17:48.187829  5063 solver.cpp:213] Iteration 12070, loss = 3.23469
I0614 16:17:48.187849  5063 solver.cpp:228]     Train net output #0: softmax = 3.23469 (* 1 = 3.23469 loss)
I0614 16:17:48.187854  5063 solver.cpp:473] Iteration 12070, lr = 0.0001
I0614 16:17:49.070832  5063 solver.cpp:213] Iteration 12080, loss = 3.28291
I0614 16:17:49.070854  5063 solver.cpp:228]     Train net output #0: softmax = 3.28291 (* 1 = 3.28291 loss)
I0614 16:17:49.070859  5063 solver.cpp:473] Iteration 12080, lr = 0.0001
I0614 16:17:49.953485  5063 solver.cpp:213] Iteration 12090, loss = 3.37465
I0614 16:17:49.953500  5063 solver.cpp:228]     Train net output #0: softmax = 3.37465 (* 1 = 3.37465 loss)
I0614 16:17:49.953505  5063 solver.cpp:473] Iteration 12090, lr = 0.0001
I0614 16:17:50.836730  5063 solver.cpp:213] Iteration 12100, loss = 3.16787
I0614 16:17:50.836746  5063 solver.cpp:228]     Train net output #0: softmax = 3.16787 (* 1 = 3.16787 loss)
I0614 16:17:50.836751  5063 solver.cpp:473] Iteration 12100, lr = 0.0001
I0614 16:17:51.719682  5063 solver.cpp:213] Iteration 12110, loss = 3.42803
I0614 16:17:51.719698  5063 solver.cpp:228]     Train net output #0: softmax = 3.42803 (* 1 = 3.42803 loss)
I0614 16:17:51.719703  5063 solver.cpp:473] Iteration 12110, lr = 0.0001
I0614 16:17:52.601563  5063 solver.cpp:213] Iteration 12120, loss = 3.08341
I0614 16:17:52.601579  5063 solver.cpp:228]     Train net output #0: softmax = 3.08341 (* 1 = 3.08341 loss)
I0614 16:17:52.601584  5063 solver.cpp:473] Iteration 12120, lr = 0.0001
I0614 16:17:53.484513  5063 solver.cpp:213] Iteration 12130, loss = 2.97546
I0614 16:17:53.484531  5063 solver.cpp:228]     Train net output #0: softmax = 2.97546 (* 1 = 2.97546 loss)
I0614 16:17:53.484535  5063 solver.cpp:473] Iteration 12130, lr = 0.0001
I0614 16:17:54.367374  5063 solver.cpp:213] Iteration 12140, loss = 3.4758
I0614 16:17:54.367394  5063 solver.cpp:228]     Train net output #0: softmax = 3.4758 (* 1 = 3.4758 loss)
I0614 16:17:54.367399  5063 solver.cpp:473] Iteration 12140, lr = 0.0001
I0614 16:17:55.251159  5063 solver.cpp:213] Iteration 12150, loss = 3.16767
I0614 16:17:55.251178  5063 solver.cpp:228]     Train net output #0: softmax = 3.16767 (* 1 = 3.16767 loss)
I0614 16:17:55.251183  5063 solver.cpp:473] Iteration 12150, lr = 0.0001
I0614 16:17:56.134997  5063 solver.cpp:213] Iteration 12160, loss = 3.37529
I0614 16:17:56.135026  5063 solver.cpp:228]     Train net output #0: softmax = 3.37529 (* 1 = 3.37529 loss)
I0614 16:17:56.135032  5063 solver.cpp:473] Iteration 12160, lr = 0.0001
I0614 16:17:57.017758  5063 solver.cpp:213] Iteration 12170, loss = 3.43167
I0614 16:17:57.017776  5063 solver.cpp:228]     Train net output #0: softmax = 3.43167 (* 1 = 3.43167 loss)
I0614 16:17:57.017781  5063 solver.cpp:473] Iteration 12170, lr = 0.0001
I0614 16:17:57.900940  5063 solver.cpp:213] Iteration 12180, loss = 3.43733
I0614 16:17:57.900962  5063 solver.cpp:228]     Train net output #0: softmax = 3.43733 (* 1 = 3.43733 loss)
I0614 16:17:57.901101  5063 solver.cpp:473] Iteration 12180, lr = 0.0001
I0614 16:17:58.783761  5063 solver.cpp:213] Iteration 12190, loss = 3.33992
I0614 16:17:58.783783  5063 solver.cpp:228]     Train net output #0: softmax = 3.33992 (* 1 = 3.33992 loss)
I0614 16:17:58.783804  5063 solver.cpp:473] Iteration 12190, lr = 0.0001
I0614 16:17:59.667214  5063 solver.cpp:213] Iteration 12200, loss = 3.1929
I0614 16:17:59.667238  5063 solver.cpp:228]     Train net output #0: softmax = 3.1929 (* 1 = 3.1929 loss)
I0614 16:17:59.667243  5063 solver.cpp:473] Iteration 12200, lr = 0.0001
I0614 16:18:00.549976  5063 solver.cpp:213] Iteration 12210, loss = 3.3647
I0614 16:18:00.549993  5063 solver.cpp:228]     Train net output #0: softmax = 3.3647 (* 1 = 3.3647 loss)
I0614 16:18:00.549998  5063 solver.cpp:473] Iteration 12210, lr = 0.0001
I0614 16:18:01.432912  5063 solver.cpp:213] Iteration 12220, loss = 3.12229
I0614 16:18:01.432929  5063 solver.cpp:228]     Train net output #0: softmax = 3.12229 (* 1 = 3.12229 loss)
I0614 16:18:01.432934  5063 solver.cpp:473] Iteration 12220, lr = 0.0001
I0614 16:18:02.316536  5063 solver.cpp:213] Iteration 12230, loss = 3.11434
I0614 16:18:02.316555  5063 solver.cpp:228]     Train net output #0: softmax = 3.11434 (* 1 = 3.11434 loss)
I0614 16:18:02.316560  5063 solver.cpp:473] Iteration 12230, lr = 0.0001
I0614 16:18:03.199631  5063 solver.cpp:213] Iteration 12240, loss = 2.99826
I0614 16:18:03.199801  5063 solver.cpp:228]     Train net output #0: softmax = 2.99826 (* 1 = 2.99826 loss)
I0614 16:18:03.199810  5063 solver.cpp:473] Iteration 12240, lr = 0.0001
I0614 16:18:04.083281  5063 solver.cpp:213] Iteration 12250, loss = 3.15126
I0614 16:18:04.083300  5063 solver.cpp:228]     Train net output #0: softmax = 3.15126 (* 1 = 3.15126 loss)
I0614 16:18:04.083305  5063 solver.cpp:473] Iteration 12250, lr = 0.0001
I0614 16:18:04.966676  5063 solver.cpp:213] Iteration 12260, loss = 3.34402
I0614 16:18:04.966699  5063 solver.cpp:228]     Train net output #0: softmax = 3.34402 (* 1 = 3.34402 loss)
I0614 16:18:04.966704  5063 solver.cpp:473] Iteration 12260, lr = 0.0001
I0614 16:18:05.849380  5063 solver.cpp:213] Iteration 12270, loss = 3.15884
I0614 16:18:05.849395  5063 solver.cpp:228]     Train net output #0: softmax = 3.15884 (* 1 = 3.15884 loss)
I0614 16:18:05.849400  5063 solver.cpp:473] Iteration 12270, lr = 0.0001
I0614 16:18:06.732596  5063 solver.cpp:213] Iteration 12280, loss = 3.31879
I0614 16:18:06.732614  5063 solver.cpp:228]     Train net output #0: softmax = 3.31879 (* 1 = 3.31879 loss)
I0614 16:18:06.732620  5063 solver.cpp:473] Iteration 12280, lr = 0.0001
I0614 16:18:07.616675  5063 solver.cpp:213] Iteration 12290, loss = 3.21048
I0614 16:18:07.616693  5063 solver.cpp:228]     Train net output #0: softmax = 3.21048 (* 1 = 3.21048 loss)
I0614 16:18:07.616698  5063 solver.cpp:473] Iteration 12290, lr = 0.0001
I0614 16:18:08.500030  5063 solver.cpp:213] Iteration 12300, loss = 3.37963
I0614 16:18:08.500052  5063 solver.cpp:228]     Train net output #0: softmax = 3.37963 (* 1 = 3.37963 loss)
I0614 16:18:08.500058  5063 solver.cpp:473] Iteration 12300, lr = 0.0001
I0614 16:18:09.383049  5063 solver.cpp:213] Iteration 12310, loss = 3.111
I0614 16:18:09.383069  5063 solver.cpp:228]     Train net output #0: softmax = 3.111 (* 1 = 3.111 loss)
I0614 16:18:09.383074  5063 solver.cpp:473] Iteration 12310, lr = 0.0001
I0614 16:18:10.265949  5063 solver.cpp:213] Iteration 12320, loss = 3.2361
I0614 16:18:10.265967  5063 solver.cpp:228]     Train net output #0: softmax = 3.2361 (* 1 = 3.2361 loss)
I0614 16:18:10.265971  5063 solver.cpp:473] Iteration 12320, lr = 0.0001
I0614 16:18:11.148890  5063 solver.cpp:213] Iteration 12330, loss = 3.05325
I0614 16:18:11.148907  5063 solver.cpp:228]     Train net output #0: softmax = 3.05325 (* 1 = 3.05325 loss)
I0614 16:18:11.148911  5063 solver.cpp:473] Iteration 12330, lr = 0.0001
I0614 16:18:12.031813  5063 solver.cpp:213] Iteration 12340, loss = 2.98133
I0614 16:18:12.031831  5063 solver.cpp:228]     Train net output #0: softmax = 2.98133 (* 1 = 2.98133 loss)
I0614 16:18:12.031836  5063 solver.cpp:473] Iteration 12340, lr = 0.0001
I0614 16:18:12.915105  5063 solver.cpp:213] Iteration 12350, loss = 3.31296
I0614 16:18:12.915122  5063 solver.cpp:228]     Train net output #0: softmax = 3.31296 (* 1 = 3.31296 loss)
I0614 16:18:12.915127  5063 solver.cpp:473] Iteration 12350, lr = 0.0001
I0614 16:18:13.798336  5063 solver.cpp:213] Iteration 12360, loss = 3.17511
I0614 16:18:13.798357  5063 solver.cpp:228]     Train net output #0: softmax = 3.17511 (* 1 = 3.17511 loss)
I0614 16:18:13.798499  5063 solver.cpp:473] Iteration 12360, lr = 0.0001
I0614 16:18:14.681341  5063 solver.cpp:213] Iteration 12370, loss = 3.26096
I0614 16:18:14.681365  5063 solver.cpp:228]     Train net output #0: softmax = 3.26096 (* 1 = 3.26096 loss)
I0614 16:18:14.681370  5063 solver.cpp:473] Iteration 12370, lr = 0.0001
I0614 16:18:15.564271  5063 solver.cpp:213] Iteration 12380, loss = 3.37998
I0614 16:18:15.564291  5063 solver.cpp:228]     Train net output #0: softmax = 3.37998 (* 1 = 3.37998 loss)
I0614 16:18:15.564296  5063 solver.cpp:473] Iteration 12380, lr = 0.0001
I0614 16:18:16.447681  5063 solver.cpp:213] Iteration 12390, loss = 3.02924
I0614 16:18:16.447701  5063 solver.cpp:228]     Train net output #0: softmax = 3.02924 (* 1 = 3.02924 loss)
I0614 16:18:16.447705  5063 solver.cpp:473] Iteration 12390, lr = 0.0001
I0614 16:18:17.330329  5063 solver.cpp:213] Iteration 12400, loss = 3.19623
I0614 16:18:17.330359  5063 solver.cpp:228]     Train net output #0: softmax = 3.19623 (* 1 = 3.19623 loss)
I0614 16:18:17.330365  5063 solver.cpp:473] Iteration 12400, lr = 0.0001
I0614 16:18:18.213531  5063 solver.cpp:213] Iteration 12410, loss = 3.35089
I0614 16:18:18.213549  5063 solver.cpp:228]     Train net output #0: softmax = 3.35089 (* 1 = 3.35089 loss)
I0614 16:18:18.213554  5063 solver.cpp:473] Iteration 12410, lr = 0.0001
I0614 16:18:19.095813  5063 solver.cpp:213] Iteration 12420, loss = 3.13817
I0614 16:18:19.095831  5063 solver.cpp:228]     Train net output #0: softmax = 3.13817 (* 1 = 3.13817 loss)
I0614 16:18:19.095963  5063 solver.cpp:473] Iteration 12420, lr = 0.0001
I0614 16:18:19.979249  5063 solver.cpp:213] Iteration 12430, loss = 3.23974
I0614 16:18:19.979269  5063 solver.cpp:228]     Train net output #0: softmax = 3.23974 (* 1 = 3.23974 loss)
I0614 16:18:19.979274  5063 solver.cpp:473] Iteration 12430, lr = 0.0001
I0614 16:18:20.861958  5063 solver.cpp:213] Iteration 12440, loss = 3.42738
I0614 16:18:20.861974  5063 solver.cpp:228]     Train net output #0: softmax = 3.42738 (* 1 = 3.42738 loss)
I0614 16:18:20.861979  5063 solver.cpp:473] Iteration 12440, lr = 0.0001
I0614 16:18:21.744607  5063 solver.cpp:213] Iteration 12450, loss = 3.3602
I0614 16:18:21.744624  5063 solver.cpp:228]     Train net output #0: softmax = 3.3602 (* 1 = 3.3602 loss)
I0614 16:18:21.744629  5063 solver.cpp:473] Iteration 12450, lr = 0.0001
I0614 16:18:22.627053  5063 solver.cpp:213] Iteration 12460, loss = 3.13912
I0614 16:18:22.627070  5063 solver.cpp:228]     Train net output #0: softmax = 3.13912 (* 1 = 3.13912 loss)
I0614 16:18:22.627075  5063 solver.cpp:473] Iteration 12460, lr = 0.0001
I0614 16:18:23.510294  5063 solver.cpp:213] Iteration 12470, loss = 3.12974
I0614 16:18:23.510313  5063 solver.cpp:228]     Train net output #0: softmax = 3.12974 (* 1 = 3.12974 loss)
I0614 16:18:23.510318  5063 solver.cpp:473] Iteration 12470, lr = 0.0001
I0614 16:18:24.393293  5063 solver.cpp:213] Iteration 12480, loss = 3.19763
I0614 16:18:24.393317  5063 solver.cpp:228]     Train net output #0: softmax = 3.19763 (* 1 = 3.19763 loss)
I0614 16:18:24.393322  5063 solver.cpp:473] Iteration 12480, lr = 0.0001
I0614 16:18:25.276204  5063 solver.cpp:213] Iteration 12490, loss = 3.1401
I0614 16:18:25.276221  5063 solver.cpp:228]     Train net output #0: softmax = 3.1401 (* 1 = 3.1401 loss)
I0614 16:18:25.276226  5063 solver.cpp:473] Iteration 12490, lr = 0.0001
I0614 16:18:26.159349  5063 solver.cpp:213] Iteration 12500, loss = 3.29595
I0614 16:18:26.159365  5063 solver.cpp:228]     Train net output #0: softmax = 3.29595 (* 1 = 3.29595 loss)
I0614 16:18:26.159370  5063 solver.cpp:473] Iteration 12500, lr = 0.0001
I0614 16:18:27.042340  5063 solver.cpp:213] Iteration 12510, loss = 3.31426
I0614 16:18:27.042362  5063 solver.cpp:228]     Train net output #0: softmax = 3.31426 (* 1 = 3.31426 loss)
I0614 16:18:27.042367  5063 solver.cpp:473] Iteration 12510, lr = 0.0001
I0614 16:18:27.926121  5063 solver.cpp:213] Iteration 12520, loss = 3.21236
I0614 16:18:27.926138  5063 solver.cpp:228]     Train net output #0: softmax = 3.21236 (* 1 = 3.21236 loss)
I0614 16:18:27.926143  5063 solver.cpp:473] Iteration 12520, lr = 0.0001
I0614 16:18:28.809020  5063 solver.cpp:213] Iteration 12530, loss = 3.2953
I0614 16:18:28.809041  5063 solver.cpp:228]     Train net output #0: softmax = 3.2953 (* 1 = 3.2953 loss)
I0614 16:18:28.809046  5063 solver.cpp:473] Iteration 12530, lr = 0.0001
I0614 16:18:29.691946  5063 solver.cpp:213] Iteration 12540, loss = 3.30511
I0614 16:18:29.691972  5063 solver.cpp:228]     Train net output #0: softmax = 3.30511 (* 1 = 3.30511 loss)
I0614 16:18:29.692095  5063 solver.cpp:473] Iteration 12540, lr = 0.0001
I0614 16:18:30.574050  5063 solver.cpp:213] Iteration 12550, loss = 3.39821
I0614 16:18:30.574066  5063 solver.cpp:228]     Train net output #0: softmax = 3.39821 (* 1 = 3.39821 loss)
I0614 16:18:30.574070  5063 solver.cpp:473] Iteration 12550, lr = 0.0001
I0614 16:18:31.457228  5063 solver.cpp:213] Iteration 12560, loss = 3.42642
I0614 16:18:31.457260  5063 solver.cpp:228]     Train net output #0: softmax = 3.42642 (* 1 = 3.42642 loss)
I0614 16:18:31.457265  5063 solver.cpp:473] Iteration 12560, lr = 0.0001
I0614 16:18:32.340032  5063 solver.cpp:213] Iteration 12570, loss = 3.44186
I0614 16:18:32.340049  5063 solver.cpp:228]     Train net output #0: softmax = 3.44186 (* 1 = 3.44186 loss)
I0614 16:18:32.340054  5063 solver.cpp:473] Iteration 12570, lr = 0.0001
I0614 16:18:33.222823  5063 solver.cpp:213] Iteration 12580, loss = 3.12023
I0614 16:18:33.222854  5063 solver.cpp:228]     Train net output #0: softmax = 3.12023 (* 1 = 3.12023 loss)
I0614 16:18:33.222861  5063 solver.cpp:473] Iteration 12580, lr = 0.0001
I0614 16:18:34.105463  5063 solver.cpp:213] Iteration 12590, loss = 3.08421
I0614 16:18:34.105479  5063 solver.cpp:228]     Train net output #0: softmax = 3.08421 (* 1 = 3.08421 loss)
I0614 16:18:34.105484  5063 solver.cpp:473] Iteration 12590, lr = 0.0001
I0614 16:18:34.988711  5063 solver.cpp:213] Iteration 12600, loss = 3.15442
I0614 16:18:34.988737  5063 solver.cpp:228]     Train net output #0: softmax = 3.15442 (* 1 = 3.15442 loss)
I0614 16:18:34.988895  5063 solver.cpp:473] Iteration 12600, lr = 0.0001
I0614 16:18:35.871407  5063 solver.cpp:213] Iteration 12610, loss = 3.22245
I0614 16:18:35.871426  5063 solver.cpp:228]     Train net output #0: softmax = 3.22245 (* 1 = 3.22245 loss)
I0614 16:18:35.871431  5063 solver.cpp:473] Iteration 12610, lr = 0.0001
I0614 16:18:36.754503  5063 solver.cpp:213] Iteration 12620, loss = 3.20377
I0614 16:18:36.754520  5063 solver.cpp:228]     Train net output #0: softmax = 3.20377 (* 1 = 3.20377 loss)
I0614 16:18:36.754525  5063 solver.cpp:473] Iteration 12620, lr = 0.0001
I0614 16:18:37.637454  5063 solver.cpp:213] Iteration 12630, loss = 3.26266
I0614 16:18:37.637470  5063 solver.cpp:228]     Train net output #0: softmax = 3.26266 (* 1 = 3.26266 loss)
I0614 16:18:37.637475  5063 solver.cpp:473] Iteration 12630, lr = 0.0001
I0614 16:18:38.520663  5063 solver.cpp:213] Iteration 12640, loss = 3.37568
I0614 16:18:38.520681  5063 solver.cpp:228]     Train net output #0: softmax = 3.37568 (* 1 = 3.37568 loss)
I0614 16:18:38.520686  5063 solver.cpp:473] Iteration 12640, lr = 0.0001
I0614 16:18:39.403256  5063 solver.cpp:213] Iteration 12650, loss = 3.28698
I0614 16:18:39.403275  5063 solver.cpp:228]     Train net output #0: softmax = 3.28698 (* 1 = 3.28698 loss)
I0614 16:18:39.403280  5063 solver.cpp:473] Iteration 12650, lr = 0.0001
I0614 16:18:40.286378  5063 solver.cpp:213] Iteration 12660, loss = 3.3481
I0614 16:18:40.286404  5063 solver.cpp:228]     Train net output #0: softmax = 3.3481 (* 1 = 3.3481 loss)
I0614 16:18:40.286415  5063 solver.cpp:473] Iteration 12660, lr = 0.0001
I0614 16:18:41.169622  5063 solver.cpp:213] Iteration 12670, loss = 3.39118
I0614 16:18:41.169639  5063 solver.cpp:228]     Train net output #0: softmax = 3.39118 (* 1 = 3.39118 loss)
I0614 16:18:41.169644  5063 solver.cpp:473] Iteration 12670, lr = 0.0001
I0614 16:18:42.053019  5063 solver.cpp:213] Iteration 12680, loss = 3.47938
I0614 16:18:42.053037  5063 solver.cpp:228]     Train net output #0: softmax = 3.47938 (* 1 = 3.47938 loss)
I0614 16:18:42.053042  5063 solver.cpp:473] Iteration 12680, lr = 0.0001
I0614 16:18:42.936627  5063 solver.cpp:213] Iteration 12690, loss = 3.3243
I0614 16:18:42.936645  5063 solver.cpp:228]     Train net output #0: softmax = 3.3243 (* 1 = 3.3243 loss)
I0614 16:18:42.936650  5063 solver.cpp:473] Iteration 12690, lr = 0.0001
I0614 16:18:43.819525  5063 solver.cpp:213] Iteration 12700, loss = 3.14872
I0614 16:18:43.819543  5063 solver.cpp:228]     Train net output #0: softmax = 3.14872 (* 1 = 3.14872 loss)
I0614 16:18:43.819548  5063 solver.cpp:473] Iteration 12700, lr = 0.0001
I0614 16:18:44.702519  5063 solver.cpp:213] Iteration 12710, loss = 2.92222
I0614 16:18:44.702540  5063 solver.cpp:228]     Train net output #0: softmax = 2.92222 (* 1 = 2.92222 loss)
I0614 16:18:44.702545  5063 solver.cpp:473] Iteration 12710, lr = 0.0001
I0614 16:18:45.585371  5063 solver.cpp:213] Iteration 12720, loss = 2.96449
I0614 16:18:45.585391  5063 solver.cpp:228]     Train net output #0: softmax = 2.96449 (* 1 = 2.96449 loss)
I0614 16:18:45.585397  5063 solver.cpp:473] Iteration 12720, lr = 0.0001
I0614 16:18:46.468108  5063 solver.cpp:213] Iteration 12730, loss = 3.14775
I0614 16:18:46.468127  5063 solver.cpp:228]     Train net output #0: softmax = 3.14775 (* 1 = 3.14775 loss)
I0614 16:18:46.468134  5063 solver.cpp:473] Iteration 12730, lr = 0.0001
I0614 16:18:47.351027  5063 solver.cpp:213] Iteration 12740, loss = 3.02947
I0614 16:18:47.351061  5063 solver.cpp:228]     Train net output #0: softmax = 3.02947 (* 1 = 3.02947 loss)
I0614 16:18:47.351068  5063 solver.cpp:473] Iteration 12740, lr = 0.0001
I0614 16:18:48.234019  5063 solver.cpp:213] Iteration 12750, loss = 3.02793
I0614 16:18:48.234035  5063 solver.cpp:228]     Train net output #0: softmax = 3.02793 (* 1 = 3.02793 loss)
I0614 16:18:48.234040  5063 solver.cpp:473] Iteration 12750, lr = 0.0001
I0614 16:18:49.116302  5063 solver.cpp:213] Iteration 12760, loss = 3.11254
I0614 16:18:49.116317  5063 solver.cpp:228]     Train net output #0: softmax = 3.11254 (* 1 = 3.11254 loss)
I0614 16:18:49.116322  5063 solver.cpp:473] Iteration 12760, lr = 0.0001
I0614 16:18:49.998783  5063 solver.cpp:213] Iteration 12770, loss = 3.06534
I0614 16:18:49.998807  5063 solver.cpp:228]     Train net output #0: softmax = 3.06534 (* 1 = 3.06534 loss)
I0614 16:18:49.998812  5063 solver.cpp:473] Iteration 12770, lr = 0.0001
I0614 16:18:50.881809  5063 solver.cpp:213] Iteration 12780, loss = 3.09756
I0614 16:18:50.881831  5063 solver.cpp:228]     Train net output #0: softmax = 3.09756 (* 1 = 3.09756 loss)
I0614 16:18:50.881841  5063 solver.cpp:473] Iteration 12780, lr = 0.0001
I0614 16:18:51.764904  5063 solver.cpp:213] Iteration 12790, loss = 3.25428
I0614 16:18:51.764922  5063 solver.cpp:228]     Train net output #0: softmax = 3.25428 (* 1 = 3.25428 loss)
I0614 16:18:51.764927  5063 solver.cpp:473] Iteration 12790, lr = 0.0001
I0614 16:18:52.648259  5063 solver.cpp:213] Iteration 12800, loss = 3.21391
I0614 16:18:52.648275  5063 solver.cpp:228]     Train net output #0: softmax = 3.21391 (* 1 = 3.21391 loss)
I0614 16:18:52.648280  5063 solver.cpp:473] Iteration 12800, lr = 0.0001
I0614 16:18:53.531489  5063 solver.cpp:213] Iteration 12810, loss = 3.09726
I0614 16:18:53.531520  5063 solver.cpp:228]     Train net output #0: softmax = 3.09726 (* 1 = 3.09726 loss)
I0614 16:18:53.531525  5063 solver.cpp:473] Iteration 12810, lr = 0.0001
I0614 16:18:54.414651  5063 solver.cpp:213] Iteration 12820, loss = 3.29696
I0614 16:18:54.414667  5063 solver.cpp:228]     Train net output #0: softmax = 3.29696 (* 1 = 3.29696 loss)
I0614 16:18:54.414672  5063 solver.cpp:473] Iteration 12820, lr = 0.0001
I0614 16:18:55.298295  5063 solver.cpp:213] Iteration 12830, loss = 3.1666
I0614 16:18:55.298318  5063 solver.cpp:228]     Train net output #0: softmax = 3.1666 (* 1 = 3.1666 loss)
I0614 16:18:55.298323  5063 solver.cpp:473] Iteration 12830, lr = 0.0001
I0614 16:18:56.181495  5063 solver.cpp:213] Iteration 12840, loss = 3.55457
I0614 16:18:56.181517  5063 solver.cpp:228]     Train net output #0: softmax = 3.55457 (* 1 = 3.55457 loss)
I0614 16:18:56.181641  5063 solver.cpp:473] Iteration 12840, lr = 0.0001
I0614 16:18:57.064661  5063 solver.cpp:213] Iteration 12850, loss = 3.04264
I0614 16:18:57.064680  5063 solver.cpp:228]     Train net output #0: softmax = 3.04264 (* 1 = 3.04264 loss)
I0614 16:18:57.064685  5063 solver.cpp:473] Iteration 12850, lr = 0.0001
I0614 16:18:57.948130  5063 solver.cpp:213] Iteration 12860, loss = 3.11391
I0614 16:18:57.948146  5063 solver.cpp:228]     Train net output #0: softmax = 3.11391 (* 1 = 3.11391 loss)
I0614 16:18:57.948151  5063 solver.cpp:473] Iteration 12860, lr = 0.0001
I0614 16:18:58.831028  5063 solver.cpp:213] Iteration 12870, loss = 3.27872
I0614 16:18:58.831046  5063 solver.cpp:228]     Train net output #0: softmax = 3.27872 (* 1 = 3.27872 loss)
I0614 16:18:58.831051  5063 solver.cpp:473] Iteration 12870, lr = 0.0001
I0614 16:18:59.714442  5063 solver.cpp:213] Iteration 12880, loss = 3.36646
I0614 16:18:59.714465  5063 solver.cpp:228]     Train net output #0: softmax = 3.36646 (* 1 = 3.36646 loss)
I0614 16:18:59.714470  5063 solver.cpp:473] Iteration 12880, lr = 0.0001
I0614 16:19:00.596823  5063 solver.cpp:213] Iteration 12890, loss = 3.42212
I0614 16:19:00.596839  5063 solver.cpp:228]     Train net output #0: softmax = 3.42212 (* 1 = 3.42212 loss)
I0614 16:19:00.596843  5063 solver.cpp:473] Iteration 12890, lr = 0.0001
I0614 16:19:01.479909  5063 solver.cpp:213] Iteration 12900, loss = 3.18995
I0614 16:19:01.479943  5063 solver.cpp:228]     Train net output #0: softmax = 3.18995 (* 1 = 3.18995 loss)
I0614 16:19:01.479948  5063 solver.cpp:473] Iteration 12900, lr = 0.0001
I0614 16:19:02.363672  5063 solver.cpp:213] Iteration 12910, loss = 3.18521
I0614 16:19:02.363689  5063 solver.cpp:228]     Train net output #0: softmax = 3.18521 (* 1 = 3.18521 loss)
I0614 16:19:02.363694  5063 solver.cpp:473] Iteration 12910, lr = 0.0001
I0614 16:19:03.247474  5063 solver.cpp:213] Iteration 12920, loss = 3.08866
I0614 16:19:03.247515  5063 solver.cpp:228]     Train net output #0: softmax = 3.08866 (* 1 = 3.08866 loss)
I0614 16:19:03.247520  5063 solver.cpp:473] Iteration 12920, lr = 0.0001
I0614 16:19:04.130709  5063 solver.cpp:213] Iteration 12930, loss = 3.37645
I0614 16:19:04.130728  5063 solver.cpp:228]     Train net output #0: softmax = 3.37645 (* 1 = 3.37645 loss)
I0614 16:19:04.130733  5063 solver.cpp:473] Iteration 12930, lr = 0.0001
I0614 16:19:05.014243  5063 solver.cpp:213] Iteration 12940, loss = 3.36894
I0614 16:19:05.014266  5063 solver.cpp:228]     Train net output #0: softmax = 3.36894 (* 1 = 3.36894 loss)
I0614 16:19:05.014271  5063 solver.cpp:473] Iteration 12940, lr = 0.0001
I0614 16:19:05.897614  5063 solver.cpp:213] Iteration 12950, loss = 3.44209
I0614 16:19:05.897635  5063 solver.cpp:228]     Train net output #0: softmax = 3.44209 (* 1 = 3.44209 loss)
I0614 16:19:05.897640  5063 solver.cpp:473] Iteration 12950, lr = 0.0001
I0614 16:19:06.781075  5063 solver.cpp:213] Iteration 12960, loss = 3.26699
I0614 16:19:06.781096  5063 solver.cpp:228]     Train net output #0: softmax = 3.26699 (* 1 = 3.26699 loss)
I0614 16:19:06.781231  5063 solver.cpp:473] Iteration 12960, lr = 0.0001
I0614 16:19:07.664392  5063 solver.cpp:213] Iteration 12970, loss = 3.08995
I0614 16:19:07.664412  5063 solver.cpp:228]     Train net output #0: softmax = 3.08995 (* 1 = 3.08995 loss)
I0614 16:19:07.664417  5063 solver.cpp:473] Iteration 12970, lr = 0.0001
I0614 16:19:08.547660  5063 solver.cpp:213] Iteration 12980, loss = 3.15355
I0614 16:19:08.547679  5063 solver.cpp:228]     Train net output #0: softmax = 3.15355 (* 1 = 3.15355 loss)
I0614 16:19:08.547684  5063 solver.cpp:473] Iteration 12980, lr = 0.0001
I0614 16:19:09.430086  5063 solver.cpp:213] Iteration 12990, loss = 2.96262
I0614 16:19:09.430104  5063 solver.cpp:228]     Train net output #0: softmax = 2.96262 (* 1 = 2.96262 loss)
I0614 16:19:09.430109  5063 solver.cpp:473] Iteration 12990, lr = 0.0001
I0614 16:19:10.256014  5063 solver.cpp:362] Snapshotting to snapshots/16-06-14_15h51m21s_0_11_pretrainClassification_iter_13000.caffemodel
I0614 16:19:10.256789  5063 solver.cpp:370] Snapshotting solver state to snapshots/16-06-14_15h51m21s_0_11_pretrainClassification_iter_13000.solverstate
I0614 16:19:10.257217  5063 solver.cpp:291] Iteration 13000, Testing net (#0)
I0614 16:19:10.370059  5063 solver.cpp:342]     Test net output #0: accuracy_top_1 = 0.240625
I0614 16:19:10.370075  5063 solver.cpp:342]     Test net output #1: accuracy_top_5 = 0.53125
I0614 16:19:10.370082  5063 solver.cpp:342]     Test net output #2: softmax = 3.12181 (* 1 = 3.12181 loss)
I0614 16:19:10.428194  5063 solver.cpp:213] Iteration 13000, loss = 3.17425
I0614 16:19:10.428207  5063 solver.cpp:228]     Train net output #0: softmax = 3.17425 (* 1 = 3.17425 loss)
I0614 16:19:10.428212  5063 solver.cpp:473] Iteration 13000, lr = 0.0001
I0614 16:19:11.310742  5063 solver.cpp:213] Iteration 13010, loss = 3.20018
I0614 16:19:11.310760  5063 solver.cpp:228]     Train net output #0: softmax = 3.20018 (* 1 = 3.20018 loss)
I0614 16:19:11.310765  5063 solver.cpp:473] Iteration 13010, lr = 0.0001
I0614 16:19:12.193482  5063 solver.cpp:213] Iteration 13020, loss = 3.19155
I0614 16:19:12.193502  5063 solver.cpp:228]     Train net output #0: softmax = 3.19155 (* 1 = 3.19155 loss)
I0614 16:19:12.193512  5063 solver.cpp:473] Iteration 13020, lr = 0.0001
I0614 16:19:13.076694  5063 solver.cpp:213] Iteration 13030, loss = 3.32099
I0614 16:19:13.076711  5063 solver.cpp:228]     Train net output #0: softmax = 3.32099 (* 1 = 3.32099 loss)
I0614 16:19:13.076716  5063 solver.cpp:473] Iteration 13030, lr = 0.0001
I0614 16:19:13.960296  5063 solver.cpp:213] Iteration 13040, loss = 3.02559
I0614 16:19:13.960315  5063 solver.cpp:228]     Train net output #0: softmax = 3.02559 (* 1 = 3.02559 loss)
I0614 16:19:13.960320  5063 solver.cpp:473] Iteration 13040, lr = 0.0001
I0614 16:19:14.843534  5063 solver.cpp:213] Iteration 13050, loss = 3.3889
I0614 16:19:14.843554  5063 solver.cpp:228]     Train net output #0: softmax = 3.3889 (* 1 = 3.3889 loss)
I0614 16:19:14.843575  5063 solver.cpp:473] Iteration 13050, lr = 0.0001
I0614 16:19:15.726742  5063 solver.cpp:213] Iteration 13060, loss = 3.11788
I0614 16:19:15.726764  5063 solver.cpp:228]     Train net output #0: softmax = 3.11788 (* 1 = 3.11788 loss)
I0614 16:19:15.726770  5063 solver.cpp:473] Iteration 13060, lr = 0.0001
I0614 16:19:16.609674  5063 solver.cpp:213] Iteration 13070, loss = 3.0553
I0614 16:19:16.609693  5063 solver.cpp:228]     Train net output #0: softmax = 3.0553 (* 1 = 3.0553 loss)
I0614 16:19:16.609697  5063 solver.cpp:473] Iteration 13070, lr = 0.0001
I0614 16:19:17.493547  5063 solver.cpp:213] Iteration 13080, loss = 3.41754
I0614 16:19:17.493568  5063 solver.cpp:228]     Train net output #0: softmax = 3.41754 (* 1 = 3.41754 loss)
I0614 16:19:17.493573  5063 solver.cpp:473] Iteration 13080, lr = 0.0001
I0614 16:19:18.377207  5063 solver.cpp:213] Iteration 13090, loss = 3.19509
I0614 16:19:18.377233  5063 solver.cpp:228]     Train net output #0: softmax = 3.19509 (* 1 = 3.19509 loss)
I0614 16:19:18.377239  5063 solver.cpp:473] Iteration 13090, lr = 0.0001
I0614 16:19:19.260795  5063 solver.cpp:213] Iteration 13100, loss = 3.2162
I0614 16:19:19.260817  5063 solver.cpp:228]     Train net output #0: softmax = 3.2162 (* 1 = 3.2162 loss)
I0614 16:19:19.260821  5063 solver.cpp:473] Iteration 13100, lr = 0.0001
I0614 16:19:20.144098  5063 solver.cpp:213] Iteration 13110, loss = 3.20273
I0614 16:19:20.144120  5063 solver.cpp:228]     Train net output #0: softmax = 3.20273 (* 1 = 3.20273 loss)
I0614 16:19:20.144125  5063 solver.cpp:473] Iteration 13110, lr = 0.0001
I0614 16:19:21.027619  5063 solver.cpp:213] Iteration 13120, loss = 3.13463
I0614 16:19:21.027637  5063 solver.cpp:228]     Train net output #0: softmax = 3.13463 (* 1 = 3.13463 loss)
I0614 16:19:21.027642  5063 solver.cpp:473] Iteration 13120, lr = 0.0001
I0614 16:19:21.909960  5063 solver.cpp:213] Iteration 13130, loss = 3.21713
I0614 16:19:21.909976  5063 solver.cpp:228]     Train net output #0: softmax = 3.21713 (* 1 = 3.21713 loss)
I0614 16:19:21.909981  5063 solver.cpp:473] Iteration 13130, lr = 0.0001
I0614 16:19:22.793035  5063 solver.cpp:213] Iteration 13140, loss = 3.37259
I0614 16:19:22.793057  5063 solver.cpp:228]     Train net output #0: softmax = 3.37259 (* 1 = 3.37259 loss)
I0614 16:19:22.793210  5063 solver.cpp:473] Iteration 13140, lr = 0.0001
I0614 16:19:23.676492  5063 solver.cpp:213] Iteration 13150, loss = 2.93889
I0614 16:19:23.676512  5063 solver.cpp:228]     Train net output #0: softmax = 2.93889 (* 1 = 2.93889 loss)
I0614 16:19:23.676515  5063 solver.cpp:473] Iteration 13150, lr = 0.0001
I0614 16:19:24.559425  5063 solver.cpp:213] Iteration 13160, loss = 2.99599
I0614 16:19:24.559443  5063 solver.cpp:228]     Train net output #0: softmax = 2.99599 (* 1 = 2.99599 loss)
I0614 16:19:24.559448  5063 solver.cpp:473] Iteration 13160, lr = 0.0001
I0614 16:19:25.443583  5063 solver.cpp:213] Iteration 13170, loss = 3.044
I0614 16:19:25.443606  5063 solver.cpp:228]     Train net output #0: softmax = 3.044 (* 1 = 3.044 loss)
I0614 16:19:25.443611  5063 solver.cpp:473] Iteration 13170, lr = 0.0001
I0614 16:19:26.326406  5063 solver.cpp:213] Iteration 13180, loss = 3.15232
I0614 16:19:26.326421  5063 solver.cpp:228]     Train net output #0: softmax = 3.15232 (* 1 = 3.15232 loss)
I0614 16:19:26.326426  5063 solver.cpp:473] Iteration 13180, lr = 0.0001
I0614 16:19:27.209712  5063 solver.cpp:213] Iteration 13190, loss = 3.3873
I0614 16:19:27.209731  5063 solver.cpp:228]     Train net output #0: softmax = 3.3873 (* 1 = 3.3873 loss)
I0614 16:19:27.209736  5063 solver.cpp:473] Iteration 13190, lr = 0.0001
I0614 16:19:28.093420  5063 solver.cpp:213] Iteration 13200, loss = 3.11515
I0614 16:19:28.093441  5063 solver.cpp:228]     Train net output #0: softmax = 3.11515 (* 1 = 3.11515 loss)
I0614 16:19:28.093564  5063 solver.cpp:473] Iteration 13200, lr = 0.0001
I0614 16:19:28.976270  5063 solver.cpp:213] Iteration 13210, loss = 3.16077
I0614 16:19:28.976286  5063 solver.cpp:228]     Train net output #0: softmax = 3.16077 (* 1 = 3.16077 loss)
I0614 16:19:28.976306  5063 solver.cpp:473] Iteration 13210, lr = 0.0001
I0614 16:19:29.859515  5063 solver.cpp:213] Iteration 13220, loss = 2.96137
I0614 16:19:29.859531  5063 solver.cpp:228]     Train net output #0: softmax = 2.96137 (* 1 = 2.96137 loss)
I0614 16:19:29.859536  5063 solver.cpp:473] Iteration 13220, lr = 0.0001
I0614 16:19:30.743316  5063 solver.cpp:213] Iteration 13230, loss = 3.38577
I0614 16:19:30.743336  5063 solver.cpp:228]     Train net output #0: softmax = 3.38577 (* 1 = 3.38577 loss)
I0614 16:19:30.743341  5063 solver.cpp:473] Iteration 13230, lr = 0.0001
I0614 16:19:31.626520  5063 solver.cpp:213] Iteration 13240, loss = 3.0841
I0614 16:19:31.626538  5063 solver.cpp:228]     Train net output #0: softmax = 3.0841 (* 1 = 3.0841 loss)
I0614 16:19:31.626543  5063 solver.cpp:473] Iteration 13240, lr = 0.0001
I0614 16:19:32.509943  5063 solver.cpp:213] Iteration 13250, loss = 3.02798
I0614 16:19:32.509966  5063 solver.cpp:228]     Train net output #0: softmax = 3.02798 (* 1 = 3.02798 loss)
I0614 16:19:32.509971  5063 solver.cpp:473] Iteration 13250, lr = 0.0001
I0614 16:19:33.394376  5063 solver.cpp:213] Iteration 13260, loss = 3.23751
I0614 16:19:33.394420  5063 solver.cpp:228]     Train net output #0: softmax = 3.23751 (* 1 = 3.23751 loss)
I0614 16:19:33.394426  5063 solver.cpp:473] Iteration 13260, lr = 0.0001
I0614 16:19:34.276948  5063 solver.cpp:213] Iteration 13270, loss = 3.49428
I0614 16:19:34.276963  5063 solver.cpp:228]     Train net output #0: softmax = 3.49428 (* 1 = 3.49428 loss)
I0614 16:19:34.276968  5063 solver.cpp:473] Iteration 13270, lr = 0.0001
I0614 16:19:35.159499  5063 solver.cpp:213] Iteration 13280, loss = 3.24642
I0614 16:19:35.159518  5063 solver.cpp:228]     Train net output #0: softmax = 3.24642 (* 1 = 3.24642 loss)
I0614 16:19:35.159523  5063 solver.cpp:473] Iteration 13280, lr = 0.0001
I0614 16:19:36.042878  5063 solver.cpp:213] Iteration 13290, loss = 3.03949
I0614 16:19:36.042897  5063 solver.cpp:228]     Train net output #0: softmax = 3.03949 (* 1 = 3.03949 loss)
I0614 16:19:36.042902  5063 solver.cpp:473] Iteration 13290, lr = 0.0001
I0614 16:19:36.927057  5063 solver.cpp:213] Iteration 13300, loss = 3.18886
I0614 16:19:36.927075  5063 solver.cpp:228]     Train net output #0: softmax = 3.18886 (* 1 = 3.18886 loss)
I0614 16:19:36.927079  5063 solver.cpp:473] Iteration 13300, lr = 0.0001
I0614 16:19:37.810670  5063 solver.cpp:213] Iteration 13310, loss = 3.20481
I0614 16:19:37.810688  5063 solver.cpp:228]     Train net output #0: softmax = 3.20481 (* 1 = 3.20481 loss)
I0614 16:19:37.810693  5063 solver.cpp:473] Iteration 13310, lr = 0.0001
I0614 16:19:38.694280  5063 solver.cpp:213] Iteration 13320, loss = 3.20593
I0614 16:19:38.694303  5063 solver.cpp:228]     Train net output #0: softmax = 3.20593 (* 1 = 3.20593 loss)
I0614 16:19:38.694452  5063 solver.cpp:473] Iteration 13320, lr = 0.0001
I0614 16:19:39.576820  5063 solver.cpp:213] Iteration 13330, loss = 3.23322
I0614 16:19:39.576836  5063 solver.cpp:228]     Train net output #0: softmax = 3.23322 (* 1 = 3.23322 loss)
I0614 16:19:39.576841  5063 solver.cpp:473] Iteration 13330, lr = 0.0001
I0614 16:19:40.459573  5063 solver.cpp:213] Iteration 13340, loss = 3.25929
I0614 16:19:40.459589  5063 solver.cpp:228]     Train net output #0: softmax = 3.25929 (* 1 = 3.25929 loss)
I0614 16:19:40.459594  5063 solver.cpp:473] Iteration 13340, lr = 0.0001
I0614 16:19:41.342195  5063 solver.cpp:213] Iteration 13350, loss = 3.23244
I0614 16:19:41.342217  5063 solver.cpp:228]     Train net output #0: softmax = 3.23244 (* 1 = 3.23244 loss)
I0614 16:19:41.342222  5063 solver.cpp:473] Iteration 13350, lr = 0.0001
I0614 16:19:42.225664  5063 solver.cpp:213] Iteration 13360, loss = 3.13143
I0614 16:19:42.225680  5063 solver.cpp:228]     Train net output #0: softmax = 3.13143 (* 1 = 3.13143 loss)
I0614 16:19:42.225685  5063 solver.cpp:473] Iteration 13360, lr = 0.0001
I0614 16:19:43.108479  5063 solver.cpp:213] Iteration 13370, loss = 3.25981
I0614 16:19:43.108495  5063 solver.cpp:228]     Train net output #0: softmax = 3.25981 (* 1 = 3.25981 loss)
I0614 16:19:43.108500  5063 solver.cpp:473] Iteration 13370, lr = 0.0001
I0614 16:19:43.991775  5063 solver.cpp:213] Iteration 13380, loss = 3.13985
I0614 16:19:43.991794  5063 solver.cpp:228]     Train net output #0: softmax = 3.13985 (* 1 = 3.13985 loss)
I0614 16:19:43.991804  5063 solver.cpp:473] Iteration 13380, lr = 0.0001
I0614 16:19:44.875115  5063 solver.cpp:213] Iteration 13390, loss = 3.16559
I0614 16:19:44.875133  5063 solver.cpp:228]     Train net output #0: softmax = 3.16559 (* 1 = 3.16559 loss)
I0614 16:19:44.875138  5063 solver.cpp:473] Iteration 13390, lr = 0.0001
I0614 16:19:45.758919  5063 solver.cpp:213] Iteration 13400, loss = 3.39408
I0614 16:19:45.758939  5063 solver.cpp:228]     Train net output #0: softmax = 3.39408 (* 1 = 3.39408 loss)
I0614 16:19:45.758944  5063 solver.cpp:473] Iteration 13400, lr = 0.0001
I0614 16:19:46.642488  5063 solver.cpp:213] Iteration 13410, loss = 3.10715
I0614 16:19:46.642515  5063 solver.cpp:228]     Train net output #0: softmax = 3.10715 (* 1 = 3.10715 loss)
I0614 16:19:46.642520  5063 solver.cpp:473] Iteration 13410, lr = 0.0001
I0614 16:19:47.526151  5063 solver.cpp:213] Iteration 13420, loss = 3.07751
I0614 16:19:47.526186  5063 solver.cpp:228]     Train net output #0: softmax = 3.07751 (* 1 = 3.07751 loss)
I0614 16:19:47.526191  5063 solver.cpp:473] Iteration 13420, lr = 0.0001
I0614 16:19:48.409536  5063 solver.cpp:213] Iteration 13430, loss = 3.37396
I0614 16:19:48.409556  5063 solver.cpp:228]     Train net output #0: softmax = 3.37396 (* 1 = 3.37396 loss)
I0614 16:19:48.409560  5063 solver.cpp:473] Iteration 13430, lr = 0.0001
I0614 16:19:49.292548  5063 solver.cpp:213] Iteration 13440, loss = 3.19204
I0614 16:19:49.292567  5063 solver.cpp:228]     Train net output #0: softmax = 3.19204 (* 1 = 3.19204 loss)
I0614 16:19:49.292577  5063 solver.cpp:473] Iteration 13440, lr = 0.0001
I0614 16:19:50.176322  5063 solver.cpp:213] Iteration 13450, loss = 3.17062
I0614 16:19:50.176342  5063 solver.cpp:228]     Train net output #0: softmax = 3.17062 (* 1 = 3.17062 loss)
I0614 16:19:50.176347  5063 solver.cpp:473] Iteration 13450, lr = 0.0001
I0614 16:19:51.059129  5063 solver.cpp:213] Iteration 13460, loss = 2.98992
I0614 16:19:51.059146  5063 solver.cpp:228]     Train net output #0: softmax = 2.98992 (* 1 = 2.98992 loss)
I0614 16:19:51.059151  5063 solver.cpp:473] Iteration 13460, lr = 0.0001
I0614 16:19:51.942345  5063 solver.cpp:213] Iteration 13470, loss = 3.40916
I0614 16:19:51.942365  5063 solver.cpp:228]     Train net output #0: softmax = 3.40916 (* 1 = 3.40916 loss)
I0614 16:19:51.942370  5063 solver.cpp:473] Iteration 13470, lr = 0.0001
I0614 16:19:52.825166  5063 solver.cpp:213] Iteration 13480, loss = 3.04664
I0614 16:19:52.825182  5063 solver.cpp:228]     Train net output #0: softmax = 3.04664 (* 1 = 3.04664 loss)
I0614 16:19:52.825187  5063 solver.cpp:473] Iteration 13480, lr = 0.0001
I0614 16:19:53.708775  5063 solver.cpp:213] Iteration 13490, loss = 3.33216
I0614 16:19:53.708794  5063 solver.cpp:228]     Train net output #0: softmax = 3.33216 (* 1 = 3.33216 loss)
I0614 16:19:53.708799  5063 solver.cpp:473] Iteration 13490, lr = 0.0001
I0614 16:19:54.591272  5063 solver.cpp:213] Iteration 13500, loss = 3.06
I0614 16:19:54.591292  5063 solver.cpp:228]     Train net output #0: softmax = 3.06 (* 1 = 3.06 loss)
I0614 16:19:54.591297  5063 solver.cpp:473] Iteration 13500, lr = 0.0001
I0614 16:19:55.474342  5063 solver.cpp:213] Iteration 13510, loss = 3.23259
I0614 16:19:55.474361  5063 solver.cpp:228]     Train net output #0: softmax = 3.23259 (* 1 = 3.23259 loss)
I0614 16:19:55.474365  5063 solver.cpp:473] Iteration 13510, lr = 0.0001
I0614 16:19:56.357920  5063 solver.cpp:213] Iteration 13520, loss = 3.31984
I0614 16:19:56.357961  5063 solver.cpp:228]     Train net output #0: softmax = 3.31984 (* 1 = 3.31984 loss)
I0614 16:19:56.357967  5063 solver.cpp:473] Iteration 13520, lr = 0.0001
I0614 16:19:57.242666  5063 solver.cpp:213] Iteration 13530, loss = 3.02718
I0614 16:19:57.242692  5063 solver.cpp:228]     Train net output #0: softmax = 3.02718 (* 1 = 3.02718 loss)
I0614 16:19:57.242697  5063 solver.cpp:473] Iteration 13530, lr = 0.0001
I0614 16:19:58.126332  5063 solver.cpp:213] Iteration 13540, loss = 3.06429
I0614 16:19:58.126351  5063 solver.cpp:228]     Train net output #0: softmax = 3.06429 (* 1 = 3.06429 loss)
I0614 16:19:58.126356  5063 solver.cpp:473] Iteration 13540, lr = 0.0001
I0614 16:19:59.009124  5063 solver.cpp:213] Iteration 13550, loss = 2.9862
I0614 16:19:59.009140  5063 solver.cpp:228]     Train net output #0: softmax = 2.9862 (* 1 = 2.9862 loss)
I0614 16:19:59.009145  5063 solver.cpp:473] Iteration 13550, lr = 0.0001
I0614 16:19:59.892813  5063 solver.cpp:213] Iteration 13560, loss = 2.94759
I0614 16:19:59.892835  5063 solver.cpp:228]     Train net output #0: softmax = 2.94759 (* 1 = 2.94759 loss)
I0614 16:19:59.892961  5063 solver.cpp:473] Iteration 13560, lr = 0.0001
I0614 16:20:00.775598  5063 solver.cpp:213] Iteration 13570, loss = 3.2093
I0614 16:20:00.775620  5063 solver.cpp:228]     Train net output #0: softmax = 3.2093 (* 1 = 3.2093 loss)
I0614 16:20:00.775625  5063 solver.cpp:473] Iteration 13570, lr = 0.0001
I0614 16:20:01.659085  5063 solver.cpp:213] Iteration 13580, loss = 3.21569
I0614 16:20:01.659116  5063 solver.cpp:228]     Train net output #0: softmax = 3.21569 (* 1 = 3.21569 loss)
I0614 16:20:01.659121  5063 solver.cpp:473] Iteration 13580, lr = 0.0001
I0614 16:20:02.541952  5063 solver.cpp:213] Iteration 13590, loss = 3.35228
I0614 16:20:02.541972  5063 solver.cpp:228]     Train net output #0: softmax = 3.35228 (* 1 = 3.35228 loss)
I0614 16:20:02.541977  5063 solver.cpp:473] Iteration 13590, lr = 0.0001
I0614 16:20:03.425503  5063 solver.cpp:213] Iteration 13600, loss = 3.17083
I0614 16:20:03.425535  5063 solver.cpp:228]     Train net output #0: softmax = 3.17083 (* 1 = 3.17083 loss)
I0614 16:20:03.425540  5063 solver.cpp:473] Iteration 13600, lr = 0.0001
I0614 16:20:04.309330  5063 solver.cpp:213] Iteration 13610, loss = 3.00291
I0614 16:20:04.309350  5063 solver.cpp:228]     Train net output #0: softmax = 3.00291 (* 1 = 3.00291 loss)
I0614 16:20:04.309355  5063 solver.cpp:473] Iteration 13610, lr = 0.0001
I0614 16:20:05.192237  5063 solver.cpp:213] Iteration 13620, loss = 3.32142
I0614 16:20:05.192258  5063 solver.cpp:228]     Train net output #0: softmax = 3.32142 (* 1 = 3.32142 loss)
I0614 16:20:05.192409  5063 solver.cpp:473] Iteration 13620, lr = 0.0001
I0614 16:20:06.075860  5063 solver.cpp:213] Iteration 13630, loss = 3.15243
I0614 16:20:06.075876  5063 solver.cpp:228]     Train net output #0: softmax = 3.15243 (* 1 = 3.15243 loss)
I0614 16:20:06.075881  5063 solver.cpp:473] Iteration 13630, lr = 0.0001
I0614 16:20:06.959206  5063 solver.cpp:213] Iteration 13640, loss = 3.14738
I0614 16:20:06.959223  5063 solver.cpp:228]     Train net output #0: softmax = 3.14738 (* 1 = 3.14738 loss)
I0614 16:20:06.959228  5063 solver.cpp:473] Iteration 13640, lr = 0.0001
I0614 16:20:07.842195  5063 solver.cpp:213] Iteration 13650, loss = 3.22145
I0614 16:20:07.842216  5063 solver.cpp:228]     Train net output #0: softmax = 3.22145 (* 1 = 3.22145 loss)
I0614 16:20:07.842221  5063 solver.cpp:473] Iteration 13650, lr = 0.0001
I0614 16:20:08.725349  5063 solver.cpp:213] Iteration 13660, loss = 3.28308
I0614 16:20:08.725368  5063 solver.cpp:228]     Train net output #0: softmax = 3.28308 (* 1 = 3.28308 loss)
I0614 16:20:08.725371  5063 solver.cpp:473] Iteration 13660, lr = 0.0001
I0614 16:20:09.609066  5063 solver.cpp:213] Iteration 13670, loss = 3.27801
I0614 16:20:09.609084  5063 solver.cpp:228]     Train net output #0: softmax = 3.27801 (* 1 = 3.27801 loss)
I0614 16:20:09.609089  5063 solver.cpp:473] Iteration 13670, lr = 0.0001
I0614 16:20:10.492624  5063 solver.cpp:213] Iteration 13680, loss = 2.97402
I0614 16:20:10.492641  5063 solver.cpp:228]     Train net output #0: softmax = 2.97402 (* 1 = 2.97402 loss)
I0614 16:20:10.492646  5063 solver.cpp:473] Iteration 13680, lr = 0.0001
I0614 16:20:11.376343  5063 solver.cpp:213] Iteration 13690, loss = 3.26827
I0614 16:20:11.376360  5063 solver.cpp:228]     Train net output #0: softmax = 3.26827 (* 1 = 3.26827 loss)
I0614 16:20:11.376365  5063 solver.cpp:473] Iteration 13690, lr = 0.0001
I0614 16:20:12.260674  5063 solver.cpp:213] Iteration 13700, loss = 3.26069
I0614 16:20:12.260697  5063 solver.cpp:228]     Train net output #0: softmax = 3.26069 (* 1 = 3.26069 loss)
I0614 16:20:12.260702  5063 solver.cpp:473] Iteration 13700, lr = 0.0001
I0614 16:20:13.143543  5063 solver.cpp:213] Iteration 13710, loss = 3.16795
I0614 16:20:13.143561  5063 solver.cpp:228]     Train net output #0: softmax = 3.16795 (* 1 = 3.16795 loss)
I0614 16:20:13.143566  5063 solver.cpp:473] Iteration 13710, lr = 0.0001
I0614 16:20:14.027199  5063 solver.cpp:213] Iteration 13720, loss = 3.05477
I0614 16:20:14.027218  5063 solver.cpp:228]     Train net output #0: softmax = 3.05477 (* 1 = 3.05477 loss)
I0614 16:20:14.027223  5063 solver.cpp:473] Iteration 13720, lr = 0.0001
I0614 16:20:14.911383  5063 solver.cpp:213] Iteration 13730, loss = 3.23246
I0614 16:20:14.911408  5063 solver.cpp:228]     Train net output #0: softmax = 3.23246 (* 1 = 3.23246 loss)
I0614 16:20:14.911414  5063 solver.cpp:473] Iteration 13730, lr = 0.0001
I0614 16:20:15.794781  5063 solver.cpp:213] Iteration 13740, loss = 3.37968
I0614 16:20:15.794805  5063 solver.cpp:228]     Train net output #0: softmax = 3.37968 (* 1 = 3.37968 loss)
I0614 16:20:15.794953  5063 solver.cpp:473] Iteration 13740, lr = 0.0001
I0614 16:20:16.678356  5063 solver.cpp:213] Iteration 13750, loss = 3.24194
I0614 16:20:16.678375  5063 solver.cpp:228]     Train net output #0: softmax = 3.24194 (* 1 = 3.24194 loss)
I0614 16:20:16.678380  5063 solver.cpp:473] Iteration 13750, lr = 0.0001
I0614 16:20:17.561127  5063 solver.cpp:213] Iteration 13760, loss = 3.00537
I0614 16:20:17.561161  5063 solver.cpp:228]     Train net output #0: softmax = 3.00537 (* 1 = 3.00537 loss)
I0614 16:20:17.561166  5063 solver.cpp:473] Iteration 13760, lr = 0.0001
I0614 16:20:18.444536  5063 solver.cpp:213] Iteration 13770, loss = 3.13559
I0614 16:20:18.444557  5063 solver.cpp:228]     Train net output #0: softmax = 3.13559 (* 1 = 3.13559 loss)
I0614 16:20:18.444563  5063 solver.cpp:473] Iteration 13770, lr = 0.0001
I0614 16:20:19.328187  5063 solver.cpp:213] Iteration 13780, loss = 3.18607
I0614 16:20:19.328207  5063 solver.cpp:228]     Train net output #0: softmax = 3.18607 (* 1 = 3.18607 loss)
I0614 16:20:19.328212  5063 solver.cpp:473] Iteration 13780, lr = 0.0001
I0614 16:20:20.211661  5063 solver.cpp:213] Iteration 13790, loss = 3.3431
I0614 16:20:20.211680  5063 solver.cpp:228]     Train net output #0: softmax = 3.3431 (* 1 = 3.3431 loss)
I0614 16:20:20.211685  5063 solver.cpp:473] Iteration 13790, lr = 0.0001
I0614 16:20:21.094643  5063 solver.cpp:213] Iteration 13800, loss = 3.09546
I0614 16:20:21.094663  5063 solver.cpp:228]     Train net output #0: softmax = 3.09546 (* 1 = 3.09546 loss)
I0614 16:20:21.094800  5063 solver.cpp:473] Iteration 13800, lr = 0.0001
I0614 16:20:21.978040  5063 solver.cpp:213] Iteration 13810, loss = 3.17356
I0614 16:20:21.978058  5063 solver.cpp:228]     Train net output #0: softmax = 3.17356 (* 1 = 3.17356 loss)
I0614 16:20:21.978063  5063 solver.cpp:473] Iteration 13810, lr = 0.0001
I0614 16:20:22.860648  5063 solver.cpp:213] Iteration 13820, loss = 3.39814
I0614 16:20:22.860666  5063 solver.cpp:228]     Train net output #0: softmax = 3.39814 (* 1 = 3.39814 loss)
I0614 16:20:22.860671  5063 solver.cpp:473] Iteration 13820, lr = 0.0001
I0614 16:20:23.743898  5063 solver.cpp:213] Iteration 13830, loss = 3.26208
I0614 16:20:23.743917  5063 solver.cpp:228]     Train net output #0: softmax = 3.26208 (* 1 = 3.26208 loss)
I0614 16:20:23.743922  5063 solver.cpp:473] Iteration 13830, lr = 0.0001
I0614 16:20:24.626691  5063 solver.cpp:213] Iteration 13840, loss = 3.25929
I0614 16:20:24.626711  5063 solver.cpp:228]     Train net output #0: softmax = 3.25929 (* 1 = 3.25929 loss)
I0614 16:20:24.626716  5063 solver.cpp:473] Iteration 13840, lr = 0.0001
I0614 16:20:25.510355  5063 solver.cpp:213] Iteration 13850, loss = 3.01388
I0614 16:20:25.510376  5063 solver.cpp:228]     Train net output #0: softmax = 3.01388 (* 1 = 3.01388 loss)
I0614 16:20:25.510381  5063 solver.cpp:473] Iteration 13850, lr = 0.0001
I0614 16:20:26.393192  5063 solver.cpp:213] Iteration 13860, loss = 3.17945
I0614 16:20:26.393211  5063 solver.cpp:228]     Train net output #0: softmax = 3.17945 (* 1 = 3.17945 loss)
I0614 16:20:26.393216  5063 solver.cpp:473] Iteration 13860, lr = 0.0001
I0614 16:20:27.276337  5063 solver.cpp:213] Iteration 13870, loss = 2.9892
I0614 16:20:27.276353  5063 solver.cpp:228]     Train net output #0: softmax = 2.9892 (* 1 = 2.9892 loss)
I0614 16:20:27.276358  5063 solver.cpp:473] Iteration 13870, lr = 0.0001
I0614 16:20:28.158897  5063 solver.cpp:213] Iteration 13880, loss = 3.31357
I0614 16:20:28.158916  5063 solver.cpp:228]     Train net output #0: softmax = 3.31357 (* 1 = 3.31357 loss)
I0614 16:20:28.158921  5063 solver.cpp:473] Iteration 13880, lr = 0.0001
I0614 16:20:29.041424  5063 solver.cpp:213] Iteration 13890, loss = 3.23225
I0614 16:20:29.041445  5063 solver.cpp:228]     Train net output #0: softmax = 3.23225 (* 1 = 3.23225 loss)
I0614 16:20:29.041451  5063 solver.cpp:473] Iteration 13890, lr = 0.0001
I0614 16:20:29.924299  5063 solver.cpp:213] Iteration 13900, loss = 3.13805
I0614 16:20:29.924321  5063 solver.cpp:228]     Train net output #0: softmax = 3.13805 (* 1 = 3.13805 loss)
I0614 16:20:29.924326  5063 solver.cpp:473] Iteration 13900, lr = 0.0001
I0614 16:20:30.808022  5063 solver.cpp:213] Iteration 13910, loss = 3.28605
I0614 16:20:30.808043  5063 solver.cpp:228]     Train net output #0: softmax = 3.28605 (* 1 = 3.28605 loss)
I0614 16:20:30.808048  5063 solver.cpp:473] Iteration 13910, lr = 0.0001
I0614 16:20:31.692139  5063 solver.cpp:213] Iteration 13920, loss = 2.97294
I0614 16:20:31.692313  5063 solver.cpp:228]     Train net output #0: softmax = 2.97294 (* 1 = 2.97294 loss)
I0614 16:20:31.692320  5063 solver.cpp:473] Iteration 13920, lr = 0.0001
I0614 16:20:32.575503  5063 solver.cpp:213] Iteration 13930, loss = 3.05872
I0614 16:20:32.575521  5063 solver.cpp:228]     Train net output #0: softmax = 3.05872 (* 1 = 3.05872 loss)
I0614 16:20:32.575526  5063 solver.cpp:473] Iteration 13930, lr = 0.0001
I0614 16:20:33.459529  5063 solver.cpp:213] Iteration 13940, loss = 3.28529
I0614 16:20:33.459568  5063 solver.cpp:228]     Train net output #0: softmax = 3.28529 (* 1 = 3.28529 loss)
I0614 16:20:33.459573  5063 solver.cpp:473] Iteration 13940, lr = 0.0001
I0614 16:20:34.342998  5063 solver.cpp:213] Iteration 13950, loss = 3.15784
I0614 16:20:34.343017  5063 solver.cpp:228]     Train net output #0: softmax = 3.15784 (* 1 = 3.15784 loss)
I0614 16:20:34.343022  5063 solver.cpp:473] Iteration 13950, lr = 0.0001
I0614 16:20:35.226460  5063 solver.cpp:213] Iteration 13960, loss = 3.36838
I0614 16:20:35.226476  5063 solver.cpp:228]     Train net output #0: softmax = 3.36838 (* 1 = 3.36838 loss)
I0614 16:20:35.226481  5063 solver.cpp:473] Iteration 13960, lr = 0.0001
I0614 16:20:36.110383  5063 solver.cpp:213] Iteration 13970, loss = 2.97282
I0614 16:20:36.110399  5063 solver.cpp:228]     Train net output #0: softmax = 2.97282 (* 1 = 2.97282 loss)
I0614 16:20:36.110404  5063 solver.cpp:473] Iteration 13970, lr = 0.0001
I0614 16:20:36.993168  5063 solver.cpp:213] Iteration 13980, loss = 3.4391
I0614 16:20:36.993187  5063 solver.cpp:228]     Train net output #0: softmax = 3.4391 (* 1 = 3.4391 loss)
I0614 16:20:36.993325  5063 solver.cpp:473] Iteration 13980, lr = 0.0001
I0614 16:20:37.877444  5063 solver.cpp:213] Iteration 13990, loss = 3.3975
I0614 16:20:37.877460  5063 solver.cpp:228]     Train net output #0: softmax = 3.3975 (* 1 = 3.3975 loss)
I0614 16:20:37.877465  5063 solver.cpp:473] Iteration 13990, lr = 0.0001
I0614 16:20:38.702919  5063 solver.cpp:362] Snapshotting to snapshots/16-06-14_15h51m21s_0_11_pretrainClassification_iter_14000.caffemodel
I0614 16:20:38.703666  5063 solver.cpp:370] Snapshotting solver state to snapshots/16-06-14_15h51m21s_0_11_pretrainClassification_iter_14000.solverstate
I0614 16:20:38.704107  5063 solver.cpp:291] Iteration 14000, Testing net (#0)
I0614 16:20:38.816756  5063 solver.cpp:342]     Test net output #0: accuracy_top_1 = 0.228125
I0614 16:20:38.816771  5063 solver.cpp:342]     Test net output #1: accuracy_top_5 = 0.525
I0614 16:20:38.816777  5063 solver.cpp:342]     Test net output #2: softmax = 3.15709 (* 1 = 3.15709 loss)
I0614 16:20:38.874976  5063 solver.cpp:213] Iteration 14000, loss = 3.21123
I0614 16:20:38.874990  5063 solver.cpp:228]     Train net output #0: softmax = 3.21123 (* 1 = 3.21123 loss)
I0614 16:20:38.874995  5063 solver.cpp:473] Iteration 14000, lr = 0.0001
I0614 16:20:39.758767  5063 solver.cpp:213] Iteration 14010, loss = 3.2791
I0614 16:20:39.758786  5063 solver.cpp:228]     Train net output #0: softmax = 3.2791 (* 1 = 3.2791 loss)
I0614 16:20:39.758791  5063 solver.cpp:473] Iteration 14010, lr = 0.0001
I0614 16:20:40.642639  5063 solver.cpp:213] Iteration 14020, loss = 3.13166
I0614 16:20:40.642659  5063 solver.cpp:228]     Train net output #0: softmax = 3.13166 (* 1 = 3.13166 loss)
I0614 16:20:40.642670  5063 solver.cpp:473] Iteration 14020, lr = 0.0001
I0614 16:20:41.526028  5063 solver.cpp:213] Iteration 14030, loss = 3.35083
I0614 16:20:41.526047  5063 solver.cpp:228]     Train net output #0: softmax = 3.35083 (* 1 = 3.35083 loss)
I0614 16:20:41.526052  5063 solver.cpp:473] Iteration 14030, lr = 0.0001
I0614 16:20:42.409796  5063 solver.cpp:213] Iteration 14040, loss = 3.36633
I0614 16:20:42.409816  5063 solver.cpp:228]     Train net output #0: softmax = 3.36633 (* 1 = 3.36633 loss)
I0614 16:20:42.409821  5063 solver.cpp:473] Iteration 14040, lr = 0.0001
I0614 16:20:43.292817  5063 solver.cpp:213] Iteration 14050, loss = 3.27862
I0614 16:20:43.292837  5063 solver.cpp:228]     Train net output #0: softmax = 3.27862 (* 1 = 3.27862 loss)
I0614 16:20:43.292843  5063 solver.cpp:473] Iteration 14050, lr = 0.0001
I0614 16:20:44.175734  5063 solver.cpp:213] Iteration 14060, loss = 3.31759
I0614 16:20:44.175750  5063 solver.cpp:228]     Train net output #0: softmax = 3.31759 (* 1 = 3.31759 loss)
I0614 16:20:44.175755  5063 solver.cpp:473] Iteration 14060, lr = 0.0001
I0614 16:20:45.059921  5063 solver.cpp:213] Iteration 14070, loss = 3.26488
I0614 16:20:45.059938  5063 solver.cpp:228]     Train net output #0: softmax = 3.26488 (* 1 = 3.26488 loss)
I0614 16:20:45.059959  5063 solver.cpp:473] Iteration 14070, lr = 0.0001
I0614 16:20:45.943295  5063 solver.cpp:213] Iteration 14080, loss = 3.04031
I0614 16:20:45.943310  5063 solver.cpp:228]     Train net output #0: softmax = 3.04031 (* 1 = 3.04031 loss)
I0614 16:20:45.943315  5063 solver.cpp:473] Iteration 14080, lr = 0.0001
I0614 16:20:46.826573  5063 solver.cpp:213] Iteration 14090, loss = 3.20732
I0614 16:20:46.826588  5063 solver.cpp:228]     Train net output #0: softmax = 3.20732 (* 1 = 3.20732 loss)
I0614 16:20:46.826593  5063 solver.cpp:473] Iteration 14090, lr = 0.0001
I0614 16:20:47.710126  5063 solver.cpp:213] Iteration 14100, loss = 3.25459
I0614 16:20:47.710144  5063 solver.cpp:228]     Train net output #0: softmax = 3.25459 (* 1 = 3.25459 loss)
I0614 16:20:47.710279  5063 solver.cpp:473] Iteration 14100, lr = 0.0001
I0614 16:20:48.592478  5063 solver.cpp:213] Iteration 14110, loss = 3.05994
I0614 16:20:48.592494  5063 solver.cpp:228]     Train net output #0: softmax = 3.05994 (* 1 = 3.05994 loss)
I0614 16:20:48.592499  5063 solver.cpp:473] Iteration 14110, lr = 0.0001
I0614 16:20:49.475438  5063 solver.cpp:213] Iteration 14120, loss = 3.40315
I0614 16:20:49.475453  5063 solver.cpp:228]     Train net output #0: softmax = 3.40315 (* 1 = 3.40315 loss)
I0614 16:20:49.475458  5063 solver.cpp:473] Iteration 14120, lr = 0.0001
I0614 16:20:50.358332  5063 solver.cpp:213] Iteration 14130, loss = 3.1605
I0614 16:20:50.358347  5063 solver.cpp:228]     Train net output #0: softmax = 3.1605 (* 1 = 3.1605 loss)
I0614 16:20:50.358352  5063 solver.cpp:473] Iteration 14130, lr = 0.0001
I0614 16:20:51.242130  5063 solver.cpp:213] Iteration 14140, loss = 3.3429
I0614 16:20:51.242147  5063 solver.cpp:228]     Train net output #0: softmax = 3.3429 (* 1 = 3.3429 loss)
I0614 16:20:51.242152  5063 solver.cpp:473] Iteration 14140, lr = 0.0001
I0614 16:20:52.125211  5063 solver.cpp:213] Iteration 14150, loss = 3.00159
I0614 16:20:52.125226  5063 solver.cpp:228]     Train net output #0: softmax = 3.00159 (* 1 = 3.00159 loss)
I0614 16:20:52.125231  5063 solver.cpp:473] Iteration 14150, lr = 0.0001
I0614 16:20:53.008594  5063 solver.cpp:213] Iteration 14160, loss = 3.24513
I0614 16:20:53.008613  5063 solver.cpp:228]     Train net output #0: softmax = 3.24513 (* 1 = 3.24513 loss)
I0614 16:20:53.008754  5063 solver.cpp:473] Iteration 14160, lr = 0.0001
I0614 16:20:53.891492  5063 solver.cpp:213] Iteration 14170, loss = 3.05614
I0614 16:20:53.891506  5063 solver.cpp:228]     Train net output #0: softmax = 3.05614 (* 1 = 3.05614 loss)
I0614 16:20:53.891511  5063 solver.cpp:473] Iteration 14170, lr = 0.0001
I0614 16:20:54.774510  5063 solver.cpp:213] Iteration 14180, loss = 3.4697
I0614 16:20:54.774524  5063 solver.cpp:228]     Train net output #0: softmax = 3.4697 (* 1 = 3.4697 loss)
I0614 16:20:54.774535  5063 solver.cpp:473] Iteration 14180, lr = 0.0001
I0614 16:20:55.658324  5063 solver.cpp:213] Iteration 14190, loss = 3.07213
I0614 16:20:55.658340  5063 solver.cpp:228]     Train net output #0: softmax = 3.07213 (* 1 = 3.07213 loss)
I0614 16:20:55.658345  5063 solver.cpp:473] Iteration 14190, lr = 0.0001
I0614 16:20:56.541430  5063 solver.cpp:213] Iteration 14200, loss = 3.18714
I0614 16:20:56.541445  5063 solver.cpp:228]     Train net output #0: softmax = 3.18714 (* 1 = 3.18714 loss)
I0614 16:20:56.541450  5063 solver.cpp:473] Iteration 14200, lr = 0.0001
I0614 16:20:57.424674  5063 solver.cpp:213] Iteration 14210, loss = 3.16756
I0614 16:20:57.424693  5063 solver.cpp:228]     Train net output #0: softmax = 3.16756 (* 1 = 3.16756 loss)
I0614 16:20:57.424698  5063 solver.cpp:473] Iteration 14210, lr = 0.0001
I0614 16:20:58.307844  5063 solver.cpp:213] Iteration 14220, loss = 3.08402
I0614 16:20:58.307863  5063 solver.cpp:228]     Train net output #0: softmax = 3.08402 (* 1 = 3.08402 loss)
I0614 16:20:58.307873  5063 solver.cpp:473] Iteration 14220, lr = 0.0001
I0614 16:20:59.191604  5063 solver.cpp:213] Iteration 14230, loss = 3.18804
I0614 16:20:59.191620  5063 solver.cpp:228]     Train net output #0: softmax = 3.18804 (* 1 = 3.18804 loss)
I0614 16:20:59.191639  5063 solver.cpp:473] Iteration 14230, lr = 0.0001
I0614 16:21:00.075001  5063 solver.cpp:213] Iteration 14240, loss = 3.37344
I0614 16:21:00.075014  5063 solver.cpp:228]     Train net output #0: softmax = 3.37344 (* 1 = 3.37344 loss)
I0614 16:21:00.075019  5063 solver.cpp:473] Iteration 14240, lr = 0.0001
I0614 16:21:00.957798  5063 solver.cpp:213] Iteration 14250, loss = 3.19391
I0614 16:21:00.957816  5063 solver.cpp:228]     Train net output #0: softmax = 3.19391 (* 1 = 3.19391 loss)
I0614 16:21:00.957821  5063 solver.cpp:473] Iteration 14250, lr = 0.0001
I0614 16:21:01.841259  5063 solver.cpp:213] Iteration 14260, loss = 3.04608
I0614 16:21:01.841276  5063 solver.cpp:228]     Train net output #0: softmax = 3.04608 (* 1 = 3.04608 loss)
I0614 16:21:01.841281  5063 solver.cpp:473] Iteration 14260, lr = 0.0001
I0614 16:21:02.724149  5063 solver.cpp:213] Iteration 14270, loss = 2.98106
I0614 16:21:02.724165  5063 solver.cpp:228]     Train net output #0: softmax = 2.98106 (* 1 = 2.98106 loss)
I0614 16:21:02.724170  5063 solver.cpp:473] Iteration 14270, lr = 0.0001
I0614 16:21:03.607151  5063 solver.cpp:213] Iteration 14280, loss = 3.1089
I0614 16:21:03.607187  5063 solver.cpp:228]     Train net output #0: softmax = 3.1089 (* 1 = 3.1089 loss)
I0614 16:21:03.607193  5063 solver.cpp:473] Iteration 14280, lr = 0.0001
I0614 16:21:04.489902  5063 solver.cpp:213] Iteration 14290, loss = 3.40183
I0614 16:21:04.489915  5063 solver.cpp:228]     Train net output #0: softmax = 3.40183 (* 1 = 3.40183 loss)
I0614 16:21:04.489920  5063 solver.cpp:473] Iteration 14290, lr = 0.0001
I0614 16:21:05.373411  5063 solver.cpp:213] Iteration 14300, loss = 3.00437
I0614 16:21:05.373425  5063 solver.cpp:228]     Train net output #0: softmax = 3.00437 (* 1 = 3.00437 loss)
I0614 16:21:05.373430  5063 solver.cpp:473] Iteration 14300, lr = 0.0001
I0614 16:21:06.257020  5063 solver.cpp:213] Iteration 14310, loss = 2.87009
I0614 16:21:06.257035  5063 solver.cpp:228]     Train net output #0: softmax = 2.87009 (* 1 = 2.87009 loss)
I0614 16:21:06.257040  5063 solver.cpp:473] Iteration 14310, lr = 0.0001
I0614 16:21:07.140463  5063 solver.cpp:213] Iteration 14320, loss = 3.21444
I0614 16:21:07.140481  5063 solver.cpp:228]     Train net output #0: softmax = 3.21444 (* 1 = 3.21444 loss)
I0614 16:21:07.140486  5063 solver.cpp:473] Iteration 14320, lr = 0.0001
I0614 16:21:08.024212  5063 solver.cpp:213] Iteration 14330, loss = 3.28406
I0614 16:21:08.024227  5063 solver.cpp:228]     Train net output #0: softmax = 3.28406 (* 1 = 3.28406 loss)
I0614 16:21:08.024232  5063 solver.cpp:473] Iteration 14330, lr = 0.0001
I0614 16:21:08.907524  5063 solver.cpp:213] Iteration 14340, loss = 3.19203
I0614 16:21:08.907542  5063 solver.cpp:228]     Train net output #0: softmax = 3.19203 (* 1 = 3.19203 loss)
I0614 16:21:08.907558  5063 solver.cpp:473] Iteration 14340, lr = 0.0001
I0614 16:21:09.790283  5063 solver.cpp:213] Iteration 14350, loss = 3.18103
I0614 16:21:09.790298  5063 solver.cpp:228]     Train net output #0: softmax = 3.18103 (* 1 = 3.18103 loss)
I0614 16:21:09.790303  5063 solver.cpp:473] Iteration 14350, lr = 0.0001
I0614 16:21:10.673388  5063 solver.cpp:213] Iteration 14360, loss = 3.20861
I0614 16:21:10.673403  5063 solver.cpp:228]     Train net output #0: softmax = 3.20861 (* 1 = 3.20861 loss)
I0614 16:21:10.673408  5063 solver.cpp:473] Iteration 14360, lr = 0.0001
I0614 16:21:11.556897  5063 solver.cpp:213] Iteration 14370, loss = 3.14794
I0614 16:21:11.556912  5063 solver.cpp:228]     Train net output #0: softmax = 3.14794 (* 1 = 3.14794 loss)
I0614 16:21:11.556917  5063 solver.cpp:473] Iteration 14370, lr = 0.0001
I0614 16:21:12.440865  5063 solver.cpp:213] Iteration 14380, loss = 3.09405
I0614 16:21:12.440882  5063 solver.cpp:228]     Train net output #0: softmax = 3.09405 (* 1 = 3.09405 loss)
I0614 16:21:12.440887  5063 solver.cpp:473] Iteration 14380, lr = 0.0001
I0614 16:21:13.324631  5063 solver.cpp:213] Iteration 14390, loss = 3.30033
I0614 16:21:13.324646  5063 solver.cpp:228]     Train net output #0: softmax = 3.30033 (* 1 = 3.30033 loss)
I0614 16:21:13.324651  5063 solver.cpp:473] Iteration 14390, lr = 0.0001
I0614 16:21:14.208631  5063 solver.cpp:213] Iteration 14400, loss = 3.13545
I0614 16:21:14.208649  5063 solver.cpp:228]     Train net output #0: softmax = 3.13545 (* 1 = 3.13545 loss)
I0614 16:21:14.208784  5063 solver.cpp:473] Iteration 14400, lr = 0.0001
I0614 16:21:15.092631  5063 solver.cpp:213] Iteration 14410, loss = 3.24151
I0614 16:21:15.092646  5063 solver.cpp:228]     Train net output #0: softmax = 3.24151 (* 1 = 3.24151 loss)
I0614 16:21:15.092651  5063 solver.cpp:473] Iteration 14410, lr = 0.0001
I0614 16:21:15.976661  5063 solver.cpp:213] Iteration 14420, loss = 3.38084
I0614 16:21:15.976676  5063 solver.cpp:228]     Train net output #0: softmax = 3.38084 (* 1 = 3.38084 loss)
I0614 16:21:15.976681  5063 solver.cpp:473] Iteration 14420, lr = 0.0001
I0614 16:21:16.860275  5063 solver.cpp:213] Iteration 14430, loss = 3.18619
I0614 16:21:16.860290  5063 solver.cpp:228]     Train net output #0: softmax = 3.18619 (* 1 = 3.18619 loss)
I0614 16:21:16.860294  5063 solver.cpp:473] Iteration 14430, lr = 0.0001
I0614 16:21:17.744720  5063 solver.cpp:213] Iteration 14440, loss = 3.46478
I0614 16:21:17.744750  5063 solver.cpp:228]     Train net output #0: softmax = 3.46478 (* 1 = 3.46478 loss)
I0614 16:21:17.744755  5063 solver.cpp:473] Iteration 14440, lr = 0.0001
I0614 16:21:18.628525  5063 solver.cpp:213] Iteration 14450, loss = 3.25734
I0614 16:21:18.628543  5063 solver.cpp:228]     Train net output #0: softmax = 3.25734 (* 1 = 3.25734 loss)
I0614 16:21:18.628548  5063 solver.cpp:473] Iteration 14450, lr = 0.0001
I0614 16:21:19.511724  5063 solver.cpp:213] Iteration 14460, loss = 3.17744
I0614 16:21:19.511740  5063 solver.cpp:228]     Train net output #0: softmax = 3.17744 (* 1 = 3.17744 loss)
I0614 16:21:19.511745  5063 solver.cpp:473] Iteration 14460, lr = 0.0001
I0614 16:21:20.395488  5063 solver.cpp:213] Iteration 14470, loss = 3.0939
I0614 16:21:20.395503  5063 solver.cpp:228]     Train net output #0: softmax = 3.0939 (* 1 = 3.0939 loss)
I0614 16:21:20.395508  5063 solver.cpp:473] Iteration 14470, lr = 0.0001
I0614 16:21:21.280736  5063 solver.cpp:213] Iteration 14480, loss = 3.33423
I0614 16:21:21.280752  5063 solver.cpp:228]     Train net output #0: softmax = 3.33423 (* 1 = 3.33423 loss)
I0614 16:21:21.280756  5063 solver.cpp:473] Iteration 14480, lr = 0.0001
I0614 16:21:22.163779  5063 solver.cpp:213] Iteration 14490, loss = 3.28677
I0614 16:21:22.163794  5063 solver.cpp:228]     Train net output #0: softmax = 3.28677 (* 1 = 3.28677 loss)
I0614 16:21:22.163800  5063 solver.cpp:473] Iteration 14490, lr = 0.0001
I0614 16:21:23.048270  5063 solver.cpp:213] Iteration 14500, loss = 3.21729
I0614 16:21:23.048290  5063 solver.cpp:228]     Train net output #0: softmax = 3.21729 (* 1 = 3.21729 loss)
I0614 16:21:23.048295  5063 solver.cpp:473] Iteration 14500, lr = 0.0001
I0614 16:21:23.931396  5063 solver.cpp:213] Iteration 14510, loss = 3.49676
I0614 16:21:23.931414  5063 solver.cpp:228]     Train net output #0: softmax = 3.49676 (* 1 = 3.49676 loss)
I0614 16:21:23.931419  5063 solver.cpp:473] Iteration 14510, lr = 0.0001
I0614 16:21:24.814313  5063 solver.cpp:213] Iteration 14520, loss = 3.35346
I0614 16:21:24.814332  5063 solver.cpp:228]     Train net output #0: softmax = 3.35346 (* 1 = 3.35346 loss)
I0614 16:21:24.814471  5063 solver.cpp:473] Iteration 14520, lr = 0.0001
I0614 16:21:25.697247  5063 solver.cpp:213] Iteration 14530, loss = 3.25215
I0614 16:21:25.697263  5063 solver.cpp:228]     Train net output #0: softmax = 3.25215 (* 1 = 3.25215 loss)
I0614 16:21:25.697268  5063 solver.cpp:473] Iteration 14530, lr = 0.0001
I0614 16:21:26.579766  5063 solver.cpp:213] Iteration 14540, loss = 3.15915
I0614 16:21:26.579782  5063 solver.cpp:228]     Train net output #0: softmax = 3.15915 (* 1 = 3.15915 loss)
I0614 16:21:26.579787  5063 solver.cpp:473] Iteration 14540, lr = 0.0001
I0614 16:21:27.463773  5063 solver.cpp:213] Iteration 14550, loss = 3.48181
I0614 16:21:27.463786  5063 solver.cpp:228]     Train net output #0: softmax = 3.48181 (* 1 = 3.48181 loss)
I0614 16:21:27.463791  5063 solver.cpp:473] Iteration 14550, lr = 0.0001
I0614 16:21:28.347630  5063 solver.cpp:213] Iteration 14560, loss = 3.44169
I0614 16:21:28.347645  5063 solver.cpp:228]     Train net output #0: softmax = 3.44169 (* 1 = 3.44169 loss)
I0614 16:21:28.347651  5063 solver.cpp:473] Iteration 14560, lr = 0.0001
I0614 16:21:29.230191  5063 solver.cpp:213] Iteration 14570, loss = 3.4725
I0614 16:21:29.230209  5063 solver.cpp:228]     Train net output #0: softmax = 3.4725 (* 1 = 3.4725 loss)
I0614 16:21:29.230214  5063 solver.cpp:473] Iteration 14570, lr = 0.0001
I0614 16:21:30.113212  5063 solver.cpp:213] Iteration 14580, loss = 3.17121
I0614 16:21:30.113230  5063 solver.cpp:228]     Train net output #0: softmax = 3.17121 (* 1 = 3.17121 loss)
I0614 16:21:30.113348  5063 solver.cpp:473] Iteration 14580, lr = 0.0001
I0614 16:21:30.997331  5063 solver.cpp:213] Iteration 14590, loss = 2.86942
I0614 16:21:30.997346  5063 solver.cpp:228]     Train net output #0: softmax = 2.86942 (* 1 = 2.86942 loss)
I0614 16:21:30.997350  5063 solver.cpp:473] Iteration 14590, lr = 0.0001
I0614 16:21:31.881070  5063 solver.cpp:213] Iteration 14600, loss = 2.9625
I0614 16:21:31.881103  5063 solver.cpp:228]     Train net output #0: softmax = 2.9625 (* 1 = 2.9625 loss)
I0614 16:21:31.881108  5063 solver.cpp:473] Iteration 14600, lr = 0.0001
I0614 16:21:32.764226  5063 solver.cpp:213] Iteration 14610, loss = 3.04886
I0614 16:21:32.764240  5063 solver.cpp:228]     Train net output #0: softmax = 3.04886 (* 1 = 3.04886 loss)
I0614 16:21:32.764245  5063 solver.cpp:473] Iteration 14610, lr = 0.0001
I0614 16:21:33.647555  5063 solver.cpp:213] Iteration 14620, loss = 3.23343
I0614 16:21:33.647589  5063 solver.cpp:228]     Train net output #0: softmax = 3.23343 (* 1 = 3.23343 loss)
I0614 16:21:33.647595  5063 solver.cpp:473] Iteration 14620, lr = 0.0001
I0614 16:21:34.530835  5063 solver.cpp:213] Iteration 14630, loss = 3.23258
I0614 16:21:34.530853  5063 solver.cpp:228]     Train net output #0: softmax = 3.23258 (* 1 = 3.23258 loss)
I0614 16:21:34.530858  5063 solver.cpp:473] Iteration 14630, lr = 0.0001
I0614 16:21:35.414805  5063 solver.cpp:213] Iteration 14640, loss = 3.2187
I0614 16:21:35.414822  5063 solver.cpp:228]     Train net output #0: softmax = 3.2187 (* 1 = 3.2187 loss)
I0614 16:21:35.414827  5063 solver.cpp:473] Iteration 14640, lr = 0.0001
I0614 16:21:36.298871  5063 solver.cpp:213] Iteration 14650, loss = 2.96299
I0614 16:21:36.298885  5063 solver.cpp:228]     Train net output #0: softmax = 2.96299 (* 1 = 2.96299 loss)
I0614 16:21:36.298889  5063 solver.cpp:473] Iteration 14650, lr = 0.0001
I0614 16:21:37.181963  5063 solver.cpp:213] Iteration 14660, loss = 2.81555
I0614 16:21:37.181983  5063 solver.cpp:228]     Train net output #0: softmax = 2.81555 (* 1 = 2.81555 loss)
I0614 16:21:37.181988  5063 solver.cpp:473] Iteration 14660, lr = 0.0001
I0614 16:21:38.065563  5063 solver.cpp:213] Iteration 14670, loss = 3.02615
I0614 16:21:38.065578  5063 solver.cpp:228]     Train net output #0: softmax = 3.02615 (* 1 = 3.02615 loss)
I0614 16:21:38.065583  5063 solver.cpp:473] Iteration 14670, lr = 0.0001
I0614 16:21:38.949618  5063 solver.cpp:213] Iteration 14680, loss = 3.49548
I0614 16:21:38.949635  5063 solver.cpp:228]     Train net output #0: softmax = 3.49548 (* 1 = 3.49548 loss)
I0614 16:21:38.949640  5063 solver.cpp:473] Iteration 14680, lr = 0.0001
I0614 16:21:39.833148  5063 solver.cpp:213] Iteration 14690, loss = 3.07637
I0614 16:21:39.833165  5063 solver.cpp:228]     Train net output #0: softmax = 3.07637 (* 1 = 3.07637 loss)
I0614 16:21:39.833170  5063 solver.cpp:473] Iteration 14690, lr = 0.0001
I0614 16:21:40.716040  5063 solver.cpp:213] Iteration 14700, loss = 2.98466
I0614 16:21:40.716058  5063 solver.cpp:228]     Train net output #0: softmax = 2.98466 (* 1 = 2.98466 loss)
I0614 16:21:40.716181  5063 solver.cpp:473] Iteration 14700, lr = 0.0001
I0614 16:21:41.599231  5063 solver.cpp:213] Iteration 14710, loss = 3.35243
I0614 16:21:41.599244  5063 solver.cpp:228]     Train net output #0: softmax = 3.35243 (* 1 = 3.35243 loss)
I0614 16:21:41.599249  5063 solver.cpp:473] Iteration 14710, lr = 0.0001
I0614 16:21:42.482852  5063 solver.cpp:213] Iteration 14720, loss = 3.16148
I0614 16:21:42.482867  5063 solver.cpp:228]     Train net output #0: softmax = 3.16148 (* 1 = 3.16148 loss)
I0614 16:21:42.482872  5063 solver.cpp:473] Iteration 14720, lr = 0.0001
I0614 16:21:43.366400  5063 solver.cpp:213] Iteration 14730, loss = 3.04382
I0614 16:21:43.366415  5063 solver.cpp:228]     Train net output #0: softmax = 3.04382 (* 1 = 3.04382 loss)
I0614 16:21:43.366420  5063 solver.cpp:473] Iteration 14730, lr = 0.0001
I0614 16:21:44.249616  5063 solver.cpp:213] Iteration 14740, loss = 3.17599
I0614 16:21:44.249631  5063 solver.cpp:228]     Train net output #0: softmax = 3.17599 (* 1 = 3.17599 loss)
I0614 16:21:44.249636  5063 solver.cpp:473] Iteration 14740, lr = 0.0001
I0614 16:21:45.132948  5063 solver.cpp:213] Iteration 14750, loss = 3.089
I0614 16:21:45.132963  5063 solver.cpp:228]     Train net output #0: softmax = 3.089 (* 1 = 3.089 loss)
I0614 16:21:45.132967  5063 solver.cpp:473] Iteration 14750, lr = 0.0001
I0614 16:21:46.015542  5063 solver.cpp:213] Iteration 14760, loss = 3.2791
I0614 16:21:46.015563  5063 solver.cpp:228]     Train net output #0: softmax = 3.2791 (* 1 = 3.2791 loss)
I0614 16:21:46.015681  5063 solver.cpp:473] Iteration 14760, lr = 0.0001
I0614 16:21:46.899760  5063 solver.cpp:213] Iteration 14770, loss = 3.24951
I0614 16:21:46.899775  5063 solver.cpp:228]     Train net output #0: softmax = 3.24951 (* 1 = 3.24951 loss)
I0614 16:21:46.899780  5063 solver.cpp:473] Iteration 14770, lr = 0.0001
I0614 16:21:47.782956  5063 solver.cpp:213] Iteration 14780, loss = 3.2692
I0614 16:21:47.782985  5063 solver.cpp:228]     Train net output #0: softmax = 3.2692 (* 1 = 3.2692 loss)
I0614 16:21:47.782990  5063 solver.cpp:473] Iteration 14780, lr = 0.0001
I0614 16:21:48.665415  5063 solver.cpp:213] Iteration 14790, loss = 3.13221
I0614 16:21:48.665429  5063 solver.cpp:228]     Train net output #0: softmax = 3.13221 (* 1 = 3.13221 loss)
I0614 16:21:48.665434  5063 solver.cpp:473] Iteration 14790, lr = 0.0001
I0614 16:21:49.548616  5063 solver.cpp:213] Iteration 14800, loss = 3.17122
I0614 16:21:49.548631  5063 solver.cpp:228]     Train net output #0: softmax = 3.17122 (* 1 = 3.17122 loss)
I0614 16:21:49.548636  5063 solver.cpp:473] Iteration 14800, lr = 0.0001
I0614 16:21:50.432198  5063 solver.cpp:213] Iteration 14810, loss = 3.24271
I0614 16:21:50.432215  5063 solver.cpp:228]     Train net output #0: softmax = 3.24271 (* 1 = 3.24271 loss)
I0614 16:21:50.432220  5063 solver.cpp:473] Iteration 14810, lr = 0.0001
I0614 16:21:51.315425  5063 solver.cpp:213] Iteration 14820, loss = 3.09722
I0614 16:21:51.315454  5063 solver.cpp:228]     Train net output #0: softmax = 3.09722 (* 1 = 3.09722 loss)
I0614 16:21:51.315464  5063 solver.cpp:473] Iteration 14820, lr = 0.0001
I0614 16:21:52.198707  5063 solver.cpp:213] Iteration 14830, loss = 3.32548
I0614 16:21:52.198722  5063 solver.cpp:228]     Train net output #0: softmax = 3.32548 (* 1 = 3.32548 loss)
I0614 16:21:52.198726  5063 solver.cpp:473] Iteration 14830, lr = 0.0001
I0614 16:21:53.082031  5063 solver.cpp:213] Iteration 14840, loss = 3.17086
I0614 16:21:53.082046  5063 solver.cpp:228]     Train net output #0: softmax = 3.17086 (* 1 = 3.17086 loss)
I0614 16:21:53.082051  5063 solver.cpp:473] Iteration 14840, lr = 0.0001
I0614 16:21:53.965693  5063 solver.cpp:213] Iteration 14850, loss = 3.04227
I0614 16:21:53.965708  5063 solver.cpp:228]     Train net output #0: softmax = 3.04227 (* 1 = 3.04227 loss)
I0614 16:21:53.965713  5063 solver.cpp:473] Iteration 14850, lr = 0.0001
I0614 16:21:54.849227  5063 solver.cpp:213] Iteration 14860, loss = 3.15256
I0614 16:21:54.849246  5063 solver.cpp:228]     Train net output #0: softmax = 3.15256 (* 1 = 3.15256 loss)
I0614 16:21:54.849251  5063 solver.cpp:473] Iteration 14860, lr = 0.0001
I0614 16:21:55.732339  5063 solver.cpp:213] Iteration 14870, loss = 3.22286
I0614 16:21:55.732357  5063 solver.cpp:228]     Train net output #0: softmax = 3.22286 (* 1 = 3.22286 loss)
I0614 16:21:55.732362  5063 solver.cpp:473] Iteration 14870, lr = 0.0001
I0614 16:21:56.616011  5063 solver.cpp:213] Iteration 14880, loss = 3.28836
I0614 16:21:56.616029  5063 solver.cpp:228]     Train net output #0: softmax = 3.28836 (* 1 = 3.28836 loss)
I0614 16:21:56.616034  5063 solver.cpp:473] Iteration 14880, lr = 0.0001
I0614 16:21:57.499243  5063 solver.cpp:213] Iteration 14890, loss = 3.10883
I0614 16:21:57.499275  5063 solver.cpp:228]     Train net output #0: softmax = 3.10883 (* 1 = 3.10883 loss)
I0614 16:21:57.499281  5063 solver.cpp:473] Iteration 14890, lr = 0.0001
I0614 16:21:58.381996  5063 solver.cpp:213] Iteration 14900, loss = 3.13596
I0614 16:21:58.382015  5063 solver.cpp:228]     Train net output #0: softmax = 3.13596 (* 1 = 3.13596 loss)
I0614 16:21:58.382021  5063 solver.cpp:473] Iteration 14900, lr = 0.0001
I0614 16:21:59.265614  5063 solver.cpp:213] Iteration 14910, loss = 3.29067
I0614 16:21:59.265630  5063 solver.cpp:228]     Train net output #0: softmax = 3.29067 (* 1 = 3.29067 loss)
I0614 16:21:59.265635  5063 solver.cpp:473] Iteration 14910, lr = 0.0001
I0614 16:22:00.149404  5063 solver.cpp:213] Iteration 14920, loss = 3.2968
I0614 16:22:00.149420  5063 solver.cpp:228]     Train net output #0: softmax = 3.2968 (* 1 = 3.2968 loss)
I0614 16:22:00.149425  5063 solver.cpp:473] Iteration 14920, lr = 0.0001
I0614 16:22:01.033148  5063 solver.cpp:213] Iteration 14930, loss = 2.95644
I0614 16:22:01.033165  5063 solver.cpp:228]     Train net output #0: softmax = 2.95644 (* 1 = 2.95644 loss)
I0614 16:22:01.033171  5063 solver.cpp:473] Iteration 14930, lr = 0.0001
I0614 16:22:01.916535  5063 solver.cpp:213] Iteration 14940, loss = 3.44926
I0614 16:22:01.916724  5063 solver.cpp:228]     Train net output #0: softmax = 3.44926 (* 1 = 3.44926 loss)
I0614 16:22:01.916731  5063 solver.cpp:473] Iteration 14940, lr = 0.0001
I0614 16:22:02.799160  5063 solver.cpp:213] Iteration 14950, loss = 3.2637
I0614 16:22:02.799175  5063 solver.cpp:228]     Train net output #0: softmax = 3.2637 (* 1 = 3.2637 loss)
I0614 16:22:02.799180  5063 solver.cpp:473] Iteration 14950, lr = 0.0001
I0614 16:22:03.682337  5063 solver.cpp:213] Iteration 14960, loss = 3.26305
I0614 16:22:03.682380  5063 solver.cpp:228]     Train net output #0: softmax = 3.26305 (* 1 = 3.26305 loss)
I0614 16:22:03.682385  5063 solver.cpp:473] Iteration 14960, lr = 0.0001
I0614 16:22:04.565232  5063 solver.cpp:213] Iteration 14970, loss = 3.12807
I0614 16:22:04.565248  5063 solver.cpp:228]     Train net output #0: softmax = 3.12807 (* 1 = 3.12807 loss)
I0614 16:22:04.565253  5063 solver.cpp:473] Iteration 14970, lr = 0.0001
I0614 16:22:05.448042  5063 solver.cpp:213] Iteration 14980, loss = 2.82147
I0614 16:22:05.448062  5063 solver.cpp:228]     Train net output #0: softmax = 2.82147 (* 1 = 2.82147 loss)
I0614 16:22:05.448067  5063 solver.cpp:473] Iteration 14980, lr = 0.0001
I0614 16:22:06.330803  5063 solver.cpp:213] Iteration 14990, loss = 3.16597
I0614 16:22:06.330818  5063 solver.cpp:228]     Train net output #0: softmax = 3.16597 (* 1 = 3.16597 loss)
I0614 16:22:06.330823  5063 solver.cpp:473] Iteration 14990, lr = 0.0001
I0614 16:22:07.156590  5063 solver.cpp:362] Snapshotting to snapshots/16-06-14_15h51m21s_0_11_pretrainClassification_iter_15000.caffemodel
I0614 16:22:07.157383  5063 solver.cpp:370] Snapshotting solver state to snapshots/16-06-14_15h51m21s_0_11_pretrainClassification_iter_15000.solverstate
I0614 16:22:07.157820  5063 solver.cpp:291] Iteration 15000, Testing net (#0)
I0614 16:22:07.270797  5063 solver.cpp:342]     Test net output #0: accuracy_top_1 = 0.24375
I0614 16:22:07.270813  5063 solver.cpp:342]     Test net output #1: accuracy_top_5 = 0.539062
I0614 16:22:07.270819  5063 solver.cpp:342]     Test net output #2: softmax = 3.11578 (* 1 = 3.11578 loss)
I0614 16:22:07.328836  5063 solver.cpp:213] Iteration 15000, loss = 3.039
I0614 16:22:07.328851  5063 solver.cpp:228]     Train net output #0: softmax = 3.039 (* 1 = 3.039 loss)
I0614 16:22:07.328855  5063 solver.cpp:473] Iteration 15000, lr = 0.0001
I0614 16:22:08.212508  5063 solver.cpp:213] Iteration 15010, loss = 3.20767
I0614 16:22:08.212524  5063 solver.cpp:228]     Train net output #0: softmax = 3.20767 (* 1 = 3.20767 loss)
I0614 16:22:08.212529  5063 solver.cpp:473] Iteration 15010, lr = 0.0001
I0614 16:22:09.096457  5063 solver.cpp:213] Iteration 15020, loss = 3.18174
I0614 16:22:09.096472  5063 solver.cpp:228]     Train net output #0: softmax = 3.18174 (* 1 = 3.18174 loss)
I0614 16:22:09.096477  5063 solver.cpp:473] Iteration 15020, lr = 0.0001
I0614 16:22:09.979620  5063 solver.cpp:213] Iteration 15030, loss = 3.12446
I0614 16:22:09.979635  5063 solver.cpp:228]     Train net output #0: softmax = 3.12446 (* 1 = 3.12446 loss)
I0614 16:22:09.979640  5063 solver.cpp:473] Iteration 15030, lr = 0.0001
I0614 16:22:10.863401  5063 solver.cpp:213] Iteration 15040, loss = 3.13353
I0614 16:22:10.863416  5063 solver.cpp:228]     Train net output #0: softmax = 3.13353 (* 1 = 3.13353 loss)
I0614 16:22:10.863421  5063 solver.cpp:473] Iteration 15040, lr = 0.0001
I0614 16:22:11.746745  5063 solver.cpp:213] Iteration 15050, loss = 3.23918
I0614 16:22:11.746760  5063 solver.cpp:228]     Train net output #0: softmax = 3.23918 (* 1 = 3.23918 loss)
I0614 16:22:11.746765  5063 solver.cpp:473] Iteration 15050, lr = 0.0001
I0614 16:22:12.629261  5063 solver.cpp:213] Iteration 15060, loss = 3.02343
I0614 16:22:12.629277  5063 solver.cpp:228]     Train net output #0: softmax = 3.02343 (* 1 = 3.02343 loss)
I0614 16:22:12.629282  5063 solver.cpp:473] Iteration 15060, lr = 0.0001
I0614 16:22:13.512681  5063 solver.cpp:213] Iteration 15070, loss = 3.42875
I0614 16:22:13.512698  5063 solver.cpp:228]     Train net output #0: softmax = 3.42875 (* 1 = 3.42875 loss)
I0614 16:22:13.512703  5063 solver.cpp:473] Iteration 15070, lr = 0.0001
I0614 16:22:14.396077  5063 solver.cpp:213] Iteration 15080, loss = 3.20953
I0614 16:22:14.396092  5063 solver.cpp:228]     Train net output #0: softmax = 3.20953 (* 1 = 3.20953 loss)
I0614 16:22:14.396097  5063 solver.cpp:473] Iteration 15080, lr = 0.0001
I0614 16:22:15.279362  5063 solver.cpp:213] Iteration 15090, loss = 3.00162
I0614 16:22:15.279377  5063 solver.cpp:228]     Train net output #0: softmax = 3.00162 (* 1 = 3.00162 loss)
I0614 16:22:15.279399  5063 solver.cpp:473] Iteration 15090, lr = 0.0001
I0614 16:22:16.162793  5063 solver.cpp:213] Iteration 15100, loss = 3.09132
I0614 16:22:16.162807  5063 solver.cpp:228]     Train net output #0: softmax = 3.09132 (* 1 = 3.09132 loss)
I0614 16:22:16.162812  5063 solver.cpp:473] Iteration 15100, lr = 0.0001
I0614 16:22:17.045464  5063 solver.cpp:213] Iteration 15110, loss = 3.08674
I0614 16:22:17.045480  5063 solver.cpp:228]     Train net output #0: softmax = 3.08674 (* 1 = 3.08674 loss)
I0614 16:22:17.045490  5063 solver.cpp:473] Iteration 15110, lr = 0.0001
I0614 16:22:17.928885  5063 solver.cpp:213] Iteration 15120, loss = 3.09262
I0614 16:22:17.928905  5063 solver.cpp:228]     Train net output #0: softmax = 3.09262 (* 1 = 3.09262 loss)
I0614 16:22:17.929045  5063 solver.cpp:473] Iteration 15120, lr = 0.0001
I0614 16:22:18.811966  5063 solver.cpp:213] Iteration 15130, loss = 3.01687
I0614 16:22:18.811985  5063 solver.cpp:228]     Train net output #0: softmax = 3.01687 (* 1 = 3.01687 loss)
I0614 16:22:18.811990  5063 solver.cpp:473] Iteration 15130, lr = 0.0001
I0614 16:22:19.694533  5063 solver.cpp:213] Iteration 15140, loss = 3.24636
I0614 16:22:19.694548  5063 solver.cpp:228]     Train net output #0: softmax = 3.24636 (* 1 = 3.24636 loss)
I0614 16:22:19.694553  5063 solver.cpp:473] Iteration 15140, lr = 0.0001
I0614 16:22:20.577481  5063 solver.cpp:213] Iteration 15150, loss = 3.13931
I0614 16:22:20.577496  5063 solver.cpp:228]     Train net output #0: softmax = 3.13931 (* 1 = 3.13931 loss)
I0614 16:22:20.577500  5063 solver.cpp:473] Iteration 15150, lr = 0.0001
I0614 16:22:21.460357  5063 solver.cpp:213] Iteration 15160, loss = 3.55146
I0614 16:22:21.460372  5063 solver.cpp:228]     Train net output #0: softmax = 3.55146 (* 1 = 3.55146 loss)
I0614 16:22:21.460377  5063 solver.cpp:473] Iteration 15160, lr = 0.0001
I0614 16:22:22.344110  5063 solver.cpp:213] Iteration 15170, loss = 3.07925
I0614 16:22:22.344125  5063 solver.cpp:228]     Train net output #0: softmax = 3.07925 (* 1 = 3.07925 loss)
I0614 16:22:22.344132  5063 solver.cpp:473] Iteration 15170, lr = 0.0001
I0614 16:22:23.227222  5063 solver.cpp:213] Iteration 15180, loss = 3.15609
I0614 16:22:23.227241  5063 solver.cpp:228]     Train net output #0: softmax = 3.15609 (* 1 = 3.15609 loss)
I0614 16:22:23.227251  5063 solver.cpp:473] Iteration 15180, lr = 0.0001
I0614 16:22:24.107611  5063 solver.cpp:213] Iteration 15190, loss = 3.06076
I0614 16:22:24.107630  5063 solver.cpp:228]     Train net output #0: softmax = 3.06076 (* 1 = 3.06076 loss)
I0614 16:22:24.107635  5063 solver.cpp:473] Iteration 15190, lr = 0.0001
I0614 16:22:24.990348  5063 solver.cpp:213] Iteration 15200, loss = 3.40023
I0614 16:22:24.990363  5063 solver.cpp:228]     Train net output #0: softmax = 3.40023 (* 1 = 3.40023 loss)
I0614 16:22:24.990368  5063 solver.cpp:473] Iteration 15200, lr = 0.0001
I0614 16:22:25.873483  5063 solver.cpp:213] Iteration 15210, loss = 3.09379
I0614 16:22:25.873498  5063 solver.cpp:228]     Train net output #0: softmax = 3.09379 (* 1 = 3.09379 loss)
I0614 16:22:25.873503  5063 solver.cpp:473] Iteration 15210, lr = 0.0001
I0614 16:22:26.756441  5063 solver.cpp:213] Iteration 15220, loss = 3.15091
I0614 16:22:26.756456  5063 solver.cpp:228]     Train net output #0: softmax = 3.15091 (* 1 = 3.15091 loss)
I0614 16:22:26.756461  5063 solver.cpp:473] Iteration 15220, lr = 0.0001
I0614 16:22:27.638599  5063 solver.cpp:213] Iteration 15230, loss = 3.04455
I0614 16:22:27.638614  5063 solver.cpp:228]     Train net output #0: softmax = 3.04455 (* 1 = 3.04455 loss)
I0614 16:22:27.638619  5063 solver.cpp:473] Iteration 15230, lr = 0.0001
I0614 16:22:28.522227  5063 solver.cpp:213] Iteration 15240, loss = 3.28914
I0614 16:22:28.522246  5063 solver.cpp:228]     Train net output #0: softmax = 3.28914 (* 1 = 3.28914 loss)
I0614 16:22:28.522251  5063 solver.cpp:473] Iteration 15240, lr = 0.0001
I0614 16:22:29.405197  5063 solver.cpp:213] Iteration 15250, loss = 2.95295
I0614 16:22:29.405213  5063 solver.cpp:228]     Train net output #0: softmax = 2.95295 (* 1 = 2.95295 loss)
I0614 16:22:29.405231  5063 solver.cpp:473] Iteration 15250, lr = 0.0001
I0614 16:22:30.287966  5063 solver.cpp:213] Iteration 15260, loss = 3.26707
I0614 16:22:30.287981  5063 solver.cpp:228]     Train net output #0: softmax = 3.26707 (* 1 = 3.26707 loss)
I0614 16:22:30.287986  5063 solver.cpp:473] Iteration 15260, lr = 0.0001
I0614 16:22:31.170464  5063 solver.cpp:213] Iteration 15270, loss = 3.10782
I0614 16:22:31.170480  5063 solver.cpp:228]     Train net output #0: softmax = 3.10782 (* 1 = 3.10782 loss)
I0614 16:22:31.170488  5063 solver.cpp:473] Iteration 15270, lr = 0.0001
I0614 16:22:32.053844  5063 solver.cpp:213] Iteration 15280, loss = 2.92493
I0614 16:22:32.053859  5063 solver.cpp:228]     Train net output #0: softmax = 2.92493 (* 1 = 2.92493 loss)
I0614 16:22:32.053864  5063 solver.cpp:473] Iteration 15280, lr = 0.0001
I0614 16:22:32.936883  5063 solver.cpp:213] Iteration 15290, loss = 3.15841
I0614 16:22:32.936897  5063 solver.cpp:228]     Train net output #0: softmax = 3.15841 (* 1 = 3.15841 loss)
I0614 16:22:32.936902  5063 solver.cpp:473] Iteration 15290, lr = 0.0001
I0614 16:22:33.814885  5063 solver.cpp:213] Iteration 15300, loss = 3.13365
I0614 16:22:33.815074  5063 solver.cpp:228]     Train net output #0: softmax = 3.13365 (* 1 = 3.13365 loss)
I0614 16:22:33.815083  5063 solver.cpp:473] Iteration 15300, lr = 0.0001
I0614 16:22:34.698544  5063 solver.cpp:213] Iteration 15310, loss = 3.14031
I0614 16:22:34.698559  5063 solver.cpp:228]     Train net output #0: softmax = 3.14031 (* 1 = 3.14031 loss)
I0614 16:22:34.698564  5063 solver.cpp:473] Iteration 15310, lr = 0.0001
I0614 16:22:35.581496  5063 solver.cpp:213] Iteration 15320, loss = 3.23988
I0614 16:22:35.581511  5063 solver.cpp:228]     Train net output #0: softmax = 3.23988 (* 1 = 3.23988 loss)
I0614 16:22:35.581516  5063 solver.cpp:473] Iteration 15320, lr = 0.0001
I0614 16:22:36.464404  5063 solver.cpp:213] Iteration 15330, loss = 3.32337
I0614 16:22:36.464419  5063 solver.cpp:228]     Train net output #0: softmax = 3.32337 (* 1 = 3.32337 loss)
I0614 16:22:36.464424  5063 solver.cpp:473] Iteration 15330, lr = 0.0001
I0614 16:22:37.346915  5063 solver.cpp:213] Iteration 15340, loss = 2.9482
I0614 16:22:37.346936  5063 solver.cpp:228]     Train net output #0: softmax = 2.9482 (* 1 = 2.9482 loss)
I0614 16:22:37.346941  5063 solver.cpp:473] Iteration 15340, lr = 0.0001
I0614 16:22:38.229676  5063 solver.cpp:213] Iteration 15350, loss = 2.98646
I0614 16:22:38.229692  5063 solver.cpp:228]     Train net output #0: softmax = 2.98646 (* 1 = 2.98646 loss)
I0614 16:22:38.229698  5063 solver.cpp:473] Iteration 15350, lr = 0.0001
I0614 16:22:39.112757  5063 solver.cpp:213] Iteration 15360, loss = 3.27329
I0614 16:22:39.112779  5063 solver.cpp:228]     Train net output #0: softmax = 3.27329 (* 1 = 3.27329 loss)
I0614 16:22:39.112948  5063 solver.cpp:473] Iteration 15360, lr = 0.0001
I0614 16:22:39.995640  5063 solver.cpp:213] Iteration 15370, loss = 3.07201
I0614 16:22:39.995654  5063 solver.cpp:228]     Train net output #0: softmax = 3.07201 (* 1 = 3.07201 loss)
I0614 16:22:39.995661  5063 solver.cpp:473] Iteration 15370, lr = 0.0001
I0614 16:22:40.878515  5063 solver.cpp:213] Iteration 15380, loss = 3.22827
I0614 16:22:40.878530  5063 solver.cpp:228]     Train net output #0: softmax = 3.22827 (* 1 = 3.22827 loss)
I0614 16:22:40.878535  5063 solver.cpp:473] Iteration 15380, lr = 0.0001
I0614 16:22:41.761274  5063 solver.cpp:213] Iteration 15390, loss = 3.26075
I0614 16:22:41.761288  5063 solver.cpp:228]     Train net output #0: softmax = 3.26075 (* 1 = 3.26075 loss)
I0614 16:22:41.761293  5063 solver.cpp:473] Iteration 15390, lr = 0.0001
I0614 16:22:42.644021  5063 solver.cpp:213] Iteration 15400, loss = 3.25162
I0614 16:22:42.644037  5063 solver.cpp:228]     Train net output #0: softmax = 3.25162 (* 1 = 3.25162 loss)
I0614 16:22:42.644042  5063 solver.cpp:473] Iteration 15400, lr = 0.0001
I0614 16:22:43.527088  5063 solver.cpp:213] Iteration 15410, loss = 3.19643
I0614 16:22:43.527107  5063 solver.cpp:228]     Train net output #0: softmax = 3.19643 (* 1 = 3.19643 loss)
I0614 16:22:43.527112  5063 solver.cpp:473] Iteration 15410, lr = 0.0001
I0614 16:22:44.410584  5063 solver.cpp:213] Iteration 15420, loss = 3.02414
I0614 16:22:44.410601  5063 solver.cpp:228]     Train net output #0: softmax = 3.02414 (* 1 = 3.02414 loss)
I0614 16:22:44.410606  5063 solver.cpp:473] Iteration 15420, lr = 0.0001
I0614 16:22:45.293546  5063 solver.cpp:213] Iteration 15430, loss = 3.18136
I0614 16:22:45.293561  5063 solver.cpp:228]     Train net output #0: softmax = 3.18136 (* 1 = 3.18136 loss)
I0614 16:22:45.293571  5063 solver.cpp:473] Iteration 15430, lr = 0.0001
I0614 16:22:46.176105  5063 solver.cpp:213] Iteration 15440, loss = 3.17548
I0614 16:22:46.176118  5063 solver.cpp:228]     Train net output #0: softmax = 3.17548 (* 1 = 3.17548 loss)
I0614 16:22:46.176123  5063 solver.cpp:473] Iteration 15440, lr = 0.0001
I0614 16:22:47.058923  5063 solver.cpp:213] Iteration 15450, loss = 3.02857
I0614 16:22:47.058938  5063 solver.cpp:228]     Train net output #0: softmax = 3.02857 (* 1 = 3.02857 loss)
I0614 16:22:47.058941  5063 solver.cpp:473] Iteration 15450, lr = 0.0001
I0614 16:22:47.941933  5063 solver.cpp:213] Iteration 15460, loss = 3.2706
I0614 16:22:47.941963  5063 solver.cpp:228]     Train net output #0: softmax = 3.2706 (* 1 = 3.2706 loss)
I0614 16:22:47.941969  5063 solver.cpp:473] Iteration 15460, lr = 0.0001
I0614 16:22:48.824894  5063 solver.cpp:213] Iteration 15470, loss = 3.15489
I0614 16:22:48.824913  5063 solver.cpp:228]     Train net output #0: softmax = 3.15489 (* 1 = 3.15489 loss)
I0614 16:22:48.824918  5063 solver.cpp:473] Iteration 15470, lr = 0.0001
I0614 16:22:49.707635  5063 solver.cpp:213] Iteration 15480, loss = 3.0329
I0614 16:22:49.707653  5063 solver.cpp:228]     Train net output #0: softmax = 3.0329 (* 1 = 3.0329 loss)
I0614 16:22:49.707782  5063 solver.cpp:473] Iteration 15480, lr = 0.0001
I0614 16:22:50.590234  5063 solver.cpp:213] Iteration 15490, loss = 3.07213
I0614 16:22:50.590250  5063 solver.cpp:228]     Train net output #0: softmax = 3.07213 (* 1 = 3.07213 loss)
I0614 16:22:50.590253  5063 solver.cpp:473] Iteration 15490, lr = 0.0001
I0614 16:22:51.473404  5063 solver.cpp:213] Iteration 15500, loss = 3.09468
I0614 16:22:51.473419  5063 solver.cpp:228]     Train net output #0: softmax = 3.09468 (* 1 = 3.09468 loss)
I0614 16:22:51.473423  5063 solver.cpp:473] Iteration 15500, lr = 0.0001
I0614 16:22:52.355949  5063 solver.cpp:213] Iteration 15510, loss = 2.94268
I0614 16:22:52.355964  5063 solver.cpp:228]     Train net output #0: softmax = 2.94268 (* 1 = 2.94268 loss)
I0614 16:22:52.355969  5063 solver.cpp:473] Iteration 15510, lr = 0.0001
I0614 16:22:53.239473  5063 solver.cpp:213] Iteration 15520, loss = 3.06552
I0614 16:22:53.239488  5063 solver.cpp:228]     Train net output #0: softmax = 3.06552 (* 1 = 3.06552 loss)
I0614 16:22:53.239493  5063 solver.cpp:473] Iteration 15520, lr = 0.0001
I0614 16:22:54.122390  5063 solver.cpp:213] Iteration 15530, loss = 3.06481
I0614 16:22:54.122408  5063 solver.cpp:228]     Train net output #0: softmax = 3.06481 (* 1 = 3.06481 loss)
I0614 16:22:54.122413  5063 solver.cpp:473] Iteration 15530, lr = 0.0001
I0614 16:22:55.004889  5063 solver.cpp:213] Iteration 15540, loss = 3.14218
I0614 16:22:55.004906  5063 solver.cpp:228]     Train net output #0: softmax = 3.14218 (* 1 = 3.14218 loss)
I0614 16:22:55.005043  5063 solver.cpp:473] Iteration 15540, lr = 0.0001
I0614 16:22:55.887703  5063 solver.cpp:213] Iteration 15550, loss = 3.35298
I0614 16:22:55.887718  5063 solver.cpp:228]     Train net output #0: softmax = 3.35298 (* 1 = 3.35298 loss)
I0614 16:22:55.887723  5063 solver.cpp:473] Iteration 15550, lr = 0.0001
I0614 16:22:56.770129  5063 solver.cpp:213] Iteration 15560, loss = 3.19998
I0614 16:22:56.770144  5063 solver.cpp:228]     Train net output #0: softmax = 3.19998 (* 1 = 3.19998 loss)
I0614 16:22:56.770149  5063 solver.cpp:473] Iteration 15560, lr = 0.0001
I0614 16:22:57.651983  5063 solver.cpp:213] Iteration 15570, loss = 2.96997
I0614 16:22:57.651999  5063 solver.cpp:228]     Train net output #0: softmax = 2.96997 (* 1 = 2.96997 loss)
I0614 16:22:57.652004  5063 solver.cpp:473] Iteration 15570, lr = 0.0001
I0614 16:22:58.534919  5063 solver.cpp:213] Iteration 15580, loss = 3.0435
I0614 16:22:58.534937  5063 solver.cpp:228]     Train net output #0: softmax = 3.0435 (* 1 = 3.0435 loss)
I0614 16:22:58.534942  5063 solver.cpp:473] Iteration 15580, lr = 0.0001
I0614 16:22:59.418207  5063 solver.cpp:213] Iteration 15590, loss = 3.29028
I0614 16:22:59.418229  5063 solver.cpp:228]     Train net output #0: softmax = 3.29028 (* 1 = 3.29028 loss)
I0614 16:22:59.418242  5063 solver.cpp:473] Iteration 15590, lr = 0.0001
I0614 16:23:00.301347  5063 solver.cpp:213] Iteration 15600, loss = 3.13055
I0614 16:23:00.301368  5063 solver.cpp:228]     Train net output #0: softmax = 3.13055 (* 1 = 3.13055 loss)
I0614 16:23:00.301373  5063 solver.cpp:473] Iteration 15600, lr = 0.0001
I0614 16:23:01.183853  5063 solver.cpp:213] Iteration 15610, loss = 3.24883
I0614 16:23:01.183871  5063 solver.cpp:228]     Train net output #0: softmax = 3.24883 (* 1 = 3.24883 loss)
I0614 16:23:01.183876  5063 solver.cpp:473] Iteration 15610, lr = 0.0001
I0614 16:23:02.066707  5063 solver.cpp:213] Iteration 15620, loss = 3.11786
I0614 16:23:02.066742  5063 solver.cpp:228]     Train net output #0: softmax = 3.11786 (* 1 = 3.11786 loss)
I0614 16:23:02.066747  5063 solver.cpp:473] Iteration 15620, lr = 0.0001
I0614 16:23:02.949956  5063 solver.cpp:213] Iteration 15630, loss = 3.33587
I0614 16:23:02.949973  5063 solver.cpp:228]     Train net output #0: softmax = 3.33587 (* 1 = 3.33587 loss)
I0614 16:23:02.949978  5063 solver.cpp:473] Iteration 15630, lr = 0.0001
I0614 16:23:03.832980  5063 solver.cpp:213] Iteration 15640, loss = 3.31744
I0614 16:23:03.833030  5063 solver.cpp:228]     Train net output #0: softmax = 3.31744 (* 1 = 3.31744 loss)
I0614 16:23:03.833036  5063 solver.cpp:473] Iteration 15640, lr = 0.0001
I0614 16:23:04.716421  5063 solver.cpp:213] Iteration 15650, loss = 3.20762
I0614 16:23:04.716440  5063 solver.cpp:228]     Train net output #0: softmax = 3.20762 (* 1 = 3.20762 loss)
I0614 16:23:04.716445  5063 solver.cpp:473] Iteration 15650, lr = 0.0001
I0614 16:23:05.599628  5063 solver.cpp:213] Iteration 15660, loss = 3.20207
I0614 16:23:05.599655  5063 solver.cpp:228]     Train net output #0: softmax = 3.20207 (* 1 = 3.20207 loss)
I0614 16:23:05.599660  5063 solver.cpp:473] Iteration 15660, lr = 0.0001
I0614 16:23:06.482929  5063 solver.cpp:213] Iteration 15670, loss = 3.35605
I0614 16:23:06.482949  5063 solver.cpp:228]     Train net output #0: softmax = 3.35605 (* 1 = 3.35605 loss)
I0614 16:23:06.482954  5063 solver.cpp:473] Iteration 15670, lr = 0.0001
I0614 16:23:07.365437  5063 solver.cpp:213] Iteration 15680, loss = 3.03899
I0614 16:23:07.365455  5063 solver.cpp:228]     Train net output #0: softmax = 3.03899 (* 1 = 3.03899 loss)
I0614 16:23:07.365460  5063 solver.cpp:473] Iteration 15680, lr = 0.0001
I0614 16:23:08.248188  5063 solver.cpp:213] Iteration 15690, loss = 2.87052
I0614 16:23:08.248204  5063 solver.cpp:228]     Train net output #0: softmax = 2.87052 (* 1 = 2.87052 loss)
I0614 16:23:08.248210  5063 solver.cpp:473] Iteration 15690, lr = 0.0001
I0614 16:23:09.131444  5063 solver.cpp:213] Iteration 15700, loss = 2.98499
I0614 16:23:09.131463  5063 solver.cpp:228]     Train net output #0: softmax = 2.98499 (* 1 = 2.98499 loss)
I0614 16:23:09.131467  5063 solver.cpp:473] Iteration 15700, lr = 0.0001
I0614 16:23:10.014554  5063 solver.cpp:213] Iteration 15710, loss = 3.15681
I0614 16:23:10.014569  5063 solver.cpp:228]     Train net output #0: softmax = 3.15681 (* 1 = 3.15681 loss)
I0614 16:23:10.014574  5063 solver.cpp:473] Iteration 15710, lr = 0.0001
I0614 16:23:10.897810  5063 solver.cpp:213] Iteration 15720, loss = 3.07713
I0614 16:23:10.897827  5063 solver.cpp:228]     Train net output #0: softmax = 3.07713 (* 1 = 3.07713 loss)
I0614 16:23:10.897833  5063 solver.cpp:473] Iteration 15720, lr = 0.0001
I0614 16:23:11.780666  5063 solver.cpp:213] Iteration 15730, loss = 3.09386
I0614 16:23:11.780684  5063 solver.cpp:228]     Train net output #0: softmax = 3.09386 (* 1 = 3.09386 loss)
I0614 16:23:11.780689  5063 solver.cpp:473] Iteration 15730, lr = 0.0001
I0614 16:23:12.663730  5063 solver.cpp:213] Iteration 15740, loss = 3.09066
I0614 16:23:12.663748  5063 solver.cpp:228]     Train net output #0: softmax = 3.09066 (* 1 = 3.09066 loss)
I0614 16:23:12.663754  5063 solver.cpp:473] Iteration 15740, lr = 0.0001
I0614 16:23:13.546447  5063 solver.cpp:213] Iteration 15750, loss = 3.333
I0614 16:23:13.546464  5063 solver.cpp:228]     Train net output #0: softmax = 3.333 (* 1 = 3.333 loss)
I0614 16:23:13.546475  5063 solver.cpp:473] Iteration 15750, lr = 0.0001
I0614 16:23:14.429216  5063 solver.cpp:213] Iteration 15760, loss = 3.07912
I0614 16:23:14.429232  5063 solver.cpp:228]     Train net output #0: softmax = 3.07912 (* 1 = 3.07912 loss)
I0614 16:23:14.429237  5063 solver.cpp:473] Iteration 15760, lr = 0.0001
I0614 16:23:15.312175  5063 solver.cpp:213] Iteration 15770, loss = 3.17331
I0614 16:23:15.312193  5063 solver.cpp:228]     Train net output #0: softmax = 3.17331 (* 1 = 3.17331 loss)
I0614 16:23:15.312198  5063 solver.cpp:473] Iteration 15770, lr = 0.0001
I0614 16:23:16.195086  5063 solver.cpp:213] Iteration 15780, loss = 3.25458
I0614 16:23:16.195104  5063 solver.cpp:228]     Train net output #0: softmax = 3.25458 (* 1 = 3.25458 loss)
I0614 16:23:16.195109  5063 solver.cpp:473] Iteration 15780, lr = 0.0001
I0614 16:23:17.077307  5063 solver.cpp:213] Iteration 15790, loss = 3.14542
I0614 16:23:17.077323  5063 solver.cpp:228]     Train net output #0: softmax = 3.14542 (* 1 = 3.14542 loss)
I0614 16:23:17.077328  5063 solver.cpp:473] Iteration 15790, lr = 0.0001
I0614 16:23:17.959053  5063 solver.cpp:213] Iteration 15800, loss = 3.23934
I0614 16:23:17.959084  5063 solver.cpp:228]     Train net output #0: softmax = 3.23934 (* 1 = 3.23934 loss)
I0614 16:23:17.959089  5063 solver.cpp:473] Iteration 15800, lr = 0.0001
I0614 16:23:18.841446  5063 solver.cpp:213] Iteration 15810, loss = 3.16718
I0614 16:23:18.841464  5063 solver.cpp:228]     Train net output #0: softmax = 3.16718 (* 1 = 3.16718 loss)
I0614 16:23:18.841470  5063 solver.cpp:473] Iteration 15810, lr = 0.0001
I0614 16:23:19.723955  5063 solver.cpp:213] Iteration 15820, loss = 3.07399
I0614 16:23:19.723971  5063 solver.cpp:228]     Train net output #0: softmax = 3.07399 (* 1 = 3.07399 loss)
I0614 16:23:19.723978  5063 solver.cpp:473] Iteration 15820, lr = 0.0001
I0614 16:23:20.606583  5063 solver.cpp:213] Iteration 15830, loss = 3.09539
I0614 16:23:20.606600  5063 solver.cpp:228]     Train net output #0: softmax = 3.09539 (* 1 = 3.09539 loss)
I0614 16:23:20.606604  5063 solver.cpp:473] Iteration 15830, lr = 0.0001
I0614 16:23:21.489089  5063 solver.cpp:213] Iteration 15840, loss = 3.06393
I0614 16:23:21.489105  5063 solver.cpp:228]     Train net output #0: softmax = 3.06393 (* 1 = 3.06393 loss)
I0614 16:23:21.489110  5063 solver.cpp:473] Iteration 15840, lr = 0.0001
I0614 16:23:22.371476  5063 solver.cpp:213] Iteration 15850, loss = 2.98932
I0614 16:23:22.371490  5063 solver.cpp:228]     Train net output #0: softmax = 2.98932 (* 1 = 2.98932 loss)
I0614 16:23:22.371495  5063 solver.cpp:473] Iteration 15850, lr = 0.0001
I0614 16:23:23.254438  5063 solver.cpp:213] Iteration 15860, loss = 3.03202
I0614 16:23:23.254456  5063 solver.cpp:228]     Train net output #0: softmax = 3.03202 (* 1 = 3.03202 loss)
I0614 16:23:23.254461  5063 solver.cpp:473] Iteration 15860, lr = 0.0001
I0614 16:23:24.136683  5063 solver.cpp:213] Iteration 15870, loss = 2.96699
I0614 16:23:24.136699  5063 solver.cpp:228]     Train net output #0: softmax = 2.96699 (* 1 = 2.96699 loss)
I0614 16:23:24.136704  5063 solver.cpp:473] Iteration 15870, lr = 0.0001
I0614 16:23:25.019654  5063 solver.cpp:213] Iteration 15880, loss = 2.95207
I0614 16:23:25.019670  5063 solver.cpp:228]     Train net output #0: softmax = 2.95207 (* 1 = 2.95207 loss)
I0614 16:23:25.019675  5063 solver.cpp:473] Iteration 15880, lr = 0.0001
I0614 16:23:25.902320  5063 solver.cpp:213] Iteration 15890, loss = 3.18955
I0614 16:23:25.902338  5063 solver.cpp:228]     Train net output #0: softmax = 3.18955 (* 1 = 3.18955 loss)
I0614 16:23:25.902343  5063 solver.cpp:473] Iteration 15890, lr = 0.0001
I0614 16:23:26.784633  5063 solver.cpp:213] Iteration 15900, loss = 3.14714
I0614 16:23:26.784651  5063 solver.cpp:228]     Train net output #0: softmax = 3.14714 (* 1 = 3.14714 loss)
I0614 16:23:26.784656  5063 solver.cpp:473] Iteration 15900, lr = 0.0001
I0614 16:23:27.667018  5063 solver.cpp:213] Iteration 15910, loss = 3.11687
I0614 16:23:27.667037  5063 solver.cpp:228]     Train net output #0: softmax = 3.11687 (* 1 = 3.11687 loss)
I0614 16:23:27.667048  5063 solver.cpp:473] Iteration 15910, lr = 0.0001
I0614 16:23:28.550112  5063 solver.cpp:213] Iteration 15920, loss = 2.98084
I0614 16:23:28.550129  5063 solver.cpp:228]     Train net output #0: softmax = 2.98084 (* 1 = 2.98084 loss)
I0614 16:23:28.550134  5063 solver.cpp:473] Iteration 15920, lr = 0.0001
I0614 16:23:29.434175  5063 solver.cpp:213] Iteration 15930, loss = 3.24504
I0614 16:23:29.434193  5063 solver.cpp:228]     Train net output #0: softmax = 3.24504 (* 1 = 3.24504 loss)
I0614 16:23:29.434198  5063 solver.cpp:473] Iteration 15930, lr = 0.0001
I0614 16:23:30.318019  5063 solver.cpp:213] Iteration 15940, loss = 3.16512
I0614 16:23:30.318035  5063 solver.cpp:228]     Train net output #0: softmax = 3.16512 (* 1 = 3.16512 loss)
I0614 16:23:30.318040  5063 solver.cpp:473] Iteration 15940, lr = 0.0001
I0614 16:23:31.202121  5063 solver.cpp:213] Iteration 15950, loss = 3.09889
I0614 16:23:31.202139  5063 solver.cpp:228]     Train net output #0: softmax = 3.09889 (* 1 = 3.09889 loss)
I0614 16:23:31.202144  5063 solver.cpp:473] Iteration 15950, lr = 0.0001
I0614 16:23:32.085798  5063 solver.cpp:213] Iteration 15960, loss = 3.18832
I0614 16:23:32.085832  5063 solver.cpp:228]     Train net output #0: softmax = 3.18832 (* 1 = 3.18832 loss)
I0614 16:23:32.085837  5063 solver.cpp:473] Iteration 15960, lr = 0.0001
I0614 16:23:32.968575  5063 solver.cpp:213] Iteration 15970, loss = 3.06319
I0614 16:23:32.968595  5063 solver.cpp:228]     Train net output #0: softmax = 3.06319 (* 1 = 3.06319 loss)
I0614 16:23:32.968598  5063 solver.cpp:473] Iteration 15970, lr = 0.0001
I0614 16:23:33.851589  5063 solver.cpp:213] Iteration 15980, loss = 3.06426
I0614 16:23:33.851637  5063 solver.cpp:228]     Train net output #0: softmax = 3.06426 (* 1 = 3.06426 loss)
I0614 16:23:33.851644  5063 solver.cpp:473] Iteration 15980, lr = 0.0001
I0614 16:23:34.734625  5063 solver.cpp:213] Iteration 15990, loss = 3.14875
I0614 16:23:34.734640  5063 solver.cpp:228]     Train net output #0: softmax = 3.14875 (* 1 = 3.14875 loss)
I0614 16:23:34.734645  5063 solver.cpp:473] Iteration 15990, lr = 0.0001
I0614 16:23:35.559146  5063 solver.cpp:362] Snapshotting to snapshots/16-06-14_15h51m21s_0_11_pretrainClassification_iter_16000.caffemodel
I0614 16:23:35.559917  5063 solver.cpp:370] Snapshotting solver state to snapshots/16-06-14_15h51m21s_0_11_pretrainClassification_iter_16000.solverstate
I0614 16:23:35.560353  5063 solver.cpp:291] Iteration 16000, Testing net (#0)
I0614 16:23:35.672924  5063 solver.cpp:342]     Test net output #0: accuracy_top_1 = 0.2125
I0614 16:23:35.672940  5063 solver.cpp:342]     Test net output #1: accuracy_top_5 = 0.496875
I0614 16:23:35.672946  5063 solver.cpp:342]     Test net output #2: softmax = 3.23245 (* 1 = 3.23245 loss)
I0614 16:23:35.731109  5063 solver.cpp:213] Iteration 16000, loss = 3.27133
I0614 16:23:35.731122  5063 solver.cpp:228]     Train net output #0: softmax = 3.27133 (* 1 = 3.27133 loss)
I0614 16:23:35.731127  5063 solver.cpp:473] Iteration 16000, lr = 0.0001
I0614 16:23:36.614042  5063 solver.cpp:213] Iteration 16010, loss = 2.99917
I0614 16:23:36.614058  5063 solver.cpp:228]     Train net output #0: softmax = 2.99917 (* 1 = 2.99917 loss)
I0614 16:23:36.614063  5063 solver.cpp:473] Iteration 16010, lr = 0.0001
I0614 16:23:37.496320  5063 solver.cpp:213] Iteration 16020, loss = 3.27769
I0614 16:23:37.496336  5063 solver.cpp:228]     Train net output #0: softmax = 3.27769 (* 1 = 3.27769 loss)
I0614 16:23:37.496341  5063 solver.cpp:473] Iteration 16020, lr = 0.0001
I0614 16:23:38.379739  5063 solver.cpp:213] Iteration 16030, loss = 3.2173
I0614 16:23:38.379756  5063 solver.cpp:228]     Train net output #0: softmax = 3.2173 (* 1 = 3.2173 loss)
I0614 16:23:38.379761  5063 solver.cpp:473] Iteration 16030, lr = 0.0001
I0614 16:23:39.262933  5063 solver.cpp:213] Iteration 16040, loss = 2.9553
I0614 16:23:39.262950  5063 solver.cpp:228]     Train net output #0: softmax = 2.9553 (* 1 = 2.9553 loss)
I0614 16:23:39.262964  5063 solver.cpp:473] Iteration 16040, lr = 0.0001
I0614 16:23:40.145802  5063 solver.cpp:213] Iteration 16050, loss = 3.02014
I0614 16:23:40.145819  5063 solver.cpp:228]     Train net output #0: softmax = 3.02014 (* 1 = 3.02014 loss)
I0614 16:23:40.145824  5063 solver.cpp:473] Iteration 16050, lr = 0.0001
I0614 16:23:41.029711  5063 solver.cpp:213] Iteration 16060, loss = 3.21173
I0614 16:23:41.029728  5063 solver.cpp:228]     Train net output #0: softmax = 3.21173 (* 1 = 3.21173 loss)
I0614 16:23:41.029733  5063 solver.cpp:473] Iteration 16060, lr = 0.0001
I0614 16:23:41.913044  5063 solver.cpp:213] Iteration 16070, loss = 2.80269
I0614 16:23:41.913064  5063 solver.cpp:228]     Train net output #0: softmax = 2.80269 (* 1 = 2.80269 loss)
I0614 16:23:41.913069  5063 solver.cpp:473] Iteration 16070, lr = 0.0001
I0614 16:23:42.795974  5063 solver.cpp:213] Iteration 16080, loss = 2.95964
I0614 16:23:42.795992  5063 solver.cpp:228]     Train net output #0: softmax = 2.95964 (* 1 = 2.95964 loss)
I0614 16:23:42.795997  5063 solver.cpp:473] Iteration 16080, lr = 0.0001
I0614 16:23:43.678475  5063 solver.cpp:213] Iteration 16090, loss = 3.03635
I0614 16:23:43.678494  5063 solver.cpp:228]     Train net output #0: softmax = 3.03635 (* 1 = 3.03635 loss)
I0614 16:23:43.678499  5063 solver.cpp:473] Iteration 16090, lr = 0.0001
I0614 16:23:44.561393  5063 solver.cpp:213] Iteration 16100, loss = 3.17788
I0614 16:23:44.561408  5063 solver.cpp:228]     Train net output #0: softmax = 3.17788 (* 1 = 3.17788 loss)
I0614 16:23:44.561414  5063 solver.cpp:473] Iteration 16100, lr = 0.0001
I0614 16:23:45.444257  5063 solver.cpp:213] Iteration 16110, loss = 3.1861
I0614 16:23:45.444274  5063 solver.cpp:228]     Train net output #0: softmax = 3.1861 (* 1 = 3.1861 loss)
I0614 16:23:45.444293  5063 solver.cpp:473] Iteration 16110, lr = 0.0001
I0614 16:23:46.326797  5063 solver.cpp:213] Iteration 16120, loss = 3.49035
I0614 16:23:46.326813  5063 solver.cpp:228]     Train net output #0: softmax = 3.49035 (* 1 = 3.49035 loss)
I0614 16:23:46.326818  5063 solver.cpp:473] Iteration 16120, lr = 0.0001
I0614 16:23:47.209483  5063 solver.cpp:213] Iteration 16130, loss = 3.12026
I0614 16:23:47.209501  5063 solver.cpp:228]     Train net output #0: softmax = 3.12026 (* 1 = 3.12026 loss)
I0614 16:23:47.209506  5063 solver.cpp:473] Iteration 16130, lr = 0.0001
I0614 16:23:48.092221  5063 solver.cpp:213] Iteration 16140, loss = 3.01984
I0614 16:23:48.092238  5063 solver.cpp:228]     Train net output #0: softmax = 3.01984 (* 1 = 3.01984 loss)
I0614 16:23:48.092243  5063 solver.cpp:473] Iteration 16140, lr = 0.0001
I0614 16:23:48.975859  5063 solver.cpp:213] Iteration 16150, loss = 3.17609
I0614 16:23:48.975878  5063 solver.cpp:228]     Train net output #0: softmax = 3.17609 (* 1 = 3.17609 loss)
I0614 16:23:48.975883  5063 solver.cpp:473] Iteration 16150, lr = 0.0001
I0614 16:23:49.858345  5063 solver.cpp:213] Iteration 16160, loss = 3.13401
I0614 16:23:49.858361  5063 solver.cpp:228]     Train net output #0: softmax = 3.13401 (* 1 = 3.13401 loss)
I0614 16:23:49.858366  5063 solver.cpp:473] Iteration 16160, lr = 0.0001
I0614 16:23:50.740833  5063 solver.cpp:213] Iteration 16170, loss = 3.03598
I0614 16:23:50.740849  5063 solver.cpp:228]     Train net output #0: softmax = 3.03598 (* 1 = 3.03598 loss)
I0614 16:23:50.740854  5063 solver.cpp:473] Iteration 16170, lr = 0.0001
I0614 16:23:51.623721  5063 solver.cpp:213] Iteration 16180, loss = 3.2452
I0614 16:23:51.623742  5063 solver.cpp:228]     Train net output #0: softmax = 3.2452 (* 1 = 3.2452 loss)
I0614 16:23:51.623747  5063 solver.cpp:473] Iteration 16180, lr = 0.0001
I0614 16:23:52.506966  5063 solver.cpp:213] Iteration 16190, loss = 3.25108
I0614 16:23:52.506981  5063 solver.cpp:228]     Train net output #0: softmax = 3.25108 (* 1 = 3.25108 loss)
I0614 16:23:52.506988  5063 solver.cpp:473] Iteration 16190, lr = 0.0001
I0614 16:23:53.390130  5063 solver.cpp:213] Iteration 16200, loss = 3.18872
I0614 16:23:53.390151  5063 solver.cpp:228]     Train net output #0: softmax = 3.18872 (* 1 = 3.18872 loss)
I0614 16:23:53.390162  5063 solver.cpp:473] Iteration 16200, lr = 0.0001
I0614 16:23:54.273077  5063 solver.cpp:213] Iteration 16210, loss = 3.02676
I0614 16:23:54.273107  5063 solver.cpp:228]     Train net output #0: softmax = 3.02676 (* 1 = 3.02676 loss)
I0614 16:23:54.273116  5063 solver.cpp:473] Iteration 16210, lr = 0.0001
I0614 16:23:55.156684  5063 solver.cpp:213] Iteration 16220, loss = 3.08579
I0614 16:23:55.156705  5063 solver.cpp:228]     Train net output #0: softmax = 3.08579 (* 1 = 3.08579 loss)
I0614 16:23:55.156710  5063 solver.cpp:473] Iteration 16220, lr = 0.0001
I0614 16:23:56.039734  5063 solver.cpp:213] Iteration 16230, loss = 2.95881
I0614 16:23:56.039752  5063 solver.cpp:228]     Train net output #0: softmax = 2.95881 (* 1 = 2.95881 loss)
I0614 16:23:56.039757  5063 solver.cpp:473] Iteration 16230, lr = 0.0001
I0614 16:23:56.923149  5063 solver.cpp:213] Iteration 16240, loss = 3.08966
I0614 16:23:56.923171  5063 solver.cpp:228]     Train net output #0: softmax = 3.08966 (* 1 = 3.08966 loss)
I0614 16:23:56.923177  5063 solver.cpp:473] Iteration 16240, lr = 0.0001
I0614 16:23:57.806291  5063 solver.cpp:213] Iteration 16250, loss = 3.07979
I0614 16:23:57.806308  5063 solver.cpp:228]     Train net output #0: softmax = 3.07979 (* 1 = 3.07979 loss)
I0614 16:23:57.806313  5063 solver.cpp:473] Iteration 16250, lr = 0.0001
I0614 16:23:58.689636  5063 solver.cpp:213] Iteration 16260, loss = 2.9421
I0614 16:23:58.689666  5063 solver.cpp:228]     Train net output #0: softmax = 2.9421 (* 1 = 2.9421 loss)
I0614 16:23:58.689671  5063 solver.cpp:473] Iteration 16260, lr = 0.0001
I0614 16:23:59.572949  5063 solver.cpp:213] Iteration 16270, loss = 2.99619
I0614 16:23:59.572967  5063 solver.cpp:228]     Train net output #0: softmax = 2.99619 (* 1 = 2.99619 loss)
I0614 16:23:59.572988  5063 solver.cpp:473] Iteration 16270, lr = 0.0001
I0614 16:24:00.456614  5063 solver.cpp:213] Iteration 16280, loss = 3.08364
I0614 16:24:00.456632  5063 solver.cpp:228]     Train net output #0: softmax = 3.08364 (* 1 = 3.08364 loss)
I0614 16:24:00.456637  5063 solver.cpp:473] Iteration 16280, lr = 0.0001
I0614 16:24:01.339138  5063 solver.cpp:213] Iteration 16290, loss = 3.30254
I0614 16:24:01.339155  5063 solver.cpp:228]     Train net output #0: softmax = 3.30254 (* 1 = 3.30254 loss)
I0614 16:24:01.339160  5063 solver.cpp:473] Iteration 16290, lr = 0.0001
I0614 16:24:02.223012  5063 solver.cpp:213] Iteration 16300, loss = 3.26541
I0614 16:24:02.223036  5063 solver.cpp:228]     Train net output #0: softmax = 3.26541 (* 1 = 3.26541 loss)
I0614 16:24:02.223040  5063 solver.cpp:473] Iteration 16300, lr = 0.0001
I0614 16:24:03.105988  5063 solver.cpp:213] Iteration 16310, loss = 3.22673
I0614 16:24:03.106005  5063 solver.cpp:228]     Train net output #0: softmax = 3.22673 (* 1 = 3.22673 loss)
I0614 16:24:03.106010  5063 solver.cpp:473] Iteration 16310, lr = 0.0001
I0614 16:24:03.988101  5063 solver.cpp:213] Iteration 16320, loss = 3.11689
I0614 16:24:03.988154  5063 solver.cpp:228]     Train net output #0: softmax = 3.11689 (* 1 = 3.11689 loss)
I0614 16:24:03.988160  5063 solver.cpp:473] Iteration 16320, lr = 0.0001
I0614 16:24:04.871184  5063 solver.cpp:213] Iteration 16330, loss = 3.19865
I0614 16:24:04.871213  5063 solver.cpp:228]     Train net output #0: softmax = 3.19865 (* 1 = 3.19865 loss)
I0614 16:24:04.871222  5063 solver.cpp:473] Iteration 16330, lr = 0.0001
I0614 16:24:05.749471  5063 solver.cpp:213] Iteration 16340, loss = 3.21734
I0614 16:24:05.749490  5063 solver.cpp:228]     Train net output #0: softmax = 3.21734 (* 1 = 3.21734 loss)
I0614 16:24:05.749495  5063 solver.cpp:473] Iteration 16340, lr = 0.0001
I0614 16:24:06.632201  5063 solver.cpp:213] Iteration 16350, loss = 3.2981
I0614 16:24:06.632220  5063 solver.cpp:228]     Train net output #0: softmax = 3.2981 (* 1 = 3.2981 loss)
I0614 16:24:06.632225  5063 solver.cpp:473] Iteration 16350, lr = 0.0001
I0614 16:24:07.515343  5063 solver.cpp:213] Iteration 16360, loss = 3.13613
I0614 16:24:07.515362  5063 solver.cpp:228]     Train net output #0: softmax = 3.13613 (* 1 = 3.13613 loss)
I0614 16:24:07.515372  5063 solver.cpp:473] Iteration 16360, lr = 0.0001
I0614 16:24:08.397210  5063 solver.cpp:213] Iteration 16370, loss = 3.11774
I0614 16:24:08.397225  5063 solver.cpp:228]     Train net output #0: softmax = 3.11774 (* 1 = 3.11774 loss)
I0614 16:24:08.397230  5063 solver.cpp:473] Iteration 16370, lr = 0.0001
I0614 16:24:09.279815  5063 solver.cpp:213] Iteration 16380, loss = 3.08515
I0614 16:24:09.279834  5063 solver.cpp:228]     Train net output #0: softmax = 3.08515 (* 1 = 3.08515 loss)
I0614 16:24:09.279839  5063 solver.cpp:473] Iteration 16380, lr = 0.0001
I0614 16:24:10.162475  5063 solver.cpp:213] Iteration 16390, loss = 3.28753
I0614 16:24:10.162503  5063 solver.cpp:228]     Train net output #0: softmax = 3.28753 (* 1 = 3.28753 loss)
I0614 16:24:10.162510  5063 solver.cpp:473] Iteration 16390, lr = 0.0001
I0614 16:24:11.046178  5063 solver.cpp:213] Iteration 16400, loss = 3.11795
I0614 16:24:11.046200  5063 solver.cpp:228]     Train net output #0: softmax = 3.11795 (* 1 = 3.11795 loss)
I0614 16:24:11.046205  5063 solver.cpp:473] Iteration 16400, lr = 0.0001
I0614 16:24:11.929164  5063 solver.cpp:213] Iteration 16410, loss = 3.3653
I0614 16:24:11.929183  5063 solver.cpp:228]     Train net output #0: softmax = 3.3653 (* 1 = 3.3653 loss)
I0614 16:24:11.929188  5063 solver.cpp:473] Iteration 16410, lr = 0.0001
I0614 16:24:12.811843  5063 solver.cpp:213] Iteration 16420, loss = 3.15553
I0614 16:24:12.811861  5063 solver.cpp:228]     Train net output #0: softmax = 3.15553 (* 1 = 3.15553 loss)
I0614 16:24:12.811864  5063 solver.cpp:473] Iteration 16420, lr = 0.0001
I0614 16:24:13.694859  5063 solver.cpp:213] Iteration 16430, loss = 3.32731
I0614 16:24:13.694876  5063 solver.cpp:228]     Train net output #0: softmax = 3.32731 (* 1 = 3.32731 loss)
I0614 16:24:13.694881  5063 solver.cpp:473] Iteration 16430, lr = 0.0001
I0614 16:24:14.578294  5063 solver.cpp:213] Iteration 16440, loss = 3.24711
I0614 16:24:14.578315  5063 solver.cpp:228]     Train net output #0: softmax = 3.24711 (* 1 = 3.24711 loss)
I0614 16:24:14.578320  5063 solver.cpp:473] Iteration 16440, lr = 0.0001
I0614 16:24:15.461668  5063 solver.cpp:213] Iteration 16450, loss = 3.19319
I0614 16:24:15.461689  5063 solver.cpp:228]     Train net output #0: softmax = 3.19319 (* 1 = 3.19319 loss)
I0614 16:24:15.461694  5063 solver.cpp:473] Iteration 16450, lr = 0.0001
I0614 16:24:16.344899  5063 solver.cpp:213] Iteration 16460, loss = 3.12582
I0614 16:24:16.344916  5063 solver.cpp:228]     Train net output #0: softmax = 3.12582 (* 1 = 3.12582 loss)
I0614 16:24:16.344921  5063 solver.cpp:473] Iteration 16460, lr = 0.0001
I0614 16:24:17.227461  5063 solver.cpp:213] Iteration 16470, loss = 3.00722
I0614 16:24:17.227479  5063 solver.cpp:228]     Train net output #0: softmax = 3.00722 (* 1 = 3.00722 loss)
I0614 16:24:17.227483  5063 solver.cpp:473] Iteration 16470, lr = 0.0001
I0614 16:24:18.110730  5063 solver.cpp:213] Iteration 16480, loss = 3.4047
I0614 16:24:18.110764  5063 solver.cpp:228]     Train net output #0: softmax = 3.4047 (* 1 = 3.4047 loss)
I0614 16:24:18.110769  5063 solver.cpp:473] Iteration 16480, lr = 0.0001
I0614 16:24:18.994472  5063 solver.cpp:213] Iteration 16490, loss = 3.30377
I0614 16:24:18.994494  5063 solver.cpp:228]     Train net output #0: softmax = 3.30377 (* 1 = 3.30377 loss)
I0614 16:24:18.994499  5063 solver.cpp:473] Iteration 16490, lr = 0.0001
I0614 16:24:19.876713  5063 solver.cpp:213] Iteration 16500, loss = 3.03091
I0614 16:24:19.876731  5063 solver.cpp:228]     Train net output #0: softmax = 3.03091 (* 1 = 3.03091 loss)
I0614 16:24:19.876736  5063 solver.cpp:473] Iteration 16500, lr = 0.0001
I0614 16:24:20.758940  5063 solver.cpp:213] Iteration 16510, loss = 3.24151
I0614 16:24:20.758968  5063 solver.cpp:228]     Train net output #0: softmax = 3.24151 (* 1 = 3.24151 loss)
I0614 16:24:20.758976  5063 solver.cpp:473] Iteration 16510, lr = 0.0001
I0614 16:24:21.642684  5063 solver.cpp:213] Iteration 16520, loss = 3.22379
I0614 16:24:21.642701  5063 solver.cpp:228]     Train net output #0: softmax = 3.22379 (* 1 = 3.22379 loss)
I0614 16:24:21.642712  5063 solver.cpp:473] Iteration 16520, lr = 0.0001
I0614 16:24:22.526291  5063 solver.cpp:213] Iteration 16530, loss = 3.07179
I0614 16:24:22.526311  5063 solver.cpp:228]     Train net output #0: softmax = 3.07179 (* 1 = 3.07179 loss)
I0614 16:24:22.526316  5063 solver.cpp:473] Iteration 16530, lr = 0.0001
I0614 16:24:23.409097  5063 solver.cpp:213] Iteration 16540, loss = 3.16708
I0614 16:24:23.409118  5063 solver.cpp:228]     Train net output #0: softmax = 3.16708 (* 1 = 3.16708 loss)
I0614 16:24:23.409122  5063 solver.cpp:473] Iteration 16540, lr = 0.0001
I0614 16:24:24.291993  5063 solver.cpp:213] Iteration 16550, loss = 3.1482
I0614 16:24:24.292011  5063 solver.cpp:228]     Train net output #0: softmax = 3.1482 (* 1 = 3.1482 loss)
I0614 16:24:24.292014  5063 solver.cpp:473] Iteration 16550, lr = 0.0001
I0614 16:24:25.174448  5063 solver.cpp:213] Iteration 16560, loss = 3.1922
I0614 16:24:25.174466  5063 solver.cpp:228]     Train net output #0: softmax = 3.1922 (* 1 = 3.1922 loss)
I0614 16:24:25.174471  5063 solver.cpp:473] Iteration 16560, lr = 0.0001
I0614 16:24:26.057466  5063 solver.cpp:213] Iteration 16570, loss = 3.1578
I0614 16:24:26.057497  5063 solver.cpp:228]     Train net output #0: softmax = 3.1578 (* 1 = 3.1578 loss)
I0614 16:24:26.057504  5063 solver.cpp:473] Iteration 16570, lr = 0.0001
I0614 16:24:26.940732  5063 solver.cpp:213] Iteration 16580, loss = 3.12774
I0614 16:24:26.940749  5063 solver.cpp:228]     Train net output #0: softmax = 3.12774 (* 1 = 3.12774 loss)
I0614 16:24:26.940754  5063 solver.cpp:473] Iteration 16580, lr = 0.0001
I0614 16:24:27.823564  5063 solver.cpp:213] Iteration 16590, loss = 3.27675
I0614 16:24:27.823580  5063 solver.cpp:228]     Train net output #0: softmax = 3.27675 (* 1 = 3.27675 loss)
I0614 16:24:27.823585  5063 solver.cpp:473] Iteration 16590, lr = 0.0001
I0614 16:24:28.705401  5063 solver.cpp:213] Iteration 16600, loss = 3.21874
I0614 16:24:28.705418  5063 solver.cpp:228]     Train net output #0: softmax = 3.21874 (* 1 = 3.21874 loss)
I0614 16:24:28.705423  5063 solver.cpp:473] Iteration 16600, lr = 0.0001
I0614 16:24:29.581926  5063 solver.cpp:213] Iteration 16610, loss = 3.11869
I0614 16:24:29.581949  5063 solver.cpp:228]     Train net output #0: softmax = 3.11869 (* 1 = 3.11869 loss)
I0614 16:24:29.581956  5063 solver.cpp:473] Iteration 16610, lr = 0.0001
I0614 16:24:30.464555  5063 solver.cpp:213] Iteration 16620, loss = 3.09644
I0614 16:24:30.464576  5063 solver.cpp:228]     Train net output #0: softmax = 3.09644 (* 1 = 3.09644 loss)
I0614 16:24:30.464581  5063 solver.cpp:473] Iteration 16620, lr = 0.0001
I0614 16:24:31.348165  5063 solver.cpp:213] Iteration 16630, loss = 3.35895
I0614 16:24:31.348194  5063 solver.cpp:228]     Train net output #0: softmax = 3.35895 (* 1 = 3.35895 loss)
I0614 16:24:31.348202  5063 solver.cpp:473] Iteration 16630, lr = 0.0001
I0614 16:24:32.230819  5063 solver.cpp:213] Iteration 16640, loss = 3.248
I0614 16:24:32.230852  5063 solver.cpp:228]     Train net output #0: softmax = 3.248 (* 1 = 3.248 loss)
I0614 16:24:32.230857  5063 solver.cpp:473] Iteration 16640, lr = 0.0001
I0614 16:24:33.114380  5063 solver.cpp:213] Iteration 16650, loss = 3.03178
I0614 16:24:33.114395  5063 solver.cpp:228]     Train net output #0: softmax = 3.03178 (* 1 = 3.03178 loss)
I0614 16:24:33.114400  5063 solver.cpp:473] Iteration 16650, lr = 0.0001
I0614 16:24:33.997753  5063 solver.cpp:213] Iteration 16660, loss = 3.2377
I0614 16:24:33.997804  5063 solver.cpp:228]     Train net output #0: softmax = 3.2377 (* 1 = 3.2377 loss)
I0614 16:24:33.997819  5063 solver.cpp:473] Iteration 16660, lr = 0.0001
I0614 16:24:34.881700  5063 solver.cpp:213] Iteration 16670, loss = 3.25623
I0614 16:24:34.881719  5063 solver.cpp:228]     Train net output #0: softmax = 3.25623 (* 1 = 3.25623 loss)
I0614 16:24:34.881723  5063 solver.cpp:473] Iteration 16670, lr = 0.0001
I0614 16:24:35.764693  5063 solver.cpp:213] Iteration 16680, loss = 3.3295
I0614 16:24:35.764710  5063 solver.cpp:228]     Train net output #0: softmax = 3.3295 (* 1 = 3.3295 loss)
I0614 16:24:35.764720  5063 solver.cpp:473] Iteration 16680, lr = 0.0001
I0614 16:24:36.647595  5063 solver.cpp:213] Iteration 16690, loss = 3.3809
I0614 16:24:36.647614  5063 solver.cpp:228]     Train net output #0: softmax = 3.3809 (* 1 = 3.3809 loss)
I0614 16:24:36.647619  5063 solver.cpp:473] Iteration 16690, lr = 0.0001
I0614 16:24:37.525857  5063 solver.cpp:213] Iteration 16700, loss = 3.2347
I0614 16:24:37.525882  5063 solver.cpp:228]     Train net output #0: softmax = 3.2347 (* 1 = 3.2347 loss)
I0614 16:24:37.525889  5063 solver.cpp:473] Iteration 16700, lr = 0.0001
I0614 16:24:38.400476  5063 solver.cpp:213] Iteration 16710, loss = 3.07215
I0614 16:24:38.400492  5063 solver.cpp:228]     Train net output #0: softmax = 3.07215 (* 1 = 3.07215 loss)
I0614 16:24:38.400497  5063 solver.cpp:473] Iteration 16710, lr = 0.0001
I0614 16:24:39.283515  5063 solver.cpp:213] Iteration 16720, loss = 3.04952
I0614 16:24:39.283531  5063 solver.cpp:228]     Train net output #0: softmax = 3.04952 (* 1 = 3.04952 loss)
I0614 16:24:39.283536  5063 solver.cpp:473] Iteration 16720, lr = 0.0001
I0614 16:24:40.166820  5063 solver.cpp:213] Iteration 16730, loss = 2.96761
I0614 16:24:40.166837  5063 solver.cpp:228]     Train net output #0: softmax = 2.96761 (* 1 = 2.96761 loss)
I0614 16:24:40.166842  5063 solver.cpp:473] Iteration 16730, lr = 0.0001
I0614 16:24:41.049521  5063 solver.cpp:213] Iteration 16740, loss = 3.14136
I0614 16:24:41.049540  5063 solver.cpp:228]     Train net output #0: softmax = 3.14136 (* 1 = 3.14136 loss)
I0614 16:24:41.049545  5063 solver.cpp:473] Iteration 16740, lr = 0.0001
I0614 16:24:41.926630  5063 solver.cpp:213] Iteration 16750, loss = 3.04617
I0614 16:24:41.926656  5063 solver.cpp:228]     Train net output #0: softmax = 3.04617 (* 1 = 3.04617 loss)
I0614 16:24:41.926661  5063 solver.cpp:473] Iteration 16750, lr = 0.0001
I0614 16:24:42.810356  5063 solver.cpp:213] Iteration 16760, loss = 3.10059
I0614 16:24:42.810374  5063 solver.cpp:228]     Train net output #0: softmax = 3.10059 (* 1 = 3.10059 loss)
I0614 16:24:42.810377  5063 solver.cpp:473] Iteration 16760, lr = 0.0001
I0614 16:24:43.693691  5063 solver.cpp:213] Iteration 16770, loss = 3.03995
I0614 16:24:43.693709  5063 solver.cpp:228]     Train net output #0: softmax = 3.03995 (* 1 = 3.03995 loss)
I0614 16:24:43.693714  5063 solver.cpp:473] Iteration 16770, lr = 0.0001
I0614 16:24:44.576702  5063 solver.cpp:213] Iteration 16780, loss = 3.17346
I0614 16:24:44.576719  5063 solver.cpp:228]     Train net output #0: softmax = 3.17346 (* 1 = 3.17346 loss)
I0614 16:24:44.576724  5063 solver.cpp:473] Iteration 16780, lr = 0.0001
I0614 16:24:45.459597  5063 solver.cpp:213] Iteration 16790, loss = 3.15601
I0614 16:24:45.459614  5063 solver.cpp:228]     Train net output #0: softmax = 3.15601 (* 1 = 3.15601 loss)
I0614 16:24:45.459617  5063 solver.cpp:473] Iteration 16790, lr = 0.0001
I0614 16:24:46.342464  5063 solver.cpp:213] Iteration 16800, loss = 3.18911
I0614 16:24:46.342481  5063 solver.cpp:228]     Train net output #0: softmax = 3.18911 (* 1 = 3.18911 loss)
I0614 16:24:46.342485  5063 solver.cpp:473] Iteration 16800, lr = 0.0001
I0614 16:24:47.226060  5063 solver.cpp:213] Iteration 16810, loss = 3.02819
I0614 16:24:47.226084  5063 solver.cpp:228]     Train net output #0: softmax = 3.02819 (* 1 = 3.02819 loss)
I0614 16:24:47.226090  5063 solver.cpp:473] Iteration 16810, lr = 0.0001
I0614 16:24:48.109331  5063 solver.cpp:213] Iteration 16820, loss = 3.30106
I0614 16:24:48.109364  5063 solver.cpp:228]     Train net output #0: softmax = 3.30106 (* 1 = 3.30106 loss)
I0614 16:24:48.109369  5063 solver.cpp:473] Iteration 16820, lr = 0.0001
I0614 16:24:48.992720  5063 solver.cpp:213] Iteration 16830, loss = 3.1415
I0614 16:24:48.992739  5063 solver.cpp:228]     Train net output #0: softmax = 3.1415 (* 1 = 3.1415 loss)
I0614 16:24:48.992743  5063 solver.cpp:473] Iteration 16830, lr = 0.0001
I0614 16:24:49.876168  5063 solver.cpp:213] Iteration 16840, loss = 3.11402
I0614 16:24:49.876185  5063 solver.cpp:228]     Train net output #0: softmax = 3.11402 (* 1 = 3.11402 loss)
I0614 16:24:49.876195  5063 solver.cpp:473] Iteration 16840, lr = 0.0001
I0614 16:24:50.759011  5063 solver.cpp:213] Iteration 16850, loss = 3.03037
I0614 16:24:50.759027  5063 solver.cpp:228]     Train net output #0: softmax = 3.03037 (* 1 = 3.03037 loss)
I0614 16:24:50.759032  5063 solver.cpp:473] Iteration 16850, lr = 0.0001
I0614 16:24:51.642390  5063 solver.cpp:213] Iteration 16860, loss = 3.10856
I0614 16:24:51.642408  5063 solver.cpp:228]     Train net output #0: softmax = 3.10856 (* 1 = 3.10856 loss)
I0614 16:24:51.642413  5063 solver.cpp:473] Iteration 16860, lr = 0.0001
I0614 16:24:52.525393  5063 solver.cpp:213] Iteration 16870, loss = 3.05517
I0614 16:24:52.525413  5063 solver.cpp:228]     Train net output #0: softmax = 3.05517 (* 1 = 3.05517 loss)
I0614 16:24:52.525418  5063 solver.cpp:473] Iteration 16870, lr = 0.0001
I0614 16:24:53.408818  5063 solver.cpp:213] Iteration 16880, loss = 3.21962
I0614 16:24:53.408838  5063 solver.cpp:228]     Train net output #0: softmax = 3.21962 (* 1 = 3.21962 loss)
I0614 16:24:53.408843  5063 solver.cpp:473] Iteration 16880, lr = 0.0001
I0614 16:24:54.292376  5063 solver.cpp:213] Iteration 16890, loss = 3.28666
I0614 16:24:54.292395  5063 solver.cpp:228]     Train net output #0: softmax = 3.28666 (* 1 = 3.28666 loss)
I0614 16:24:54.292400  5063 solver.cpp:473] Iteration 16890, lr = 0.0001
I0614 16:24:55.174552  5063 solver.cpp:213] Iteration 16900, loss = 3.08025
I0614 16:24:55.174572  5063 solver.cpp:228]     Train net output #0: softmax = 3.08025 (* 1 = 3.08025 loss)
I0614 16:24:55.174577  5063 solver.cpp:473] Iteration 16900, lr = 0.0001
I0614 16:24:56.057703  5063 solver.cpp:213] Iteration 16910, loss = 3.13244
I0614 16:24:56.057720  5063 solver.cpp:228]     Train net output #0: softmax = 3.13244 (* 1 = 3.13244 loss)
I0614 16:24:56.057725  5063 solver.cpp:473] Iteration 16910, lr = 0.0001
I0614 16:24:56.941500  5063 solver.cpp:213] Iteration 16920, loss = 3.10976
I0614 16:24:56.941520  5063 solver.cpp:228]     Train net output #0: softmax = 3.10976 (* 1 = 3.10976 loss)
I0614 16:24:56.941525  5063 solver.cpp:473] Iteration 16920, lr = 0.0001
I0614 16:24:57.825533  5063 solver.cpp:213] Iteration 16930, loss = 3.16345
I0614 16:24:57.825563  5063 solver.cpp:228]     Train net output #0: softmax = 3.16345 (* 1 = 3.16345 loss)
I0614 16:24:57.825572  5063 solver.cpp:473] Iteration 16930, lr = 0.0001
I0614 16:24:58.709450  5063 solver.cpp:213] Iteration 16940, loss = 3.17215
I0614 16:24:58.709471  5063 solver.cpp:228]     Train net output #0: softmax = 3.17215 (* 1 = 3.17215 loss)
I0614 16:24:58.709476  5063 solver.cpp:473] Iteration 16940, lr = 0.0001
I0614 16:24:59.592887  5063 solver.cpp:213] Iteration 16950, loss = 3.23331
I0614 16:24:59.592907  5063 solver.cpp:228]     Train net output #0: softmax = 3.23331 (* 1 = 3.23331 loss)
I0614 16:24:59.592912  5063 solver.cpp:473] Iteration 16950, lr = 0.0001
I0614 16:25:00.475564  5063 solver.cpp:213] Iteration 16960, loss = 2.97756
I0614 16:25:00.475581  5063 solver.cpp:228]     Train net output #0: softmax = 2.97756 (* 1 = 2.97756 loss)
I0614 16:25:00.475586  5063 solver.cpp:473] Iteration 16960, lr = 0.0001
I0614 16:25:01.358999  5063 solver.cpp:213] Iteration 16970, loss = 3.21605
I0614 16:25:01.359020  5063 solver.cpp:228]     Train net output #0: softmax = 3.21605 (* 1 = 3.21605 loss)
I0614 16:25:01.359025  5063 solver.cpp:473] Iteration 16970, lr = 0.0001
I0614 16:25:02.242563  5063 solver.cpp:213] Iteration 16980, loss = 3.13167
I0614 16:25:02.242600  5063 solver.cpp:228]     Train net output #0: softmax = 3.13167 (* 1 = 3.13167 loss)
I0614 16:25:02.242606  5063 solver.cpp:473] Iteration 16980, lr = 0.0001
I0614 16:25:03.125968  5063 solver.cpp:213] Iteration 16990, loss = 3.20503
I0614 16:25:03.125988  5063 solver.cpp:228]     Train net output #0: softmax = 3.20503 (* 1 = 3.20503 loss)
I0614 16:25:03.125993  5063 solver.cpp:473] Iteration 16990, lr = 0.0001
I0614 16:25:03.950526  5063 solver.cpp:362] Snapshotting to snapshots/16-06-14_15h51m21s_0_11_pretrainClassification_iter_17000.caffemodel
I0614 16:25:03.951292  5063 solver.cpp:370] Snapshotting solver state to snapshots/16-06-14_15h51m21s_0_11_pretrainClassification_iter_17000.solverstate
I0614 16:25:03.951738  5063 solver.cpp:291] Iteration 17000, Testing net (#0)
I0614 16:25:04.064460  5063 solver.cpp:342]     Test net output #0: accuracy_top_1 = 0.239063
I0614 16:25:04.064513  5063 solver.cpp:342]     Test net output #1: accuracy_top_5 = 0.528125
I0614 16:25:04.064530  5063 solver.cpp:342]     Test net output #2: softmax = 3.1657 (* 1 = 3.1657 loss)
I0614 16:25:04.122772  5063 solver.cpp:213] Iteration 17000, loss = 3.02514
I0614 16:25:04.122786  5063 solver.cpp:228]     Train net output #0: softmax = 3.02514 (* 1 = 3.02514 loss)
I0614 16:25:04.122792  5063 solver.cpp:473] Iteration 17000, lr = 0.0001
I0614 16:25:05.006202  5063 solver.cpp:213] Iteration 17010, loss = 3.12184
I0614 16:25:05.006220  5063 solver.cpp:228]     Train net output #0: softmax = 3.12184 (* 1 = 3.12184 loss)
I0614 16:25:05.006225  5063 solver.cpp:473] Iteration 17010, lr = 0.0001
I0614 16:25:05.889024  5063 solver.cpp:213] Iteration 17020, loss = 3.14617
I0614 16:25:05.889040  5063 solver.cpp:228]     Train net output #0: softmax = 3.14617 (* 1 = 3.14617 loss)
I0614 16:25:05.889045  5063 solver.cpp:473] Iteration 17020, lr = 0.0001
I0614 16:25:06.772018  5063 solver.cpp:213] Iteration 17030, loss = 3.19412
I0614 16:25:06.772037  5063 solver.cpp:228]     Train net output #0: softmax = 3.19412 (* 1 = 3.19412 loss)
I0614 16:25:06.772042  5063 solver.cpp:473] Iteration 17030, lr = 0.0001
I0614 16:25:07.655680  5063 solver.cpp:213] Iteration 17040, loss = 3.26079
I0614 16:25:07.655697  5063 solver.cpp:228]     Train net output #0: softmax = 3.26079 (* 1 = 3.26079 loss)
I0614 16:25:07.655701  5063 solver.cpp:473] Iteration 17040, lr = 0.0001
I0614 16:25:08.539839  5063 solver.cpp:213] Iteration 17050, loss = 3.00804
I0614 16:25:08.539860  5063 solver.cpp:228]     Train net output #0: softmax = 3.00804 (* 1 = 3.00804 loss)
I0614 16:25:08.539865  5063 solver.cpp:473] Iteration 17050, lr = 0.0001
I0614 16:25:09.423425  5063 solver.cpp:213] Iteration 17060, loss = 3.24149
I0614 16:25:09.423444  5063 solver.cpp:228]     Train net output #0: softmax = 3.24149 (* 1 = 3.24149 loss)
I0614 16:25:09.423449  5063 solver.cpp:473] Iteration 17060, lr = 0.0001
I0614 16:25:10.306568  5063 solver.cpp:213] Iteration 17070, loss = 3.4955
I0614 16:25:10.306591  5063 solver.cpp:228]     Train net output #0: softmax = 3.4955 (* 1 = 3.4955 loss)
I0614 16:25:10.306596  5063 solver.cpp:473] Iteration 17070, lr = 0.0001
I0614 16:25:11.189170  5063 solver.cpp:213] Iteration 17080, loss = 3.04314
I0614 16:25:11.189187  5063 solver.cpp:228]     Train net output #0: softmax = 3.04314 (* 1 = 3.04314 loss)
I0614 16:25:11.189191  5063 solver.cpp:473] Iteration 17080, lr = 0.0001
I0614 16:25:12.072402  5063 solver.cpp:213] Iteration 17090, loss = 3.17286
I0614 16:25:12.072419  5063 solver.cpp:228]     Train net output #0: softmax = 3.17286 (* 1 = 3.17286 loss)
I0614 16:25:12.072424  5063 solver.cpp:473] Iteration 17090, lr = 0.0001
I0614 16:25:12.955523  5063 solver.cpp:213] Iteration 17100, loss = 3.30875
I0614 16:25:12.955540  5063 solver.cpp:228]     Train net output #0: softmax = 3.30875 (* 1 = 3.30875 loss)
I0614 16:25:12.955545  5063 solver.cpp:473] Iteration 17100, lr = 0.0001
I0614 16:25:13.839011  5063 solver.cpp:213] Iteration 17110, loss = 2.82286
I0614 16:25:13.839030  5063 solver.cpp:228]     Train net output #0: softmax = 2.82286 (* 1 = 2.82286 loss)
I0614 16:25:13.839035  5063 solver.cpp:473] Iteration 17110, lr = 0.0001
I0614 16:25:14.722249  5063 solver.cpp:213] Iteration 17120, loss = 3.00105
I0614 16:25:14.722267  5063 solver.cpp:228]     Train net output #0: softmax = 3.00105 (* 1 = 3.00105 loss)
I0614 16:25:14.722272  5063 solver.cpp:473] Iteration 17120, lr = 0.0001
I0614 16:25:15.605932  5063 solver.cpp:213] Iteration 17130, loss = 3.17066
I0614 16:25:15.605955  5063 solver.cpp:228]     Train net output #0: softmax = 3.17066 (* 1 = 3.17066 loss)
I0614 16:25:15.605960  5063 solver.cpp:473] Iteration 17130, lr = 0.0001
I0614 16:25:16.488790  5063 solver.cpp:213] Iteration 17140, loss = 3.23132
I0614 16:25:16.488806  5063 solver.cpp:228]     Train net output #0: softmax = 3.23132 (* 1 = 3.23132 loss)
I0614 16:25:16.488811  5063 solver.cpp:473] Iteration 17140, lr = 0.0001
I0614 16:25:17.372217  5063 solver.cpp:213] Iteration 17150, loss = 3.13751
I0614 16:25:17.372251  5063 solver.cpp:228]     Train net output #0: softmax = 3.13751 (* 1 = 3.13751 loss)
I0614 16:25:17.372256  5063 solver.cpp:473] Iteration 17150, lr = 0.0001
I0614 16:25:18.255201  5063 solver.cpp:213] Iteration 17160, loss = 3.09784
I0614 16:25:18.255218  5063 solver.cpp:228]     Train net output #0: softmax = 3.09784 (* 1 = 3.09784 loss)
I0614 16:25:18.255223  5063 solver.cpp:473] Iteration 17160, lr = 0.0001
I0614 16:25:19.138592  5063 solver.cpp:213] Iteration 17170, loss = 2.92116
I0614 16:25:19.138612  5063 solver.cpp:228]     Train net output #0: softmax = 2.92116 (* 1 = 2.92116 loss)
I0614 16:25:19.138617  5063 solver.cpp:473] Iteration 17170, lr = 0.0001
I0614 16:25:20.021402  5063 solver.cpp:213] Iteration 17180, loss = 3.2015
I0614 16:25:20.021420  5063 solver.cpp:228]     Train net output #0: softmax = 3.2015 (* 1 = 3.2015 loss)
I0614 16:25:20.021425  5063 solver.cpp:473] Iteration 17180, lr = 0.0001
I0614 16:25:20.904237  5063 solver.cpp:213] Iteration 17190, loss = 3.13176
I0614 16:25:20.904258  5063 solver.cpp:228]     Train net output #0: softmax = 3.13176 (* 1 = 3.13176 loss)
I0614 16:25:20.904263  5063 solver.cpp:473] Iteration 17190, lr = 0.0001
I0614 16:25:21.787411  5063 solver.cpp:213] Iteration 17200, loss = 3.11413
I0614 16:25:21.787431  5063 solver.cpp:228]     Train net output #0: softmax = 3.11413 (* 1 = 3.11413 loss)
I0614 16:25:21.787436  5063 solver.cpp:473] Iteration 17200, lr = 0.0001
I0614 16:25:22.671195  5063 solver.cpp:213] Iteration 17210, loss = 3.2694
I0614 16:25:22.671211  5063 solver.cpp:228]     Train net output #0: softmax = 3.2694 (* 1 = 3.2694 loss)
I0614 16:25:22.671216  5063 solver.cpp:473] Iteration 17210, lr = 0.0001
I0614 16:25:23.555474  5063 solver.cpp:213] Iteration 17220, loss = 3.22702
I0614 16:25:23.555492  5063 solver.cpp:228]     Train net output #0: softmax = 3.22702 (* 1 = 3.22702 loss)
I0614 16:25:23.555497  5063 solver.cpp:473] Iteration 17220, lr = 0.0001
I0614 16:25:24.439724  5063 solver.cpp:213] Iteration 17230, loss = 3.06598
I0614 16:25:24.439744  5063 solver.cpp:228]     Train net output #0: softmax = 3.06598 (* 1 = 3.06598 loss)
I0614 16:25:24.439749  5063 solver.cpp:473] Iteration 17230, lr = 0.0001
I0614 16:25:25.323786  5063 solver.cpp:213] Iteration 17240, loss = 3.01415
I0614 16:25:25.323807  5063 solver.cpp:228]     Train net output #0: softmax = 3.01415 (* 1 = 3.01415 loss)
I0614 16:25:25.323812  5063 solver.cpp:473] Iteration 17240, lr = 0.0001
I0614 16:25:26.207170  5063 solver.cpp:213] Iteration 17250, loss = 3.03416
I0614 16:25:26.207192  5063 solver.cpp:228]     Train net output #0: softmax = 3.03416 (* 1 = 3.03416 loss)
I0614 16:25:26.207196  5063 solver.cpp:473] Iteration 17250, lr = 0.0001
I0614 16:25:27.090850  5063 solver.cpp:213] Iteration 17260, loss = 3.04101
I0614 16:25:27.090868  5063 solver.cpp:228]     Train net output #0: softmax = 3.04101 (* 1 = 3.04101 loss)
I0614 16:25:27.090873  5063 solver.cpp:473] Iteration 17260, lr = 0.0001
I0614 16:25:27.973687  5063 solver.cpp:213] Iteration 17270, loss = 3.17165
I0614 16:25:27.973706  5063 solver.cpp:228]     Train net output #0: softmax = 3.17165 (* 1 = 3.17165 loss)
I0614 16:25:27.973711  5063 solver.cpp:473] Iteration 17270, lr = 0.0001
I0614 16:25:28.856914  5063 solver.cpp:213] Iteration 17280, loss = 3.09717
I0614 16:25:28.856933  5063 solver.cpp:228]     Train net output #0: softmax = 3.09717 (* 1 = 3.09717 loss)
I0614 16:25:28.856938  5063 solver.cpp:473] Iteration 17280, lr = 0.0001
I0614 16:25:29.740155  5063 solver.cpp:213] Iteration 17290, loss = 3.12115
I0614 16:25:29.740183  5063 solver.cpp:228]     Train net output #0: softmax = 3.12115 (* 1 = 3.12115 loss)
I0614 16:25:29.740191  5063 solver.cpp:473] Iteration 17290, lr = 0.0001
I0614 16:25:30.623474  5063 solver.cpp:213] Iteration 17300, loss = 3.10288
I0614 16:25:30.623492  5063 solver.cpp:228]     Train net output #0: softmax = 3.10288 (* 1 = 3.10288 loss)
I0614 16:25:30.623497  5063 solver.cpp:473] Iteration 17300, lr = 0.0001
I0614 16:25:31.506516  5063 solver.cpp:213] Iteration 17310, loss = 3.298
I0614 16:25:31.506549  5063 solver.cpp:228]     Train net output #0: softmax = 3.298 (* 1 = 3.298 loss)
I0614 16:25:31.506554  5063 solver.cpp:473] Iteration 17310, lr = 0.0001
I0614 16:25:32.389494  5063 solver.cpp:213] Iteration 17320, loss = 3.2516
I0614 16:25:32.389514  5063 solver.cpp:228]     Train net output #0: softmax = 3.2516 (* 1 = 3.2516 loss)
I0614 16:25:32.389519  5063 solver.cpp:473] Iteration 17320, lr = 0.0001
I0614 16:25:33.270015  5063 solver.cpp:213] Iteration 17330, loss = 3.14744
I0614 16:25:33.270035  5063 solver.cpp:228]     Train net output #0: softmax = 3.14744 (* 1 = 3.14744 loss)
I0614 16:25:33.270040  5063 solver.cpp:473] Iteration 17330, lr = 0.0001
I0614 16:25:34.150615  5063 solver.cpp:213] Iteration 17340, loss = 3.37423
I0614 16:25:34.150650  5063 solver.cpp:228]     Train net output #0: softmax = 3.37423 (* 1 = 3.37423 loss)
I0614 16:25:34.150655  5063 solver.cpp:473] Iteration 17340, lr = 0.0001
I0614 16:25:35.033900  5063 solver.cpp:213] Iteration 17350, loss = 2.88243
I0614 16:25:35.033918  5063 solver.cpp:228]     Train net output #0: softmax = 2.88243 (* 1 = 2.88243 loss)
I0614 16:25:35.033923  5063 solver.cpp:473] Iteration 17350, lr = 0.0001
I0614 16:25:35.917651  5063 solver.cpp:213] Iteration 17360, loss = 3.18367
I0614 16:25:35.917668  5063 solver.cpp:228]     Train net output #0: softmax = 3.18367 (* 1 = 3.18367 loss)
I0614 16:25:35.917673  5063 solver.cpp:473] Iteration 17360, lr = 0.0001
I0614 16:25:36.800787  5063 solver.cpp:213] Iteration 17370, loss = 3.07921
I0614 16:25:36.800809  5063 solver.cpp:228]     Train net output #0: softmax = 3.07921 (* 1 = 3.07921 loss)
I0614 16:25:36.800814  5063 solver.cpp:473] Iteration 17370, lr = 0.0001
I0614 16:25:37.684223  5063 solver.cpp:213] Iteration 17380, loss = 3.01495
I0614 16:25:37.684243  5063 solver.cpp:228]     Train net output #0: softmax = 3.01495 (* 1 = 3.01495 loss)
I0614 16:25:37.684248  5063 solver.cpp:473] Iteration 17380, lr = 0.0001
I0614 16:25:38.567840  5063 solver.cpp:213] Iteration 17390, loss = 3.11584
I0614 16:25:38.567859  5063 solver.cpp:228]     Train net output #0: softmax = 3.11584 (* 1 = 3.11584 loss)
I0614 16:25:38.567864  5063 solver.cpp:473] Iteration 17390, lr = 0.0001
I0614 16:25:39.451238  5063 solver.cpp:213] Iteration 17400, loss = 2.93517
I0614 16:25:39.451256  5063 solver.cpp:228]     Train net output #0: softmax = 2.93517 (* 1 = 2.93517 loss)
I0614 16:25:39.451261  5063 solver.cpp:473] Iteration 17400, lr = 0.0001
I0614 16:25:40.334656  5063 solver.cpp:213] Iteration 17410, loss = 3.20777
I0614 16:25:40.334684  5063 solver.cpp:228]     Train net output #0: softmax = 3.20777 (* 1 = 3.20777 loss)
I0614 16:25:40.334692  5063 solver.cpp:473] Iteration 17410, lr = 0.0001
I0614 16:25:41.217702  5063 solver.cpp:213] Iteration 17420, loss = 2.82655
I0614 16:25:41.217717  5063 solver.cpp:228]     Train net output #0: softmax = 2.82655 (* 1 = 2.82655 loss)
I0614 16:25:41.217722  5063 solver.cpp:473] Iteration 17420, lr = 0.0001
I0614 16:25:42.101557  5063 solver.cpp:213] Iteration 17430, loss = 3.27994
I0614 16:25:42.101577  5063 solver.cpp:228]     Train net output #0: softmax = 3.27994 (* 1 = 3.27994 loss)
I0614 16:25:42.101582  5063 solver.cpp:473] Iteration 17430, lr = 0.0001
I0614 16:25:42.984959  5063 solver.cpp:213] Iteration 17440, loss = 3.1524
I0614 16:25:42.984977  5063 solver.cpp:228]     Train net output #0: softmax = 3.1524 (* 1 = 3.1524 loss)
I0614 16:25:42.984982  5063 solver.cpp:473] Iteration 17440, lr = 0.0001
I0614 16:25:43.868409  5063 solver.cpp:213] Iteration 17450, loss = 3.21531
I0614 16:25:43.868424  5063 solver.cpp:228]     Train net output #0: softmax = 3.21531 (* 1 = 3.21531 loss)
I0614 16:25:43.868429  5063 solver.cpp:473] Iteration 17450, lr = 0.0001
I0614 16:25:44.751792  5063 solver.cpp:213] Iteration 17460, loss = 3.18653
I0614 16:25:44.751811  5063 solver.cpp:228]     Train net output #0: softmax = 3.18653 (* 1 = 3.18653 loss)
I0614 16:25:44.751816  5063 solver.cpp:473] Iteration 17460, lr = 0.0001
I0614 16:25:45.634395  5063 solver.cpp:213] Iteration 17470, loss = 3.05154
I0614 16:25:45.634413  5063 solver.cpp:228]     Train net output #0: softmax = 3.05154 (* 1 = 3.05154 loss)
I0614 16:25:45.634418  5063 solver.cpp:473] Iteration 17470, lr = 0.0001
I0614 16:25:46.517961  5063 solver.cpp:213] Iteration 17480, loss = 3.21802
I0614 16:25:46.517977  5063 solver.cpp:228]     Train net output #0: softmax = 3.21802 (* 1 = 3.21802 loss)
I0614 16:25:46.517982  5063 solver.cpp:473] Iteration 17480, lr = 0.0001
I0614 16:25:47.400436  5063 solver.cpp:213] Iteration 17490, loss = 3.19911
I0614 16:25:47.400455  5063 solver.cpp:228]     Train net output #0: softmax = 3.19911 (* 1 = 3.19911 loss)
I0614 16:25:47.400460  5063 solver.cpp:473] Iteration 17490, lr = 0.0001
I0614 16:25:48.283442  5063 solver.cpp:213] Iteration 17500, loss = 3.26237
I0614 16:25:48.283473  5063 solver.cpp:228]     Train net output #0: softmax = 3.26237 (* 1 = 3.26237 loss)
I0614 16:25:48.283478  5063 solver.cpp:473] Iteration 17500, lr = 0.0001
I0614 16:25:49.165416  5063 solver.cpp:213] Iteration 17510, loss = 3.01019
I0614 16:25:49.165431  5063 solver.cpp:228]     Train net output #0: softmax = 3.01019 (* 1 = 3.01019 loss)
I0614 16:25:49.165436  5063 solver.cpp:473] Iteration 17510, lr = 0.0001
I0614 16:25:50.048296  5063 solver.cpp:213] Iteration 17520, loss = 3.21375
I0614 16:25:50.048312  5063 solver.cpp:228]     Train net output #0: softmax = 3.21375 (* 1 = 3.21375 loss)
I0614 16:25:50.048317  5063 solver.cpp:473] Iteration 17520, lr = 0.0001
I0614 16:25:50.931123  5063 solver.cpp:213] Iteration 17530, loss = 3.17986
I0614 16:25:50.931143  5063 solver.cpp:228]     Train net output #0: softmax = 3.17986 (* 1 = 3.17986 loss)
I0614 16:25:50.931148  5063 solver.cpp:473] Iteration 17530, lr = 0.0001
I0614 16:25:51.813601  5063 solver.cpp:213] Iteration 17540, loss = 3.12836
I0614 16:25:51.813618  5063 solver.cpp:228]     Train net output #0: softmax = 3.12836 (* 1 = 3.12836 loss)
I0614 16:25:51.813623  5063 solver.cpp:473] Iteration 17540, lr = 0.0001
I0614 16:25:52.696367  5063 solver.cpp:213] Iteration 17550, loss = 3.41583
I0614 16:25:52.696389  5063 solver.cpp:228]     Train net output #0: softmax = 3.41583 (* 1 = 3.41583 loss)
I0614 16:25:52.696394  5063 solver.cpp:473] Iteration 17550, lr = 0.0001
I0614 16:25:53.580162  5063 solver.cpp:213] Iteration 17560, loss = 3.03882
I0614 16:25:53.580183  5063 solver.cpp:228]     Train net output #0: softmax = 3.03882 (* 1 = 3.03882 loss)
I0614 16:25:53.580188  5063 solver.cpp:473] Iteration 17560, lr = 0.0001
I0614 16:25:54.463640  5063 solver.cpp:213] Iteration 17570, loss = 2.91096
I0614 16:25:54.463662  5063 solver.cpp:228]     Train net output #0: softmax = 2.91096 (* 1 = 2.91096 loss)
I0614 16:25:54.463667  5063 solver.cpp:473] Iteration 17570, lr = 0.0001
I0614 16:25:55.346997  5063 solver.cpp:213] Iteration 17580, loss = 3.09854
I0614 16:25:55.347018  5063 solver.cpp:228]     Train net output #0: softmax = 3.09854 (* 1 = 3.09854 loss)
I0614 16:25:55.347023  5063 solver.cpp:473] Iteration 17580, lr = 0.0001
I0614 16:25:56.230360  5063 solver.cpp:213] Iteration 17590, loss = 3.05274
I0614 16:25:56.230377  5063 solver.cpp:228]     Train net output #0: softmax = 3.05274 (* 1 = 3.05274 loss)
I0614 16:25:56.230382  5063 solver.cpp:473] Iteration 17590, lr = 0.0001
I0614 16:25:57.112634  5063 solver.cpp:213] Iteration 17600, loss = 3.09902
I0614 16:25:57.112666  5063 solver.cpp:228]     Train net output #0: softmax = 3.09902 (* 1 = 3.09902 loss)
I0614 16:25:57.112676  5063 solver.cpp:473] Iteration 17600, lr = 0.0001
I0614 16:25:57.993113  5063 solver.cpp:213] Iteration 17610, loss = 3.07603
I0614 16:25:57.993135  5063 solver.cpp:228]     Train net output #0: softmax = 3.07603 (* 1 = 3.07603 loss)
I0614 16:25:57.993139  5063 solver.cpp:473] Iteration 17610, lr = 0.0001
I0614 16:25:58.876365  5063 solver.cpp:213] Iteration 17620, loss = 3.10146
I0614 16:25:58.876385  5063 solver.cpp:228]     Train net output #0: softmax = 3.10146 (* 1 = 3.10146 loss)
I0614 16:25:58.876390  5063 solver.cpp:473] Iteration 17620, lr = 0.0001
I0614 16:25:59.761253  5063 solver.cpp:213] Iteration 17630, loss = 3.13833
I0614 16:25:59.761283  5063 solver.cpp:228]     Train net output #0: softmax = 3.13833 (* 1 = 3.13833 loss)
I0614 16:25:59.761288  5063 solver.cpp:473] Iteration 17630, lr = 0.0001
I0614 16:26:00.644923  5063 solver.cpp:213] Iteration 17640, loss = 2.90618
I0614 16:26:00.644944  5063 solver.cpp:228]     Train net output #0: softmax = 2.90618 (* 1 = 2.90618 loss)
I0614 16:26:00.644948  5063 solver.cpp:473] Iteration 17640, lr = 0.0001
I0614 16:26:01.526864  5063 solver.cpp:213] Iteration 17650, loss = 3.19943
I0614 16:26:01.526878  5063 solver.cpp:228]     Train net output #0: softmax = 3.19943 (* 1 = 3.19943 loss)
I0614 16:26:01.526883  5063 solver.cpp:473] Iteration 17650, lr = 0.0001
I0614 16:26:02.410723  5063 solver.cpp:213] Iteration 17660, loss = 2.9769
I0614 16:26:02.410759  5063 solver.cpp:228]     Train net output #0: softmax = 2.9769 (* 1 = 2.9769 loss)
I0614 16:26:02.410764  5063 solver.cpp:473] Iteration 17660, lr = 0.0001
I0614 16:26:03.294934  5063 solver.cpp:213] Iteration 17670, loss = 3.09194
I0614 16:26:03.294956  5063 solver.cpp:228]     Train net output #0: softmax = 3.09194 (* 1 = 3.09194 loss)
I0614 16:26:03.294961  5063 solver.cpp:473] Iteration 17670, lr = 0.0001
I0614 16:26:04.178961  5063 solver.cpp:213] Iteration 17680, loss = 2.83561
I0614 16:26:04.179018  5063 solver.cpp:228]     Train net output #0: softmax = 2.83561 (* 1 = 2.83561 loss)
I0614 16:26:04.179033  5063 solver.cpp:473] Iteration 17680, lr = 0.0001
I0614 16:26:05.063529  5063 solver.cpp:213] Iteration 17690, loss = 3.17865
I0614 16:26:05.063549  5063 solver.cpp:228]     Train net output #0: softmax = 3.17865 (* 1 = 3.17865 loss)
I0614 16:26:05.063554  5063 solver.cpp:473] Iteration 17690, lr = 0.0001
I0614 16:26:05.947393  5063 solver.cpp:213] Iteration 17700, loss = 3.13238
I0614 16:26:05.947414  5063 solver.cpp:228]     Train net output #0: softmax = 3.13238 (* 1 = 3.13238 loss)
I0614 16:26:05.947418  5063 solver.cpp:473] Iteration 17700, lr = 0.0001
I0614 16:26:06.831507  5063 solver.cpp:213] Iteration 17710, loss = 3.19723
I0614 16:26:06.831527  5063 solver.cpp:228]     Train net output #0: softmax = 3.19723 (* 1 = 3.19723 loss)
I0614 16:26:06.831532  5063 solver.cpp:473] Iteration 17710, lr = 0.0001
I0614 16:26:07.714825  5063 solver.cpp:213] Iteration 17720, loss = 2.9094
I0614 16:26:07.714844  5063 solver.cpp:228]     Train net output #0: softmax = 2.9094 (* 1 = 2.9094 loss)
I0614 16:26:07.714849  5063 solver.cpp:473] Iteration 17720, lr = 0.0001
I0614 16:26:08.598482  5063 solver.cpp:213] Iteration 17730, loss = 3.32751
I0614 16:26:08.598508  5063 solver.cpp:228]     Train net output #0: softmax = 3.32751 (* 1 = 3.32751 loss)
I0614 16:26:08.598513  5063 solver.cpp:473] Iteration 17730, lr = 0.0001
I0614 16:26:09.482343  5063 solver.cpp:213] Iteration 17740, loss = 2.83284
I0614 16:26:09.482365  5063 solver.cpp:228]     Train net output #0: softmax = 2.83284 (* 1 = 2.83284 loss)
I0614 16:26:09.482370  5063 solver.cpp:473] Iteration 17740, lr = 0.0001
I0614 16:26:10.365520  5063 solver.cpp:213] Iteration 17750, loss = 3.2974
I0614 16:26:10.365537  5063 solver.cpp:228]     Train net output #0: softmax = 3.2974 (* 1 = 3.2974 loss)
I0614 16:26:10.365542  5063 solver.cpp:473] Iteration 17750, lr = 0.0001
I0614 16:26:11.248875  5063 solver.cpp:213] Iteration 17760, loss = 3.3467
I0614 16:26:11.248893  5063 solver.cpp:228]     Train net output #0: softmax = 3.3467 (* 1 = 3.3467 loss)
I0614 16:26:11.248898  5063 solver.cpp:473] Iteration 17760, lr = 0.0001
I0614 16:26:12.132930  5063 solver.cpp:213] Iteration 17770, loss = 3.18709
I0614 16:26:12.132951  5063 solver.cpp:228]     Train net output #0: softmax = 3.18709 (* 1 = 3.18709 loss)
I0614 16:26:12.132956  5063 solver.cpp:473] Iteration 17770, lr = 0.0001
I0614 16:26:13.015923  5063 solver.cpp:213] Iteration 17780, loss = 3.02701
I0614 16:26:13.015940  5063 solver.cpp:228]     Train net output #0: softmax = 3.02701 (* 1 = 3.02701 loss)
I0614 16:26:13.015945  5063 solver.cpp:473] Iteration 17780, lr = 0.0001
I0614 16:26:13.900022  5063 solver.cpp:213] Iteration 17790, loss = 2.88309
I0614 16:26:13.900055  5063 solver.cpp:228]     Train net output #0: softmax = 2.88309 (* 1 = 2.88309 loss)
I0614 16:26:13.900063  5063 solver.cpp:473] Iteration 17790, lr = 0.0001
I0614 16:26:14.783138  5063 solver.cpp:213] Iteration 17800, loss = 3.18266
I0614 16:26:14.783159  5063 solver.cpp:228]     Train net output #0: softmax = 3.18266 (* 1 = 3.18266 loss)
I0614 16:26:14.783164  5063 solver.cpp:473] Iteration 17800, lr = 0.0001
I0614 16:26:15.665931  5063 solver.cpp:213] Iteration 17810, loss = 2.80477
I0614 16:26:15.665948  5063 solver.cpp:228]     Train net output #0: softmax = 2.80477 (* 1 = 2.80477 loss)
I0614 16:26:15.665953  5063 solver.cpp:473] Iteration 17810, lr = 0.0001
I0614 16:26:16.549157  5063 solver.cpp:213] Iteration 17820, loss = 3.25505
I0614 16:26:16.549175  5063 solver.cpp:228]     Train net output #0: softmax = 3.25505 (* 1 = 3.25505 loss)
I0614 16:26:16.549180  5063 solver.cpp:473] Iteration 17820, lr = 0.0001
I0614 16:26:17.433174  5063 solver.cpp:213] Iteration 17830, loss = 3.29848
I0614 16:26:17.433193  5063 solver.cpp:228]     Train net output #0: softmax = 3.29848 (* 1 = 3.29848 loss)
I0614 16:26:17.433198  5063 solver.cpp:473] Iteration 17830, lr = 0.0001
I0614 16:26:18.316126  5063 solver.cpp:213] Iteration 17840, loss = 3.19581
I0614 16:26:18.316161  5063 solver.cpp:228]     Train net output #0: softmax = 3.19581 (* 1 = 3.19581 loss)
I0614 16:26:18.316166  5063 solver.cpp:473] Iteration 17840, lr = 0.0001
I0614 16:26:19.199956  5063 solver.cpp:213] Iteration 17850, loss = 3.08348
I0614 16:26:19.199980  5063 solver.cpp:228]     Train net output #0: softmax = 3.08348 (* 1 = 3.08348 loss)
I0614 16:26:19.199985  5063 solver.cpp:473] Iteration 17850, lr = 0.0001
I0614 16:26:20.082131  5063 solver.cpp:213] Iteration 17860, loss = 2.86909
I0614 16:26:20.082149  5063 solver.cpp:228]     Train net output #0: softmax = 2.86909 (* 1 = 2.86909 loss)
I0614 16:26:20.082154  5063 solver.cpp:473] Iteration 17860, lr = 0.0001
I0614 16:26:20.966140  5063 solver.cpp:213] Iteration 17870, loss = 2.91926
I0614 16:26:20.966158  5063 solver.cpp:228]     Train net output #0: softmax = 2.91926 (* 1 = 2.91926 loss)
I0614 16:26:20.966163  5063 solver.cpp:473] Iteration 17870, lr = 0.0001
I0614 16:26:21.850361  5063 solver.cpp:213] Iteration 17880, loss = 2.96787
I0614 16:26:21.850381  5063 solver.cpp:228]     Train net output #0: softmax = 2.96787 (* 1 = 2.96787 loss)
I0614 16:26:21.850386  5063 solver.cpp:473] Iteration 17880, lr = 0.0001
I0614 16:26:22.734051  5063 solver.cpp:213] Iteration 17890, loss = 3.07996
I0614 16:26:22.734072  5063 solver.cpp:228]     Train net output #0: softmax = 3.07996 (* 1 = 3.07996 loss)
I0614 16:26:22.734076  5063 solver.cpp:473] Iteration 17890, lr = 0.0001
I0614 16:26:23.617660  5063 solver.cpp:213] Iteration 17900, loss = 3.01463
I0614 16:26:23.617676  5063 solver.cpp:228]     Train net output #0: softmax = 3.01463 (* 1 = 3.01463 loss)
I0614 16:26:23.617681  5063 solver.cpp:473] Iteration 17900, lr = 0.0001
I0614 16:26:24.501287  5063 solver.cpp:213] Iteration 17910, loss = 3.0932
I0614 16:26:24.501310  5063 solver.cpp:228]     Train net output #0: softmax = 3.0932 (* 1 = 3.0932 loss)
I0614 16:26:24.501315  5063 solver.cpp:473] Iteration 17910, lr = 0.0001
I0614 16:26:25.385278  5063 solver.cpp:213] Iteration 17920, loss = 3.17947
I0614 16:26:25.385296  5063 solver.cpp:228]     Train net output #0: softmax = 3.17947 (* 1 = 3.17947 loss)
I0614 16:26:25.385301  5063 solver.cpp:473] Iteration 17920, lr = 0.0001
I0614 16:26:26.269470  5063 solver.cpp:213] Iteration 17930, loss = 3.18012
I0614 16:26:26.269490  5063 solver.cpp:228]     Train net output #0: softmax = 3.18012 (* 1 = 3.18012 loss)
I0614 16:26:26.269500  5063 solver.cpp:473] Iteration 17930, lr = 0.0001
I0614 16:26:27.153539  5063 solver.cpp:213] Iteration 17940, loss = 3.07297
I0614 16:26:27.153558  5063 solver.cpp:228]     Train net output #0: softmax = 3.07297 (* 1 = 3.07297 loss)
I0614 16:26:27.153563  5063 solver.cpp:473] Iteration 17940, lr = 0.0001
I0614 16:26:28.037731  5063 solver.cpp:213] Iteration 17950, loss = 3.08903
I0614 16:26:28.037750  5063 solver.cpp:228]     Train net output #0: softmax = 3.08903 (* 1 = 3.08903 loss)
I0614 16:26:28.037755  5063 solver.cpp:473] Iteration 17950, lr = 0.0001
I0614 16:26:28.921490  5063 solver.cpp:213] Iteration 17960, loss = 3.14565
I0614 16:26:28.921509  5063 solver.cpp:228]     Train net output #0: softmax = 3.14565 (* 1 = 3.14565 loss)
I0614 16:26:28.921514  5063 solver.cpp:473] Iteration 17960, lr = 0.0001
I0614 16:26:29.805639  5063 solver.cpp:213] Iteration 17970, loss = 3.2088
I0614 16:26:29.805673  5063 solver.cpp:228]     Train net output #0: softmax = 3.2088 (* 1 = 3.2088 loss)
I0614 16:26:29.805681  5063 solver.cpp:473] Iteration 17970, lr = 0.0001
I0614 16:26:30.690317  5063 solver.cpp:213] Iteration 17980, loss = 2.98631
I0614 16:26:30.690338  5063 solver.cpp:228]     Train net output #0: softmax = 2.98631 (* 1 = 2.98631 loss)
I0614 16:26:30.690343  5063 solver.cpp:473] Iteration 17980, lr = 0.0001
I0614 16:26:31.573585  5063 solver.cpp:213] Iteration 17990, loss = 2.91842
I0614 16:26:31.573601  5063 solver.cpp:228]     Train net output #0: softmax = 2.91842 (* 1 = 2.91842 loss)
I0614 16:26:31.573606  5063 solver.cpp:473] Iteration 17990, lr = 0.0001
I0614 16:26:32.398757  5063 solver.cpp:362] Snapshotting to snapshots/16-06-14_15h51m21s_0_11_pretrainClassification_iter_18000.caffemodel
I0614 16:26:32.399507  5063 solver.cpp:370] Snapshotting solver state to snapshots/16-06-14_15h51m21s_0_11_pretrainClassification_iter_18000.solverstate
I0614 16:26:32.399938  5063 solver.cpp:291] Iteration 18000, Testing net (#0)
I0614 16:26:32.512459  5063 solver.cpp:342]     Test net output #0: accuracy_top_1 = 0.242188
I0614 16:26:32.512473  5063 solver.cpp:342]     Test net output #1: accuracy_top_5 = 0.53125
I0614 16:26:32.512480  5063 solver.cpp:342]     Test net output #2: softmax = 3.08751 (* 1 = 3.08751 loss)
I0614 16:26:32.570497  5063 solver.cpp:213] Iteration 18000, loss = 3.18147
I0614 16:26:32.570510  5063 solver.cpp:228]     Train net output #0: softmax = 3.18147 (* 1 = 3.18147 loss)
I0614 16:26:32.570515  5063 solver.cpp:473] Iteration 18000, lr = 0.0001
I0614 16:26:33.453969  5063 solver.cpp:213] Iteration 18010, loss = 3.00889
I0614 16:26:33.453984  5063 solver.cpp:228]     Train net output #0: softmax = 3.00889 (* 1 = 3.00889 loss)
I0614 16:26:33.453989  5063 solver.cpp:473] Iteration 18010, lr = 0.0001
I0614 16:26:34.338253  5063 solver.cpp:213] Iteration 18020, loss = 3.25464
I0614 16:26:34.338311  5063 solver.cpp:228]     Train net output #0: softmax = 3.25464 (* 1 = 3.25464 loss)
I0614 16:26:34.338316  5063 solver.cpp:473] Iteration 18020, lr = 0.0001
I0614 16:26:35.222064  5063 solver.cpp:213] Iteration 18030, loss = 3.24355
I0614 16:26:35.222089  5063 solver.cpp:228]     Train net output #0: softmax = 3.24355 (* 1 = 3.24355 loss)
I0614 16:26:35.222093  5063 solver.cpp:473] Iteration 18030, lr = 0.0001
I0614 16:26:36.105427  5063 solver.cpp:213] Iteration 18040, loss = 3.03314
I0614 16:26:36.105443  5063 solver.cpp:228]     Train net output #0: softmax = 3.03314 (* 1 = 3.03314 loss)
I0614 16:26:36.105448  5063 solver.cpp:473] Iteration 18040, lr = 0.0001
I0614 16:26:36.988932  5063 solver.cpp:213] Iteration 18050, loss = 2.83871
I0614 16:26:36.988950  5063 solver.cpp:228]     Train net output #0: softmax = 2.83871 (* 1 = 2.83871 loss)
I0614 16:26:36.988955  5063 solver.cpp:473] Iteration 18050, lr = 0.0001
I0614 16:26:37.871510  5063 solver.cpp:213] Iteration 18060, loss = 3.11464
I0614 16:26:37.871532  5063 solver.cpp:228]     Train net output #0: softmax = 3.11464 (* 1 = 3.11464 loss)
I0614 16:26:37.871537  5063 solver.cpp:473] Iteration 18060, lr = 0.0001
I0614 16:26:38.754842  5063 solver.cpp:213] Iteration 18070, loss = 3.06935
I0614 16:26:38.754864  5063 solver.cpp:228]     Train net output #0: softmax = 3.06935 (* 1 = 3.06935 loss)
I0614 16:26:38.754869  5063 solver.cpp:473] Iteration 18070, lr = 0.0001
I0614 16:26:39.637511  5063 solver.cpp:213] Iteration 18080, loss = 3.07281
I0614 16:26:39.637528  5063 solver.cpp:228]     Train net output #0: softmax = 3.07281 (* 1 = 3.07281 loss)
I0614 16:26:39.637533  5063 solver.cpp:473] Iteration 18080, lr = 0.0001
I0614 16:26:40.520299  5063 solver.cpp:213] Iteration 18090, loss = 3.15939
I0614 16:26:40.520323  5063 solver.cpp:228]     Train net output #0: softmax = 3.15939 (* 1 = 3.15939 loss)
I0614 16:26:40.520328  5063 solver.cpp:473] Iteration 18090, lr = 0.0001
I0614 16:26:41.403470  5063 solver.cpp:213] Iteration 18100, loss = 2.94506
I0614 16:26:41.403488  5063 solver.cpp:228]     Train net output #0: softmax = 2.94506 (* 1 = 2.94506 loss)
I0614 16:26:41.403492  5063 solver.cpp:473] Iteration 18100, lr = 0.0001
I0614 16:26:42.287063  5063 solver.cpp:213] Iteration 18110, loss = 3.19739
I0614 16:26:42.287082  5063 solver.cpp:228]     Train net output #0: softmax = 3.19739 (* 1 = 3.19739 loss)
I0614 16:26:42.287087  5063 solver.cpp:473] Iteration 18110, lr = 0.0001
I0614 16:26:43.169811  5063 solver.cpp:213] Iteration 18120, loss = 3.09855
I0614 16:26:43.169827  5063 solver.cpp:228]     Train net output #0: softmax = 3.09855 (* 1 = 3.09855 loss)
I0614 16:26:43.169831  5063 solver.cpp:473] Iteration 18120, lr = 0.0001
I0614 16:26:44.053351  5063 solver.cpp:213] Iteration 18130, loss = 2.75986
I0614 16:26:44.053369  5063 solver.cpp:228]     Train net output #0: softmax = 2.75986 (* 1 = 2.75986 loss)
I0614 16:26:44.053374  5063 solver.cpp:473] Iteration 18130, lr = 0.0001
I0614 16:26:44.937690  5063 solver.cpp:213] Iteration 18140, loss = 3.10151
I0614 16:26:44.937708  5063 solver.cpp:228]     Train net output #0: softmax = 3.10151 (* 1 = 3.10151 loss)
I0614 16:26:44.937712  5063 solver.cpp:473] Iteration 18140, lr = 0.0001
I0614 16:26:45.820904  5063 solver.cpp:213] Iteration 18150, loss = 3.17018
I0614 16:26:45.820936  5063 solver.cpp:228]     Train net output #0: softmax = 3.17018 (* 1 = 3.17018 loss)
I0614 16:26:45.820943  5063 solver.cpp:473] Iteration 18150, lr = 0.0001
I0614 16:26:46.703884  5063 solver.cpp:213] Iteration 18160, loss = 3.40101
I0614 16:26:46.703909  5063 solver.cpp:228]     Train net output #0: softmax = 3.40101 (* 1 = 3.40101 loss)
I0614 16:26:46.703914  5063 solver.cpp:473] Iteration 18160, lr = 0.0001
I0614 16:26:47.588290  5063 solver.cpp:213] Iteration 18170, loss = 3.2136
I0614 16:26:47.588310  5063 solver.cpp:228]     Train net output #0: softmax = 3.2136 (* 1 = 3.2136 loss)
I0614 16:26:47.588315  5063 solver.cpp:473] Iteration 18170, lr = 0.0001
I0614 16:26:48.471083  5063 solver.cpp:213] Iteration 18180, loss = 3.00216
I0614 16:26:48.471120  5063 solver.cpp:228]     Train net output #0: softmax = 3.00216 (* 1 = 3.00216 loss)
I0614 16:26:48.471125  5063 solver.cpp:473] Iteration 18180, lr = 0.0001
I0614 16:26:49.354102  5063 solver.cpp:213] Iteration 18190, loss = 3.0322
I0614 16:26:49.354122  5063 solver.cpp:228]     Train net output #0: softmax = 3.0322 (* 1 = 3.0322 loss)
I0614 16:26:49.354127  5063 solver.cpp:473] Iteration 18190, lr = 0.0001
I0614 16:26:50.237161  5063 solver.cpp:213] Iteration 18200, loss = 2.94903
I0614 16:26:50.237181  5063 solver.cpp:228]     Train net output #0: softmax = 2.94903 (* 1 = 2.94903 loss)
I0614 16:26:50.237186  5063 solver.cpp:473] Iteration 18200, lr = 0.0001
I0614 16:26:51.121050  5063 solver.cpp:213] Iteration 18210, loss = 3.2126
I0614 16:26:51.121076  5063 solver.cpp:228]     Train net output #0: softmax = 3.2126 (* 1 = 3.2126 loss)
I0614 16:26:51.121081  5063 solver.cpp:473] Iteration 18210, lr = 0.0001
I0614 16:26:52.004885  5063 solver.cpp:213] Iteration 18220, loss = 2.92178
I0614 16:26:52.004904  5063 solver.cpp:228]     Train net output #0: softmax = 2.92178 (* 1 = 2.92178 loss)
I0614 16:26:52.004909  5063 solver.cpp:473] Iteration 18220, lr = 0.0001
I0614 16:26:52.887423  5063 solver.cpp:213] Iteration 18230, loss = 3.05635
I0614 16:26:52.887445  5063 solver.cpp:228]     Train net output #0: softmax = 3.05635 (* 1 = 3.05635 loss)
I0614 16:26:52.887450  5063 solver.cpp:473] Iteration 18230, lr = 0.0001
I0614 16:26:53.770591  5063 solver.cpp:213] Iteration 18240, loss = 3.21445
I0614 16:26:53.770612  5063 solver.cpp:228]     Train net output #0: softmax = 3.21445 (* 1 = 3.21445 loss)
I0614 16:26:53.770615  5063 solver.cpp:473] Iteration 18240, lr = 0.0001
I0614 16:26:54.654171  5063 solver.cpp:213] Iteration 18250, loss = 2.80336
I0614 16:26:54.654192  5063 solver.cpp:228]     Train net output #0: softmax = 2.80336 (* 1 = 2.80336 loss)
I0614 16:26:54.654197  5063 solver.cpp:473] Iteration 18250, lr = 0.0001
I0614 16:26:55.537431  5063 solver.cpp:213] Iteration 18260, loss = 2.89153
I0614 16:26:55.537451  5063 solver.cpp:228]     Train net output #0: softmax = 2.89153 (* 1 = 2.89153 loss)
I0614 16:26:55.537456  5063 solver.cpp:473] Iteration 18260, lr = 0.0001
I0614 16:26:56.421607  5063 solver.cpp:213] Iteration 18270, loss = 3.2407
I0614 16:26:56.421629  5063 solver.cpp:228]     Train net output #0: softmax = 3.2407 (* 1 = 3.2407 loss)
I0614 16:26:56.421634  5063 solver.cpp:473] Iteration 18270, lr = 0.0001
I0614 16:26:57.303722  5063 solver.cpp:213] Iteration 18280, loss = 3.06469
I0614 16:26:57.303747  5063 solver.cpp:228]     Train net output #0: softmax = 3.06469 (* 1 = 3.06469 loss)
I0614 16:26:57.303752  5063 solver.cpp:473] Iteration 18280, lr = 0.0001
I0614 16:26:58.186739  5063 solver.cpp:213] Iteration 18290, loss = 2.95872
I0614 16:26:58.186763  5063 solver.cpp:228]     Train net output #0: softmax = 2.95872 (* 1 = 2.95872 loss)
I0614 16:26:58.186767  5063 solver.cpp:473] Iteration 18290, lr = 0.0001
I0614 16:26:59.069735  5063 solver.cpp:213] Iteration 18300, loss = 3.32581
I0614 16:26:59.069752  5063 solver.cpp:228]     Train net output #0: softmax = 3.32581 (* 1 = 3.32581 loss)
I0614 16:26:59.069757  5063 solver.cpp:473] Iteration 18300, lr = 0.0001
I0614 16:26:59.952883  5063 solver.cpp:213] Iteration 18310, loss = 3.32997
I0614 16:26:59.952898  5063 solver.cpp:228]     Train net output #0: softmax = 3.32997 (* 1 = 3.32997 loss)
I0614 16:26:59.952903  5063 solver.cpp:473] Iteration 18310, lr = 0.0001
I0614 16:27:00.836621  5063 solver.cpp:213] Iteration 18320, loss = 3.04992
I0614 16:27:00.836642  5063 solver.cpp:228]     Train net output #0: softmax = 3.04992 (* 1 = 3.04992 loss)
I0614 16:27:00.836647  5063 solver.cpp:473] Iteration 18320, lr = 0.0001
I0614 16:27:01.720258  5063 solver.cpp:213] Iteration 18330, loss = 3.13049
I0614 16:27:01.720286  5063 solver.cpp:228]     Train net output #0: softmax = 3.13049 (* 1 = 3.13049 loss)
I0614 16:27:01.720294  5063 solver.cpp:473] Iteration 18330, lr = 0.0001
I0614 16:27:02.604353  5063 solver.cpp:213] Iteration 18340, loss = 3.09269
I0614 16:27:02.604388  5063 solver.cpp:228]     Train net output #0: softmax = 3.09269 (* 1 = 3.09269 loss)
I0614 16:27:02.604393  5063 solver.cpp:473] Iteration 18340, lr = 0.0001
I0614 16:27:03.488329  5063 solver.cpp:213] Iteration 18350, loss = 3.03787
I0614 16:27:03.488349  5063 solver.cpp:228]     Train net output #0: softmax = 3.03787 (* 1 = 3.03787 loss)
I0614 16:27:03.488354  5063 solver.cpp:473] Iteration 18350, lr = 0.0001
I0614 16:27:04.372496  5063 solver.cpp:213] Iteration 18360, loss = 3.28347
I0614 16:27:04.372552  5063 solver.cpp:228]     Train net output #0: softmax = 3.28347 (* 1 = 3.28347 loss)
I0614 16:27:04.372566  5063 solver.cpp:473] Iteration 18360, lr = 0.0001
I0614 16:27:05.256312  5063 solver.cpp:213] Iteration 18370, loss = 3.0239
I0614 16:27:05.256331  5063 solver.cpp:228]     Train net output #0: softmax = 3.0239 (* 1 = 3.0239 loss)
I0614 16:27:05.256336  5063 solver.cpp:473] Iteration 18370, lr = 0.0001
I0614 16:27:06.140456  5063 solver.cpp:213] Iteration 18380, loss = 2.90715
I0614 16:27:06.140475  5063 solver.cpp:228]     Train net output #0: softmax = 2.90715 (* 1 = 2.90715 loss)
I0614 16:27:06.140480  5063 solver.cpp:473] Iteration 18380, lr = 0.0001
I0614 16:27:07.024930  5063 solver.cpp:213] Iteration 18390, loss = 3.34744
I0614 16:27:07.024969  5063 solver.cpp:228]     Train net output #0: softmax = 3.34744 (* 1 = 3.34744 loss)
I0614 16:27:07.025149  5063 solver.cpp:473] Iteration 18390, lr = 0.0001
I0614 16:27:07.908059  5063 solver.cpp:213] Iteration 18400, loss = 2.94649
I0614 16:27:07.908080  5063 solver.cpp:228]     Train net output #0: softmax = 2.94649 (* 1 = 2.94649 loss)
I0614 16:27:07.908085  5063 solver.cpp:473] Iteration 18400, lr = 0.0001
I0614 16:27:08.790442  5063 solver.cpp:213] Iteration 18410, loss = 3.21834
I0614 16:27:08.790467  5063 solver.cpp:228]     Train net output #0: softmax = 3.21834 (* 1 = 3.21834 loss)
I0614 16:27:08.790472  5063 solver.cpp:473] Iteration 18410, lr = 0.0001
I0614 16:27:09.673588  5063 solver.cpp:213] Iteration 18420, loss = 3.20961
I0614 16:27:09.673610  5063 solver.cpp:228]     Train net output #0: softmax = 3.20961 (* 1 = 3.20961 loss)
I0614 16:27:09.673615  5063 solver.cpp:473] Iteration 18420, lr = 0.0001
I0614 16:27:10.556326  5063 solver.cpp:213] Iteration 18430, loss = 3.17085
I0614 16:27:10.556347  5063 solver.cpp:228]     Train net output #0: softmax = 3.17085 (* 1 = 3.17085 loss)
I0614 16:27:10.556352  5063 solver.cpp:473] Iteration 18430, lr = 0.0001
I0614 16:27:11.439092  5063 solver.cpp:213] Iteration 18440, loss = 3.14073
I0614 16:27:11.439112  5063 solver.cpp:228]     Train net output #0: softmax = 3.14073 (* 1 = 3.14073 loss)
I0614 16:27:11.439117  5063 solver.cpp:473] Iteration 18440, lr = 0.0001
I0614 16:27:12.321630  5063 solver.cpp:213] Iteration 18450, loss = 3.08357
I0614 16:27:12.321655  5063 solver.cpp:228]     Train net output #0: softmax = 3.08357 (* 1 = 3.08357 loss)
I0614 16:27:12.321780  5063 solver.cpp:473] Iteration 18450, lr = 0.0001
I0614 16:27:13.204689  5063 solver.cpp:213] Iteration 18460, loss = 3.05245
I0614 16:27:13.204710  5063 solver.cpp:228]     Train net output #0: softmax = 3.05245 (* 1 = 3.05245 loss)
I0614 16:27:13.204715  5063 solver.cpp:473] Iteration 18460, lr = 0.0001
I0614 16:27:14.087721  5063 solver.cpp:213] Iteration 18470, loss = 3.07179
I0614 16:27:14.087748  5063 solver.cpp:228]     Train net output #0: softmax = 3.07179 (* 1 = 3.07179 loss)
I0614 16:27:14.087752  5063 solver.cpp:473] Iteration 18470, lr = 0.0001
I0614 16:27:14.971323  5063 solver.cpp:213] Iteration 18480, loss = 2.94951
I0614 16:27:14.971345  5063 solver.cpp:228]     Train net output #0: softmax = 2.94951 (* 1 = 2.94951 loss)
I0614 16:27:14.971350  5063 solver.cpp:473] Iteration 18480, lr = 0.0001
I0614 16:27:15.854535  5063 solver.cpp:213] Iteration 18490, loss = 2.88848
I0614 16:27:15.854557  5063 solver.cpp:228]     Train net output #0: softmax = 2.88848 (* 1 = 2.88848 loss)
I0614 16:27:15.854562  5063 solver.cpp:473] Iteration 18490, lr = 0.0001
I0614 16:27:16.738103  5063 solver.cpp:213] Iteration 18500, loss = 2.89388
I0614 16:27:16.738126  5063 solver.cpp:228]     Train net output #0: softmax = 2.89388 (* 1 = 2.89388 loss)
I0614 16:27:16.738129  5063 solver.cpp:473] Iteration 18500, lr = 0.0001
I0614 16:27:17.621695  5063 solver.cpp:213] Iteration 18510, loss = 3.16598
I0614 16:27:17.621721  5063 solver.cpp:228]     Train net output #0: softmax = 3.16598 (* 1 = 3.16598 loss)
I0614 16:27:17.621726  5063 solver.cpp:473] Iteration 18510, lr = 0.0001
I0614 16:27:18.504899  5063 solver.cpp:213] Iteration 18520, loss = 2.87663
I0614 16:27:18.504940  5063 solver.cpp:228]     Train net output #0: softmax = 2.87663 (* 1 = 2.87663 loss)
I0614 16:27:18.504945  5063 solver.cpp:473] Iteration 18520, lr = 0.0001
I0614 16:27:19.388821  5063 solver.cpp:213] Iteration 18530, loss = 3.11989
I0614 16:27:19.388844  5063 solver.cpp:228]     Train net output #0: softmax = 3.11989 (* 1 = 3.11989 loss)
I0614 16:27:19.388849  5063 solver.cpp:473] Iteration 18530, lr = 0.0001
I0614 16:27:20.272800  5063 solver.cpp:213] Iteration 18540, loss = 3.05513
I0614 16:27:20.272822  5063 solver.cpp:228]     Train net output #0: softmax = 3.05513 (* 1 = 3.05513 loss)
I0614 16:27:20.272827  5063 solver.cpp:473] Iteration 18540, lr = 0.0001
I0614 16:27:21.156448  5063 solver.cpp:213] Iteration 18550, loss = 3.18283
I0614 16:27:21.156469  5063 solver.cpp:228]     Train net output #0: softmax = 3.18283 (* 1 = 3.18283 loss)
I0614 16:27:21.156474  5063 solver.cpp:473] Iteration 18550, lr = 0.0001
I0614 16:27:22.039712  5063 solver.cpp:213] Iteration 18560, loss = 3.07792
I0614 16:27:22.039733  5063 solver.cpp:228]     Train net output #0: softmax = 3.07792 (* 1 = 3.07792 loss)
I0614 16:27:22.039738  5063 solver.cpp:473] Iteration 18560, lr = 0.0001
I0614 16:27:22.922796  5063 solver.cpp:213] Iteration 18570, loss = 2.98455
I0614 16:27:22.922821  5063 solver.cpp:228]     Train net output #0: softmax = 2.98455 (* 1 = 2.98455 loss)
I0614 16:27:22.922968  5063 solver.cpp:473] Iteration 18570, lr = 0.0001
I0614 16:27:23.806300  5063 solver.cpp:213] Iteration 18580, loss = 2.89094
I0614 16:27:23.806324  5063 solver.cpp:228]     Train net output #0: softmax = 2.89094 (* 1 = 2.89094 loss)
I0614 16:27:23.806329  5063 solver.cpp:473] Iteration 18580, lr = 0.0001
I0614 16:27:24.690376  5063 solver.cpp:213] Iteration 18590, loss = 2.85338
I0614 16:27:24.690398  5063 solver.cpp:228]     Train net output #0: softmax = 2.85338 (* 1 = 2.85338 loss)
I0614 16:27:24.690402  5063 solver.cpp:473] Iteration 18590, lr = 0.0001
I0614 16:27:25.573627  5063 solver.cpp:213] Iteration 18600, loss = 3.08919
I0614 16:27:25.573648  5063 solver.cpp:228]     Train net output #0: softmax = 3.08919 (* 1 = 3.08919 loss)
I0614 16:27:25.573652  5063 solver.cpp:473] Iteration 18600, lr = 0.0001
I0614 16:27:26.457509  5063 solver.cpp:213] Iteration 18610, loss = 3.04886
I0614 16:27:26.457530  5063 solver.cpp:228]     Train net output #0: softmax = 3.04886 (* 1 = 3.04886 loss)
I0614 16:27:26.457535  5063 solver.cpp:473] Iteration 18610, lr = 0.0001
I0614 16:27:27.341792  5063 solver.cpp:213] Iteration 18620, loss = 3.15019
I0614 16:27:27.341823  5063 solver.cpp:228]     Train net output #0: softmax = 3.15019 (* 1 = 3.15019 loss)
I0614 16:27:27.341828  5063 solver.cpp:473] Iteration 18620, lr = 0.0001
I0614 16:27:28.224990  5063 solver.cpp:213] Iteration 18630, loss = 3.28533
I0614 16:27:28.225016  5063 solver.cpp:228]     Train net output #0: softmax = 3.28533 (* 1 = 3.28533 loss)
I0614 16:27:28.225170  5063 solver.cpp:473] Iteration 18630, lr = 0.0001
I0614 16:27:29.108196  5063 solver.cpp:213] Iteration 18640, loss = 2.86942
I0614 16:27:29.108219  5063 solver.cpp:228]     Train net output #0: softmax = 2.86942 (* 1 = 2.86942 loss)
I0614 16:27:29.108224  5063 solver.cpp:473] Iteration 18640, lr = 0.0001
I0614 16:27:29.991452  5063 solver.cpp:213] Iteration 18650, loss = 3.02178
I0614 16:27:29.991472  5063 solver.cpp:228]     Train net output #0: softmax = 3.02178 (* 1 = 3.02178 loss)
I0614 16:27:29.991477  5063 solver.cpp:473] Iteration 18650, lr = 0.0001
I0614 16:27:30.875253  5063 solver.cpp:213] Iteration 18660, loss = 3.08714
I0614 16:27:30.875274  5063 solver.cpp:228]     Train net output #0: softmax = 3.08714 (* 1 = 3.08714 loss)
I0614 16:27:30.875279  5063 solver.cpp:473] Iteration 18660, lr = 0.0001
I0614 16:27:31.758815  5063 solver.cpp:213] Iteration 18670, loss = 3.05351
I0614 16:27:31.758836  5063 solver.cpp:228]     Train net output #0: softmax = 3.05351 (* 1 = 3.05351 loss)
I0614 16:27:31.758841  5063 solver.cpp:473] Iteration 18670, lr = 0.0001
I0614 16:27:32.641470  5063 solver.cpp:213] Iteration 18680, loss = 3.02234
I0614 16:27:32.641510  5063 solver.cpp:228]     Train net output #0: softmax = 3.02234 (* 1 = 3.02234 loss)
I0614 16:27:32.641515  5063 solver.cpp:473] Iteration 18680, lr = 0.0001
I0614 16:27:33.526154  5063 solver.cpp:213] Iteration 18690, loss = 3.2657
I0614 16:27:33.526178  5063 solver.cpp:228]     Train net output #0: softmax = 3.2657 (* 1 = 3.2657 loss)
I0614 16:27:33.526183  5063 solver.cpp:473] Iteration 18690, lr = 0.0001
I0614 16:27:34.409904  5063 solver.cpp:213] Iteration 18700, loss = 3.13183
I0614 16:27:34.409947  5063 solver.cpp:228]     Train net output #0: softmax = 3.13183 (* 1 = 3.13183 loss)
I0614 16:27:34.409953  5063 solver.cpp:473] Iteration 18700, lr = 0.0001
I0614 16:27:35.294752  5063 solver.cpp:213] Iteration 18710, loss = 2.93231
I0614 16:27:35.294773  5063 solver.cpp:228]     Train net output #0: softmax = 2.93231 (* 1 = 2.93231 loss)
I0614 16:27:35.294777  5063 solver.cpp:473] Iteration 18710, lr = 0.0001
I0614 16:27:36.178498  5063 solver.cpp:213] Iteration 18720, loss = 2.94715
I0614 16:27:36.178517  5063 solver.cpp:228]     Train net output #0: softmax = 2.94715 (* 1 = 2.94715 loss)
I0614 16:27:36.178522  5063 solver.cpp:473] Iteration 18720, lr = 0.0001
I0614 16:27:37.062376  5063 solver.cpp:213] Iteration 18730, loss = 3.07968
I0614 16:27:37.062397  5063 solver.cpp:228]     Train net output #0: softmax = 3.07968 (* 1 = 3.07968 loss)
I0614 16:27:37.062402  5063 solver.cpp:473] Iteration 18730, lr = 0.0001
I0614 16:27:37.945622  5063 solver.cpp:213] Iteration 18740, loss = 2.98849
I0614 16:27:37.945642  5063 solver.cpp:228]     Train net output #0: softmax = 2.98849 (* 1 = 2.98849 loss)
I0614 16:27:37.945647  5063 solver.cpp:473] Iteration 18740, lr = 0.0001
I0614 16:27:38.829107  5063 solver.cpp:213] Iteration 18750, loss = 3.1503
I0614 16:27:38.829135  5063 solver.cpp:228]     Train net output #0: softmax = 3.1503 (* 1 = 3.1503 loss)
I0614 16:27:38.829272  5063 solver.cpp:473] Iteration 18750, lr = 0.0001
I0614 16:27:39.712210  5063 solver.cpp:213] Iteration 18760, loss = 3.08164
I0614 16:27:39.712230  5063 solver.cpp:228]     Train net output #0: softmax = 3.08164 (* 1 = 3.08164 loss)
I0614 16:27:39.712235  5063 solver.cpp:473] Iteration 18760, lr = 0.0001
I0614 16:27:40.595656  5063 solver.cpp:213] Iteration 18770, loss = 3.09873
I0614 16:27:40.595677  5063 solver.cpp:228]     Train net output #0: softmax = 3.09873 (* 1 = 3.09873 loss)
I0614 16:27:40.595682  5063 solver.cpp:473] Iteration 18770, lr = 0.0001
I0614 16:27:41.478682  5063 solver.cpp:213] Iteration 18780, loss = 3.13572
I0614 16:27:41.478703  5063 solver.cpp:228]     Train net output #0: softmax = 3.13572 (* 1 = 3.13572 loss)
I0614 16:27:41.478708  5063 solver.cpp:473] Iteration 18780, lr = 0.0001
I0614 16:27:42.362262  5063 solver.cpp:213] Iteration 18790, loss = 3.13634
I0614 16:27:42.362282  5063 solver.cpp:228]     Train net output #0: softmax = 3.13634 (* 1 = 3.13634 loss)
I0614 16:27:42.362287  5063 solver.cpp:473] Iteration 18790, lr = 0.0001
I0614 16:27:43.245905  5063 solver.cpp:213] Iteration 18800, loss = 3.19893
I0614 16:27:43.245928  5063 solver.cpp:228]     Train net output #0: softmax = 3.19893 (* 1 = 3.19893 loss)
I0614 16:27:43.245932  5063 solver.cpp:473] Iteration 18800, lr = 0.0001
I0614 16:27:44.129963  5063 solver.cpp:213] Iteration 18810, loss = 3.28388
I0614 16:27:44.129992  5063 solver.cpp:228]     Train net output #0: softmax = 3.28388 (* 1 = 3.28388 loss)
I0614 16:27:44.130115  5063 solver.cpp:473] Iteration 18810, lr = 0.0001
I0614 16:27:45.012569  5063 solver.cpp:213] Iteration 18820, loss = 3.15384
I0614 16:27:45.012589  5063 solver.cpp:228]     Train net output #0: softmax = 3.15384 (* 1 = 3.15384 loss)
I0614 16:27:45.012594  5063 solver.cpp:473] Iteration 18820, lr = 0.0001
I0614 16:27:45.895859  5063 solver.cpp:213] Iteration 18830, loss = 3.05562
I0614 16:27:45.895879  5063 solver.cpp:228]     Train net output #0: softmax = 3.05562 (* 1 = 3.05562 loss)
I0614 16:27:45.895882  5063 solver.cpp:473] Iteration 18830, lr = 0.0001
I0614 16:27:46.779466  5063 solver.cpp:213] Iteration 18840, loss = 3.01182
I0614 16:27:46.779489  5063 solver.cpp:228]     Train net output #0: softmax = 3.01182 (* 1 = 3.01182 loss)
I0614 16:27:46.779492  5063 solver.cpp:473] Iteration 18840, lr = 0.0001
I0614 16:27:47.662456  5063 solver.cpp:213] Iteration 18850, loss = 2.95594
I0614 16:27:47.662475  5063 solver.cpp:228]     Train net output #0: softmax = 2.95594 (* 1 = 2.95594 loss)
I0614 16:27:47.662480  5063 solver.cpp:473] Iteration 18850, lr = 0.0001
I0614 16:27:48.545624  5063 solver.cpp:213] Iteration 18860, loss = 3.03854
I0614 16:27:48.545656  5063 solver.cpp:228]     Train net output #0: softmax = 3.03854 (* 1 = 3.03854 loss)
I0614 16:27:48.545662  5063 solver.cpp:473] Iteration 18860, lr = 0.0001
I0614 16:27:49.428321  5063 solver.cpp:213] Iteration 18870, loss = 3.07794
I0614 16:27:49.428349  5063 solver.cpp:228]     Train net output #0: softmax = 3.07794 (* 1 = 3.07794 loss)
I0614 16:27:49.428352  5063 solver.cpp:473] Iteration 18870, lr = 0.0001
I0614 16:27:50.311056  5063 solver.cpp:213] Iteration 18880, loss = 3.18423
I0614 16:27:50.311076  5063 solver.cpp:228]     Train net output #0: softmax = 3.18423 (* 1 = 3.18423 loss)
I0614 16:27:50.311081  5063 solver.cpp:473] Iteration 18880, lr = 0.0001
I0614 16:27:51.194919  5063 solver.cpp:213] Iteration 18890, loss = 3.27519
I0614 16:27:51.194941  5063 solver.cpp:228]     Train net output #0: softmax = 3.27519 (* 1 = 3.27519 loss)
I0614 16:27:51.194947  5063 solver.cpp:473] Iteration 18890, lr = 0.0001
I0614 16:27:52.078119  5063 solver.cpp:213] Iteration 18900, loss = 3.11326
I0614 16:27:52.078138  5063 solver.cpp:228]     Train net output #0: softmax = 3.11326 (* 1 = 3.11326 loss)
I0614 16:27:52.078143  5063 solver.cpp:473] Iteration 18900, lr = 0.0001
I0614 16:27:52.961891  5063 solver.cpp:213] Iteration 18910, loss = 3.1234
I0614 16:27:52.961911  5063 solver.cpp:228]     Train net output #0: softmax = 3.1234 (* 1 = 3.1234 loss)
I0614 16:27:52.961916  5063 solver.cpp:473] Iteration 18910, lr = 0.0001
I0614 16:27:53.844815  5063 solver.cpp:213] Iteration 18920, loss = 3.26575
I0614 16:27:53.844837  5063 solver.cpp:228]     Train net output #0: softmax = 3.26575 (* 1 = 3.26575 loss)
I0614 16:27:53.844842  5063 solver.cpp:473] Iteration 18920, lr = 0.0001
I0614 16:27:54.729115  5063 solver.cpp:213] Iteration 18930, loss = 3.34847
I0614 16:27:54.729136  5063 solver.cpp:228]     Train net output #0: softmax = 3.34847 (* 1 = 3.34847 loss)
I0614 16:27:54.729261  5063 solver.cpp:473] Iteration 18930, lr = 0.0001
I0614 16:27:55.612448  5063 solver.cpp:213] Iteration 18940, loss = 3.2292
I0614 16:27:55.612468  5063 solver.cpp:228]     Train net output #0: softmax = 3.2292 (* 1 = 3.2292 loss)
I0614 16:27:55.612471  5063 solver.cpp:473] Iteration 18940, lr = 0.0001
I0614 16:27:56.495964  5063 solver.cpp:213] Iteration 18950, loss = 2.84708
I0614 16:27:56.495981  5063 solver.cpp:228]     Train net output #0: softmax = 2.84708 (* 1 = 2.84708 loss)
I0614 16:27:56.495986  5063 solver.cpp:473] Iteration 18950, lr = 0.0001
I0614 16:27:57.379899  5063 solver.cpp:213] Iteration 18960, loss = 2.77006
I0614 16:27:57.379920  5063 solver.cpp:228]     Train net output #0: softmax = 2.77006 (* 1 = 2.77006 loss)
I0614 16:27:57.379925  5063 solver.cpp:473] Iteration 18960, lr = 0.0001
I0614 16:27:58.264348  5063 solver.cpp:213] Iteration 18970, loss = 2.81779
I0614 16:27:58.264371  5063 solver.cpp:228]     Train net output #0: softmax = 2.81779 (* 1 = 2.81779 loss)
I0614 16:27:58.264376  5063 solver.cpp:473] Iteration 18970, lr = 0.0001
I0614 16:27:59.147805  5063 solver.cpp:213] Iteration 18980, loss = 2.96963
I0614 16:27:59.147828  5063 solver.cpp:228]     Train net output #0: softmax = 2.96963 (* 1 = 2.96963 loss)
I0614 16:27:59.147833  5063 solver.cpp:473] Iteration 18980, lr = 0.0001
I0614 16:28:00.030747  5063 solver.cpp:213] Iteration 18990, loss = 2.90118
I0614 16:28:00.030786  5063 solver.cpp:228]     Train net output #0: softmax = 2.90118 (* 1 = 2.90118 loss)
I0614 16:28:00.030908  5063 solver.cpp:473] Iteration 18990, lr = 0.0001
I0614 16:28:00.856416  5063 solver.cpp:362] Snapshotting to snapshots/16-06-14_15h51m21s_0_11_pretrainClassification_iter_19000.caffemodel
I0614 16:28:00.857218  5063 solver.cpp:370] Snapshotting solver state to snapshots/16-06-14_15h51m21s_0_11_pretrainClassification_iter_19000.solverstate
I0614 16:28:00.857648  5063 solver.cpp:291] Iteration 19000, Testing net (#0)
I0614 16:28:00.970407  5063 solver.cpp:342]     Test net output #0: accuracy_top_1 = 0.259375
I0614 16:28:00.970422  5063 solver.cpp:342]     Test net output #1: accuracy_top_5 = 0.55
I0614 16:28:00.970428  5063 solver.cpp:342]     Test net output #2: softmax = 3.00263 (* 1 = 3.00263 loss)
I0614 16:28:01.028620  5063 solver.cpp:213] Iteration 19000, loss = 2.88576
I0614 16:28:01.028635  5063 solver.cpp:228]     Train net output #0: softmax = 2.88576 (* 1 = 2.88576 loss)
I0614 16:28:01.028640  5063 solver.cpp:473] Iteration 19000, lr = 0.0001
I0614 16:28:01.912206  5063 solver.cpp:213] Iteration 19010, loss = 2.99407
I0614 16:28:01.912225  5063 solver.cpp:228]     Train net output #0: softmax = 2.99407 (* 1 = 2.99407 loss)
I0614 16:28:01.912230  5063 solver.cpp:473] Iteration 19010, lr = 0.0001
I0614 16:28:02.795083  5063 solver.cpp:213] Iteration 19020, loss = 2.84508
I0614 16:28:02.795101  5063 solver.cpp:228]     Train net output #0: softmax = 2.84508 (* 1 = 2.84508 loss)
I0614 16:28:02.795106  5063 solver.cpp:473] Iteration 19020, lr = 0.0001
I0614 16:28:03.678153  5063 solver.cpp:213] Iteration 19030, loss = 2.88228
I0614 16:28:03.678169  5063 solver.cpp:228]     Train net output #0: softmax = 2.88228 (* 1 = 2.88228 loss)
I0614 16:28:03.678174  5063 solver.cpp:473] Iteration 19030, lr = 0.0001
I0614 16:28:04.561085  5063 solver.cpp:213] Iteration 19040, loss = 3.04335
I0614 16:28:04.561148  5063 solver.cpp:228]     Train net output #0: softmax = 3.04335 (* 1 = 3.04335 loss)
I0614 16:28:04.561164  5063 solver.cpp:473] Iteration 19040, lr = 0.0001
I0614 16:28:05.444233  5063 solver.cpp:213] Iteration 19050, loss = 3.07212
I0614 16:28:05.444253  5063 solver.cpp:228]     Train net output #0: softmax = 3.07212 (* 1 = 3.07212 loss)
I0614 16:28:05.444258  5063 solver.cpp:473] Iteration 19050, lr = 0.0001
I0614 16:28:06.327121  5063 solver.cpp:213] Iteration 19060, loss = 2.81434
I0614 16:28:06.327136  5063 solver.cpp:228]     Train net output #0: softmax = 2.81434 (* 1 = 2.81434 loss)
I0614 16:28:06.327141  5063 solver.cpp:473] Iteration 19060, lr = 0.0001
I0614 16:28:07.210561  5063 solver.cpp:213] Iteration 19070, loss = 3.08105
I0614 16:28:07.210577  5063 solver.cpp:228]     Train net output #0: softmax = 3.08105 (* 1 = 3.08105 loss)
I0614 16:28:07.210582  5063 solver.cpp:473] Iteration 19070, lr = 0.0001
I0614 16:28:08.094189  5063 solver.cpp:213] Iteration 19080, loss = 2.97012
I0614 16:28:08.094208  5063 solver.cpp:228]     Train net output #0: softmax = 2.97012 (* 1 = 2.97012 loss)
I0614 16:28:08.094213  5063 solver.cpp:473] Iteration 19080, lr = 0.0001
I0614 16:28:08.977869  5063 solver.cpp:213] Iteration 19090, loss = 3.48295
I0614 16:28:08.977890  5063 solver.cpp:228]     Train net output #0: softmax = 3.48295 (* 1 = 3.48295 loss)
I0614 16:28:08.977893  5063 solver.cpp:473] Iteration 19090, lr = 0.0001
I0614 16:28:09.861243  5063 solver.cpp:213] Iteration 19100, loss = 2.90334
I0614 16:28:09.861268  5063 solver.cpp:228]     Train net output #0: softmax = 2.90334 (* 1 = 2.90334 loss)
I0614 16:28:09.861274  5063 solver.cpp:473] Iteration 19100, lr = 0.0001
I0614 16:28:10.744765  5063 solver.cpp:213] Iteration 19110, loss = 3.02142
I0614 16:28:10.744787  5063 solver.cpp:228]     Train net output #0: softmax = 3.02142 (* 1 = 3.02142 loss)
I0614 16:28:10.744915  5063 solver.cpp:473] Iteration 19110, lr = 0.0001
I0614 16:28:11.628996  5063 solver.cpp:213] Iteration 19120, loss = 3.11063
I0614 16:28:11.629016  5063 solver.cpp:228]     Train net output #0: softmax = 3.11063 (* 1 = 3.11063 loss)
I0614 16:28:11.629021  5063 solver.cpp:473] Iteration 19120, lr = 0.0001
I0614 16:28:12.512403  5063 solver.cpp:213] Iteration 19130, loss = 3.25438
I0614 16:28:12.512418  5063 solver.cpp:228]     Train net output #0: softmax = 3.25438 (* 1 = 3.25438 loss)
I0614 16:28:12.512423  5063 solver.cpp:473] Iteration 19130, lr = 0.0001
I0614 16:28:13.395171  5063 solver.cpp:213] Iteration 19140, loss = 3.15841
I0614 16:28:13.395189  5063 solver.cpp:228]     Train net output #0: softmax = 3.15841 (* 1 = 3.15841 loss)
I0614 16:28:13.395192  5063 solver.cpp:473] Iteration 19140, lr = 0.0001
I0614 16:28:14.278156  5063 solver.cpp:213] Iteration 19150, loss = 2.98678
I0614 16:28:14.278174  5063 solver.cpp:228]     Train net output #0: softmax = 2.98678 (* 1 = 2.98678 loss)
I0614 16:28:14.278179  5063 solver.cpp:473] Iteration 19150, lr = 0.0001
I0614 16:28:15.161407  5063 solver.cpp:213] Iteration 19160, loss = 3.08136
I0614 16:28:15.161432  5063 solver.cpp:228]     Train net output #0: softmax = 3.08136 (* 1 = 3.08136 loss)
I0614 16:28:15.161437  5063 solver.cpp:473] Iteration 19160, lr = 0.0001
I0614 16:28:16.044232  5063 solver.cpp:213] Iteration 19170, loss = 3.00268
I0614 16:28:16.044252  5063 solver.cpp:228]     Train net output #0: softmax = 3.00268 (* 1 = 3.00268 loss)
I0614 16:28:16.044262  5063 solver.cpp:473] Iteration 19170, lr = 0.0001
I0614 16:28:16.927355  5063 solver.cpp:213] Iteration 19180, loss = 3.13476
I0614 16:28:16.927372  5063 solver.cpp:228]     Train net output #0: softmax = 3.13476 (* 1 = 3.13476 loss)
I0614 16:28:16.927376  5063 solver.cpp:473] Iteration 19180, lr = 0.0001
I0614 16:28:17.810575  5063 solver.cpp:213] Iteration 19190, loss = 3.23717
I0614 16:28:17.810595  5063 solver.cpp:228]     Train net output #0: softmax = 3.23717 (* 1 = 3.23717 loss)
I0614 16:28:17.810600  5063 solver.cpp:473] Iteration 19190, lr = 0.0001
I0614 16:28:18.693478  5063 solver.cpp:213] Iteration 19200, loss = 3.24625
I0614 16:28:18.693511  5063 solver.cpp:228]     Train net output #0: softmax = 3.24625 (* 1 = 3.24625 loss)
I0614 16:28:18.693517  5063 solver.cpp:473] Iteration 19200, lr = 0.0001
I0614 16:28:19.576731  5063 solver.cpp:213] Iteration 19210, loss = 3.09319
I0614 16:28:19.576755  5063 solver.cpp:228]     Train net output #0: softmax = 3.09319 (* 1 = 3.09319 loss)
I0614 16:28:19.576759  5063 solver.cpp:473] Iteration 19210, lr = 0.0001
I0614 16:28:20.460295  5063 solver.cpp:213] Iteration 19220, loss = 2.92798
I0614 16:28:20.460314  5063 solver.cpp:228]     Train net output #0: softmax = 2.92798 (* 1 = 2.92798 loss)
I0614 16:28:20.460319  5063 solver.cpp:473] Iteration 19220, lr = 0.0001
I0614 16:28:21.342648  5063 solver.cpp:213] Iteration 19230, loss = 3.00891
I0614 16:28:21.342667  5063 solver.cpp:228]     Train net output #0: softmax = 3.00891 (* 1 = 3.00891 loss)
I0614 16:28:21.342828  5063 solver.cpp:473] Iteration 19230, lr = 0.0001
I0614 16:28:22.226197  5063 solver.cpp:213] Iteration 19240, loss = 2.81823
I0614 16:28:22.226214  5063 solver.cpp:228]     Train net output #0: softmax = 2.81823 (* 1 = 2.81823 loss)
I0614 16:28:22.226219  5063 solver.cpp:473] Iteration 19240, lr = 0.0001
I0614 16:28:23.108907  5063 solver.cpp:213] Iteration 19250, loss = 2.98007
I0614 16:28:23.108923  5063 solver.cpp:228]     Train net output #0: softmax = 2.98007 (* 1 = 2.98007 loss)
I0614 16:28:23.108927  5063 solver.cpp:473] Iteration 19250, lr = 0.0001
I0614 16:28:23.992671  5063 solver.cpp:213] Iteration 19260, loss = 3.16714
I0614 16:28:23.992691  5063 solver.cpp:228]     Train net output #0: softmax = 3.16714 (* 1 = 3.16714 loss)
I0614 16:28:23.992696  5063 solver.cpp:473] Iteration 19260, lr = 0.0001
I0614 16:28:24.876416  5063 solver.cpp:213] Iteration 19270, loss = 3.02389
I0614 16:28:24.876436  5063 solver.cpp:228]     Train net output #0: softmax = 3.02389 (* 1 = 3.02389 loss)
I0614 16:28:24.876441  5063 solver.cpp:473] Iteration 19270, lr = 0.0001
I0614 16:28:25.760071  5063 solver.cpp:213] Iteration 19280, loss = 3.06719
I0614 16:28:25.760090  5063 solver.cpp:228]     Train net output #0: softmax = 3.06719 (* 1 = 3.06719 loss)
I0614 16:28:25.760095  5063 solver.cpp:473] Iteration 19280, lr = 0.0001
I0614 16:28:26.643606  5063 solver.cpp:213] Iteration 19290, loss = 2.88437
I0614 16:28:26.643627  5063 solver.cpp:228]     Train net output #0: softmax = 2.88437 (* 1 = 2.88437 loss)
I0614 16:28:26.643632  5063 solver.cpp:473] Iteration 19290, lr = 0.0001
I0614 16:28:27.526479  5063 solver.cpp:213] Iteration 19300, loss = 3.30246
I0614 16:28:27.526497  5063 solver.cpp:228]     Train net output #0: softmax = 3.30246 (* 1 = 3.30246 loss)
I0614 16:28:27.526502  5063 solver.cpp:473] Iteration 19300, lr = 0.0001
I0614 16:28:28.408877  5063 solver.cpp:213] Iteration 19310, loss = 2.88029
I0614 16:28:28.408893  5063 solver.cpp:228]     Train net output #0: softmax = 2.88029 (* 1 = 2.88029 loss)
I0614 16:28:28.408898  5063 solver.cpp:473] Iteration 19310, lr = 0.0001
I0614 16:28:29.291949  5063 solver.cpp:213] Iteration 19320, loss = 2.89345
I0614 16:28:29.291975  5063 solver.cpp:228]     Train net output #0: softmax = 2.89345 (* 1 = 2.89345 loss)
I0614 16:28:29.291980  5063 solver.cpp:473] Iteration 19320, lr = 0.0001
I0614 16:28:30.175731  5063 solver.cpp:213] Iteration 19330, loss = 3.25021
I0614 16:28:30.175753  5063 solver.cpp:228]     Train net output #0: softmax = 3.25021 (* 1 = 3.25021 loss)
I0614 16:28:30.175758  5063 solver.cpp:473] Iteration 19330, lr = 0.0001
I0614 16:28:31.059469  5063 solver.cpp:213] Iteration 19340, loss = 3.10055
I0614 16:28:31.059486  5063 solver.cpp:228]     Train net output #0: softmax = 3.10055 (* 1 = 3.10055 loss)
I0614 16:28:31.059491  5063 solver.cpp:473] Iteration 19340, lr = 0.0001
I0614 16:28:31.942566  5063 solver.cpp:213] Iteration 19350, loss = 2.90566
I0614 16:28:31.942586  5063 solver.cpp:228]     Train net output #0: softmax = 2.90566 (* 1 = 2.90566 loss)
I0614 16:28:31.942713  5063 solver.cpp:473] Iteration 19350, lr = 0.0001
I0614 16:28:32.826026  5063 solver.cpp:213] Iteration 19360, loss = 2.9071
I0614 16:28:32.826059  5063 solver.cpp:228]     Train net output #0: softmax = 2.9071 (* 1 = 2.9071 loss)
I0614 16:28:32.826064  5063 solver.cpp:473] Iteration 19360, lr = 0.0001
I0614 16:28:33.709200  5063 solver.cpp:213] Iteration 19370, loss = 2.96382
I0614 16:28:33.709218  5063 solver.cpp:228]     Train net output #0: softmax = 2.96382 (* 1 = 2.96382 loss)
I0614 16:28:33.709223  5063 solver.cpp:473] Iteration 19370, lr = 0.0001
I0614 16:28:34.591552  5063 solver.cpp:213] Iteration 19380, loss = 3.13552
I0614 16:28:34.591588  5063 solver.cpp:228]     Train net output #0: softmax = 3.13552 (* 1 = 3.13552 loss)
I0614 16:28:34.591593  5063 solver.cpp:473] Iteration 19380, lr = 0.0001
I0614 16:28:35.474934  5063 solver.cpp:213] Iteration 19390, loss = 3.13422
I0614 16:28:35.474951  5063 solver.cpp:228]     Train net output #0: softmax = 3.13422 (* 1 = 3.13422 loss)
I0614 16:28:35.474956  5063 solver.cpp:473] Iteration 19390, lr = 0.0001
I0614 16:28:36.358054  5063 solver.cpp:213] Iteration 19400, loss = 2.86781
I0614 16:28:36.358072  5063 solver.cpp:228]     Train net output #0: softmax = 2.86781 (* 1 = 2.86781 loss)
I0614 16:28:36.358075  5063 solver.cpp:473] Iteration 19400, lr = 0.0001
I0614 16:28:37.241001  5063 solver.cpp:213] Iteration 19410, loss = 2.95383
I0614 16:28:37.241020  5063 solver.cpp:228]     Train net output #0: softmax = 2.95383 (* 1 = 2.95383 loss)
I0614 16:28:37.241183  5063 solver.cpp:473] Iteration 19410, lr = 0.0001
I0614 16:28:38.117415  5063 solver.cpp:213] Iteration 19420, loss = 2.90531
I0614 16:28:38.117434  5063 solver.cpp:228]     Train net output #0: softmax = 2.90531 (* 1 = 2.90531 loss)
I0614 16:28:38.117439  5063 solver.cpp:473] Iteration 19420, lr = 0.0001
I0614 16:28:39.000978  5063 solver.cpp:213] Iteration 19430, loss = 3.03727
I0614 16:28:39.001003  5063 solver.cpp:228]     Train net output #0: softmax = 3.03727 (* 1 = 3.03727 loss)
I0614 16:28:39.001008  5063 solver.cpp:473] Iteration 19430, lr = 0.0001
I0614 16:28:39.884671  5063 solver.cpp:213] Iteration 19440, loss = 3.24098
I0614 16:28:39.884691  5063 solver.cpp:228]     Train net output #0: softmax = 3.24098 (* 1 = 3.24098 loss)
I0614 16:28:39.884696  5063 solver.cpp:473] Iteration 19440, lr = 0.0001
I0614 16:28:40.767948  5063 solver.cpp:213] Iteration 19450, loss = 3.01285
I0614 16:28:40.767967  5063 solver.cpp:228]     Train net output #0: softmax = 3.01285 (* 1 = 3.01285 loss)
I0614 16:28:40.767972  5063 solver.cpp:473] Iteration 19450, lr = 0.0001
I0614 16:28:41.651362  5063 solver.cpp:213] Iteration 19460, loss = 2.93598
I0614 16:28:41.651381  5063 solver.cpp:228]     Train net output #0: softmax = 2.93598 (* 1 = 2.93598 loss)
I0614 16:28:41.651386  5063 solver.cpp:473] Iteration 19460, lr = 0.0001
I0614 16:28:42.535508  5063 solver.cpp:213] Iteration 19470, loss = 2.95078
I0614 16:28:42.535527  5063 solver.cpp:228]     Train net output #0: softmax = 2.95078 (* 1 = 2.95078 loss)
I0614 16:28:42.535532  5063 solver.cpp:473] Iteration 19470, lr = 0.0001
I0614 16:28:43.417955  5063 solver.cpp:213] Iteration 19480, loss = 3.24207
I0614 16:28:43.417978  5063 solver.cpp:228]     Train net output #0: softmax = 3.24207 (* 1 = 3.24207 loss)
I0614 16:28:43.417982  5063 solver.cpp:473] Iteration 19480, lr = 0.0001
I0614 16:28:44.301861  5063 solver.cpp:213] Iteration 19490, loss = 2.90434
I0614 16:28:44.301879  5063 solver.cpp:228]     Train net output #0: softmax = 2.90434 (* 1 = 2.90434 loss)
I0614 16:28:44.301884  5063 solver.cpp:473] Iteration 19490, lr = 0.0001
I0614 16:28:45.185941  5063 solver.cpp:213] Iteration 19500, loss = 2.84914
I0614 16:28:45.185964  5063 solver.cpp:228]     Train net output #0: softmax = 2.84914 (* 1 = 2.84914 loss)
I0614 16:28:45.185968  5063 solver.cpp:473] Iteration 19500, lr = 0.0001
I0614 16:28:46.068279  5063 solver.cpp:213] Iteration 19510, loss = 3.12564
I0614 16:28:46.068296  5063 solver.cpp:228]     Train net output #0: softmax = 3.12564 (* 1 = 3.12564 loss)
I0614 16:28:46.068301  5063 solver.cpp:473] Iteration 19510, lr = 0.0001
I0614 16:28:46.951136  5063 solver.cpp:213] Iteration 19520, loss = 3.23986
I0614 16:28:46.951155  5063 solver.cpp:228]     Train net output #0: softmax = 3.23986 (* 1 = 3.23986 loss)
I0614 16:28:46.951160  5063 solver.cpp:473] Iteration 19520, lr = 0.0001
I0614 16:28:47.833793  5063 solver.cpp:213] Iteration 19530, loss = 3.11046
I0614 16:28:47.833814  5063 solver.cpp:228]     Train net output #0: softmax = 3.11046 (* 1 = 3.11046 loss)
I0614 16:28:47.833824  5063 solver.cpp:473] Iteration 19530, lr = 0.0001
I0614 16:28:48.716928  5063 solver.cpp:213] Iteration 19540, loss = 2.86513
I0614 16:28:48.716965  5063 solver.cpp:228]     Train net output #0: softmax = 2.86513 (* 1 = 2.86513 loss)
I0614 16:28:48.716969  5063 solver.cpp:473] Iteration 19540, lr = 0.0001
I0614 16:28:49.599617  5063 solver.cpp:213] Iteration 19550, loss = 3.09766
I0614 16:28:49.599633  5063 solver.cpp:228]     Train net output #0: softmax = 3.09766 (* 1 = 3.09766 loss)
I0614 16:28:49.599638  5063 solver.cpp:473] Iteration 19550, lr = 0.0001
I0614 16:28:50.483224  5063 solver.cpp:213] Iteration 19560, loss = 2.98673
I0614 16:28:50.483247  5063 solver.cpp:228]     Train net output #0: softmax = 2.98673 (* 1 = 2.98673 loss)
I0614 16:28:50.483250  5063 solver.cpp:473] Iteration 19560, lr = 0.0001
I0614 16:28:51.365790  5063 solver.cpp:213] Iteration 19570, loss = 3.04364
I0614 16:28:51.365809  5063 solver.cpp:228]     Train net output #0: softmax = 3.04364 (* 1 = 3.04364 loss)
I0614 16:28:51.365813  5063 solver.cpp:473] Iteration 19570, lr = 0.0001
I0614 16:28:52.248637  5063 solver.cpp:213] Iteration 19580, loss = 3.03582
I0614 16:28:52.248662  5063 solver.cpp:228]     Train net output #0: softmax = 3.03582 (* 1 = 3.03582 loss)
I0614 16:28:52.248667  5063 solver.cpp:473] Iteration 19580, lr = 0.0001
I0614 16:28:53.131863  5063 solver.cpp:213] Iteration 19590, loss = 3.03358
I0614 16:28:53.131886  5063 solver.cpp:228]     Train net output #0: softmax = 3.03358 (* 1 = 3.03358 loss)
I0614 16:28:53.132025  5063 solver.cpp:473] Iteration 19590, lr = 0.0001
I0614 16:28:54.015437  5063 solver.cpp:213] Iteration 19600, loss = 3.0565
I0614 16:28:54.015456  5063 solver.cpp:228]     Train net output #0: softmax = 3.0565 (* 1 = 3.0565 loss)
I0614 16:28:54.015461  5063 solver.cpp:473] Iteration 19600, lr = 0.0001
I0614 16:28:54.898542  5063 solver.cpp:213] Iteration 19610, loss = 2.95634
I0614 16:28:54.898564  5063 solver.cpp:228]     Train net output #0: softmax = 2.95634 (* 1 = 2.95634 loss)
I0614 16:28:54.898569  5063 solver.cpp:473] Iteration 19610, lr = 0.0001
I0614 16:28:55.780598  5063 solver.cpp:213] Iteration 19620, loss = 3.14954
I0614 16:28:55.780614  5063 solver.cpp:228]     Train net output #0: softmax = 3.14954 (* 1 = 3.14954 loss)
I0614 16:28:55.780619  5063 solver.cpp:473] Iteration 19620, lr = 0.0001
I0614 16:28:56.663702  5063 solver.cpp:213] Iteration 19630, loss = 3.0123
I0614 16:28:56.663718  5063 solver.cpp:228]     Train net output #0: softmax = 3.0123 (* 1 = 3.0123 loss)
I0614 16:28:56.663723  5063 solver.cpp:473] Iteration 19630, lr = 0.0001
I0614 16:28:57.546808  5063 solver.cpp:213] Iteration 19640, loss = 3.00127
I0614 16:28:57.546831  5063 solver.cpp:228]     Train net output #0: softmax = 3.00127 (* 1 = 3.00127 loss)
I0614 16:28:57.546836  5063 solver.cpp:473] Iteration 19640, lr = 0.0001
I0614 16:28:58.430336  5063 solver.cpp:213] Iteration 19650, loss = 3.33349
I0614 16:28:58.430354  5063 solver.cpp:228]     Train net output #0: softmax = 3.33349 (* 1 = 3.33349 loss)
I0614 16:28:58.430359  5063 solver.cpp:473] Iteration 19650, lr = 0.0001
I0614 16:28:59.313961  5063 solver.cpp:213] Iteration 19660, loss = 2.85755
I0614 16:28:59.313976  5063 solver.cpp:228]     Train net output #0: softmax = 2.85755 (* 1 = 2.85755 loss)
I0614 16:28:59.313980  5063 solver.cpp:473] Iteration 19660, lr = 0.0001
I0614 16:29:00.196548  5063 solver.cpp:213] Iteration 19670, loss = 3.08321
I0614 16:29:00.196569  5063 solver.cpp:228]     Train net output #0: softmax = 3.08321 (* 1 = 3.08321 loss)
I0614 16:29:00.196574  5063 solver.cpp:473] Iteration 19670, lr = 0.0001
I0614 16:29:01.078894  5063 solver.cpp:213] Iteration 19680, loss = 3.17517
I0614 16:29:01.078912  5063 solver.cpp:228]     Train net output #0: softmax = 3.17517 (* 1 = 3.17517 loss)
I0614 16:29:01.078917  5063 solver.cpp:473] Iteration 19680, lr = 0.0001
I0614 16:29:01.961863  5063 solver.cpp:213] Iteration 19690, loss = 3.02174
I0614 16:29:01.961879  5063 solver.cpp:228]     Train net output #0: softmax = 3.02174 (* 1 = 3.02174 loss)
I0614 16:29:01.961884  5063 solver.cpp:473] Iteration 19690, lr = 0.0001
I0614 16:29:02.844766  5063 solver.cpp:213] Iteration 19700, loss = 2.99026
I0614 16:29:02.844799  5063 solver.cpp:228]     Train net output #0: softmax = 2.99026 (* 1 = 2.99026 loss)
I0614 16:29:02.844804  5063 solver.cpp:473] Iteration 19700, lr = 0.0001
I0614 16:29:03.726883  5063 solver.cpp:213] Iteration 19710, loss = 2.89499
I0614 16:29:03.726902  5063 solver.cpp:228]     Train net output #0: softmax = 2.89499 (* 1 = 2.89499 loss)
I0614 16:29:03.726912  5063 solver.cpp:473] Iteration 19710, lr = 0.0001
I0614 16:29:04.609635  5063 solver.cpp:213] Iteration 19720, loss = 3.25689
I0614 16:29:04.609665  5063 solver.cpp:228]     Train net output #0: softmax = 3.25689 (* 1 = 3.25689 loss)
I0614 16:29:04.609671  5063 solver.cpp:473] Iteration 19720, lr = 0.0001
I0614 16:29:05.492455  5063 solver.cpp:213] Iteration 19730, loss = 2.95279
I0614 16:29:05.492477  5063 solver.cpp:228]     Train net output #0: softmax = 2.95279 (* 1 = 2.95279 loss)
I0614 16:29:05.492481  5063 solver.cpp:473] Iteration 19730, lr = 0.0001
I0614 16:29:06.375535  5063 solver.cpp:213] Iteration 19740, loss = 3.07098
I0614 16:29:06.375550  5063 solver.cpp:228]     Train net output #0: softmax = 3.07098 (* 1 = 3.07098 loss)
I0614 16:29:06.375555  5063 solver.cpp:473] Iteration 19740, lr = 0.0001
I0614 16:29:07.258504  5063 solver.cpp:213] Iteration 19750, loss = 2.8599
I0614 16:29:07.258522  5063 solver.cpp:228]     Train net output #0: softmax = 2.8599 (* 1 = 2.8599 loss)
I0614 16:29:07.258527  5063 solver.cpp:473] Iteration 19750, lr = 0.0001
I0614 16:29:08.141919  5063 solver.cpp:213] Iteration 19760, loss = 3.02694
I0614 16:29:08.141938  5063 solver.cpp:228]     Train net output #0: softmax = 3.02694 (* 1 = 3.02694 loss)
I0614 16:29:08.141943  5063 solver.cpp:473] Iteration 19760, lr = 0.0001
I0614 16:29:09.025173  5063 solver.cpp:213] Iteration 19770, loss = 3.19593
I0614 16:29:09.025193  5063 solver.cpp:228]     Train net output #0: softmax = 3.19593 (* 1 = 3.19593 loss)
I0614 16:29:09.025203  5063 solver.cpp:473] Iteration 19770, lr = 0.0001
I0614 16:29:09.908639  5063 solver.cpp:213] Iteration 19780, loss = 2.80583
I0614 16:29:09.908661  5063 solver.cpp:228]     Train net output #0: softmax = 2.80583 (* 1 = 2.80583 loss)
I0614 16:29:09.908666  5063 solver.cpp:473] Iteration 19780, lr = 0.0001
I0614 16:29:10.791518  5063 solver.cpp:213] Iteration 19790, loss = 2.87607
I0614 16:29:10.791534  5063 solver.cpp:228]     Train net output #0: softmax = 2.87607 (* 1 = 2.87607 loss)
I0614 16:29:10.791539  5063 solver.cpp:473] Iteration 19790, lr = 0.0001
I0614 16:29:11.674679  5063 solver.cpp:213] Iteration 19800, loss = 2.86008
I0614 16:29:11.674695  5063 solver.cpp:228]     Train net output #0: softmax = 2.86008 (* 1 = 2.86008 loss)
I0614 16:29:11.674700  5063 solver.cpp:473] Iteration 19800, lr = 0.0001
I0614 16:29:12.557709  5063 solver.cpp:213] Iteration 19810, loss = 2.86463
I0614 16:29:12.557726  5063 solver.cpp:228]     Train net output #0: softmax = 2.86463 (* 1 = 2.86463 loss)
I0614 16:29:12.557731  5063 solver.cpp:473] Iteration 19810, lr = 0.0001
I0614 16:29:13.440292  5063 solver.cpp:213] Iteration 19820, loss = 3.1499
I0614 16:29:13.440307  5063 solver.cpp:228]     Train net output #0: softmax = 3.1499 (* 1 = 3.1499 loss)
I0614 16:29:13.440311  5063 solver.cpp:473] Iteration 19820, lr = 0.0001
I0614 16:29:14.323545  5063 solver.cpp:213] Iteration 19830, loss = 3.1513
I0614 16:29:14.323565  5063 solver.cpp:228]     Train net output #0: softmax = 3.1513 (* 1 = 3.1513 loss)
I0614 16:29:14.323679  5063 solver.cpp:473] Iteration 19830, lr = 0.0001
I0614 16:29:15.206856  5063 solver.cpp:213] Iteration 19840, loss = 3.11632
I0614 16:29:15.206878  5063 solver.cpp:228]     Train net output #0: softmax = 3.11632 (* 1 = 3.11632 loss)
I0614 16:29:15.206883  5063 solver.cpp:473] Iteration 19840, lr = 0.0001
I0614 16:29:16.089390  5063 solver.cpp:213] Iteration 19850, loss = 3.03772
I0614 16:29:16.089406  5063 solver.cpp:228]     Train net output #0: softmax = 3.03772 (* 1 = 3.03772 loss)
I0614 16:29:16.089411  5063 solver.cpp:473] Iteration 19850, lr = 0.0001
I0614 16:29:16.972543  5063 solver.cpp:213] Iteration 19860, loss = 2.99231
I0614 16:29:16.972560  5063 solver.cpp:228]     Train net output #0: softmax = 2.99231 (* 1 = 2.99231 loss)
I0614 16:29:16.972565  5063 solver.cpp:473] Iteration 19860, lr = 0.0001
I0614 16:29:17.855631  5063 solver.cpp:213] Iteration 19870, loss = 3.25787
I0614 16:29:17.855648  5063 solver.cpp:228]     Train net output #0: softmax = 3.25787 (* 1 = 3.25787 loss)
I0614 16:29:17.855651  5063 solver.cpp:473] Iteration 19870, lr = 0.0001
I0614 16:29:18.738708  5063 solver.cpp:213] Iteration 19880, loss = 2.91159
I0614 16:29:18.738739  5063 solver.cpp:228]     Train net output #0: softmax = 2.91159 (* 1 = 2.91159 loss)
I0614 16:29:18.738744  5063 solver.cpp:473] Iteration 19880, lr = 0.0001
I0614 16:29:19.621918  5063 solver.cpp:213] Iteration 19890, loss = 2.83285
I0614 16:29:19.621937  5063 solver.cpp:228]     Train net output #0: softmax = 2.83285 (* 1 = 2.83285 loss)
I0614 16:29:19.621940  5063 solver.cpp:473] Iteration 19890, lr = 0.0001
I0614 16:29:20.504297  5063 solver.cpp:213] Iteration 19900, loss = 3.08033
I0614 16:29:20.504319  5063 solver.cpp:228]     Train net output #0: softmax = 3.08033 (* 1 = 3.08033 loss)
I0614 16:29:20.504324  5063 solver.cpp:473] Iteration 19900, lr = 0.0001
I0614 16:29:21.387544  5063 solver.cpp:213] Iteration 19910, loss = 3.17184
I0614 16:29:21.387560  5063 solver.cpp:228]     Train net output #0: softmax = 3.17184 (* 1 = 3.17184 loss)
I0614 16:29:21.387565  5063 solver.cpp:473] Iteration 19910, lr = 0.0001
I0614 16:29:22.270800  5063 solver.cpp:213] Iteration 19920, loss = 3.1408
I0614 16:29:22.270817  5063 solver.cpp:228]     Train net output #0: softmax = 3.1408 (* 1 = 3.1408 loss)
I0614 16:29:22.270822  5063 solver.cpp:473] Iteration 19920, lr = 0.0001
I0614 16:29:23.154047  5063 solver.cpp:213] Iteration 19930, loss = 2.79754
I0614 16:29:23.154062  5063 solver.cpp:228]     Train net output #0: softmax = 2.79754 (* 1 = 2.79754 loss)
I0614 16:29:23.154067  5063 solver.cpp:473] Iteration 19930, lr = 0.0001
I0614 16:29:24.036672  5063 solver.cpp:213] Iteration 19940, loss = 3.12703
I0614 16:29:24.036690  5063 solver.cpp:228]     Train net output #0: softmax = 3.12703 (* 1 = 3.12703 loss)
I0614 16:29:24.036695  5063 solver.cpp:473] Iteration 19940, lr = 0.0001
I0614 16:29:24.918787  5063 solver.cpp:213] Iteration 19950, loss = 2.89494
I0614 16:29:24.918810  5063 solver.cpp:228]     Train net output #0: softmax = 2.89494 (* 1 = 2.89494 loss)
I0614 16:29:24.918961  5063 solver.cpp:473] Iteration 19950, lr = 0.0001
I0614 16:29:25.801482  5063 solver.cpp:213] Iteration 19960, loss = 3.04835
I0614 16:29:25.801498  5063 solver.cpp:228]     Train net output #0: softmax = 3.04835 (* 1 = 3.04835 loss)
I0614 16:29:25.801502  5063 solver.cpp:473] Iteration 19960, lr = 0.0001
I0614 16:29:26.684581  5063 solver.cpp:213] Iteration 19970, loss = 2.8429
I0614 16:29:26.684597  5063 solver.cpp:228]     Train net output #0: softmax = 2.8429 (* 1 = 2.8429 loss)
I0614 16:29:26.684602  5063 solver.cpp:473] Iteration 19970, lr = 0.0001
I0614 16:29:27.567734  5063 solver.cpp:213] Iteration 19980, loss = 3.04141
I0614 16:29:27.567750  5063 solver.cpp:228]     Train net output #0: softmax = 3.04141 (* 1 = 3.04141 loss)
I0614 16:29:27.567755  5063 solver.cpp:473] Iteration 19980, lr = 0.0001
I0614 16:29:28.451020  5063 solver.cpp:213] Iteration 19990, loss = 3.1044
I0614 16:29:28.451038  5063 solver.cpp:228]     Train net output #0: softmax = 3.1044 (* 1 = 3.1044 loss)
I0614 16:29:28.451043  5063 solver.cpp:473] Iteration 19990, lr = 0.0001
I0614 16:29:29.276278  5063 solver.cpp:362] Snapshotting to snapshots/16-06-14_15h51m21s_0_11_pretrainClassification_iter_20000.caffemodel
I0614 16:29:29.277045  5063 solver.cpp:370] Snapshotting solver state to snapshots/16-06-14_15h51m21s_0_11_pretrainClassification_iter_20000.solverstate
I0614 16:29:29.299676  5063 solver.cpp:273] Iteration 20000, loss = 3.07465
I0614 16:29:29.299690  5063 solver.cpp:291] Iteration 20000, Testing net (#0)
I0614 16:29:29.412504  5063 solver.cpp:342]     Test net output #0: accuracy_top_1 = 0.24375
I0614 16:29:29.412519  5063 solver.cpp:342]     Test net output #1: accuracy_top_5 = 0.526563
I0614 16:29:29.412525  5063 solver.cpp:342]     Test net output #2: softmax = 3.13114 (* 1 = 3.13114 loss)
I0614 16:29:29.412529  5063 solver.cpp:278] Optimization Done.
I0614 16:29:29.412533  5063 caffe.cpp:121] Optimization Done.
